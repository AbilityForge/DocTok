,question,heading,text,content
0,What key topics are discussed in LangSmith?,Concepts,"Explanations, clarification and discussion of key topics in LangSmith.","Concepts Explanations, clarification and discussion of key topics in LangSmith."
1,What are the different components related to administration in the given text?,Admin,OrganizationsWorkspacesUsersAPI keysPersonal Access Tokens (PATs)Service keysRolesOrganization rolesWorkspace roles,Admin OrganizationsWorkspacesUsersAPI keysPersonal Access Tokens (PATs)Service keysRolesOrganization rolesWorkspace roles
2,What elements are associated with the concept of tracing?,Tracing,RunsTracesProjectsFeedbackTagsMetadata,Tracing RunsTracesProjectsFeedbackTagsMetadata
3,What are the key components involved in the evaluation process?,Evaluation,Datasets and examplesExperimentsEvaluators,Evaluation Datasets and examplesExperimentsEvaluators
4,What are the types of prompts mentioned in the text?,Prompts,Prompt typesTemplate formats,Prompts Prompt typesTemplate formats
5,What are the key aspects of Usage and Billing related to Data Retention and Usage Limits?,Usage and Billing,Data RetentionUsage Limits,Usage and Billing Data RetentionUsage Limits
6,What is LangSmith and what capabilities does it offer for building LLM applications?,Get started with LangSmith,"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!","Get started with LangSmith LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!"
7,What are the commands to install LangSmith for Python and TypeScript?,1. Install LangSmith,PythonTypeScriptpip install -U langsmithyarn add langsmith,1. Install LangSmith PythonTypeScriptpip install -U langsmithyarn add langsmith
8,How can you create an API key?,2. Create an API key,To create an API key head to the Settings page. Then click Create API Key.,2. Create an API key To create an API key head to the Settings page. Then click Create API Key.
9,What environment variables need to be set up for using LangChain with the OpenAI API?,3. Set up your environment,"Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>","3. Set up your environment Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>"
10,What are the steps to log traces to LangSmith for LangChain users?,4. Log your first trace,"Tracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here. We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use traceable. See more on the Annotate code for tracing page. PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{""role"": ""user"", ""content"": user_input}],        model=""gpt-3.5-turbo""    )    return result.choices[0].message.contentpipeline(""Hello, world!"")# Out:  Hello there! How can I assist you today?import { OpenAI } from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: ""user"", content: user_input }],        model: ""gpt-3.5-turbo"",    });    return result.choices[0].message.content;});await pipeline(""Hello, world!"")// Out: Hello there! How can I assist you today? View a sample output trace.Learn more about tracing in the how-to guides.","4. Log your first trace Tracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here. We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use traceable. See more on the Annotate code for tracing page. PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{""role"": ""user"", ""content"": user_input}],        model=""gpt-3.5-turbo""    )    return result.choices[0].message.contentpipeline(""Hello, world!"")# Out:  Hello there! How can I assist you today?import { OpenAI } from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: ""user"", content: user_input }],        model: ""gpt-3.5-turbo"",    });    return result.choices[0].message.content;});await pipeline(""Hello, world!"")// Out: Hello there! How can I assist you today? View a sample output trace.Learn more about tracing in the how-to guides."
11,What are the necessary components for running an evaluation in LangSmith?,5. Run your first evaluation,"Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator. PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = ""Sample Dataset""dataset = client.create_dataset(dataset_name, description=""A sample dataset in LangSmith."")client.create_examples(    inputs=[        {""postfix"": ""to LangSmith""},        {""postfix"": ""to Evaluations in LangSmith""},    ],    outputs=[        {""output"": ""Welcome to LangSmith""},        {""output"": ""Welcome to Evaluations in LangSmith""},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {""score"": run.outputs[""output""] == example.outputs[""output""]}experiment_results = evaluate(    lambda input: ""Welcome "" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=""sample-experiment"", # The name of the experiment    metadata={      ""version"": ""1.0.0"",      ""revision_id"": ""beta""    },)import { Client, Run, Example } from ""langsmith"";import { evaluate } from ""langsmith/evaluation"";import { EvaluationResult } from ""langsmith/evaluation"";const client = new Client();// Define dataset: these are your test casesconst datasetName = ""Sample Dataset"";const dataset = await client.createDataset(datasetName, {  description: ""A sample dataset in LangSmith."",});await client.createExamples({  inputs: [    { postfix: ""to LangSmith"" },    { postfix: ""to Evaluations in LangSmith"" },  ],  outputs: [    { output: ""Welcome to LangSmith"" },    { output: ""Welcome to Evaluations in LangSmith"" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: ""exact_match"",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: ""1.0.0"",      revision_id: ""beta"",    },  }); Learn more about evaluation in the how-to guides.","5. Run your first evaluation Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator. PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = ""Sample Dataset""dataset = client.create_dataset(dataset_name, description=""A sample dataset in LangSmith."")client.create_examples(    inputs=[        {""postfix"": ""to LangSmith""},        {""postfix"": ""to Evaluations in LangSmith""},    ],    outputs=[        {""output"": ""Welcome to LangSmith""},        {""output"": ""Welcome to Evaluations in LangSmith""},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {""score"": run.outputs[""output""] == example.outputs[""output""]}experiment_results = evaluate(    lambda input: ""Welcome "" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=""sample-experiment"", # The name of the experiment    metadata={      ""version"": ""1.0.0"",      ""revision_id"": ""beta""    },)import { Client, Run, Example } from ""langsmith"";import { evaluate } from ""langsmith/evaluation"";import { EvaluationResult } from ""langsmith/evaluation"";const client = new Client();// Define dataset: these are your test casesconst datasetName = ""Sample Dataset"";const dataset = await client.createDataset(datasetName, {  description: ""A sample dataset in LangSmith."",});await client.createExamples({  inputs: [    { postfix: ""to LangSmith"" },    { postfix: ""to Evaluations in LangSmith"" },  ],  outputs: [    { output: ""Welcome to LangSmith"" },    { output: ""Welcome to Evaluations in LangSmith"" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: ""exact_match"",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: ""1.0.0"",      revision_id: ""beta"",    },  }); Learn more about evaluation in the how-to guides."
12,How does LangSmith integrate with LangChain?,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications.","Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
13,What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core","Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
14,What environment variables should be configured for using LangChain with the OpenAI API?,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
","1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
15,What is required to log a trace to LangSmith when using LangChain?,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });","2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
16,What is the default name of the trace logged to the project?,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here.","3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
17,How can you trace specific invocations of LangChain runnables in Python and JS/TS?,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });","Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
18,How can you configure a custom project name for a LangSmith application run?,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project","Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
19,How can you set the project name for a LangChainTracer instance in Python?,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });","Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
20,What types of information can be associated with traces using metadata and tags in LangSmith?,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})","Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
21,How can you specify a custom name for a run in LangSmith?,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})","Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
22,How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);","Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
23,What methods does LangChain provide to ensure all traces are submitted before exiting an application?,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}","Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
24,How can you configure tracing programmatically without setting environment variables?,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });","Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
25,How does LangSmith support distributed tracing with LangChain in Python?,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})","Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
26,How can LangChain objects be traced when used in conjunction with the LangSmith SDK?,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
","Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
27,How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });","Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
28,What are the limitations of combining LangChain with traceable when using the RunTree API?,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});","Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
29,What does the technical reference for LangSmith cover?,Reference,"Technical reference that covers components, APIs, and other aspects of LangSmith.","Reference Technical reference that covers components, APIs, and other aspects of LangSmith."
30,What is the title of the API reference document?,API reference,LangSmith API Reference,API reference LangSmith API Reference
31,What programming language do the off-the-shelf evaluators in the LangChain SDK reference support?,SDK reference,LangChain off-the-shelf evaluators (Python only),SDK reference LangChain off-the-shelf evaluators (Python only)
32,What is the relationship between cloud architecture and scalability in different regions?,Architecture reference,Cloud architecture and scalabilityRegions FAQ,Architecture reference Cloud architecture and scalabilityRegions FAQ
33,What are the different data formats mentioned in the text?,Data formats,Run (span) data formatFeedback data formatTrace query syntaxFilter argumentsFilter query language,Data formats Run (span) data formatFeedback data formatTrace query syntaxFilter argumentsFilter query language
34,What are the different methods of authentication?,Authentication and authorization,Authentication methods,Authentication and authorization Authentication methods
35,What are the key steps involved in installing and configuring a self-hosted instance of LangSmith?,Self-Hosting LangSmith,"Step-by-step guides that cover the installation, configuration, and scaling of your Self-Hosted LangSmith instance. Architectural overview: A high-level overview of the LangSmith architecture.Storage services: The storage services used by LangSmith.Services: The services that make up LangSmith.Installation: How to install LangSmith on your own infrastructure.Kubernetes: Deploy LangSmith on Kubernetes.Docker: Deploy LangSmith using Docker.Configuration: How to configure your self-hosted instance of LangSmith.SSO with OAuth2.0 and OIDC: Configure LangSmith to use OAuth2.0 and OIDC for SSO.Connect to an external ClickHouse database: Configure LangSmith to use an external ClickHouse database.Connect to an external Postgres database: Configure LangSmith to use an external Postgres database.Connect to an external Redis instance: Configure LangSmith to use an external Redis instance.Usage: How to use your self-hosted instance of LangSmith.Upgrades: How to upgrade your self-hosted instance of LangSmith.Release notes: The latest release notes for LangSmith.Week of June 17, 2024 - LangSmith v0.6: Release notes for version 0.6 of LangSmith.Week of May 13, 2024 - LangSmith v0.5: Release notes for version 0.5 of LangSmith.Week of March 25, 2024 - LangSmith v0.4: Release notes for version 0.4 of LangSmith.Week of Februrary 21, 2024 - LangSmith v0.3: Release notes for version 0.3 of LangSmith.Week of January 29, 2024 - LangSmith v0.2: Release notes for version 0.2 of LangSmith.FAQ: Frequently asked questions about LangSmith.","Self-Hosting LangSmith Step-by-step guides that cover the installation, configuration, and scaling of your Self-Hosted LangSmith instance. Architectural overview: A high-level overview of the LangSmith architecture.Storage services: The storage services used by LangSmith.Services: The services that make up LangSmith.Installation: How to install LangSmith on your own infrastructure.Kubernetes: Deploy LangSmith on Kubernetes.Docker: Deploy LangSmith using Docker.Configuration: How to configure your self-hosted instance of LangSmith.SSO with OAuth2.0 and OIDC: Configure LangSmith to use OAuth2.0 and OIDC for SSO.Connect to an external ClickHouse database: Configure LangSmith to use an external ClickHouse database.Connect to an external Postgres database: Configure LangSmith to use an external Postgres database.Connect to an external Redis instance: Configure LangSmith to use an external Redis instance.Usage: How to use your self-hosted instance of LangSmith.Upgrades: How to upgrade your self-hosted instance of LangSmith.Release notes: The latest release notes for LangSmith.Week of June 17, 2024 - LangSmith v0.6: Release notes for version 0.6 of LangSmith.Week of May 13, 2024 - LangSmith v0.5: Release notes for version 0.5 of LangSmith.Week of March 25, 2024 - LangSmith v0.4: Release notes for version 0.4 of LangSmith.Week of Februrary 21, 2024 - LangSmith v0.3: Release notes for version 0.3 of LangSmith.Week of January 29, 2024 - LangSmith v0.2: Release notes for version 0.2 of LangSmith.FAQ: Frequently asked questions about LangSmith."
36,What type of content do the how-to guides in LangSmith provide?,How-to guides,Step-by-step guides that cover key tasks and operations in LangSmith.,How-to guides Step-by-step guides that cover key tasks and operations in LangSmith.
37,What steps are involved in setting up a LangSmith account and managing its features?,Setup,"See the following guides to set up your LangSmith account. Create an account and API keySet up an organizationCreate an organizationManage and navigate workspacesManage usersSet up a workspaceCreate a workspaceManage usersConfigure workspace settingsSet up billingUpdate invoice email, tax id and, business informationSet up access control (enterprise only)Create a roleAssign a role to a user","Setup See the following guides to set up your LangSmith account. Create an account and API keySet up an organizationCreate an organizationManage and navigate workspacesManage usersSet up a workspaceCreate a workspaceManage usersConfigure workspace settingsSet up billingUpdate invoice email, tax id and, business informationSet up access control (enterprise only)Create a roleAssign a role to a user"
38,What features does LangSmith offer for adding observability to LLM applications?,Tracing,Get started with LangSmith's tracing features to start adding observability to your LLM applications. Annotate code for tracingUse @traceable/traceableWrap the OpenAI API clientUse the RunTree APIUse the trace context manager (Python only)Toggle tracing on and offLog traces to specific projectSet the destination project staticallySet the destination project dynamicallySet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingDistributed tracing in PythonDistributed tracing in TypeScriptAccess the current span within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesChat-style modelsSpecify model nameStream outputsManually provide token countsInstruct-style modelsPrevent logging of sensitive data in tracesRule-based masking of inputs and outputsExport tracesUse filter argumentsUse filter query languageShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChainInstallationQuick startTrace selectivelyLog to specific projectAdd metadata and tags to tracesCustomize run nameAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTrace with LangGraphInteroperability between LangChain and LangGraphInteroperability between @traceable/traceable and LangGraphTrace with Instructor (Python only)Trace with the Vercel AI SDK (JS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for traces,Tracing Get started with LangSmith's tracing features to start adding observability to your LLM applications. Annotate code for tracingUse @traceable/traceableWrap the OpenAI API clientUse the RunTree APIUse the trace context manager (Python only)Toggle tracing on and offLog traces to specific projectSet the destination project staticallySet the destination project dynamicallySet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingDistributed tracing in PythonDistributed tracing in TypeScriptAccess the current span within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesChat-style modelsSpecify model nameStream outputsManually provide token countsInstruct-style modelsPrevent logging of sensitive data in tracesRule-based masking of inputs and outputsExport tracesUse filter argumentsUse filter query languageShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChainInstallationQuick startTrace selectivelyLog to specific projectAdd metadata and tags to tracesCustomize run nameAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTrace with LangGraphInteroperability between LangChain and LangGraphInteroperability between @traceable/traceable and LangGraphTrace with Instructor (Python only)Trace with the Vercel AI SDK (JS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for traces
39,What are the ways to create a new dataset in LangSmith?,Datasets,Manage datasets in LangSmith to evaluate and improve your LLM applications. Manage datasets in the applicationCreate a new dataset and add examples manuallyDataset schema validationAdd inputs and outputs from traces to datasetsUpload a CSV file to create a datasetGenerate synthetic examplesExport a datasetCreate and manage dataset splitsManage datasets programmaticallyCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from a pandas DataFrameFetch datasetsFetch examplesUpdate examplesBulk update examplesVersion datasetsCreate a new version of a datasetTag a versionShare or unshare a dataset publicly,Datasets Manage datasets in LangSmith to evaluate and improve your LLM applications. Manage datasets in the applicationCreate a new dataset and add examples manuallyDataset schema validationAdd inputs and outputs from traces to datasetsUpload a CSV file to create a datasetGenerate synthetic examplesExport a datasetCreate and manage dataset splitsManage datasets programmaticallyCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from a pandas DataFrameFetch datasetsFetch examplesUpdate examplesBulk update examplesVersion datasetsCreate a new version of a datasetTag a versionShare or unshare a dataset publicly
40,What methods can be used to evaluate the performance of LLM applications over time?,Evaluation,Evaluate your LLM applications to measure their performance over time. Evaluate an LLM applicationRun an evaluationUse custom evaluatorsEvaluate on a particular version of a datasetEvaluate on a subset of a datasetEvaluate on a dataset splitEvaluate on a dataset with repetitionsUse a summary evaluatorEvaluate a LangChain runnableReturn multiple scoresBind an evaluator to a dataset in the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse LangChain off-the-shelf evaluators (Python only)Use question and answer (correctness) evaluatorsUse criteria evaluatorsUse labeled criteria evaluatorsUse string or embedding distance metricsUse a custom LLM in off-the-shelf evaluatorsHandle multiple input or output fieldsCompare experiment resultsOpen the comparison viewView regressions and improvementsFilter on regressions or improvementsUpdate baseline experimentSelect feedback keyOpen a traceExpand detailed viewUpdate display settingsEvaluate an existing experimentUnit test LLM applications (Python only)Run pairwise evaluationsUse the evaluate_comparative functionConfigure inputs and outputs for pairwise evaluatorsCompare two experiments with LLM-based pairwise evaluatorsView pairwise experimentsAudit evaluator scoresIn the comparison viewIn the runs tableIn the SDKCreate few-shot evaluatorsCreate your evaluatorMake correctionsView your corrections datasetFetch performance metrics for an experimentRun evals using the API onlyCreate a datasetRun a single experimentRun a pairwise experimentUpload experiments run outside of LangSmith with the REST APIRequest body schemaConsiderationsExample requestView the experiment in the UI,Evaluation Evaluate your LLM applications to measure their performance over time. Evaluate an LLM applicationRun an evaluationUse custom evaluatorsEvaluate on a particular version of a datasetEvaluate on a subset of a datasetEvaluate on a dataset splitEvaluate on a dataset with repetitionsUse a summary evaluatorEvaluate a LangChain runnableReturn multiple scoresBind an evaluator to a dataset in the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse LangChain off-the-shelf evaluators (Python only)Use question and answer (correctness) evaluatorsUse criteria evaluatorsUse labeled criteria evaluatorsUse string or embedding distance metricsUse a custom LLM in off-the-shelf evaluatorsHandle multiple input or output fieldsCompare experiment resultsOpen the comparison viewView regressions and improvementsFilter on regressions or improvementsUpdate baseline experimentSelect feedback keyOpen a traceExpand detailed viewUpdate display settingsEvaluate an existing experimentUnit test LLM applications (Python only)Run pairwise evaluationsUse the evaluate_comparative functionConfigure inputs and outputs for pairwise evaluatorsCompare two experiments with LLM-based pairwise evaluatorsView pairwise experimentsAudit evaluator scoresIn the comparison viewIn the runs tableIn the SDKCreate few-shot evaluatorsCreate your evaluatorMake correctionsView your corrections datasetFetch performance metrics for an experimentRun evals using the API onlyCreate a datasetRun a single experimentRun a pairwise experimentUpload experiments run outside of LangSmith with the REST APIRequest body schemaConsiderationsExample requestView the experiment in the UI
41,What steps can be taken to collect and utilize human feedback for improving LLM applications?,Human feedback,Collect human feedback to improve your LLM applications. Capture user feedback from your application to tracesSet up a new feedback criteriaAnnotate traces inlineUse annotation queuesCreate an annotation queueAssign runs to an annotation queueReview runs in an annotation queue,Human feedback Collect human feedback to improve your LLM applications. Capture user feedback from your application to tracesSet up a new feedback criteriaAnnotate traces inlineUse annotation queuesCreate an annotation queueAssign runs to an annotation queueReview runs in an annotation queue
42,What features does LangSmith offer for monitoring and automations of production data?,Monitoring and automations,Leverage LangSmith's powerful monitoring and automations features to make sense of your production data. Filter traces in the applicationCreate a filterFilter for intermediate runs (spans)Advanced: filter for intermediate runs (spans) on properties of the rootAdvanced: filter for runs (spans) whose child runs have some attributeFilter based on inputs and outputsFilter based on input / output key-value pairsCopy the filterManually specify a raw query in LangSmith query languageUse an AI Query to auto-generate a queryUse monitoring chartsChange the time periodSlice data by metadata or tagDrill down into specific subsetsSet up automation rulesCreate a ruleView logs for your automationsSet up online evaluationsConfigure online evaluationsSet API keysSet up webhook notifications for rulesWebhook payloadExample with ModalSet up threadsGroup traces into threadsView threads,Monitoring and automations Leverage LangSmith's powerful monitoring and automations features to make sense of your production data. Filter traces in the applicationCreate a filterFilter for intermediate runs (spans)Advanced: filter for intermediate runs (spans) on properties of the rootAdvanced: filter for runs (spans) whose child runs have some attributeFilter based on inputs and outputsFilter based on input / output key-value pairsCopy the filterManually specify a raw query in LangSmith query languageUse an AI Query to auto-generate a queryUse monitoring chartsChange the time periodSlice data by metadata or tagDrill down into specific subsetsSet up automation rulesCreate a ruleView logs for your automationsSet up online evaluationsConfigure online evaluationsSet API keysSet up webhook notifications for rulesWebhook payloadExample with ModalSet up threadsGroup traces into threadsView threads
43,What functionalities does LangSmith offer for managing prompts in LLM development?,Prompts,"Organize and manage prompts in LangSmith to streamline your LLM development workflow. Create a promptCompose your promptSave your promptView your promptsAdd metadataUpdate a promptUpdate metadataUpdate the prompt contentVersion a promptManage prompts programmaticallyConfigure environment variablesPush a promptPull a promptUse a prompt without LangChainList, delete, and like promptsLangChain Hub","Prompts Organize and manage prompts in LangSmith to streamline your LLM development workflow. Create a promptCompose your promptSave your promptView your promptsAdd metadataUpdate a promptUpdate metadataUpdate the prompt contentVersion a promptManage prompts programmaticallyConfigure environment variablesPush a promptPull a promptUse a prompt without LangChainList, delete, and like promptsLangChain Hub"
44,What features are available in the LangSmith Playground for iterating on prompts and models?,Playground,Quickly iterate on prompts and models in the LangSmith Playground. Use custom TLS certificatesUse a custom modelSave settings configuration,Playground Quickly iterate on prompts and models in the LangSmith Playground. Use custom TLS certificatesUse a custom modelSave settings configuration
45,What are the key features and pricing details for the different plans offered by LangSmith?,Plans,"StartupsDeveloperPlusEnterpriseDesigned for early stage startups building AI applicationsDesigned for hobbyists who want to start their adventure soloEverything in Developer, plus team features and higher rate limitsDesigned for teams with more security, deployment, and support needsContact us to learn moreFree for 1 user5,000 free traces per monthAdditional traces billed starting @ 0.05/trace$39/user10,000 free traces per monthAdditional traces billed starting @ 0.05/traceCustomWhat to expect:We want all early stage companies to build with LangSmith. LangSmith for Startups offers discounted prices and a generous free, monthly trace allotment, so you can have the right tooling in place as you grow your business.Key features:1 Developer seatDebugging tracesDataset collectionTesting and evaluationPrompt managementMonitoringKey features:All features in Developer tierUp to 10 seatsHosted LangServe (beta)Higher rate limitsEmail supportKey features:All features in Plus tierSingle Sign On (SSO)Negotiable SLAsDeployment options in customers environmentCustom rate limitsTeam trainingsShared Slack channelArchitectural guidanceDedicated customer success manager","Plans StartupsDeveloperPlusEnterpriseDesigned for early stage startups building AI applicationsDesigned for hobbyists who want to start their adventure soloEverything in Developer, plus team features and higher rate limitsDesigned for teams with more security, deployment, and support needsContact us to learn moreFree for 1 user5,000 free traces per monthAdditional traces billed starting @ 0.05/trace$39/user10,000 free traces per monthAdditional traces billed starting @ 0.05/traceCustomWhat to expect:We want all early stage companies to build with LangSmith. LangSmith for Startups offers discounted prices and a generous free, monthly trace allotment, so you can have the right tooling in place as you grow your business.Key features:1 Developer seatDebugging tracesDataset collectionTesting and evaluationPrompt managementMonitoringKey features:All features in Developer tierUp to 10 seatsHosted LangServe (beta)Higher rate limitsEmail supportKey features:All features in Plus tierSingle Sign On (SSO)Negotiable SLAsDeployment options in customers environmentCustom rate limitsTeam trainingsShared Slack channelArchitectural guidanceDedicated customer success manager"
46,What are the differences in team seats and pricing between the Developer and Enterprise plans?,Plan Comparison,"DeveloperPlusEnterpriseFeaturesDebugging TracesDataset CollectionHuman LabelingTesting and EvaluationPrompt ManagementHosted LangServe--MonitoringRole-Based Access Controls (RBAC)----TeamDeveloper Seats1 Free SeatMaximum 10 seats$39 per seat/month1Custom pricingUsageTraces2First 5k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)First 10k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)CustomMax ingested events / hour350,0003 / 250,000500,000CustomTotal trace size storage / hour4500MB3 / 2.5GB5GBCustomSecurity ControlsSingle Sign On--GoogleGitHubCustom SSODeploymentHosted in USHosted in USAdd-on for self-hosted deployment in customer's VPCSupportSupport ChannelsCommunityEmailEmailShared Slack ChannelShared Slack Channel----Team Training----Application Architectural Guidance----Dedicated Customer Success Manager----SLA----ProcurementBillingMonthly, self-serveCredit CardMonthly, self-serveCredit CardAnnual InvoiceACHCustom Terms and Data Privacy Agreement----Infosec Review----WorkspacesSingle, default Workspace under Personal OrganizationUp to 3 Workspaces per OrganizationUp to 10 Workspaces per Organization (contact support@langchain.dev for more)Organization Roles (User and Admin)-- 1 2 3 4","Plan Comparison DeveloperPlusEnterpriseFeaturesDebugging TracesDataset CollectionHuman LabelingTesting and EvaluationPrompt ManagementHosted LangServe--MonitoringRole-Based Access Controls (RBAC)----TeamDeveloper Seats1 Free SeatMaximum 10 seats$39 per seat/month1Custom pricingUsageTraces2First 5k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)First 10k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)CustomMax ingested events / hour350,0003 / 250,000500,000CustomTotal trace size storage / hour4500MB3 / 2.5GB5GBCustomSecurity ControlsSingle Sign On--GoogleGitHubCustom SSODeploymentHosted in USHosted in USAdd-on for self-hosted deployment in customer's VPCSupportSupport ChannelsCommunityEmailEmailShared Slack ChannelShared Slack Channel----Team Training----Application Architectural Guidance----Dedicated Customer Success Manager----SLA----ProcurementBillingMonthly, self-serveCredit CardMonthly, self-serveCredit CardAnnual InvoiceACHCustom Terms and Data Privacy Agreement----Infosec Review----WorkspacesSingle, default Workspace under Personal OrganizationUp to 3 Workspaces per OrganizationUp to 10 Workspaces per Organization (contact support@langchain.dev for more)Organization Roles (User and Admin)-- 1 2 3 4"
47,When will my usage of LangSmith become billable?,Ive been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?,"If youve been using LangSmith already, your usage will be billable starting in July. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by reaching out to sales@langchain.dev.","Ive been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account? If youve been using LangSmith already, your usage will be billable starting in July. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by reaching out to sales@langchain.dev."
48,What plan should I choose if I am an individual developer working on small projects?,Which plan is right for me?,"If youre an individual developer, the Developer plan is a great choice for small projects. For teams that want to collaborate in LangSmith, check out the Plus plan. If you are an early-stage startup building an AI application, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our Startup Contact Form for more details. If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our Sales Contact Form for more details.","Which plan is right for me? If youre an individual developer, the Developer plan is a great choice for small projects. For teams that want to collaborate in LangSmith, check out the Plus plan. If you are an early-stage startup building an AI application, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our Startup Contact Form for more details. If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our Sales Contact Form for more details."
49,What is considered a seat in an organization?,What is a seat?,A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.,What is a seat? A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.
50,"What does the term ""trace"" refer to in the context of an application chain or agent?",What is a trace?,"A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an example of a single trace.","What is a trace? A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an example of a single trace."
51,What types of data are included in an ingested event sent to LangSmith?,What is an ingested event?,"An ingested event is any distinct, trace-related data sent to LangSmith. This includes: Inputs, outputs and metadata sent at the start of a run step within a traceInputs, outputs and metadata sent at the end of a run step within a traceFeedback on run steps or traces","What is an ingested event? An ingested event is any distinct, trace-related data sent to LangSmith. This includes: Inputs, outputs and metadata sent at the start of a run step within a traceInputs, outputs and metadata sent at the end of a run step within a traceFeedback on run steps or traces"
52,What options do I have if I've reached my rate or usage limits on LangSmith?,Ive hit my rate or usage limits. What can I do?,"If youve consumed the monthly allotment of free traces in your account, you can add a credit card on the Developer and Plus plans to continue sending traces to LangSmith. If youve hit the rate limits on your tier, you can upgrade to a higher plan to get higher limits, or reach out to support@langchain.dev with questions.","Ive hit my rate or usage limits. What can I do? If youve consumed the monthly allotment of free traces in your account, you can add a credit card on the Developer and Plus plans to continue sending traces to LangSmith. If youve hit the rate limits on your tier, you can upgrade to a higher plan to get higher limits, or reach out to support@langchain.dev with questions."
53,Can a Developer account be upgraded to the Plus or Enterprise plans?,"I have a developer account, can I upgrade my account to the Plus or Enterprise plan?","Every user will have a unique personal account on the Developer plan. We cannot upgrade a Developer account to the Plus or Enterprise plans. If youre interested in working as a team, create a separate LangSmith Organization on the Plus plan. This plan can upgraded to the Enterprise plan at a later date.","I have a developer account, can I upgrade my account to the Plus or Enterprise plan? Every user will have a unique personal account on the Developer plan. We cannot upgrade a Developer account to the Plus or Enterprise plans. If youre interested in working as a team, create a separate LangSmith Organization on the Plus plan. This plan can upgraded to the Enterprise plan at a later date."
54,What is the billing process for Seats Traces?,How will billing work?,Seats Traces,How will billing work? Seats Traces
55,Can I set a spend limit on tracing in LangSmith?,Can I limit how much I spend on tracing?,"You can set limits on the number of traces that can be sent to LangSmith per month on
the Plans and Billing settings page. noteWhile we do show you the dollar value of your usage limit for convenience, this limit evaluated
in terms of number of traces instead of dollar amount. For example, if you are approved for our
startup plan tier where you are given a generous allotment of free traces, your usage limit will
not automatically change.You are not currently able to set a spend limit in the product.","Can I limit how much I spend on tracing? You can set limits on the number of traces that can be sent to LangSmith per month on
the Plans and Billing settings page. noteWhile we do show you the dollar value of your usage limit for convenience, this limit evaluated
in terms of number of traces instead of dollar amount. For example, if you are approved for our
startup plan tier where you are given a generous allotment of free traces, your usage limit will
not automatically change.You are not currently able to set a spend limit in the product."
56,How can I view the daily number of billable LangSmith traces for my organization?,How can my track my usage so far this month?,"Under the Settings section for your Organization you will see subsection for Usage. There, you will able to see a graph of the daily nunber of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.","How can my track my usage so far this month? Under the Settings section for your Organization you will see subsection for Usage. There, you will able to see a graph of the daily nunber of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day."
57,What should customers on the Developer and Plus plan tiers do if they have a question about their bill?,I have a question about my bill...,Customers on the Developer and Plus plan tiers should email support@langchain.dev. Customers on the Enterprise plan should contact their sales representative directly. Enterprise plan customers are billed annually by invoice.,I have a question about my bill... Customers on the Developer and Plus plan tiers should email support@langchain.dev. Customers on the Enterprise plan should contact their sales representative directly. Enterprise plan customers are billed annually by invoice.
58,What types of support are available for each plan offered?,What can I expect from Support?,"On the Developer plan, community-based support is available on Discord. On the Plus plan, you will also receive preferential, email support at support@langchain.dev for LangSmith-related questions only and we'll do our best to respond within the next business day. On the Enterprise plan, youll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, well also support deployments and new releases with our infra engineering team on-call.","What can I expect from Support? On the Developer plan, community-based support is available on Discord. On the Plus plan, you will also receive preferential, email support at support@langchain.dev for LangSmith-related questions only and we'll do our best to respond within the next business day. On the Enterprise plan, youll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, well also support deployments and new releases with our infra engineering team on-call."
59,In which regions can I choose to sign up for data storage?,Where is my data stored?,"You may choose to sign up in either the US or EU region. See the cloud architecture reference for more details. If youre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment.","Where is my data stored? You may choose to sign up in either the US or EU region. See the cloud architecture reference for more details. If youre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment."
60,What security frameworks is LangSmith compliant with?,Which security frameworks is LangSmith compliant with?,"We are SOC 2 Type II, GDPR, and HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com. Please note we only enter into BAAs with customers on our Enterprise plan.","Which security frameworks is LangSmith compliant with? We are SOC 2 Type II, GDPR, and HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com. Please note we only enter into BAAs with customers on our Enterprise plan."
61,Will LangSmith train on the data that I send?,Will you train on the data that I send LangSmith?,"We will not train on your data, and you own all rights to your data. See LangSmith Terms of Service for more information.","Will you train on the data that I send LangSmith? We will not train on your data, and you own all rights to your data. See LangSmith Terms of Service for more information."
62,What are the key features of LangGraph Cloud for deploying LangGraph applications?,LangGraph Cloud,"LangGraph Cloud is a managed service for deploying and hosting LangGraph applications. Deploying applications with LangGraph Cloud shortens the time-to-market for developers. With one click, deploy a production-ready API with built-in persistence for your LangGraph application. LangGraph Cloud APIs are horizontally scalable and deployed with durable storage. LangGraph Cloud is seamlessly integrated with LangSmith and is accessible from the Deployments section in the left-hand navigation bar. See the official LangGraph Cloud documentation for more details.","LangGraph Cloud LangGraph Cloud is a managed service for deploying and hosting LangGraph applications. Deploying applications with LangGraph Cloud shortens the time-to-market for developers. With one click, deploy a production-ready API with built-in persistence for your LangGraph application. LangGraph Cloud APIs are horizontally scalable and deployed with durable storage. LangGraph Cloud is seamlessly integrated with LangSmith and is accessible from the Deployments section in the left-hand navigation bar. See the official LangGraph Cloud documentation for more details."
63,What can beginners find in the LangSmith tutorials?,Tutorials,"New to LangSmith? This is the place to start. Here, you'll find a hands-on introduction to key LangSmith workflows.","Tutorials New to LangSmith? This is the place to start. Here, you'll find a hands-on introduction to key LangSmith workflows."
64,What are some key activities developers can undertake to enhance their LLM applications?,Developers,Add observability to your LLM applicationEvaluate your LLM applicationOptimize a classifierRAG EvaluationsBacktestingAgent Evaluations,Developers Add observability to your LLM applicationEvaluate your LLM applicationOptimize a classifierRAG EvaluationsBacktestingAgent Evaluations
65,How can administrators optimize tracing spend on LangSmith?,Administrators,Optimize tracing spend on LangSmith,Administrators Optimize tracing spend on LangSmith
66,What should you do if you are using LangChain in relation to logging traces to LangSmith?,Annotate code for tracing,"There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.","Annotate code for tracing There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."
67,What environment variables must be set to log traces using the @traceable decorator in Python and the traceable function in TypeScript?,Use@traceable/traceable,"LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.
Note that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.
import { traceable } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: ""formatPrompt"" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();","Use@traceable/traceable LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.
Note that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.
import { traceable } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: ""formatPrompt"" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();"
68,What environment variables must be set to log traces when using the wrap_openai or wrapOpenAI methods?,Wrap the OpenAI client,"The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!
The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceable(name=""Chat Pipeline"")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(""Can you summarize this morning's meetings?"")import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", run_type: ""tool"" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: ""system"",            content:                ""You are a helpful assistant. Please respond to the user's request only based on the given context."",        },        { role: ""user"", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: ""Chat Pipeline"" });await chatPipeline(""Can you summarize this morning's meetings?"");","Wrap the OpenAI client The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!
The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceable(name=""Chat Pipeline"")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(""Can you summarize this morning's meetings?"")import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", run_type: ""tool"" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: ""system"",            content:                ""You are a helpful assistant. Please respond to the user's request only based on the given context."",        },        { role: ""user"", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: ""Chat Pipeline"" });await chatPipeline(""Can you summarize this morning's meetings?"");"
69,What is the purpose of the RunTree API in LangSmith?,Use theRunTreeAPI,"Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually
create runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not
necessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# Create a top-level runpipeline = RunTree(    name=""Chat Pipeline"",    run_type=""chain"",    inputs={""question"": question})# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}]# Create a child runchild_llm_run = pipeline.create_child(    name=""OpenAI Call"",    run_type=""llm"",    inputs={""messages"": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={""answer"": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from ""openai"";import { RunTree } from ""langsmith"";// This can be a user input to your appconst question = ""Can you summarize this morning's meetings?"";const pipeline = new RunTree({    name: ""Chat Pipeline"",    run_type: ""chain"",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = ""During this morning's meeting, we solved all world conflict."";const messages = [    { role: ""system"", content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { role: ""user"", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: ""OpenAI Call"",    run_type: ""llm"",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();","Use theRunTreeAPI Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually
create runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not
necessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# Create a top-level runpipeline = RunTree(    name=""Chat Pipeline"",    run_type=""chain"",    inputs={""question"": question})# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}]# Create a child runchild_llm_run = pipeline.create_child(    name=""OpenAI Call"",    run_type=""llm"",    inputs={""messages"": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={""answer"": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from ""openai"";import { RunTree } from ""langsmith"";// This can be a user input to your appconst question = ""Can you summarize this morning's meetings?"";const pipeline = new RunTree({    name: ""Chat Pipeline"",    run_type: ""chain"",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = ""During this morning's meeting, we solved all world conflict."";const messages = [    { role: ""system"", content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { role: ""user"", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: ""OpenAI Call"",    run_type: ""llm"",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"
70,What is the purpose of using the trace context manager in Python?,Use thetracecontext manager (Python only),"In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {""input"": ""Can you summarize this morning's meetings?""}with trace(""Chat Pipeline"", ""chain"", project_name=""my_test"", inputs=app_inputs) as rt:    output = chat_pipeline(""Can you summarize this morning's meetings?"")    rt.end(outputs={""output"": output})","Use thetracecontext manager (Python only) In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {""input"": ""Can you summarize this morning's meetings?""}with trace(""Chat Pipeline"", ""chain"", project_name=""my_test"", inputs=app_inputs) as rt:    output = chat_pipeline(""Can you summarize this morning's meetings?"")    rt.end(outputs={""output"": output})"
71,What is LangSmith and what capabilities does it offer for building LLM applications?,Get started with LangSmith,"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!","Get started with LangSmith LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!"
72,What are the commands to install LangSmith for Python and TypeScript?,1. Install LangSmith,PythonTypeScriptpip install -U langsmithyarn add langsmith,1. Install LangSmith PythonTypeScriptpip install -U langsmithyarn add langsmith
73,How can you create an API key?,2. Create an API key,To create an API key head to the Settings page. Then click Create API Key.,2. Create an API key To create an API key head to the Settings page. Then click Create API Key.
74,What environment variables need to be set up for using LangChain with the OpenAI API?,3. Set up your environment,"Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>","3. Set up your environment Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>"
75,What are the steps to log traces to LangSmith for LangChain users?,4. Log your first trace,"Tracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here. We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use traceable. See more on the Annotate code for tracing page. PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{""role"": ""user"", ""content"": user_input}],        model=""gpt-3.5-turbo""    )    return result.choices[0].message.contentpipeline(""Hello, world!"")# Out:  Hello there! How can I assist you today?import { OpenAI } from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: ""user"", content: user_input }],        model: ""gpt-3.5-turbo"",    });    return result.choices[0].message.content;});await pipeline(""Hello, world!"")// Out: Hello there! How can I assist you today? View a sample output trace.Learn more about tracing in the how-to guides.","4. Log your first trace Tracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here. We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use traceable. See more on the Annotate code for tracing page. PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{""role"": ""user"", ""content"": user_input}],        model=""gpt-3.5-turbo""    )    return result.choices[0].message.contentpipeline(""Hello, world!"")# Out:  Hello there! How can I assist you today?import { OpenAI } from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: ""user"", content: user_input }],        model: ""gpt-3.5-turbo"",    });    return result.choices[0].message.content;});await pipeline(""Hello, world!"")// Out: Hello there! How can I assist you today? View a sample output trace.Learn more about tracing in the how-to guides."
76,What are the necessary components for running an evaluation in LangSmith?,5. Run your first evaluation,"Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator. PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = ""Sample Dataset""dataset = client.create_dataset(dataset_name, description=""A sample dataset in LangSmith."")client.create_examples(    inputs=[        {""postfix"": ""to LangSmith""},        {""postfix"": ""to Evaluations in LangSmith""},    ],    outputs=[        {""output"": ""Welcome to LangSmith""},        {""output"": ""Welcome to Evaluations in LangSmith""},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {""score"": run.outputs[""output""] == example.outputs[""output""]}experiment_results = evaluate(    lambda input: ""Welcome "" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=""sample-experiment"", # The name of the experiment    metadata={      ""version"": ""1.0.0"",      ""revision_id"": ""beta""    },)import { Client, Run, Example } from ""langsmith"";import { evaluate } from ""langsmith/evaluation"";import { EvaluationResult } from ""langsmith/evaluation"";const client = new Client();// Define dataset: these are your test casesconst datasetName = ""Sample Dataset"";const dataset = await client.createDataset(datasetName, {  description: ""A sample dataset in LangSmith."",});await client.createExamples({  inputs: [    { postfix: ""to LangSmith"" },    { postfix: ""to Evaluations in LangSmith"" },  ],  outputs: [    { output: ""Welcome to LangSmith"" },    { output: ""Welcome to Evaluations in LangSmith"" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: ""exact_match"",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: ""1.0.0"",      revision_id: ""beta"",    },  }); Learn more about evaluation in the how-to guides.","5. Run your first evaluation Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator. PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = ""Sample Dataset""dataset = client.create_dataset(dataset_name, description=""A sample dataset in LangSmith."")client.create_examples(    inputs=[        {""postfix"": ""to LangSmith""},        {""postfix"": ""to Evaluations in LangSmith""},    ],    outputs=[        {""output"": ""Welcome to LangSmith""},        {""output"": ""Welcome to Evaluations in LangSmith""},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {""score"": run.outputs[""output""] == example.outputs[""output""]}experiment_results = evaluate(    lambda input: ""Welcome "" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=""sample-experiment"", # The name of the experiment    metadata={      ""version"": ""1.0.0"",      ""revision_id"": ""beta""    },)import { Client, Run, Example } from ""langsmith"";import { evaluate } from ""langsmith/evaluation"";import { EvaluationResult } from ""langsmith/evaluation"";const client = new Client();// Define dataset: these are your test casesconst datasetName = ""Sample Dataset"";const dataset = await client.createDataset(datasetName, {  description: ""A sample dataset in LangSmith."",});await client.createExamples({  inputs: [    { postfix: ""to LangSmith"" },    { postfix: ""to Evaluations in LangSmith"" },  ],  outputs: [    { output: ""Welcome to LangSmith"" },    { output: ""Welcome to Evaluations in LangSmith"" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: ""exact_match"",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: ""1.0.0"",      revision_id: ""beta"",    },  }); Learn more about evaluation in the how-to guides."
