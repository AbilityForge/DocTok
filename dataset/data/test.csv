url,heading,text
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets#create-a-new-version-of-a-dataset,Version datasets,"In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets#create-a-new-version-of-a-dataset,Create a new version of a dataset,"Any time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved. By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the ""Examples"" tab, you can see the state of the dataset at that point in time. Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the ""latest"" version of the dataset. Also, by default the latest version of the dataset is shown in the ""Examples"" tab and experiments from all versions are shown in the ""Tests"" tab. In the ""Tests"" tab, you can see the results of tests run on the dataset at different versions."
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets#create-a-new-version-of-a-dataset,Tag a version,"You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history. For example, you might tag a version of your dataset as ""prod"" and use it to run tests against your LLM pipeline. Tagging can be done in the UI by clicking on ""+ Tag this version"" in the ""Examples"" tab. You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK: from langsmith import Clientfromt datetime import datetimeclient = Client()initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like ""prod""client.update_dataset_tag(    dataset_name=toxic_dataset_name, as_of=initial_time, tag=""prod"") To run an evaluation on a particular tagged version of a dataset, you can follow this guide."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,LangSmith Release Notes,noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,"Week of June 17, 2024 - LangSmith v0.6","LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,New Features,Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More...
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Performance and Reliability Changes,Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Infrastructure changes,"[Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Admin changes,Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Deprecation notices,With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,"Week of May 13, 2024 - LangSmith v0.5","LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Breaking changes,We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,New Features,Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Performance and Reliability Changes,"Split ingest, session deletion, and automation jobs to execute within separate resource pools."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Infrastructure changes,"As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Admin changes,Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Deprecation notices,With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,"Week of March 25, 2024 - LangSmith v0.4","LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Breaking changes,"This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Performance and Reliability Changes,"Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Infrastructure changes,"Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Admin changes,Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Deprecation notices,With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,"Week of Februrary 21, 2024 - LangSmith v0.3","LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Breaking changes,"This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Performance and Reliability Changes,"Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Admin changes,None
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Deprecation notices,With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,"Week of January 29, 2024 - LangSmith v0.2","LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Requirements,This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Breaking changes,"The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {""user_id"": ..., ""user_name"":...,}) and then search using has(metadata, '{""user_name"": ...}')"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Performance and Reliability Changes,"Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Infrastructure Changes,"Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Admin changes,Increased the maximum number of users per organization from 5 to 100 for new organizations.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-january-29-2024---langsmith-v02,Deprecation notices,With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/concepts/admin,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#quick-start,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_api,Trace using the LangSmith REST API,"It is HIGHLY recommended to use our Python or TypeScript SDKs to send traces to LangSmith.
We have designed these SDKs with several optimizations, including batching and backgrounding, to ensure that your application's performance is not impacted by sending traces to LangSmith.
However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application.
This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation here for a full list of endpoints and request/response schemas. noteWhen using the LangSmith REST API, you will need to provide your API key in the request headers as ""x-api-key"".Additionally, you should IGNORE and not set the dotted_order and trace_id fields in the request body. These fields will be automatically generated by the system. The following example shows how you might leverage our API directly in Python. The same principles apply to other languages. import openaiimport osimport requestsfrom datetime import datetimefrom uuid import uuid4def post_run(run_id, name, run_type, inputs, parent_id=None):    """"""Function to post a new run to the API.""""""    data = {        ""id"": run_id.hex,        ""name"": name,        ""run_type"": run_type,        ""inputs"": inputs,        ""start_time"": datetime.utcnow().isoformat(),    }    if parent_id:        data[""parent_run_id""] = parent_id.hex    requests.post(        ""https://api.smith.langchain.com/runs"", # Update appropriately for self-hosted installations or the EU region        json=data,        headers=headers    )def patch_run(run_id, outputs):    """"""Function to patch a run with outputs.""""""    requests.patch(        f""https://api.smith.langchain.com/runs/{run_id}"",        json={            ""outputs"": outputs,            ""end_time"": datetime.utcnow().isoformat(),        },        headers=headers,    )# Send your API Key in the request headersheaders = {""x-api-key"": os.environ[""LANGCHAIN_API_KEY""]}# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    {""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context.""},    {""role"": ""user"", ""content"": f""Question: {question}\\nContext: {context}""}]# Create parent runparent_run_id = uuid4()post_run(parent_run_id, ""Chat Pipeline"", ""chain"", {""question"": question})# Create child runchild_run_id = uuid4()post_run(child_run_id, ""OpenAI Call"", ""llm"", {""messages"": messages}, parent_run_id)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(model=""gpt-3.5-turbo"", messages=messages)# End runspatch_run(child_run_id, chat_completion.dict())patch_run(parent_run_id, {""answer"": chat_completion.choices[0].message.content})"
https://docs.smith.langchain.com/concepts/admin#roles,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#roles,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#roles,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#roles,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#roles,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#roles,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#roles,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#roles,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#roles,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#pull_a_prompt,Manage prompts programmatically,"You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.
All functionality going forward will live in the langsmith package."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#pull_a_prompt,Install packages,PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#pull_a_prompt,Configure environment variables,"If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=""lsv2_..."" TerminologyWhat we refer to as ""prompts"" used to be called ""repos"", so any references to ""repo"" in the code are referring to a prompt."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#pull_a_prompt,Push a prompt,"To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = client.push_prompt(""joke-generator"", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = prompts.push(""joke-generator"", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");url = prompts.push(""joke-generator"", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.
This is useful for storing the model configuration you want to use with this prompt.
The provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelclient.push_prompt(""joke-generator-with-model"", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelurl = prompts.push(""joke-generator-with-model"", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { ChatOpenAI } from ""langchain-openai"";const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");const chain = prompt.pipe(model);prompts.push(""joke-generator-with-model"", chain);"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#pull_a_prompt,Pull a prompt,"To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { ChatOpenAI } from ""langchain-openai"";const prompt = prompts.pull(""joke-generator"");const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const chain = prompt.pipe(model);chain.invoke({""topic"": ""cats""}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.
Just specify include_model when pulling the prompt.
If the stored prompt includes a model, it will be returned as a RunnableSequence.
Make sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})from langchain import hub as promptschain = prompts.pull(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { Runnable } from ""@langchain/core/runnables"";const chain = prompts.pull<Runnable>(""joke-generator-with-model"", { includeModel: true });chain.invoke({""topic"": ""cats""}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#pull_a_prompt,Use a prompt without LangChain,"If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.
These convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(""joke-generator"")prompt_value = prompt.invoke({""topic"": ""cats""})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#pull_a_prompt,"List, delete, and like prompts","You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.
See the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include ""joke""prompts = client.list_prompts(query=""joke"", is_public=False)# Delete a promptclient.delete_prompt(""joke-generator"")# Like a promptclient.like_prompt(""efriis/my-first-prompt"")# Unlike a promptclient.unlike_prompt(""efriis/my-first-prompt"")// List all prompts in my workspaceimport Client from ""langsmith"";const client = new Client({ apiKey: ""lsv2_..."" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include ""joke""const private_joke_prompts = client.listPrompts({ query: ""joke"", isPublic: false});// Delete a promptclient.deletePrompt(""joke-generator"");// Like a promptclient.likePrompt(""efriis/my-first-prompt"");// Unlike a promptclient.unlikePrompt(""efriis/my-first-prompt""); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.
However, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues,Use annotation queues,"Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.
While you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues,Create an annotation queue,"To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.
Then click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.
You can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as ""Done"" for it to be removed from the queue. If you check ""All workspace members review each run,"" then a run will remain in the queue until all workspace members have marked it ""Done"".Enable reservations on runs: We recommend enabling reservations.
This will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified ""reservation length"". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it ""Done"", the reservation will expire after the specified ""reservation length"".
The run is then released back into the queue and can be reserved by another reviewer. noteClicking ""Requeue at end"" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues,Assign runs to an annotation queue,"To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.
Select multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.
Set up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.
To learn more about how to capture user feedback from your LLM application, follow this guide."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues,Review runs in an annotation queue,"To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.
Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.
You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to ""View run"". The keyboard shortcuts shown can help streamline the review process."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#view-the-experiment-in-the-ui,Upload experiments run outside of LangSmith with the REST API,"Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#view-the-experiment-in-the-ui,Request body schema,"Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within
the experiment. Each object in the results represents a ""row"" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name
refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset
in LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  ""experiment_name"": ""string (required)"",  ""experiment_description"": ""string (optional)"",  ""experiment_start_time"": ""datetime (required)"",  ""experiment_end_time"": ""datetime (required)"",  ""dataset_id"": ""uuid (optional - an external dataset id, used to group experiments together)"",  ""dataset_name"": ""string (optional - must provide either dataset_id or dataset_name)"",  ""dataset_description"": ""string (optional)"",  ""experiment_metadata"": { // Object (any shape - optional)    ""key"": ""value""  },  ""summary_experiment_scores"": [ // List of summary feedback objects (optional)    {      ""key"": ""string (required)"",      ""score"": ""number (optional)"",      ""value"": ""string (optional)"",      ""comment"": ""string (optional)"",      ""feedback_source"": { // Object (optional)        ""type"": ""string (required)""      },      ""feedback_config"": { // Object (optional)        ""type"": ""string enum: continuous, categorical, or freeform"",        ""min"": ""number (optional)"",        ""max"": ""number (optional)"",        ""categories"": [ // List of feedback category objects (optional)            ""value"": ""number (required)"",            ""label"": ""string (optional)""        ]      },      ""created_at"": ""datetime (optional - defaults to now)"",      ""modified_at"": ""datetime (optional - defaults to now)"",      ""correction"": ""Object or string (optional)""    }  ],  ""results"": [ // List of experiment row objects (required)    {      ""row_id"": ""uuid (required)"",      ""inputs"": {     // Object (required - any shape). This will        ""key"": ""val""  // be the input to both the run and the dataset example.      },      ""expected_outputs"": { // Object (optional - any shape).        ""key"": ""val""        // These will be the outputs of the dataset examples.      },      ""actual_outputs"": { // Object (optional - any shape).        ""key"": ""val       // These will be the outputs of the runs.      },      ""evaluation_scores"": [ // List of feedback objects for the run (optional)        {            ""key"": ""string (required)"",            ""score"": ""number (optional)"",            ""value"": ""string (optional)"",            ""comment"": ""string (optional)"",            ""feedback_source"": { // Object (optional)                ""type"": ""string (required)""            },            ""feedback_config"": { // Object (optional)                ""type"": ""string enum: continuous, categorical, or freeform"",                ""min"": ""number (optional)"",                ""max"": ""number (optional)"",                ""categories"": [ // List of feedback category objects (optional)                    ""value"": ""number (required)"",                    ""label"": ""string (optional)""                ]            },            ""created_at"": ""datetime (optional - defaults to now)"",            ""modified_at"": ""datetime (optional - defaults to now)"",            ""correction"": ""Object or string (optional)""        }      ],      ""start_time"": ""datetime (required)"", // The start/end times for the runs will be used to      ""end_time"": ""datetime (required)"",   // calculate latency. They must all fall between the      ""run_name"": ""string (optional)"",     // start and end times for the experiment.      ""error"": ""string (optional)"",      ""run_metadata"": { // Object (any shape - optional)        ""key"": ""value""      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#view-the-experiment-in-the-ui,Considerations,"You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together
under a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if
you only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#view-the-experiment-in-the-ui,Example request,"Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  ""experiment_name"": ""My external experiment"",  ""experiment_description"": ""An experiment uploaded to LangSmith"",  ""dataset_name"": ""my-external-dataset"",  ""summary_experiment_scores"": [    {      ""key"": ""summary_accuracy"",      ""score"": 0.9,      ""comment"": ""Great job!""    }  ],  ""results"": [    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the weather in San Francisco today?""      },      ""expected_outputs"": {        ""output"": ""Sorry, I am unable to provide information about the current weather.""      },      ""actual_outputs"": {        ""output"": ""The weather is partly cloudy with a high of 65.""      },      ""evaluation_scores"": [        {          ""key"": ""hallucination"",          ""score"": 1,          ""comment"": ""The chatbot made up the weather instead of identifying that ""                     ""they don't have enough info to answer the question. This is ""                     ""a hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:39"",      ""end_time"": ""2024-08-03T00:12:41"",      ""run_name"": ""Chatbot""    },    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the square root of 49?""      },      ""expected_outputs"": {        ""output"": ""The square root of 49 is 7.""      },      ""actual_outputs"": {        ""output"": ""7.""      },      ""evaluation_scores"": [       {          ""key"": ""hallucination"",          ""score"": 0,          ""comment"": ""The chatbot correctly identified the answer. This is not a ""                     ""hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:40"",      ""end_time"": ""2024-08-03T00:12:42"",      ""run_name"": ""Chatbot""    }  ],  ""experiment_start_time"": ""2024-08-03T00:12:38"",  ""experiment_end_time"": ""2024-08-03T00:12:43""}resp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/upload-experiment"",    json=body,    headers={""x-api-key"": os.environ[""LANGCHAIN_API_KEY""]})print(resp.json()) Below is the response received: {  ""dataset"": {    ""name"": ""my-external-dataset"",    ""description"": null,    ""created_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""data_type"": ""kv"",    ""inputs_schema_definition"": null,    ""outputs_schema_definition"": null,    ""externally_managed"": true,    ""id"": ""<<uuid>>"",    ""tenant_id"": ""<<uuid>>"",    ""example_count"": 0,    ""session_count"": 0,    ""modified_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""last_session_start_time"": null  },  ""experiment"": {    ""start_time"": ""2024-08-03T00:12:38"",    ""end_time"": ""2024-08-03T00:12:43+00:00"",    ""extra"": null,    ""name"": ""My external experiment"",    ""description"": ""An experiment uploaded to LangSmith"",    ""default_dataset_id"": null,    ""reference_dataset_id"": ""<<uuid>>"",    ""trace_tier"": ""longlived"",    ""id"": ""<<uuid>>"",    ""run_count"": null,    ""latency_p50"": null,    ""latency_p99"": null,    ""first_token_p50"": null,    ""first_token_p99"": null,    ""total_tokens"": null,    ""prompt_tokens"": null,    ""completion_tokens"": null,    ""total_cost"": null,    ""prompt_cost"": null,    ""completion_cost"": null,    ""tenant_id"": ""<<uuid>>"",    ""last_run_start_time"": null,    ""last_run_start_time_live"": null,    ""feedback_stats"": null,    ""session_feedback_stats"": null,    ""run_facets"": null,    ""error_rate"": null,    ""streaming_rate"": null,    ""test_run_number"": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.
If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this
information in the request body)."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#view-the-experiment-in-the-ui,View the experiment in the UI,"Now, login to the UI and click on your newly-created dataset! You should see a single experiment:
 Your examples will have been uploaded:
 Clicking on your experiment will bring you to the comparison view:
 As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators,Use LangChain off-the-shelf evaluators (Python only),"Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.
These evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Ankush"", ""Hello Ankush""),    (""Harrison"", ""Hello Harrison""),]dataset_name = ""Hello Set""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": input}, {""expected"": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators,Use question and answer (correctness) evaluators,"Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!
Three QA evaluators you can load are: ""qa"", ""context_qa"", ""cot_qa"". Based on our meta-evals, we recommend using ""cot_qa"", or Chain of Thought QA. Here is a trivial example that uses a ""cot_qa"" evaluator to evaluate a simple pipeline that prefixes the input with ""Hello"": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(""cot_qa"")client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators,Use criteria evaluators,"If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the ""criteria"" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The ""criteria"" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    ""criteria"",    config={        ""criteria"": {            ""says_hello"": ""Does the submission say hello?"",        }    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.
To specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {""creativity"": ""Is this submission creative, imaginative, or novel?""}criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={""criteria"": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent ""direction"" (i.e., higher is not necessarily better).
The direction of the score depends on the criteria being evaluated. For example, a score of 1 for ""helpfulness"" means that the prediction was deemed to be helpful by the model.
However, a score of 1 for ""maliciousness"" means that the prediction contains malicious content, which, of course, is ""bad""."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators,Use labeled criteria evaluators,"If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the ""labeled_criteria"" or ""labeled_score_string"" evaluators. The ""labeled_criteria"" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe ""labeled_score_string"" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    ""labeled_score_string"",    config={        ""criteria"": {            ""accuracy"": ""How accurate is this prediction compared to the reference on a scale of 1-10?""        },        ""normalize_by"": 10,    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators,Use string or embedding distance metrics,"To measure the similarity between a predicted string and a reference, you can use string distance metrics: The ""string_distance"" evaluator computes a normalized string edit distance between the prediction and referenceThe ""embedding_distance"" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    ""string_distance"",    config={""distance"": ""levenshtein"", ""normalize_score"": True})embedding_distance_evaluator = LangChainStringEvaluator(    ""embedding_distance"",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # ""embeddings"": HuggingFaceEmbeddings(model=""distilbert-base-uncased""),      # Can also choose ""euclidean"", ""chebyshev"", ""hamming"", and ""manhattan""        ""distance_metric"": ""cosine"",      })evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators,Use a custom LLM in off-the-shelf evaluators,"You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=""gpt-3.5-turbo"")cot_qa_evaluator = LangChainStringEvaluator(""cot_qa"", config={""llm"": eval_llm})evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-labeled-criteria-evaluators,Handle multiple input or output fields,"LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.
These map the keys ""prediction"", ""reference"", and ""input"" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: ""greeting"" and ""foo"". We want to evaluate the ""greeting"" field against the ""expected"" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    },    prepare_data=lambda run, example: {        ""prediction"": run.outputs[""greeting""],        ""reference"": example.outputs[""expected""],        ""input"": example.inputs[""input""],    })client = Client()evaluate(    lambda input: {""greeting"": ""Hello "" + input[""input""], ""foo"": ""bar""},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,Set up automation rules,"While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users.
LangSmith provides a powerful feature called automations that allow you to trigger certain actions on your trace data.
At a high level, automations are defined by a filter, sampling rate, and action. Automation rules can trigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an annotation queue, and triggering a webhook. An example of an automation you can set up can be ""trigger an online evaluation that grades on vagueness for all of my downvoted traces."""
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,Create a rule,We will outline the steps for creating an automation rule in LangSmith below.
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,Step 1: Navigate to rule creation,"To create a rule, head click on Rules in the top right corner of any project details page, then scroll to the bottom and click on + Add Rule. Alternatively, you can access rules in settings by navigating to this link, click on + Add Rule, then Project Rule. noteThere are currently two types of rules you can create: Project Rule and Dataset Rule.Project Rule: This rule will apply to traces in the specified project. Actions allowed are adding to a dataset, adding to an annotation queue, running online evaluation, and triggering a webhook.Dataset Rule: This rule will apply to traces that are part of an experiment in the specified dataset. Actions allowed are only running an evaluator on the experiment results. To see this in action, you can follow this guide. Give your rule a name, for example ""my_rule"":"
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,Step 2: Define the filter,"You can create a filter as you normally would to filter traces in the project. For more information on filters, you can refer to this guide."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,(Optional) Step 3: Apply Rule to Past Runs,"When creating a new rule, you can apply the rule to past runs as well. To do this, select Apply to Past Runs checkbox and enter Backfill From date as the
start date to apply the rule. This will start from the Backfill From date and apply the run rules until it is caught up with the latest runs. Note that you will have to expand the date range for logs if you wanted to look at the progress of the backfill, see View logs for your automations for details."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,Step 4: Define the sampling rate,"You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,Step 5: Define the action,"There are four actions you can take with an automation rule: Add to dataset: Add the inputs and outputs of the trace to a dataset.Add to annotation queue: Add the trace to an annotation queue.Run online evaluation: Run an online evaluation on the trace. For more information on online evaluations, you can refer to this guide.Trigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.Extend data retention: Extends the data retention period on matching traces that use base retention.
Note that all other rules will also extend data retention on matching traces through the
auto-upgrade mechanism,
but this rule takes no additional action."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#view-logs-for-your-automations,View logs for your automations,"You can view logs for your automations by going to Settings -> Rules and click on the Logs button in any row. You can also get to logs by clicking on Rules in the top right hand corner of any project details page, then clicking on See Logs for any rule. Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced.
If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon. By default, rule logs only show results for runs that occurred in the last day. To see results for older runs, you can select Last 1 day and enter the desired date range.
When applying a rule to past runs, the processing will start from the start date and go forward, so this would be needed to view logs while the backfill is proceeding."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Set up webhook notifications for rules,"When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Webhook payload,"The payload we send to your webhook endpoint contains ""rule_id"" this is the ID of the automation that sent this payload""start_time"" and ""end_time"" these are the time boundaries where we found matching runs""runs"" this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.""feedback_stats"" this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below. ""feedback_stats"": {    ""about_langchain"": {        ""n"": 1,        ""avg"": 0.0,        ""show_feedback_arrow"": true,        ""values"": {}    },    ""category"": {        ""n"": 0,        ""avg"": null,        ""show_feedback_arrow"": true,        ""values"": {            ""CONCEPTUAL"": 1        }    },    ""user_score"": {        ""n"": 2,        ""avg"": 0.0,        ""show_feedback_arrow"": false,        ""values"": {}    },    ""vagueness"": {        ""n"": 1,        ""avg"": 0.0,        ""show_feedback_arrow"": true,        ""values"": {}    }}, fetching from S3 URLsDepending on how recent your runs are, the inputs_s3_urls and outputs_s3_urls fields may contain S3 URLs to the actual data instead of the data itself.The inputs and outputs can be fetched by the ROOT.presigned_url provided in inputs_s3_urls and outputs_s3_urls respectively. This is an example of the entire payload we send to your webhook endpoint: {  ""rule_id"": ""d75d7417-0c57-4655-88fe-1db3cda3a47a"",  ""start_time"": ""2024-04-05T01:28:54.734491+00:00"",  ""end_time"": ""2024-04-05T01:28:56.492563+00:00"",  ""runs"": [    {      ""status"": ""success"",      ""is_root"": true,      ""trace_id"": ""6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""dotted_order"": ""20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""run_type"": ""tool"",      ""modified_at"": ""2024-04-05T01:28:54.145062"",      ""tenant_id"": ""2ebda79f-2946-4491-a9ad-d642f49e0815"",      ""end_time"": ""2024-04-05T01:28:54.085649"",      ""name"": ""Search"",      ""start_time"": ""2024-04-05T01:28:54.085646"",      ""id"": ""6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""session_id"": ""6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5"",      ""parent_run_ids"": [],      ""child_run_ids"": null,      ""direct_child_run_ids"": null,      ""total_tokens"": 0,      ""completion_tokens"": 0,      ""prompt_tokens"": 0,      ""total_cost"": null,      ""completion_cost"": null,      ""prompt_cost"": null,      ""first_token_time"": null,      ""app_path"": ""/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809"",      ""in_dataset"": false,      ""last_queued_at"": null,      ""inputs"": null,      ""inputs_s3_urls"": null,      ""outputs"": null,      ""outputs_s3_urls"": null,      ""extra"": null,      ""events"": null,      ""feedback_stats"": null,      ""serialized"": null,      ""share_token"": null    }  ]}"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Webhook Security,"We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications. An example would be https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Webhook custom HTTP headers,"If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the Headers option next to the URL field and add your headers. noteHeaders are stored in encrypted format."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Webhook Delivery,"When delivering events to your webhook endpoint we follow these guidelines If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.Anything your endpoint returns in the body will be ignored"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Setup,"For an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here. First, create a Modal account. Then, locally install the Modal SDK: pip install modal To finish setting up your account, run the command: modal setup and follow the instructions"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Secrets,"Next, you will need to set up some secrets in Modal. First, LangSmith will need to authenticate to Modal by passing in a secret.
The easiest way to do this is to pass in a secret in the query parameters.
To validate this secret, we will need to add a secret in Modal to validate it.
We will do that by creating a Modal secret.
You can see instructions for secrets here.
For this purpose, let's call our secret ls-webhook and have it set an environment variable with the name LS_WEBHOOK. We can also set up a LangSmith secret - luckily there is already an integration template for this!"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Service,"After that, you can create a Python file that will serve as your endpoint.
An example is below, with comments explaining what is going on: from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub(""auth-example"", image=Image.debian_slim().pip_install(""langsmith""))@stub.function(    secrets=[Secret.from_name(""ls-webhook""), Secret.from_name(""my-langsmith-secret"")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method=""POST"")# We set up a `secret` query parameterdef f(data: dict, secret: str = Query(...)):    # You can import dependencies you don't have locally inside Modal funxtions    from langsmith import Client    # First, we validate the secret key we pass    import os    if secret != os.environ[""LS_WEBHOOK""]:        raise HTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=""Incorrect bearer token"",            headers={""WWW-Authenticate"": ""Bearer""},        )    # This is where we put the logic for what should happen inside this webhook    ls_client = Client()    runs = data[""runs""]    ids = [r[""id""] for r in runs]    feedback = list(ls_client.list_feedback(run_ids=ids))    for r, f in zip(runs, feedback):        try:            ls_client.create_example(                inputs=r[""inputs""],                outputs={""output"": f.correction},                dataset_name=""classifier-github-issues"",            )        except Exception:            raise ValueError(f""{r} and {f}"")    # Function body    return ""success!"" We can now deploy this easily with modal deploy ... (see docs here). You should now get something like:  Created objects.  Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py  Created mount PythonPackage:langsmith  Created f => https://hwchase17--auth-example-f.modal.run App deployed! View Deployment: https://modal.com/apps/hwchase17/auth-example The important thing to remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.
NOTE: this is NOT the final deployment URL, make sure not to accidentally use that."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#example-with-modal,Hooking it up,"We can now take the function URL we create above and add it as a webhook.
We have to remember to also pass in the secret key as a query parameter.
Putting it all together, it should look something like: https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET} Replace {SECRET} with the secret key you created to access the Modal service."
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project#set-the-destination-project-dynamically,Log traces to specific project,You can change the destination project of your traces both statically through environment variables and dynamically at runtime.
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project#set-the-destination-project-dynamically,Set the destination project statically,"As mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-custom-project If the project specified does not exist, it will be created automatically when the first trace is ingested."
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project#set-the-destination-project-dynamically,Set the destination project dynamically,"You can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application. noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""Hello!""}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(    run_type=""llm"",    name=""OpenAI Call Decorator"",    project_name=""My Project"")def call_openai(    messages: list[dict], model: str = ""gpt-3.5-turbo"") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(    messages,    langsmith_extra={""project_name"": ""My Overriden Project""},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=messages,    langsmith_extra={""project_name"": ""My Project""},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(    run_type=""llm"",    name=""OpenAI Call RunTree"",    inputs={""messages"": messages},    project_name=""My Project"")chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";import { RunTree} from ""langsmith"";const client = new OpenAI();const messages = [    {role: ""system"", content: ""You are a helpful assistant.""},    {role: ""user"", content: ""Hello!""}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: ""llm"",    name: ""OpenAI Call Traceable"",    project_name: ""My Project""});// Call the traceable functionawait traceableCallOpenAI(messages, ""gpt-3.5-turbo"");// Create and use a RunTree objectconst rt = new RunTree({    runType: ""llm"",    name: ""OpenAI Call RunTree"",    inputs: { messages },    project_name: ""My Project""});// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.postRun();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax,Trace query syntax,"Using the list_runs method in the SDK or /runs/query endpoint in the API, you can filter runs to analyze and export."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax,Filter arguments,"KeysDescriptionproject_id / project_nameThe project(s) to fetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a specific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name / dataset_idFetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.reference_example_idFetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.errorFetch runs that errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other filtering arguments.filterFetch runs that match a given structured filter statement. See the guide below for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to return in the response. By default, all fields are returned.query (experimental)Natural language query, which translates your query into a filter statement."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax,Filter query language,"LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs. The filtering grammar is based on common comparators on fields in the run object. Supported comparators include: gte (greater than or equal to)gt (greater than)lte (less than or equal to)lt (less than)eq (equal to)neq (not equal to)has (check if run contains a tag or metadata json blob)search (search for a substring in a string field) Additionally, you can combine multiple comparisons through and and or operators. These can be applied on fields of the run object, such as its id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and any associated feedback through feedback_key and feedback_score."
https://docs.smith.langchain.com/how_to_guides/prompts/open_a_prompt_from_a_trace,Open a prompt from a trace,"If you pull a prompt into your code and begin logging traces that use it, you can find a link to the prompt in the Trace UI. In the run that used the prompt, hover over the Prompt tag. Clicking on this will take you to the prompt. (If you used a LangChain Hub prompt, this tag will say Hub)
] In the metadata of the run, you can see more details. Click on an individual prompt metadata value to filter your traces by that attribute. You can filter by prompt handle, prompt name, or prompt commit hash.
"
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,Architectural overview,"Enterprise License RequiredSelf-Hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith can be run via Kubernetes (recommended) or Docker in a Cloud environment that you control. The LangSmith application consists of several components including 5 LangSmith servers and 3 stateful services: LangSmith FrontendLangSmith BackendLangSmith Platform BackendLangSmith PlaygroundLangSmith QueueClickHousePostgresRedis To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine."
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,Storage Services,"noteLangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services.
In a production setting, we strongly recommend using external Storage Services."
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,ClickHouse,"ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data)."
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,PostgreSQL,"PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback)."
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,Redis,"Redis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching. LangSmith uses Redis to back queuing/caching operations."
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,LangSmith Frontend,The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,LangSmith Backend,"The backend is the primary entrypoint for API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and sdk, preparing traces for ingestion, and supporting the hub API."
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,LangSmith Queue,"The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database."
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,LangSmith Platform Backend,The platform backend is an internal service that primarily handles authentication and other high-volume tasks. The user should not need to interact with this service directly.
https://docs.smith.langchain.com/self_hosting/architectural_overview#datastores,LangSmith Playground,The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.
https://docs.smith.langchain.com/pricing,Plans,"StartupsDeveloperPlusEnterpriseDesigned for early stage startups building AI applicationsDesigned for hobbyists who want to start their adventure soloEverything in Developer, plus team features and higher rate limitsDesigned for teams with more security, deployment, and support needsContact us to learn moreFree for 1 user5,000 free traces per monthAdditional traces billed starting @ 0.05/trace$39/user10,000 free traces per monthAdditional traces billed starting @ 0.05/traceCustomWhat to expect:We want all early stage companies to build with LangSmith. LangSmith for Startups offers discounted prices and a generous free, monthly trace allotment, so you can have the right tooling in place as you grow your business.Key features:1 Developer seatDebugging tracesDataset collectionTesting and evaluationPrompt managementMonitoringKey features:All features in Developer tierUp to 10 seatsHosted LangServe (beta)Higher rate limitsEmail supportKey features:All features in Plus tierSingle Sign On (SSO)Negotiable SLAsDeployment options in customers environmentCustom rate limitsTeam trainingsShared Slack channelArchitectural guidanceDedicated customer success manager"
https://docs.smith.langchain.com/pricing,Plan Comparison,"DeveloperPlusEnterpriseFeaturesDebugging TracesDataset CollectionHuman LabelingTesting and EvaluationPrompt ManagementHosted LangServe--MonitoringRole-Based Access Controls (RBAC)----TeamDeveloper Seats1 Free SeatMaximum 10 seats$39 per seat/month1Custom pricingUsageTraces2First 5k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)First 10k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)CustomMax ingested events / hour350,0003 / 250,000500,000CustomTotal trace size storage / hour4500MB3 / 2.5GB5GBCustomSecurity ControlsSingle Sign On--GoogleGitHubCustom SSODeploymentHosted in USHosted in USAdd-on for self-hosted deployment in customer's VPCSupportSupport ChannelsCommunityEmailEmailShared Slack ChannelShared Slack Channel----Team Training----Application Architectural Guidance----Dedicated Customer Success Manager----SLA----ProcurementBillingMonthly, self-serveCredit CardMonthly, self-serveCredit CardAnnual InvoiceACHCustom Terms and Data Privacy Agreement----Infosec Review----WorkspacesSingle, default Workspace under Personal OrganizationUp to 3 Workspaces per OrganizationUp to 10 Workspaces per Organization (contact support@langchain.dev for more)Organization Roles (User and Admin)-- 1 2 3 4"
https://docs.smith.langchain.com/pricing,Ive been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?,"If youve been using LangSmith already, your usage will be billable starting in July. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by reaching out to sales@langchain.dev."
https://docs.smith.langchain.com/pricing,Which plan is right for me?,"If youre an individual developer, the Developer plan is a great choice for small projects. For teams that want to collaborate in LangSmith, check out the Plus plan. If you are an early-stage startup building an AI application, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our Startup Contact Form for more details. If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our Sales Contact Form for more details."
https://docs.smith.langchain.com/pricing,What is a seat?,A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.
https://docs.smith.langchain.com/pricing,What is a trace?,"A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an example of a single trace."
https://docs.smith.langchain.com/pricing,What is an ingested event?,"An ingested event is any distinct, trace-related data sent to LangSmith. This includes: Inputs, outputs and metadata sent at the start of a run step within a traceInputs, outputs and metadata sent at the end of a run step within a traceFeedback on run steps or traces"
https://docs.smith.langchain.com/pricing,Ive hit my rate or usage limits. What can I do?,"If youve consumed the monthly allotment of free traces in your account, you can add a credit card on the Developer and Plus plans to continue sending traces to LangSmith. If youve hit the rate limits on your tier, you can upgrade to a higher plan to get higher limits, or reach out to support@langchain.dev with questions."
https://docs.smith.langchain.com/pricing,"I have a developer account, can I upgrade my account to the Plus or Enterprise plan?","Every user will have a unique personal account on the Developer plan. We cannot upgrade a Developer account to the Plus or Enterprise plans. If youre interested in working as a team, create a separate LangSmith Organization on the Plus plan. This plan can upgraded to the Enterprise plan at a later date."
https://docs.smith.langchain.com/pricing,How will billing work?,Seats Traces
https://docs.smith.langchain.com/pricing,Can I limit how much I spend on tracing?,"You can set limits on the number of traces that can be sent to LangSmith per month on
the Plans and Billing settings page. noteWhile we do show you the dollar value of your usage limit for convenience, this limit evaluated
in terms of number of traces instead of dollar amount. For example, if you are approved for our
startup plan tier where you are given a generous allotment of free traces, your usage limit will
not automatically change.You are not currently able to set a spend limit in the product."
https://docs.smith.langchain.com/pricing,How can my track my usage so far this month?,"Under the Settings section for your Organization you will see subsection for Usage. There, you will able to see a graph of the daily nunber of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day."
https://docs.smith.langchain.com/pricing,I have a question about my bill...,Customers on the Developer and Plus plan tiers should email support@langchain.dev. Customers on the Enterprise plan should contact their sales representative directly. Enterprise plan customers are billed annually by invoice.
https://docs.smith.langchain.com/pricing,What can I expect from Support?,"On the Developer plan, community-based support is available on Discord. On the Plus plan, you will also receive preferential, email support at support@langchain.dev for LangSmith-related questions only and we'll do our best to respond within the next business day. On the Enterprise plan, youll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, well also support deployments and new releases with our infra engineering team on-call."
https://docs.smith.langchain.com/pricing,Where is my data stored?,"You may choose to sign up in either the US or EU region. See the cloud architecture reference for more details. If youre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment."
https://docs.smith.langchain.com/pricing,Which security frameworks is LangSmith compliant with?,"We are SOC 2 Type II, GDPR, and HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com. Please note we only enter into BAAs with customers on our Enterprise plan."
https://docs.smith.langchain.com/pricing,Will you train on the data that I send LangSmith?,"We will not train on your data, and you own all rights to your data. See LangSmith Terms of Service for more information."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#copy-the-filter,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/concepts/usage_and_billing,Usage and Billing,Data RetentionUsage LimitsRate Limits
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#generate-synthetic-examples,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#example-request,Upload experiments run outside of LangSmith with the REST API,"Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#example-request,Request body schema,"Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within
the experiment. Each object in the results represents a ""row"" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name
refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset
in LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  ""experiment_name"": ""string (required)"",  ""experiment_description"": ""string (optional)"",  ""experiment_start_time"": ""datetime (required)"",  ""experiment_end_time"": ""datetime (required)"",  ""dataset_id"": ""uuid (optional - an external dataset id, used to group experiments together)"",  ""dataset_name"": ""string (optional - must provide either dataset_id or dataset_name)"",  ""dataset_description"": ""string (optional)"",  ""experiment_metadata"": { // Object (any shape - optional)    ""key"": ""value""  },  ""summary_experiment_scores"": [ // List of summary feedback objects (optional)    {      ""key"": ""string (required)"",      ""score"": ""number (optional)"",      ""value"": ""string (optional)"",      ""comment"": ""string (optional)"",      ""feedback_source"": { // Object (optional)        ""type"": ""string (required)""      },      ""feedback_config"": { // Object (optional)        ""type"": ""string enum: continuous, categorical, or freeform"",        ""min"": ""number (optional)"",        ""max"": ""number (optional)"",        ""categories"": [ // List of feedback category objects (optional)            ""value"": ""number (required)"",            ""label"": ""string (optional)""        ]      },      ""created_at"": ""datetime (optional - defaults to now)"",      ""modified_at"": ""datetime (optional - defaults to now)"",      ""correction"": ""Object or string (optional)""    }  ],  ""results"": [ // List of experiment row objects (required)    {      ""row_id"": ""uuid (required)"",      ""inputs"": {     // Object (required - any shape). This will        ""key"": ""val""  // be the input to both the run and the dataset example.      },      ""expected_outputs"": { // Object (optional - any shape).        ""key"": ""val""        // These will be the outputs of the dataset examples.      },      ""actual_outputs"": { // Object (optional - any shape).        ""key"": ""val       // These will be the outputs of the runs.      },      ""evaluation_scores"": [ // List of feedback objects for the run (optional)        {            ""key"": ""string (required)"",            ""score"": ""number (optional)"",            ""value"": ""string (optional)"",            ""comment"": ""string (optional)"",            ""feedback_source"": { // Object (optional)                ""type"": ""string (required)""            },            ""feedback_config"": { // Object (optional)                ""type"": ""string enum: continuous, categorical, or freeform"",                ""min"": ""number (optional)"",                ""max"": ""number (optional)"",                ""categories"": [ // List of feedback category objects (optional)                    ""value"": ""number (required)"",                    ""label"": ""string (optional)""                ]            },            ""created_at"": ""datetime (optional - defaults to now)"",            ""modified_at"": ""datetime (optional - defaults to now)"",            ""correction"": ""Object or string (optional)""        }      ],      ""start_time"": ""datetime (required)"", // The start/end times for the runs will be used to      ""end_time"": ""datetime (required)"",   // calculate latency. They must all fall between the      ""run_name"": ""string (optional)"",     // start and end times for the experiment.      ""error"": ""string (optional)"",      ""run_metadata"": { // Object (any shape - optional)        ""key"": ""value""      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#example-request,Considerations,"You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together
under a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if
you only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#example-request,Example request,"Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  ""experiment_name"": ""My external experiment"",  ""experiment_description"": ""An experiment uploaded to LangSmith"",  ""dataset_name"": ""my-external-dataset"",  ""summary_experiment_scores"": [    {      ""key"": ""summary_accuracy"",      ""score"": 0.9,      ""comment"": ""Great job!""    }  ],  ""results"": [    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the weather in San Francisco today?""      },      ""expected_outputs"": {        ""output"": ""Sorry, I am unable to provide information about the current weather.""      },      ""actual_outputs"": {        ""output"": ""The weather is partly cloudy with a high of 65.""      },      ""evaluation_scores"": [        {          ""key"": ""hallucination"",          ""score"": 1,          ""comment"": ""The chatbot made up the weather instead of identifying that ""                     ""they don't have enough info to answer the question. This is ""                     ""a hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:39"",      ""end_time"": ""2024-08-03T00:12:41"",      ""run_name"": ""Chatbot""    },    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the square root of 49?""      },      ""expected_outputs"": {        ""output"": ""The square root of 49 is 7.""      },      ""actual_outputs"": {        ""output"": ""7.""      },      ""evaluation_scores"": [       {          ""key"": ""hallucination"",          ""score"": 0,          ""comment"": ""The chatbot correctly identified the answer. This is not a ""                     ""hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:40"",      ""end_time"": ""2024-08-03T00:12:42"",      ""run_name"": ""Chatbot""    }  ],  ""experiment_start_time"": ""2024-08-03T00:12:38"",  ""experiment_end_time"": ""2024-08-03T00:12:43""}resp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/upload-experiment"",    json=body,    headers={""x-api-key"": os.environ[""LANGCHAIN_API_KEY""]})print(resp.json()) Below is the response received: {  ""dataset"": {    ""name"": ""my-external-dataset"",    ""description"": null,    ""created_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""data_type"": ""kv"",    ""inputs_schema_definition"": null,    ""outputs_schema_definition"": null,    ""externally_managed"": true,    ""id"": ""<<uuid>>"",    ""tenant_id"": ""<<uuid>>"",    ""example_count"": 0,    ""session_count"": 0,    ""modified_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""last_session_start_time"": null  },  ""experiment"": {    ""start_time"": ""2024-08-03T00:12:38"",    ""end_time"": ""2024-08-03T00:12:43+00:00"",    ""extra"": null,    ""name"": ""My external experiment"",    ""description"": ""An experiment uploaded to LangSmith"",    ""default_dataset_id"": null,    ""reference_dataset_id"": ""<<uuid>>"",    ""trace_tier"": ""longlived"",    ""id"": ""<<uuid>>"",    ""run_count"": null,    ""latency_p50"": null,    ""latency_p99"": null,    ""first_token_p50"": null,    ""first_token_p99"": null,    ""total_tokens"": null,    ""prompt_tokens"": null,    ""completion_tokens"": null,    ""total_cost"": null,    ""prompt_cost"": null,    ""completion_cost"": null,    ""tenant_id"": ""<<uuid>>"",    ""last_run_start_time"": null,    ""last_run_start_time_live"": null,    ""feedback_stats"": null,    ""session_feedback_stats"": null,    ""run_facets"": null,    ""error_rate"": null,    ""streaming_rate"": null,    ""test_run_number"": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.
If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this
information in the request body)."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#example-request,View the experiment in the UI,"Now, login to the UI and click on your newly-created dataset! You should see a single experiment:
 Your examples will have been uploaded:
 Clicking on your experiment will bring you to the comparison view:
 As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."
https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization,Deleting Organizations,"The LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table. This command using the Organization ID as an argument."
https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization,Prerequisites,"Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/PostgreSQL clienthttps://www.postgresql.org/download/PostgreSQL database connection:HostPortUsernameIf using the bundled version, this is postgresPasswordIf using the bundled version, this is postgresDatabase nameIf using the bundled version, this is postgresClickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the PostgreSQL database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the postgresql service to your local machine.Run kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.Connectivity to the Clickhouse database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port 8443"
https://docs.smith.langchain.com/self_hosting/scripts/delete_an_organization,Running the deletion script for a single organization,"Run the following command to run the organization removal script: sh delete_organization.sh <postgres_url> <clickhouse_url> --organization_id <organization_id> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_organization.sh ""postgres://postgres:postgres@localhost:5432/postgres"" ""clickhouse://default:password@localhost:8123/default"" --organization_id 4ec70ec7-0808-416a-b836-7100aeec934b If you visit the Langsmith UI, you should now see organization is no longer present."
https://docs.smith.langchain.com/concepts/tracing#runs,Tracing,"This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"
https://docs.smith.langchain.com/concepts/tracing#runs,Runs,"A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#runs,Traces,"A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
"
https://docs.smith.langchain.com/concepts/tracing#runs,Projects,"A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.
"
https://docs.smith.langchain.com/concepts/tracing#runs,Feedback,"Feedback allows you to score an individual run based on certain criteria.
Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.
Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#runs,Tags,"Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces
"
https://docs.smith.langchain.com/concepts/tracing#runs,Metadata,"Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.
Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces
"
https://docs.smith.langchain.com/concepts/tracing#runs,Data storage and retention,"For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/concepts/tracing#runs,Deleting traces from LangSmith,"If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the ""Delete"" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control#assign-a-role-to-a-user,Set up access control,"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Read more about roles under admin concepts Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces LangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it.
Only users with the workspace:manage permission can manage access control settings for a workspace."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control#assign-a-role-to-a-user,Create a role,"By default, LangSmith comes with a set of system roles: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) If these do not fit your access model, Organization Admins can create custom roles to suit your needs. To create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization. Click on the Create Role button to create a new role. You should see a form like the one below: Assign permissions for the different LangSmith resources that you want to control access to."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control#assign-a-role-to-a-user,Assign a role to a user,"Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page Each user will have a Role dropdown that you can use to assign a role to them. You can also invite new users with a given role."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchainjs-and-langsmith-sdk,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/concepts,Concepts,"Explanations, clarification and discussion of key topics in LangSmith."
https://docs.smith.langchain.com/concepts,Admin,OrganizationsWorkspacesUsersAPI keysPersonal Access Tokens (PATs)Service keysRolesOrganization rolesWorkspace roles
https://docs.smith.langchain.com/concepts,Tracing,RunsTracesProjectsFeedbackTagsMetadata
https://docs.smith.langchain.com/concepts,Evaluation,Datasets and examplesExperimentsEvaluators
https://docs.smith.langchain.com/concepts,Prompts,Prompt typesTemplate formats
https://docs.smith.langchain.com/concepts,Usage and Billing,Data RetentionUsage Limits
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-a-summary-evaluator,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/tracing/mask_inputs_outputs,Prevent logging of sensitive data in traces,"In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend. If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application: LANGCHAIN_HIDE_INPUTS=trueLANGCHAIN_HIDE_OUTPUTS=true This works for both the LangSmith SDK (Python and TypeScript) and LangChain. You can also customize and override this behavior for a given Client instance. This can be done by setting the hide_inputs and hide_outputs parameters on the Client object (hideInputs and hideOutputs in TypeScript). For the example below, we will simply return an empty object for both hide_inputs and hide_outputs, but you can customize this to your needs. PythonTypeScriptimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(openai.Client())langsmith_client = Client(    hide_inputs=lambda inputs: {}, hide_outputs=lambda outputs: {})# The trace produced will have its metadata present, but the inputs will be hiddenopenai_client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=[        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},        {""role"": ""user"", ""content"": ""Hello!""},    ],    langsmith_extra={""client"": langsmith_client},)# The trace produced will not have hidden inputs and outputsopenai_client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=[        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},        {""role"": ""user"", ""content"": ""Hello!""},    ],)import OpenAI from ""openai"";import { Client } from ""langsmith"";import { wrapOpenAI } from ""langsmith/wrappers"";const langsmithClient = new Client({  hideInputs: (inputs) => ({}),  hideOutputs: (outputs) => ({}),});// The trace produced will have its metadata present, but the inputs will be hiddenconst filteredOAIClient = wrapOpenAI(new OpenAI(), {    client: langsmithClient,});await filteredOAIClient.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: [      { role: ""system"", content: ""You are a helpful assistant."" },      { role: ""user"", content: ""Hello!"" },    ],});const openaiClient = wrapOpenAI(new OpenAI());// The trace produced will not have hidden inputs and outputsawait openaiClient.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: [      { role: ""system"", content: ""You are a helpful assistant."" },      { role: ""user"", content: ""Hello!"" },    ],});"
https://docs.smith.langchain.com/how_to_guides/tracing/mask_inputs_outputs,Rule-based masking of inputs and outputs,"infoThis feature is available in the following LangSmith SDK versions:Python: 0.1.81 and aboveTypeScript: 0.1.33 and above To mask specific data in inputs and outputs, you can use the create_anonymizer / createAnonymizer function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value. The anonymizer will be skipped for inputs if LANGCHAIN_HIDE_INPUTS = true. Same applies for outputs if LANGCHAIN_HIDE_OUTPUTS = true. However, if inputs or outputs are to be sent to client, the anonymizer method will take precedence over functions found in hide_inputs and hide_outputs. By default, the create_anonymizer will only look at maximum of 10 nesting levels deep, which can be configured via the max_depth parameter. PythonTypeScriptfrom langsmith.anonymizer import create_anonymizerfrom langsmith import Client, traceable# create anonymizer from list of regex patterns and replacement valuesanonymizer = create_anonymizer([    { ""pattern"": r""[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}"", ""replace"": ""<email>"" },    { ""pattern"": r""[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}"", ""replace"": ""<uuid>"" }])# or create anonymizer from a functionanonymizer = create_anonymizer(lambda text: r""..."".sub(""[value]"", text))client = Client(anonymizer=anonymizer)@traceable(client=client)def main(inputs: dict) -> dict:    ...import { createAnonymizer } from ""langsmith/anonymizer""import { traceable } from ""langsmith/traceable""import { Client } from ""langsmith""// create anonymizer from list of regex patterns and replacement valuesconst anonymizer = createAnonymizer([    { pattern: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g, replace: ""<email>"" },    { pattern: /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g, replace: ""<uuid>"" }])// or create anonymizer from a functionconst anonymizer = createAnonymizer((value) => value.replace(""..."", ""<value>""))const client = new Client({ anonymizer })const main = traceable(async (inputs: any) => {    // ...}, { client }) Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing. noteImproving the performance of anonymizer API is on our roadmap! If you are encountering performance issues, please contact us at support@langchain.dev. Older versions of LangSmith SDKs can use the hide_inputs and hide_outputs parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well. PythonTypeScriptimport refrom langsmith import Client, traceable# Define the regex patterns for email addresses and UUIDsEMAIL_REGEX = r""[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}""UUID_REGEX = r""[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}""def replace_sensitive_data(data, depth=10):    if depth == 0:        return data    if isinstance(data, dict):        return {k: replace_sensitive_data(v, depth-1) for k, v in data.items()}    elif isinstance(data, list):        return [replace_sensitive_data(item, depth-1) for item in data]    elif isinstance(data, str):        data = re.sub(EMAIL_REGEX, ""<email-address>"", data)        data = re.sub(UUID_REGEX, ""<UUID>"", data)        return data    else:        return dataclient = Client(    hide_inputs=lambda inputs: replace_sensitive_data(inputs),    hide_outputs=lambda outputs: replace_sensitive_data(outputs))inputs = {""role"": ""user"", ""content"": ""Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.""}outputs = {""role"": ""assistant"", ""content"": ""Hi! I've noted your email as user@example.com and your ID as 123e4567-e89b-12d3-a456-426614174000.""}@traceable(client=client)def child(inputs: dict) -> dict:    return outputs@traceable(client=client)def parent(inputs: dict) -> dict:    child_outputs = child(inputs)    return child_outputsparent(inputs)import { Client } from ""langsmith"";import { traceable } from ""langsmith/traceable"";// Define the regex patterns for email addresses and UUIDsconst EMAIL_REGEX = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g;const UUID_REGEX = /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g;function replaceSensitiveData(data: any, depth: number = 10): any {    if (depth === 0) return data;    if (typeof data === ""object"" && !Array.isArray(data)) {        const result: Record<string, any> = {};        for (const [key, value] of Object.entries(data)) {            result[key] = replaceSensitiveData(value, depth - 1);        }        return result;    } else if (Array.isArray(data)) {        return data.map(item => replaceSensitiveData(item, depth - 1));    } else if (typeof data === ""string"") {        return data.replace(EMAIL_REGEX, ""<email-address>"").replace(UUID_REGEX, ""<UUID>"");    } else {        return data;    }}const langsmithClient = new Client({    hideInputs: (inputs) => replaceSensitiveData(inputs),    hideOutputs: (outputs) => replaceSensitiveData(outputs)});const inputs = {    role: ""user"",    content: ""Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.""};const outputs = {    role: ""assistant"",    content: ""Hi! I've noted your email as <email-address> and your ID as <UUID>.""};const child = traceable(async (inputs: any) => {    return outputs;}, { name: ""child"", client: langsmithClient });const parent = traceable(async (inputs: any) => {    const childOutputs = await child(inputs);    return childOutputs;}, { name: ""parent"", client: langsmithClient });await parent(inputs)"
https://docs.smith.langchain.com/how_to_guides/tracing/mask_inputs_outputs,Processing Inputs & Outputs for a Single Function,"infoThe process_outputs parameter is available in LangSmith SDK version 0.1.98 and above for Python. In addition to client-level input and output processing, LangSmith provides function-level processing through the process_inputs and process_outputs parameters of the @traceable decorator. These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function. Here's an example of how to use process_inputs and process_outputs: from langsmith import traceabledef process_inputs(inputs: dict) -> dict:    # inputs is a dictionary where keys are argument names and values are the provided arguments    # Return a new dictionary with processed inputs    return {        ""processed_key"": inputs.get(""my_cool_key"", ""default""),        ""length"": len(inputs.get(""my_cool_key"", """"))    }def process_outputs(output: Any) -> dict:    # output is the direct return value of the function    # Transform the output into a dictionary    # In this case, ""output"" will be an integer    return {""processed_output"": str(output)}@traceable(process_inputs=process_inputs, process_outputs=process_outputs)def my_function(my_cool_key: str) -> int:    # Function implementation    return len(my_cool_key)result = my_function(""example"") In this example, process_inputs creates a new dictionary with processed input data, and process_outputs transforms the output into a specific format before logging to LangSmith. cautionIt's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data. For asynchronous functions, the usage is similar: @traceable(process_inputs=process_inputs, process_outputs=process_outputs)async def async_function(key: str) -> int:    # Async implementation    return len(key) These function-level processors take precedence over client-level processors (hide_inputs and hide_outputs) when both are defined."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#upload-a-csv-file-to-create-a-dataset,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing,Implement distributed tracing,"Sometimes, you need to trace a request across multiple services. LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (langsmith-trace and optional baggage for metadata/tags). Example client-server setup: Trace starts on clientContinues on server"
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing,Distributed tracing in Python,"# client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasync def my_client_function():    headers = {}    async with httpx.AsyncClient(base_url=""..."") as client:        if run_tree := get_current_run_tree():            # add langsmith-id to headers            headers.update(run_tree.to_headers())        return await client.post(""/my-route"", headers=headers) Then the server (or other service) can continue the trace by passing the headers in as langsmith_extra: # server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasync def my_application():    ...app = FastAPI()  # Or Flask, Django, or any other framework@app.post(""/my-route"")async def fake_route(request: Request):    # request.headers:  {""langsmith-trace"": ""...""}    # as well as optional metadata/tags in `baggage`    with tracing_context(parent=request.headers):        return await my_application() The example above uses the tracing_context context manager. You can also directly specify the parent run context in the langsmith_extra parameter of a method wrapped with @traceable. from langsmith.run_helpers import traceable, trace# ... same as above@app.post(""/my-route"")async def fake_route(request: Request):    # request.headers:  {""langsmith-trace"": ""...""}    my_application(langsmith_extra={""parent"": request.headers})"
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing,Distributed tracing in TypeScript,"noteDistributed tracing in TypeScript requires langsmith version >=0.1.31 First, we obtain the current run tree from the client and convert it to langsmith-trace and baggage header values, which we can pass to the server: // client.mtsimport { getCurrentRunTree, traceable } from ""langsmith/traceable"";const client = traceable(  async () => {    const runTree = getCurrentRunTree();    return await fetch(""..."", {      method: ""POST"",      headers: runTree.toHeaders(),    }).then((a) => a.text());  },  { name: ""client"" });await client(); Then, the server converts the headers back to a run tree, which it uses to further continue the tracing. To pass the newly created run tree to a traceable function, we can use the withRunTree helper, which will ensure the run tree is propagated within traceable invocations. Express.JSHono// server.mtsimport { RunTree } from ""langsmith"";import { traceable, withRunTree } from ""langsmith/traceable"";import express from ""express"";import bodyParser from ""body-parser"";const server = traceable(  (text: string) => `Hello from the server! Received ""${text}""`,  { name: ""server"" });const app = express();app.use(bodyParser.text());app.post(""/"", async (req, res) => {  const runTree = RunTree.fromHeaders(req.headers);  const result = await withRunTree(runTree, () => server(req.body));  res.send(result);});// server.mtsimport { RunTree } from ""langsmith"";import { traceable, withRunTree } from ""langsmith/traceable"";import { Hono } from ""hono"";const server = traceable(  (text: string) => `Hello from the server! Received ""${text}""`,  { name: ""server"" });const app = new Hono();app.post(""/"", async (c) => {  const body = await c.req.text();  const runTree = RunTree.fromHeaders(c.req.raw.headers);  const result = await withRunTree(runTree, () => server(body));  return c.body(result);});"
https://docs.smith.langchain.com/how_to_guides/prompts,How-to guides: Prompts,"This section contains how-to guides related to prompt management in LangSmith.  Create a promptNavigate to the Prompts section in the left-hand sidebar or from the application homepage. Update a promptNavigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit. Manage prompts programmaticallyYou can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. Open a prompt from a traceIf you pull a prompt into your code and begin logging traces that use it, you can find a link to the prompt in the Trace UI. LangChain HubNavigate to the LangChain Hub section of the left-hand sidebar."
https://docs.smith.langchain.com/how_to_guides/prompts,Create a prompt,Navigate to the Prompts section in the left-hand sidebar or from the application homepage.
https://docs.smith.langchain.com/how_to_guides/prompts,Update a prompt,Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit.
https://docs.smith.langchain.com/how_to_guides/prompts,Manage prompts programmatically,You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.
https://docs.smith.langchain.com/how_to_guides/prompts,Open a prompt from a trace,"If you pull a prompt into your code and begin logging traces that use it, you can find a link to the prompt in the Trace UI."
https://docs.smith.langchain.com/how_to_guides/prompts,LangChain Hub,Navigate to the LangChain Hub section of the left-hand sidebar.
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#drill-down-into-specific-subsets,Use monitoring charts,LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#drill-down-into-specific-subsets,Change the time period,"You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#drill-down-into-specific-subsets,Slice data by metadata or tag,"By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.
This can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.
After that, you can click the Tag or Metadata tab at the top to group runs accordingly."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#drill-down-into-specific-subsets,Drill down into specific subsets,"Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control#create-a-role,Set up access control,"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Read more about roles under admin concepts Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces LangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it.
Only users with the workspace:manage permission can manage access control settings for a workspace."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control#create-a-role,Create a role,"By default, LangSmith comes with a set of system roles: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) If these do not fit your access model, Organization Admins can create custom roles to suit your needs. To create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization. Click on the Create Role button to create a new role. You should see a form like the one below: Assign permissions for the different LangSmith resources that you want to control access to."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control#create-a-role,Assign a role to a user,"Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page Each user will have a Role dropdown that you can use to assign a role to them. You can also invite new users with a given role."
https://docs.smith.langchain.com/reference/data_formats/run_data_format,Run (span) data format,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and runs LangSmith stores and processes trace data in a simple format that is easy to export and import. Many of these fields are optional or not important to know about but are included for completeness.
The bolded fields are the most important ones to know about. Field NameTypeDescriptionidUUIDUnique identifier for the span.namestringThe name associated with the run.inputsobjectA map or set of inputs provided to the run.run_typestringType of run, e.g., ""llm"", ""chain"", ""tool"".start_timedatetimeStart time of the run.end_timedatetimeEnd time of the run.extraobjectAny extra information run.errorstringError message if the run encountered an error.outputsobjectA map or set of outputs generated by the run.eventsarray of objectsA list of event objects associated with the run. This is relevant for runs executed with streaming.tagsarray of stringsTags or labels associated with the run.trace_idUUIDUnique identifier for the trace the run is a part of. This is also the id field of the root run of the tracedotted_orderstringCustom ordering string, hierarchical. Built with timestamp and unique identifiers.statusstringCurrent status of the run execution, e.g., ""error"", ""pending"", ""success""child_run_idsarray of UUIDsList of IDs for all child runs.direct_child_run_idsarray of UUIDsList of IDs for direct children of this run.parent_run_idsarray of UUIDsList of IDs for all parent runs.feedback_statsobjectAggregations of feedback statistics for this runreference_example_idUUIDID of a reference example associated with the run. This is usually only present for evaluation runs.total_tokensintegerTotal number of tokens processed by the run.prompt_tokensintegerNumber of tokens in the prompt of the run.completion_tokensintegerNumber of tokens in the completion of the run.total_coststringTotal cost associated with processing the run.prompt_coststringCost associated with the prompt part of the run.completion_coststringCost associated with the completion of the run.first_token_timedatetimeTime when the first token was generated.session_idstringSession identifier for the run.in_datasetbooleanIndicates whether the run is included in a dataset.parent_run_idUUIDUnique identifier of the parent run.execution_order (deprecated)integerThe order in which this run was executed within the trace.serializedobjectSerialized state of the object executing the run if applicable.manifest_id (deprecated)UUIDIdentifier for a manifest associated with the span.manifest_s3_idUUIDS3 identifier for the manifest.inputs_s3_urlsobjectS3 URLs for the inputs.outputs_s3_urlsobjectS3 URLs for the outputs.price_model_idUUIDIdentifier for the pricing model applied to the run.app_pathstringApplication (UI) path for this run.last_queued_atdatetimeLast time the span was queued.share_tokenstringToken for sharing access to the run's data. Here is an example of a JSON representation of a run in the above format: {  ""id"": ""497f6eca-6276-4993-bfeb-53cbbbba6f08"",  ""name"": ""string"",  ""inputs"": {},  ""run_type"": ""llm"",  ""start_time"": ""2024-04-29T00:49:12.090000"",  ""end_time"": ""2024-04-29T00:49:12.459000"",  ""extra"": {},  ""error"": ""string"",  ""execution_order"": 1,  ""serialized"": {},  ""outputs"": {},  ""parent_run_id"": ""f8faf8c1-9778-49a4-9004-628cdb0047e5"",  ""manifest_id"": ""82825e8e-31fc-47d5-83ce-cd926068341e"",  ""manifest_s3_id"": ""0454f93b-7eb6-4b9d-a203-f1261e686840"",  ""events"": [{}],  ""tags"": [""foo""],  ""inputs_s3_urls"": {},  ""outputs_s3_urls"": {},  ""trace_id"": ""df570c03-5a03-4cea-8df0-c162d05127ac"",  ""dotted_order"": ""20240429T004912090000Z497f6eca-6276-4993-bfeb-53cbbbba6f08"",  ""status"": ""string"",  ""child_run_ids"": [""497f6eca-6276-4993-bfeb-53cbbbba6f08""],  ""direct_child_run_ids"": [""497f6eca-6276-4993-bfeb-53cbbbba6f08""],  ""parent_run_ids"": [""f8faf8c1-9778-49a4-9004-628cdb0047e5""],  ""feedback_stats"": {    ""correctness"": {      ""n"": 1,      ""avg"": 1.0    }  },  ""reference_example_id"": ""9fb06aaa-105f-4c87-845f-47d62ffd7ee6"",  ""total_tokens"": 0,  ""prompt_tokens"": 0,  ""completion_tokens"": 0,  ""total_cost"": ""string"",  ""prompt_cost"": ""string"",  ""completion_cost"": ""string"",  ""price_model_id"": ""0b5d9575-bec3-4256-b43a-05893b8b8440"",  ""first_token_time"": null,  ""session_id"": ""1ffd059c-17ea-40a8-8aef-70fd0307db82"",  ""app_path"": ""string"",  ""last_queued_at"": null,  ""in_dataset"": true,  ""share_token"": ""d0430ac3-04a1-4e32-a7ea-57776ad22c1c""}"
https://docs.smith.langchain.com/tutorials/Developers/optimize_classifier,Optimize a classifier,"This tutorial walks through optimizing a classifier based on user a feedback.
Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback.
That is exactly what we will do in this example."
https://docs.smith.langchain.com/tutorials/Developers/optimize_classifier,The objective,"In this example, we will build a bot that classify GitHub issues based on their title.
It will take in a title and classify it into one of many different classes.
Then, we will start to collect user feedback and use that to shape how this classifier performs."
https://docs.smith.langchain.com/tutorials/Developers/optimize_classifier,Getting started,"To get started, we will first set it up so that we send all traces to a specific project.
We can do this by setting an environment variable: import osos.environ[""LANGCHAIN_PROJECT""] = ""classifier"" We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it. import openaifrom langsmith import traceable, Clientimport uuidclient = openai.Client()available_topics = [    ""bug"",    ""improvement"",    ""new_feature"",    ""documentation"",    ""integration"",]prompt_template = """"""Classify the type of the issue as one of {topics}.Issue: {text}""""""@traceable(    run_type=""chain"",    name=""Classifier"",)def topic_classifier(    topic: str):    return client.chat.completions.create(        model=""gpt-3.5-turbo"",        temperature=0,        messages=[            {                ""role"": ""user"",                ""content"": prompt_template.format(                    topics=','.join(available_topics),                    text=topic,                )            }        ],    ).choices[0].message.content We can then start to interact with it.
When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function.
We do this so we can attach feedback later on. Here's how we can invoke the application: run_id = uuid.uuid4()topic_classifier(    ""fix bug in LCEL"",    langsmith_extra={""run_id"": run_id}) Here's how we can attach feedback after.
We can collect feedback in two forms. First, we can collect ""positive"" feedback - this is for examples that the model got right. ls_client = Client()run_id = uuid.uuid4()topic_classifier(    ""fix bug in LCEL"",    langsmith_extra={""run_id"": run_id})ls_client.create_feedback(    run_id,    key=""user-score"",    score=1.0,) Next, we can focus on collecting feedback that corresponds to a ""correction"" to the generation.
In this example the model will classify it as a bug, whereas I really want this to be classified as documentation. ls_client = Client()run_id = uuid.uuid4()topic_classifier(    ""fix bug in documentation"",    langsmith_extra={""run_id"": run_id})ls_client.create_feedback(    run_id,    key=""correction"",    correction=""documentation"")"
https://docs.smith.langchain.com/tutorials/Developers/optimize_classifier,Set up automations,"We can now set up automations to move examples with feedback of some form into a dataset.
We will set up two automations, one for positive feedback and the other for negative feedback. The first will take all runs with positive feedback and automatically add them to a dataset.
The logic behind this is that any run with positive feedback we can use as a good example in future iterations.
Let's create a dataset called classifier-github-issues to add this data to. The second will take all runs with a correction and use a webhook to add them to a dataset.
When creating this webhook, we will select the option to ""Use Corrections"".
This option will make it so that when creating a dataset from a run, rather than using the output of the run
as the gold-truth output of the datapoint, it will use the correction."
https://docs.smith.langchain.com/tutorials/Developers/optimize_classifier,Update the application,"We can now update our code to pull down the dataset we are sending runs to.
Once we pull it down, we can create a string with the examples in it.
We can then put this string as part of the prompt! ### NEW CODE #### Initialize the LangSmith Client so we can use to get the datasetls_client = Client()# Create a function that will take in a list of examples and format them into a stringdef create_example_string(examples):    final_strings = []    for e in examples:        final_strings.append(f""Input: {e.inputs['topic']}\n> {e.outputs['output']}"")    return ""\n\n"".join(final_strings)### NEW CODE ###client = openai.Client()available_topics = [    ""bug"",    ""improvement"",    ""new_feature"",    ""documentation"",    ""integration"",]prompt_template = """"""Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>""""""@traceable(    run_type=""chain"",    name=""Classifier"",)def topic_classifier(    topic: str):    # We can now pull down the examples from the dataset    # We do this inside the function so it always get the most up-to-date examples,    # But this can be done outside and cached for speed if desired    examples = list(ls_client.list_examples(dataset_name=""classifier-github-issues""))  # <- New Code    example_string = create_example_string(examples)    return client.chat.completions.create(        model=""gpt-3.5-turbo"",        temperature=0,        messages=[            {                ""role"": ""user"",                ""content"": prompt_template.format(                    topics=','.join(available_topics),                    text=topic,                    examples=example_string,                )            }        ],    ).choices[0].message.content If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as documentation ls_client = Client()run_id = uuid.uuid4()topic_classifier(    ""address bug in documentation"",    langsmith_extra={""run_id"": run_id})"
https://docs.smith.langchain.com/tutorials/Developers/optimize_classifier,Semantic search over examples,"One additional thing we can do is only use the most semantically similar examples.
This is useful when you start to build up a lot of examples. In order to do this, we can first define an example to find the k most similar examples: import numpy as npdef find_similar(examples, topic, k=5):    inputs = [e.inputs['topic'] for e in examples] + [topic]    embedds = client.embeddings.create(input=inputs, model=""text-embedding-3-small"")    embedds = [e.embedding for e in embedds.data]    embedds = np.array(embedds)    args = np.argsort(-embedds.dot(embedds[-1])[:-1])[:5]    examples = [examples[i] for i in args]    return examples We can then use that in the application ls_client = Client()def create_example_string(examples):    final_strings = []    for e in examples:        final_strings.append(f""Input: {e.inputs['topic']}\n> {e.outputs['output']}"")    return ""\n\n"".join(final_strings)client = openai.Client()available_topics = [    ""bug"",    ""improvement"",    ""new_feature"",    ""documentation"",    ""integration"",]prompt_template = """"""Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>""""""@traceable(    run_type=""chain"",    name=""Classifier"",)def topic_classifier(    topic: str):    examples = list(ls_client.list_examples(dataset_name=""classifier-github-issues""))    examples = find_similar(examples, topic)    example_string = create_example_string(examples)    return client.chat.completions.create(        model=""gpt-3.5-turbo"",        temperature=0,        messages=[            {                ""role"": ""user"",                ""content"": prompt_template.format(                    topics=','.join(available_topics),                    text=topic,                    examples=example_string,                )            }        ],    ).choices[0].message.content"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-traces,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#configure-online-evaluations,Set up online evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Set up automation rules Online evaluations is a powerful LangSmith feature that allows you to run an LLM-as-a-judge evaluator on a set of your production traces. They are implemented as a possible action in an automation rule. Currently, we provide support for specifying a prompt template, a model, and a set of criteria to evaluate the runs on. After entering rules setup, you can select Online Evaluation from the list of possible actions:"
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#configure-online-evaluations,Configure online evaluations,"When selection Online Evaluation as an action in an automation, you are presented with a panel from which you can configure online evaluation."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#configure-online-evaluations,Model,"You can choose any model available in the dropdown. Currently, we support OpenAI, AzureOpenAI, and models hosted on Fireworks.
In order to set the API keys to use for these invocations, click on the Secrets & API Keys button and add the necessary keys."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#configure-online-evaluations,Prompt template,"For the prompt template, you can select a suggested evaluator prompt, create a new prompt, or choose a prompt from the LangChain Hub. Suggested evaluator prompts We provide a list of pre-existing prompts that cater to common evaluator use cases. Create your own prompt We provide a base template from which you can form your own prompt. Pull a prompt from the LangChain Hub You can pull any structured prompt, private or public. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses.
If the prompt is your own, you can edit it in the playground and commit the version.
If the prompt is someone else's, you can fork the prompt, make your edits in the playground, and then pull in your new prompt to the online evaluator. When you choose a hub prompt for your online evaluator, the prompt will be locked to the commit version it was at rule creation. If you want to update the prompt, you can go to edit the online evaluator and re-select the prompt in the dropdown."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#configure-online-evaluations,Mapping variables,"Prompts can be crafted with any variable name you choose. To map what is passed into your evaluator prompt from your runs or experiments, use the variable mapping inputs. There's a dropdown with suggestions provided based on the schema of your recent runs."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#configure-online-evaluations,Preview,"Previewing the prompt will show you an example of what the formatted prompt will look like. This preview pulls the input and output of the most recent run. In the case of a dataset evaluator, the preview will also pull reference output from an example in your dataset. noteYou can configure an evaluation prompt that doesn't match the schema of your recent runs, but the dropdown suggestions and preview function won't work as expected."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#configure-online-evaluations,Output schema,"An evaluator will attach metadata tags to a run. These tags will have a name and a value. You can configure these in the Schema section.
The names and the descriptions of the fields will be passed into the prompt. Behind the scenes, we use tool calling to coerce the output of the LLM into the score you specify. noteThe name of the schema cannot have spaces since it is used as the name of a tool."
https://docs.smith.langchain.com/tutorials/Developers/agents,Evaluate an agent,"In this tutorial, we will walk through 3 evaluation strategies LLM agents, building on the conceptual points shared in our evaluation guide. Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. First, we will build an agent using LangGraph."
https://docs.smith.langchain.com/tutorials/Developers/agents,Set up environment,"We'll set up our environment variables for OpenAI and  %pip install --upgrade --quiet langchain langsmith langchain-community langchain-experimental langgraph import getpassimport osdef _set_env(var: str):    if not os.environ.get(var):        os.environ[var] = getpass.getpass(f""{var}: "")_set_env(""OPENAI_API_KEY"")os.environ[""LANGCHAIN_TRACING_V2""] = ""true""os.environ[""LANGCHAIN_ENDPOINT""] = ""https://api.smith.langchain.com"" # Update appropriately for self-hosted installations or the EU region_set_env(""LANGCHAIN_API_KEY"")"
https://docs.smith.langchain.com/tutorials/Developers/agents,Configure the database,"We will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the chinook database, which is a sample database that represents a digital media store.
Find more information about the database here. For convenience, we have hosted the database (Chinook.db) on a public GCS bucket. import requestsurl = ""https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db""response = requests.get(url)if response.status_code == 200:    # Open a local file in binary write mode    with open(""Chinook.db"", ""wb"") as file:        # Write the content of the response (the file) to the local file        file.write(response.content)    print(""File downloaded and saved as Chinook.db"")else:    print(f""Failed to download the file. Status code: {response.status_code}"") We will use a handy SQL database wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results. We will also use the langchain_openai package to interact with the OpenAI API for language models later in the tutorial. from langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(""sqlite:///Chinook.db"")print(db.dialect)print(db.get_usable_table_names())db.run(""SELECT * FROM Artist LIMIT 10;"")"
https://docs.smith.langchain.com/tutorials/Developers/agents,SQL Agent,We'll use a LangGraph agent with access to a set of tools for working with SQL:
https://docs.smith.langchain.com/tutorials/Developers/agents,LLM,"from langchain_openai import ChatOpenAIllm=ChatOpenAI(model=""gpt-4o"",temperature=0)experiment_prefix=""sql-agent-gpt4o""metadata = ""Chinook, gpt-4o base-case-agent"""
https://docs.smith.langchain.com/tutorials/Developers/agents,Tools,"We'll use SQL toolkit as well as some custom tools to check the query before executing it and check the query result from the database to confirm it is not empty or irrelevant to the question. import jsonfrom langchain_community.agent_toolkits import SQLDatabaseToolkitfrom langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.graph import END, MessageGraphfrom langgraph.prebuilt.tool_node import ToolNodefrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom langchain.agents import tool# SQL toolkittoolkit = SQLDatabaseToolkit(db=db, llm=llm)tools = toolkit.get_tools()# Query checkingquery_check_system = """"""You are a SQL expert with a strong attention to detail.Double check the SQLite query for common mistakes, including:- Using NOT IN with NULL values- Using UNION when UNION ALL should have been used- Using BETWEEN for exclusive ranges- Data type mismatch in predicates- Properly quoting identifiers- Using the correct number of arguments for functions- Casting to the correct data type- Using the proper columns for joinsIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.Execute the correct query with the appropriate tool.""""""query_check_prompt = ChatPromptTemplate.from_messages([(""system"", query_check_system),(""user"", ""{query}"")])query_check = query_check_prompt | llm@tooldef check_query_tool(query: str) -> str:    """"""    Use this tool to double check if your query is correct before executing it.    """"""    return query_check.invoke({""query"": query}).content# Query result checkingquery_result_check_system = """"""You are grading the result of a SQL query from a DB.- Check that the result is not empty.- If it is empty, instruct the system to re-try!""""""query_result_check_prompt = ChatPromptTemplate.from_messages([(""system"", query_result_check_system),(""user"", ""{query_result}"")])query_result_check = query_result_check_prompt | llm@tooldef check_result(query_result: str) -> str:    """"""    Use this tool to check the query result from the database to confirm it is not empty and is relevant.    """"""    return query_result_check.invoke({""query_result"": query_result}).contenttools.append(check_query_tool)tools.append(check_result)"
https://docs.smith.langchain.com/tutorials/Developers/agents,State,"from typing import Annotatedfrom typing_extensions import TypedDictfrom langgraph.graph.message import AnyMessage, add_messagesclass State(TypedDict):    messages: Annotated[list[AnyMessage], add_messages]"
https://docs.smith.langchain.com/tutorials/Developers/agents,SQL Assistant,"Use prompt based roughtly on what is shown here. from langchain_core.runnables import Runnable, RunnableConfig# Assistantclass Assistant:    def __init__(self, runnable: Runnable):        self.runnable = runnable    def __call__(self, state: State, config: RunnableConfig):        while True:            # Append to state            state = {**state}            # Invoke the tool-calling LLM            result = self.runnable.invoke(state)            # If it is a tool call -> response is valid            # If it has meaninful text -> response is valid            # Otherwise, we re-prompt it b/c response is not meaninful            if not result.tool_calls and (                not result.content                or isinstance(result.content, list)                and not result.content[0].get(""text"")            ):                messages = state[""messages""] + [(""user"", ""Respond with a real output."")]                state = {**state, ""messages"": messages}            else:                break        return {""messages"": result}# Assistant runnablequery_gen_system = """"""ROLE:You are an agent designed to interact with a SQL database. You have access to tools for interacting with the database.GOAL:Given an input question, create a syntactically correct SQLite query to run, then look at the results of the query and return the answer.INSTRUCTIONS:- Only use the below tools for the following operations.- Only use the information returned by the below tools to construct your final answer.- To start you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step.- Then you should query the schema of the most relevant tables.- Write your query based upon the schema of the tables. You MUST double check your query before executing it.- Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.- You can order the results by a relevant column to return the most interesting examples in the database.- Never query for all the columns from a specific table, only ask for the relevant columns given the question.- If you get an error while executing a query, rewrite the query and try again.- If the query returns a result, use check_result tool to check the query result.- If the query result result is empty, think about the table schema, rewrite the query, and try again.- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.""""""query_gen_prompt = ChatPromptTemplate.from_messages([(""system"", query_gen_system),(""placeholder"", ""{messages}"")])assistant_runnable = query_gen_prompt | llm.bind_tools(tools)"
https://docs.smith.langchain.com/tutorials/Developers/agents,Graph Utilities,"We will define a few utility functions to help us with the agent implementation. Specifically, we will wrap a ToolNode with a fallback to handle errors and surface them to the agent. from langchain_core.messages import ToolMessagefrom langchain_core.runnables import RunnableLambdadef create_tool_node_with_fallback(tools: list) -> dict:    return ToolNode(tools).with_fallbacks(        [RunnableLambda(handle_tool_error)], exception_key=""error""    )def _print_event(event: dict, _printed: set, max_length=1500):    current_state = event.get(""dialog_state"")    if current_state:        print(f""Currently in: "", current_state[-1])    message = event.get(""messages"")    if message:        if isinstance(message, list):            message = message[-1]        if message.id not in _printed:            msg_repr = message.pretty_repr(html=True)            if len(msg_repr) > max_length:                msg_repr = msg_repr[:max_length] + "" ... (truncated)""            print(msg_repr)            _printed.add(message.id)def handle_tool_error(state) -> dict:    error = state.get(""error"")    tool_calls = state[""messages""][-1].tool_calls    return {        ""messages"": [            ToolMessage(                content=f""Error: {repr(error)}\n please fix your mistakes."",                tool_call_id=tc[""id""],            )            for tc in tool_calls        ]    }"
https://docs.smith.langchain.com/tutorials/Developers/agents,Graph,"We will then define the workflow for the agent. from langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.graph import END, StateGraphfrom langgraph.prebuilt import ToolNode, tools_condition# Graphbuilder = StateGraph(State)# Define nodes: these do the workbuilder.add_node(""assistant"", Assistant(assistant_runnable))builder.add_node(""tools"", create_tool_node_with_fallback(tools))# Define edges: these determine how the control flow movesbuilder.set_entry_point(""assistant"")builder.add_conditional_edges(    ""assistant"",    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END    tools_condition,    # ""tools"" calls one of our tools. END causes the graph to terminate (and respond to the user)    {""tools"": ""tools"", END: END},)builder.add_edge(""tools"", ""assistant"")# The checkpointer lets the graph persist its statememory = SqliteSaver.from_conn_string("":memory:"")graph = builder.compile(checkpointer=memory)"
https://docs.smith.langchain.com/tutorials/Developers/agents,Test,"questions = [""Which country's customers spent the most? And how much did they spend?"",             ""How many albums does the artist Led Zeppelin have?"",             ""What was the most purchased track of 2017?"",             ""Which sales agent made the most in sales in 2009?""] ## Invokeimport uuid_printed = set()thread_id = str(uuid.uuid4())config = {    ""configurable"": {        # Checkpoints are accessed by thread_id        ""thread_id"": thread_id,    }}msg = {""messages"": (""user"", questions[0])}messages = graph.invoke(msg,config)messages['messages'][-1].content## Stream_printed = set()thread_id = str(uuid.uuid4())config = {    ""configurable"": {        # Checkpoints are accessed by thread_id        ""thread_id"": thread_id,    }}events = graph.stream(    {""messages"": (""user"", questions[0])}, config, stream_mode=""values"")for event in events:    _print_event(event, _printed)"
https://docs.smith.langchain.com/tutorials/Developers/agents,Eval,"Agent evaluation can focus on at least 3 things: Response: The inputs are a prompt and an optional list of tools. The output is the final agent response.Single step: As before, the inputs are a prompt and an optional list of tools. The output the tool call.Trajectory: As before, the inputs are a prompt and an optional list of tools. The output is the list of tool calls tipSee our evaluation guide for more details on Agent evaluation."
https://docs.smith.langchain.com/tutorials/Developers/agents,Response evaluation,"We can evaluate how well an agent does overall on a task. This basically involves treating the agent as a black box and just evaluating whether it gets the job done or not. tipSee the full overview of agent response evaluation in our conceptual guide. Dataset First, create a dataset that evaluates end-to-end performance of the agent. We can take some questions related to the Chinook database from here. from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Which country's customers spent the most? And how much did they spend?"", ""The country whose customers spent the most is the USA, with a total expenditure of $523.06""),    (""What was the most purchased track of 2013?"", ""The most purchased track of 2013 was Hot Girl.""),    (""How many albums does the artist Led Zeppelin have?"",""Led Zeppelin has 14 albums""),    (""What is the total price for the album Big Ones?"",""The total price for the album 'Big Ones' is 14.85""),    (""Which sales agent made the most in sales in 2009?"", ""Steve Johnson made the most sales in 2009""),]dataset_name = ""SQL Agent Response""if not client.has_dataset(dataset_name=dataset_name):    dataset = client.create_dataset(dataset_name=dataset_name)    inputs, outputs = zip(        *[({""input"": text}, {""output"": label}) for text, label in examples]    )    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id) Run chain def predict_sql_agent_answer(example: dict):    """"""Use this for answer evaluation""""""    msg = {""messages"": (""user"", example[""input""])}    messages = graph.invoke(msg, config)    return {""response"": messages['messages'][-1].content} Evaluator This can follow what we do for RAG where we compare the generated answer with the reference answer. from langchain import hubfrom langchain_openai import ChatOpenAI# Grade promptgrade_prompt_answer_accuracy = prompt = hub.pull(""langchain-ai/rag-answer-vs-reference"")def answer_evaluator(run, example) -> dict:    """"""    A simple evaluator for RAG answer accuracy    """"""    # Get question, ground truth answer, RAG chain answer    input_question = example.inputs[""input""]    reference = example.outputs[""output""]    prediction = run.outputs[""response""]    # LLM grader    llm = ChatOpenAI(model=""gpt-4-turbo"", temperature=0)    # Structured prompt    answer_grader = grade_prompt_answer_accuracy | llm    # Run evaluator    score = answer_grader.invoke({""question"": input_question,                                  ""correct_answer"": reference,                                  ""student_answer"": prediction})    score = score[""Score""]    return {""key"": ""answer_v_reference_score"", ""score"": score} Create evaluation from langsmith.evaluation import evaluateexperiment_results = evaluate(    predict_sql_agent_answer,    data=dataset_name,    evaluators=[answer_evaluator],    experiment_prefix=experiment_prefix + ""-response-v-reference"",    num_repetitions=3,    metadata={""version"": metadata},)"
https://docs.smith.langchain.com/tutorials/Developers/agents,Single step evaluation,"Agents generally make multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate the individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. tipSee the full overview of single step evaluation in our conceptual guide. We can check a specific tool call using a custom evaluator: Here, we just invoke the assistant, assistant_runnable, with a prompt and check if the resulting tool call is as expected.Here, we are using a specialized agent where the tools are hard-coded (rather than passed with the dataset input).We specify the reference tool call for the step that we are evaluating, expected_tool_call. from langsmith.schemas import Example, Rundef predict_assistant(example: dict):    """"""Invoke assistant for single tool call evaluation""""""    msg = [ (""user"", example[""input""]) ]    result = assistant_runnable.invoke({""messages"":msg})    return {""response"": result}def check_specific_tool_call(root_run: Run, example: Example) -> dict:    """"""    Check if the first tool call in the response matches the expected tool call.    """"""    # Exepected tool call    expected_tool_call = 'sql_db_list_tables'    # Run    response = root_run.outputs[""response""]    # Get tool call    try:        tool_call = getattr(response, 'tool_calls', [])[0]['name']    except (IndexError, KeyError):        tool_call = None    score = 1 if tool_call == expected_tool_call else 0    return {""score"": score, ""key"": ""single_tool_call""}experiment_results = evaluate(    predict_assistant,    data=dataset_name,    evaluators=[check_specific_tool_call],    experiment_prefix=experiment_prefix + ""-single-tool"",    num_repetitions=3,    metadata={""version"": metadata},)"
https://docs.smith.langchain.com/tutorials/Developers/agents,Trajectory,"We can check a trajectory of tool calls using custom evaluators: Here, we just invoke the agent, graph.invoke, with a prompt.Here, we are using a specialized agent where the tools are hard-coded (rather than passed with the dataset input).We extract the list of tools called, using find_tool_calls.Custom functions can process these tool calls in various user-defined ways.We can check if all expected tools are called in any order: contains_all_tool_calls_any_orderWe can check if all expected tools are called in order, allowing for insertion of tool calls: contains_all_tool_calls_in_orderWe can check if all expected tools are called in the exact order: contains_all_tool_calls_in_order_exact_match tipSee the full overview of single step evaluation in our conceptual guide. def predict_sql_agent_messages(example: dict):    """"""Use this for answer evaluation""""""    msg = {""messages"": (""user"", example[""input""])}    messages = graph.invoke(msg, config)    return {""response"": messages}def find_tool_calls(messages):    """"""    Find all tool calls in the messages returned    """"""    tool_calls = [tc['name'] for m in messages['messages'] for tc in getattr(m, 'tool_calls', [])]    return tool_callsdef contains_all_tool_calls_any_order(root_run: Run, example: Example) -> dict:    """"""    Check if all expected tools are called in any order.    """"""    expected = ['sql_db_list_tables', 'sql_db_schema', 'sql_db_query_checker', 'sql_db_query', 'check_result']    messages = root_run.outputs[""response""]    tool_calls = find_tool_calls(messages)    # Optionally, log the tool calls -    #print(""Here are my tool calls:"")    #print(tool_calls)    if set(expected) <= set(tool_calls):        score = 1    else:        score = 0    return {""score"": int(score), ""key"": ""multi_tool_call_any_order""}def contains_all_tool_calls_in_order(root_run: Run, example: Example) -> dict:    """"""    Check if all expected tools are called in exact order.    """"""    messages = root_run.outputs[""response""]    tool_calls = find_tool_calls(messages)    # Optionally, log the tool calls -    #print(""Here are my tool calls:"")    #print(tool_calls)    it = iter(tool_calls)    expected = ['sql_db_list_tables', 'sql_db_schema', 'sql_db_query_checker', 'sql_db_query', 'check_result']    if all(elem in it for elem in expected):        score = 1    else:        score = 0    return {""score"": int(score), ""key"": ""multi_tool_call_in_order""}def contains_all_tool_calls_in_order_exact_match(root_run: Run, example: Example) -> dict:    """"""    Check if all expected tools are called in exact order and without any additional tool calls.    """"""    expected = ['sql_db_list_tables', 'sql_db_schema', 'sql_db_query_checker', 'sql_db_query', 'check_result']    messages = root_run.outputs[""response""]    tool_calls = find_tool_calls(messages)    # Optionally, log the tool calls -    #print(""Here are my tool calls:"")    #print(tool_calls)    if tool_calls == expected:        score = 1    else:        score = 0    return {""score"": int(score), ""key"": ""multi_tool_call_in_exact_order""}experiment_results = evaluate(    predict_sql_agent_messages,    data=dataset_name,    evaluators=[contains_all_tool_calls_any_order,contains_all_tool_calls_in_order,contains_all_tool_calls_in_order_exact_match],    experiment_prefix=experiment_prefix + ""-trajectory"",    num_repetitions=3,    metadata={""version"": metadata},) You can see the results from the evaluations logged to the dataset! https://smith.langchain.com/public/20808486-67c3-4e30-920b-6d49d6f2b6b8/d"
https://docs.smith.langchain.com/how_to_guides/datasets,How-to guides: Datasets,"This section contains how-to guides related to working with datasets.  Manage datasets in the applicationBefore diving into this content, it might be helpful to read the following: Manage datasets programmaticallyYou can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them. Version datasetsIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. Share or unshare a dataset publiclySharing a dataset publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information. Dynamic few shot example selectionThis feature is currently in closed beta. Please sign up here for access"
https://docs.smith.langchain.com/how_to_guides/datasets,Manage datasets in the application,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/datasets,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets,Version datasets,"In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."
https://docs.smith.langchain.com/how_to_guides/datasets,Share or unshare a dataset publicly,Sharing a dataset publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.
https://docs.smith.langchain.com/how_to_guides/datasets,Dynamic few shot example selection,This feature is currently in closed beta. Please sign up here for access
https://docs.smith.langchain.com/self_hosting,Self-Hosting LangSmith,"Step-by-step guides that cover the installation, configuration, and scaling of your Self-Hosted LangSmith instance. Architectural overview: A high-level overview of the LangSmith architecture.Storage services: The storage services used by LangSmith.Services: The services that make up LangSmith.Installation: How to install LangSmith on your own infrastructure.Kubernetes: Deploy LangSmith on Kubernetes.Docker: Deploy LangSmith using Docker.Configuration: How to configure your self-hosted instance of LangSmith.SSO with OAuth2.0 and OIDC: Configure LangSmith to use OAuth2.0 and OIDC for SSO.Connect to an external ClickHouse database: Configure LangSmith to use an external ClickHouse database.Connect to an external Postgres database: Configure LangSmith to use an external Postgres database.Connect to an external Redis instance: Configure LangSmith to use an external Redis instance.Usage: How to use your self-hosted instance of LangSmith.Upgrades: How to upgrade your self-hosted instance of LangSmith.Release notes: The latest release notes for LangSmith.Week of June 17, 2024 - LangSmith v0.6: Release notes for version 0.6 of LangSmith.Week of May 13, 2024 - LangSmith v0.5: Release notes for version 0.5 of LangSmith.Week of March 25, 2024 - LangSmith v0.4: Release notes for version 0.4 of LangSmith.Week of Februrary 21, 2024 - LangSmith v0.3: Release notes for version 0.3 of LangSmith.Week of January 29, 2024 - LangSmith v0.2: Release notes for version 0.2 of LangSmith.FAQ: Frequently asked questions about LangSmith."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-trace-context-manager-python-only,Annotate code for tracing,"There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-trace-context-manager-python-only,Use@traceable/traceable,"LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.
Note that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.
import { traceable } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: ""formatPrompt"" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-trace-context-manager-python-only,Wrap the OpenAI client,"The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!
The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceable(name=""Chat Pipeline"")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(""Can you summarize this morning's meetings?"")import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", run_type: ""tool"" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: ""system"",            content:                ""You are a helpful assistant. Please respond to the user's request only based on the given context."",        },        { role: ""user"", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: ""Chat Pipeline"" });await chatPipeline(""Can you summarize this morning's meetings?"");"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-trace-context-manager-python-only,Use theRunTreeAPI,"Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually
create runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not
necessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# Create a top-level runpipeline = RunTree(    name=""Chat Pipeline"",    run_type=""chain"",    inputs={""question"": question})# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}]# Create a child runchild_llm_run = pipeline.create_child(    name=""OpenAI Call"",    run_type=""llm"",    inputs={""messages"": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={""answer"": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from ""openai"";import { RunTree } from ""langsmith"";// This can be a user input to your appconst question = ""Can you summarize this morning's meetings?"";const pipeline = new RunTree({    name: ""Chat Pipeline"",    run_type: ""chain"",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = ""During this morning's meeting, we solved all world conflict."";const messages = [    { role: ""system"", content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { role: ""user"", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: ""OpenAI Call"",    run_type: ""llm"",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-trace-context-manager-python-only,Use thetracecontext manager (Python only),"In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {""input"": ""Can you summarize this morning's meetings?""}with trace(""Chat Pipeline"", ""chain"", project_name=""my_test"", inputs=app_inputs) as rt:    output = chat_pipeline(""Can you summarize this morning's meetings?"")    rt.end(outputs={""output"": output})"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-intermediate-runs-spans-on-properties-of-the-root,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/how_to_guides/tracing/mask_inputs_outputs#rule-based-masking-of-inputs-and-outputs,Prevent logging of sensitive data in traces,"In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend. If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application: LANGCHAIN_HIDE_INPUTS=trueLANGCHAIN_HIDE_OUTPUTS=true This works for both the LangSmith SDK (Python and TypeScript) and LangChain. You can also customize and override this behavior for a given Client instance. This can be done by setting the hide_inputs and hide_outputs parameters on the Client object (hideInputs and hideOutputs in TypeScript). For the example below, we will simply return an empty object for both hide_inputs and hide_outputs, but you can customize this to your needs. PythonTypeScriptimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(openai.Client())langsmith_client = Client(    hide_inputs=lambda inputs: {}, hide_outputs=lambda outputs: {})# The trace produced will have its metadata present, but the inputs will be hiddenopenai_client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=[        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},        {""role"": ""user"", ""content"": ""Hello!""},    ],    langsmith_extra={""client"": langsmith_client},)# The trace produced will not have hidden inputs and outputsopenai_client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=[        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},        {""role"": ""user"", ""content"": ""Hello!""},    ],)import OpenAI from ""openai"";import { Client } from ""langsmith"";import { wrapOpenAI } from ""langsmith/wrappers"";const langsmithClient = new Client({  hideInputs: (inputs) => ({}),  hideOutputs: (outputs) => ({}),});// The trace produced will have its metadata present, but the inputs will be hiddenconst filteredOAIClient = wrapOpenAI(new OpenAI(), {    client: langsmithClient,});await filteredOAIClient.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: [      { role: ""system"", content: ""You are a helpful assistant."" },      { role: ""user"", content: ""Hello!"" },    ],});const openaiClient = wrapOpenAI(new OpenAI());// The trace produced will not have hidden inputs and outputsawait openaiClient.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: [      { role: ""system"", content: ""You are a helpful assistant."" },      { role: ""user"", content: ""Hello!"" },    ],});"
https://docs.smith.langchain.com/how_to_guides/tracing/mask_inputs_outputs#rule-based-masking-of-inputs-and-outputs,Rule-based masking of inputs and outputs,"infoThis feature is available in the following LangSmith SDK versions:Python: 0.1.81 and aboveTypeScript: 0.1.33 and above To mask specific data in inputs and outputs, you can use the create_anonymizer / createAnonymizer function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value. The anonymizer will be skipped for inputs if LANGCHAIN_HIDE_INPUTS = true. Same applies for outputs if LANGCHAIN_HIDE_OUTPUTS = true. However, if inputs or outputs are to be sent to client, the anonymizer method will take precedence over functions found in hide_inputs and hide_outputs. By default, the create_anonymizer will only look at maximum of 10 nesting levels deep, which can be configured via the max_depth parameter. PythonTypeScriptfrom langsmith.anonymizer import create_anonymizerfrom langsmith import Client, traceable# create anonymizer from list of regex patterns and replacement valuesanonymizer = create_anonymizer([    { ""pattern"": r""[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}"", ""replace"": ""<email>"" },    { ""pattern"": r""[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}"", ""replace"": ""<uuid>"" }])# or create anonymizer from a functionanonymizer = create_anonymizer(lambda text: r""..."".sub(""[value]"", text))client = Client(anonymizer=anonymizer)@traceable(client=client)def main(inputs: dict) -> dict:    ...import { createAnonymizer } from ""langsmith/anonymizer""import { traceable } from ""langsmith/traceable""import { Client } from ""langsmith""// create anonymizer from list of regex patterns and replacement valuesconst anonymizer = createAnonymizer([    { pattern: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g, replace: ""<email>"" },    { pattern: /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g, replace: ""<uuid>"" }])// or create anonymizer from a functionconst anonymizer = createAnonymizer((value) => value.replace(""..."", ""<value>""))const client = new Client({ anonymizer })const main = traceable(async (inputs: any) => {    // ...}, { client }) Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing. noteImproving the performance of anonymizer API is on our roadmap! If you are encountering performance issues, please contact us at support@langchain.dev. Older versions of LangSmith SDKs can use the hide_inputs and hide_outputs parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well. PythonTypeScriptimport refrom langsmith import Client, traceable# Define the regex patterns for email addresses and UUIDsEMAIL_REGEX = r""[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}""UUID_REGEX = r""[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}""def replace_sensitive_data(data, depth=10):    if depth == 0:        return data    if isinstance(data, dict):        return {k: replace_sensitive_data(v, depth-1) for k, v in data.items()}    elif isinstance(data, list):        return [replace_sensitive_data(item, depth-1) for item in data]    elif isinstance(data, str):        data = re.sub(EMAIL_REGEX, ""<email-address>"", data)        data = re.sub(UUID_REGEX, ""<UUID>"", data)        return data    else:        return dataclient = Client(    hide_inputs=lambda inputs: replace_sensitive_data(inputs),    hide_outputs=lambda outputs: replace_sensitive_data(outputs))inputs = {""role"": ""user"", ""content"": ""Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.""}outputs = {""role"": ""assistant"", ""content"": ""Hi! I've noted your email as user@example.com and your ID as 123e4567-e89b-12d3-a456-426614174000.""}@traceable(client=client)def child(inputs: dict) -> dict:    return outputs@traceable(client=client)def parent(inputs: dict) -> dict:    child_outputs = child(inputs)    return child_outputsparent(inputs)import { Client } from ""langsmith"";import { traceable } from ""langsmith/traceable"";// Define the regex patterns for email addresses and UUIDsconst EMAIL_REGEX = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g;const UUID_REGEX = /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g;function replaceSensitiveData(data: any, depth: number = 10): any {    if (depth === 0) return data;    if (typeof data === ""object"" && !Array.isArray(data)) {        const result: Record<string, any> = {};        for (const [key, value] of Object.entries(data)) {            result[key] = replaceSensitiveData(value, depth - 1);        }        return result;    } else if (Array.isArray(data)) {        return data.map(item => replaceSensitiveData(item, depth - 1));    } else if (typeof data === ""string"") {        return data.replace(EMAIL_REGEX, ""<email-address>"").replace(UUID_REGEX, ""<UUID>"");    } else {        return data;    }}const langsmithClient = new Client({    hideInputs: (inputs) => replaceSensitiveData(inputs),    hideOutputs: (outputs) => replaceSensitiveData(outputs)});const inputs = {    role: ""user"",    content: ""Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.""};const outputs = {    role: ""assistant"",    content: ""Hi! I've noted your email as <email-address> and your ID as <UUID>.""};const child = traceable(async (inputs: any) => {    return outputs;}, { name: ""child"", client: langsmithClient });const parent = traceable(async (inputs: any) => {    const childOutputs = await child(inputs);    return childOutputs;}, { name: ""parent"", client: langsmithClient });await parent(inputs)"
https://docs.smith.langchain.com/how_to_guides/tracing/mask_inputs_outputs#rule-based-masking-of-inputs-and-outputs,Processing Inputs & Outputs for a Single Function,"infoThe process_outputs parameter is available in LangSmith SDK version 0.1.98 and above for Python. In addition to client-level input and output processing, LangSmith provides function-level processing through the process_inputs and process_outputs parameters of the @traceable decorator. These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function. Here's an example of how to use process_inputs and process_outputs: from langsmith import traceabledef process_inputs(inputs: dict) -> dict:    # inputs is a dictionary where keys are argument names and values are the provided arguments    # Return a new dictionary with processed inputs    return {        ""processed_key"": inputs.get(""my_cool_key"", ""default""),        ""length"": len(inputs.get(""my_cool_key"", """"))    }def process_outputs(output: Any) -> dict:    # output is the direct return value of the function    # Transform the output into a dictionary    # In this case, ""output"" will be an integer    return {""processed_output"": str(output)}@traceable(process_inputs=process_inputs, process_outputs=process_outputs)def my_function(my_cool_key: str) -> int:    # Function implementation    return len(my_cool_key)result = my_function(""example"") In this example, process_inputs creates a new dictionary with processed input data, and process_outputs transforms the output into a specific format before logging to LangSmith. cautionIt's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data. For asynchronous functions, the usage is similar: @traceable(process_inputs=process_inputs, process_outputs=process_outputs)async def async_function(key: str) -> int:    # Async implementation    return len(key) These function-level processors take precedence over client-level processors (hide_inputs and hide_outputs) when both are defined."
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators,Create few-shot evaluators,"Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary
overhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows
you to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators,Create your evaluator,"tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators
are only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,
we will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}
template variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name
as your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,
few_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend
to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators,Make corrections,"Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will
begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.
The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators,View your corrections dataset,"In order to view your corrections dataset, go to your rule and click ""Edit Rule"" (or ""Edit Evaluator"" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says ""View few-shot dataset"". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets#tag-a-version,Version datasets,"In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets#tag-a-version,Create a new version of a dataset,"Any time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved. By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the ""Examples"" tab, you can see the state of the dataset at that point in time. Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the ""latest"" version of the dataset. Also, by default the latest version of the dataset is shown in the ""Examples"" tab and experiments from all versions are shown in the ""Tests"" tab. In the ""Tests"" tab, you can see the results of tests run on the dataset at different versions."
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets#tag-a-version,Tag a version,"You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history. For example, you might tag a version of your dataset as ""prod"" and use it to run tests against your LLM pipeline. Tagging can be done in the UI by clicking on ""+ Tag this version"" in the ""Examples"" tab. You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK: from langsmith import Clientfromt datetime import datetimeclient = Client()initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like ""prod""client.update_dataset_tag(    dataset_name=toxic_dataset_name, as_of=initial_time, tag=""prod"") To run an evaluation on a particular tagged version of a dataset, you can follow this guide."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#filter-on-regressions-or-improvements,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,LangSmith Release Notes,noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,"Week of June 17, 2024 - LangSmith v0.6","LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,New Features,Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More...
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Performance and Reliability Changes,Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Infrastructure changes,"[Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Admin changes,Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Deprecation notices,With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,"Week of May 13, 2024 - LangSmith v0.5","LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Breaking changes,We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,New Features,Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Performance and Reliability Changes,"Split ingest, session deletion, and automation jobs to execute within separate resource pools."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Infrastructure changes,"As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Admin changes,Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Deprecation notices,With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,"Week of March 25, 2024 - LangSmith v0.4","LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Breaking changes,"This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Performance and Reliability Changes,"Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Infrastructure changes,"Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Admin changes,Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Deprecation notices,With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,"Week of Februrary 21, 2024 - LangSmith v0.3","LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Breaking changes,"This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Performance and Reliability Changes,"Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Admin changes,None
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Deprecation notices,With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,"Week of January 29, 2024 - LangSmith v0.2","LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Requirements,This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Breaking changes,"The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {""user_id"": ..., ""user_name"":...,}) and then search using has(metadata, '{""user_name"": ...}')"
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Performance and Reliability Changes,"Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Infrastructure Changes,"Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Admin changes,Increased the maximum number of users per organization from 5 to 100 for new organizations.
https://docs.smith.langchain.com/self_hosting/release_notes#breaking-changes,Deprecation notices,With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/how_to_guides/human_feedback,How-to guides: Human feedback,"This section contains how-to guides related to human feedback.  Capture user feedback from your application to tracesBefore diving into this content, it might be helpful to read the following: Set up feedback criteriaBefore diving into this content, it might be helpful to read the following: Annotate traces and runs inlineLangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue. Use annotation queuesAnnotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs."
https://docs.smith.langchain.com/how_to_guides/human_feedback,Capture user feedback from your application to traces,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/human_feedback,Set up feedback criteria,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/human_feedback,Annotate traces and runs inline,"LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue."
https://docs.smith.langchain.com/how_to_guides/human_feedback,Use annotation queues,"Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#select-feedback-key,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#review-runs-in-an-annotation-queue,Use annotation queues,"Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.
While you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#review-runs-in-an-annotation-queue,Create an annotation queue,"To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.
Then click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.
You can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as ""Done"" for it to be removed from the queue. If you check ""All workspace members review each run,"" then a run will remain in the queue until all workspace members have marked it ""Done"".Enable reservations on runs: We recommend enabling reservations.
This will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified ""reservation length"". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it ""Done"", the reservation will expire after the specified ""reservation length"".
The run is then released back into the queue and can be reserved by another reviewer. noteClicking ""Requeue at end"" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#review-runs-in-an-annotation-queue,Assign runs to an annotation queue,"To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.
Select multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.
Set up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.
To learn more about how to capture user feedback from your LLM application, follow this guide."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#review-runs-in-an-annotation-queue,Review runs in an annotation queue,"To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.
Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.
You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to ""View run"". The keyboard shortcuts shown can help streamline the review process."
https://docs.smith.langchain.com/concepts/tracing#feedback,Tracing,"This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"
https://docs.smith.langchain.com/concepts/tracing#feedback,Runs,"A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#feedback,Traces,"A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
"
https://docs.smith.langchain.com/concepts/tracing#feedback,Projects,"A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.
"
https://docs.smith.langchain.com/concepts/tracing#feedback,Feedback,"Feedback allows you to score an individual run based on certain criteria.
Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.
Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#feedback,Tags,"Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces
"
https://docs.smith.langchain.com/concepts/tracing#feedback,Metadata,"Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.
Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces
"
https://docs.smith.langchain.com/concepts/tracing#feedback,Data storage and retention,"For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/concepts/tracing#feedback,Deleting traces from LangSmith,"If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the ""Delete"" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#manage-users,Set up a workspace,"infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.
Workspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#manage-users,Create a workspace,"To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.
Once your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.
Please see the pricing page for more information."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#manage-users,Manage users,"infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.
Users may also be invited directly to one or more workspaces when they are invited to an organization."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#manage-users,Configure workspace settings,"Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace,Set up a workspace,"infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.
Workspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace,Create a workspace,"To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.
Once your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.
Please see the pricing page for more information."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace,Manage users,"infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.
Users may also be invited directly to one or more workspaces when they are invited to an organization."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace,Configure workspace settings,"Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,LangSmith Release Notes,noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,"Week of June 17, 2024 - LangSmith v0.6","LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,New Features,Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More...
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Performance and Reliability Changes,Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Infrastructure changes,"[Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Admin changes,Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Deprecation notices,With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,"Week of May 13, 2024 - LangSmith v0.5","LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Breaking changes,We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,New Features,Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Performance and Reliability Changes,"Split ingest, session deletion, and automation jobs to execute within separate resource pools."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Infrastructure changes,"As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Admin changes,Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Deprecation notices,With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,"Week of March 25, 2024 - LangSmith v0.4","LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Breaking changes,"This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Performance and Reliability Changes,"Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Infrastructure changes,"Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Admin changes,Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Deprecation notices,With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,"Week of Februrary 21, 2024 - LangSmith v0.3","LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Breaking changes,"This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Performance and Reliability Changes,"Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Admin changes,None
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Deprecation notices,With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,"Week of January 29, 2024 - LangSmith v0.2","LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Requirements,This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Breaking changes,"The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {""user_id"": ..., ""user_name"":...,}) and then search using has(metadata, '{""user_name"": ...}')"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Performance and Reliability Changes,"Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Infrastructure Changes,"Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Admin changes,Increased the maximum number of users per organization from 5 to 100 for new organizations.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-march-25-2024---langsmith-v04,Deprecation notices,With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#considerations,Upload experiments run outside of LangSmith with the REST API,"Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#considerations,Request body schema,"Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within
the experiment. Each object in the results represents a ""row"" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name
refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset
in LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  ""experiment_name"": ""string (required)"",  ""experiment_description"": ""string (optional)"",  ""experiment_start_time"": ""datetime (required)"",  ""experiment_end_time"": ""datetime (required)"",  ""dataset_id"": ""uuid (optional - an external dataset id, used to group experiments together)"",  ""dataset_name"": ""string (optional - must provide either dataset_id or dataset_name)"",  ""dataset_description"": ""string (optional)"",  ""experiment_metadata"": { // Object (any shape - optional)    ""key"": ""value""  },  ""summary_experiment_scores"": [ // List of summary feedback objects (optional)    {      ""key"": ""string (required)"",      ""score"": ""number (optional)"",      ""value"": ""string (optional)"",      ""comment"": ""string (optional)"",      ""feedback_source"": { // Object (optional)        ""type"": ""string (required)""      },      ""feedback_config"": { // Object (optional)        ""type"": ""string enum: continuous, categorical, or freeform"",        ""min"": ""number (optional)"",        ""max"": ""number (optional)"",        ""categories"": [ // List of feedback category objects (optional)            ""value"": ""number (required)"",            ""label"": ""string (optional)""        ]      },      ""created_at"": ""datetime (optional - defaults to now)"",      ""modified_at"": ""datetime (optional - defaults to now)"",      ""correction"": ""Object or string (optional)""    }  ],  ""results"": [ // List of experiment row objects (required)    {      ""row_id"": ""uuid (required)"",      ""inputs"": {     // Object (required - any shape). This will        ""key"": ""val""  // be the input to both the run and the dataset example.      },      ""expected_outputs"": { // Object (optional - any shape).        ""key"": ""val""        // These will be the outputs of the dataset examples.      },      ""actual_outputs"": { // Object (optional - any shape).        ""key"": ""val       // These will be the outputs of the runs.      },      ""evaluation_scores"": [ // List of feedback objects for the run (optional)        {            ""key"": ""string (required)"",            ""score"": ""number (optional)"",            ""value"": ""string (optional)"",            ""comment"": ""string (optional)"",            ""feedback_source"": { // Object (optional)                ""type"": ""string (required)""            },            ""feedback_config"": { // Object (optional)                ""type"": ""string enum: continuous, categorical, or freeform"",                ""min"": ""number (optional)"",                ""max"": ""number (optional)"",                ""categories"": [ // List of feedback category objects (optional)                    ""value"": ""number (required)"",                    ""label"": ""string (optional)""                ]            },            ""created_at"": ""datetime (optional - defaults to now)"",            ""modified_at"": ""datetime (optional - defaults to now)"",            ""correction"": ""Object or string (optional)""        }      ],      ""start_time"": ""datetime (required)"", // The start/end times for the runs will be used to      ""end_time"": ""datetime (required)"",   // calculate latency. They must all fall between the      ""run_name"": ""string (optional)"",     // start and end times for the experiment.      ""error"": ""string (optional)"",      ""run_metadata"": { // Object (any shape - optional)        ""key"": ""value""      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#considerations,Considerations,"You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together
under a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if
you only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#considerations,Example request,"Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  ""experiment_name"": ""My external experiment"",  ""experiment_description"": ""An experiment uploaded to LangSmith"",  ""dataset_name"": ""my-external-dataset"",  ""summary_experiment_scores"": [    {      ""key"": ""summary_accuracy"",      ""score"": 0.9,      ""comment"": ""Great job!""    }  ],  ""results"": [    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the weather in San Francisco today?""      },      ""expected_outputs"": {        ""output"": ""Sorry, I am unable to provide information about the current weather.""      },      ""actual_outputs"": {        ""output"": ""The weather is partly cloudy with a high of 65.""      },      ""evaluation_scores"": [        {          ""key"": ""hallucination"",          ""score"": 1,          ""comment"": ""The chatbot made up the weather instead of identifying that ""                     ""they don't have enough info to answer the question. This is ""                     ""a hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:39"",      ""end_time"": ""2024-08-03T00:12:41"",      ""run_name"": ""Chatbot""    },    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the square root of 49?""      },      ""expected_outputs"": {        ""output"": ""The square root of 49 is 7.""      },      ""actual_outputs"": {        ""output"": ""7.""      },      ""evaluation_scores"": [       {          ""key"": ""hallucination"",          ""score"": 0,          ""comment"": ""The chatbot correctly identified the answer. This is not a ""                     ""hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:40"",      ""end_time"": ""2024-08-03T00:12:42"",      ""run_name"": ""Chatbot""    }  ],  ""experiment_start_time"": ""2024-08-03T00:12:38"",  ""experiment_end_time"": ""2024-08-03T00:12:43""}resp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/upload-experiment"",    json=body,    headers={""x-api-key"": os.environ[""LANGCHAIN_API_KEY""]})print(resp.json()) Below is the response received: {  ""dataset"": {    ""name"": ""my-external-dataset"",    ""description"": null,    ""created_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""data_type"": ""kv"",    ""inputs_schema_definition"": null,    ""outputs_schema_definition"": null,    ""externally_managed"": true,    ""id"": ""<<uuid>>"",    ""tenant_id"": ""<<uuid>>"",    ""example_count"": 0,    ""session_count"": 0,    ""modified_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""last_session_start_time"": null  },  ""experiment"": {    ""start_time"": ""2024-08-03T00:12:38"",    ""end_time"": ""2024-08-03T00:12:43+00:00"",    ""extra"": null,    ""name"": ""My external experiment"",    ""description"": ""An experiment uploaded to LangSmith"",    ""default_dataset_id"": null,    ""reference_dataset_id"": ""<<uuid>>"",    ""trace_tier"": ""longlived"",    ""id"": ""<<uuid>>"",    ""run_count"": null,    ""latency_p50"": null,    ""latency_p99"": null,    ""first_token_p50"": null,    ""first_token_p99"": null,    ""total_tokens"": null,    ""prompt_tokens"": null,    ""completion_tokens"": null,    ""total_cost"": null,    ""prompt_cost"": null,    ""completion_cost"": null,    ""tenant_id"": ""<<uuid>>"",    ""last_run_start_time"": null,    ""last_run_start_time_live"": null,    ""feedback_stats"": null,    ""session_feedback_stats"": null,    ""run_facets"": null,    ""error_rate"": null,    ""streaming_rate"": null,    ""test_run_number"": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.
If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this
information in the request body)."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#considerations,View the experiment in the UI,"Now, login to the UI and click on your newly-created dataset! You should see a single experiment:
 Your examples will have been uploaded:
 Clicking on your experiment will bring you to the comparison view:
 As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."
https://docs.smith.langchain.com/how_to_guides/playground/custom_endpoint,Use a custom model,"The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via LangServe, an open source library for serving LangChain applications.
Behind the scenes, the playground will interact with your model server to generate responses."
https://docs.smith.langchain.com/how_to_guides/playground/custom_endpoint,Deploy a custom model server,"For your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server here
We highly recommend using the sample model server as a starting point. Depending on your model is an instruct-style or chat-style model, you will need to implement either custom_model.py or custom_chat_model.py respectively."
https://docs.smith.langchain.com/how_to_guides/playground/custom_endpoint,Adding configurable fields,"It is often useful to configure your model with different parameters. These might include temperature, model_name, max_tokens, etc. To make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground. You can add configurable fields by implementing the with_configurable_fields function in the config.py file. You can def with_configurable_fields(self) -> Runnable:    """"""Expose fields you want to be configurable in the playground. We will automatically expose these to the    playground. If you don't want to expose any fields, you can remove this method.""""""    return self.configurable_fields(n=ConfigurableField(        id=""n"",        name=""Num Characters"",        description=""Number of characters to return from the input prompt."",    ))"
https://docs.smith.langchain.com/how_to_guides/playground/custom_endpoint,Use the model in the LangSmith Playground,"Once you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the ChatCustomModel or the CustomModel provider for chat-style model or instruct-style models. Enter the URL. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters. If everything is set up correctly, you should see the model's response in the playground as well as the configurable fields specified in the with_configurable_fields. See how to store your model configuration for later use here."
https://docs.smith.langchain.com/concepts/admin#api-keys,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#api-keys,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#api-keys,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#api-keys,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#api-keys,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#api-keys,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#api-keys,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#api-keys,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#api-keys,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#make-corrections,Create few-shot evaluators,"Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary
overhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows
you to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#make-corrections,Create your evaluator,"tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators
are only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,
we will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}
template variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name
as your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,
few_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend
to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#make-corrections,Make corrections,"Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will
begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.
The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#make-corrections,View your corrections dataset,"In order to view your corrections dataset, go to your rule and click ""Edit Rule"" (or ""Edit Evaluator"" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says ""View few-shot dataset"". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"
https://docs.smith.langchain.com/concepts/prompts#prompt-types,Prompts,"Writing good prompts is key to getting the best performance out of your applications. LangSmith provides ways to create, test, and manage prompts."
https://docs.smith.langchain.com/concepts/prompts#prompt-types,Prompt types,"We support three types of prompt templates: StringPromptTemplateChatPromptTemplateStructuredPromptTemplate For detailed information about these templates, please refer to the LangChain documentation. In LangSmith, you can create prompts using the Playground. From the prompt view in the Playground, you can select either ""Chat-style prompt"" or ""Instruct-style prompt"" to get started. Chat-style prompt Chat prompts are used for chat-style models that accept a list of messages as an input and respond with an assistant message. A chat-style prompt is represented in LangSmith as a ChatPromptTemplate, which can contain multiple messages, each with prompt variables. You can also specify an output schema which is represented in LangSmith as a StructuredPromptTemplate. Instruct-style prompt An instruct-style prompt is represented as a StringPromptTemplate that gets formatted to a single string input for your model."
https://docs.smith.langchain.com/concepts/prompts#prompt-types,Template formats,We support two types of template formats: f-string and mustache. You can switch between these formats when editing prompts in the Playground.
https://docs.smith.langchain.com/concepts/prompts#prompt-types,F-string,"F-strings are a Python-specific string formatting method that allows you to embed expressions inside string literals, using curly braces {}.
Here's an example of an f-string template: Hello, {name}!"
https://docs.smith.langchain.com/concepts/prompts#prompt-types,Mustache,"Mustache is a template syntax that allows you to embed variables inside double curly braces {{}}. NotePlease see the Mustache documentation for more detailed information on how to use Mustache templates. Here's an example of a mustache template: Hello, {{name}}! Mustache is more robust than f-strings since it supports more complex logic. You can use conditionals, loops, and access nested keys in mustache templates. Conditional Hello, {{#name}}{{name}}{{/name}}{{^name}}world{{/name}}! The template will output ""Hello, Bob!"" if ""Bob"" is provided as the name variable, and ""Hello, world!"" if the name variable is not provided. Loop {{#names}}{{name}}{{/names}} input: {  ""names"": [{ ""name"": ""Alice"" }, { ""name"": ""Bob"" }, { ""name"": ""Charlie"" }]} output: ""AliceBobCharlie"" Loop with nesting {{#people}}{{name}} is {{age}} years old. {{/people}} input: {  ""people"": [    { ""name"": ""Alice"", ""age"": 30 },    { ""name"": ""Bob"", ""age"": 25 },    { ""name"": ""Charlie"", ""age"": 35 }  ]} output: ""Alice is 30 years old. Bob is 25 years old. Charlie is 35 years old."" Dot notation {{person.name}} is {{person.age}} years old. input: {  ""person"": {    ""name"": ""Alice"",    ""age"": 30  }} output: ""Alice is 30 years old."""
https://docs.smith.langchain.com/self_hosting/scripts/delete_traces,Deleting Traces,"The LangSmith UI does not currently support the deletion of an invidual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs_history views) and the runs and feedback tables themselves. This command can either be run using a trace ID as an argument or using a file that is a list of trace IDs."
https://docs.smith.langchain.com/self_hosting/scripts/delete_traces,Prerequisites,"Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/Clickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the Clickhouse database from the machine you will be running the delete_trace_by_id script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine."
https://docs.smith.langchain.com/self_hosting/scripts/delete_traces,Running the deletion script for a single trace,"Run the following command to run the trace deletion script using a single trace ID: sh delete_trace_by_id.sh <clickhouse_url> --trace_id <trace_id> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_trace_by_id.sh ""clickhouse://default:password@localhost:8123/default"" --trace_id 4ec70ec7-0808-416a-b836-7100aeec934b If you visit the Langsmith UI, you should now see specified trace ID is no longer present nor reflected in stats."
https://docs.smith.langchain.com/self_hosting/scripts/delete_traces,Running the deletion script for a multiple traces from a file with one trace ID per line,"Run the following command to run the trace deletion script using a list of trace IDs: sh delete_trace_by_id.sh <clickhouse_url> --file <path/to/foo.txt> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_trace_by_id.sh ""clickhouse://default:password@localhost:8123/default"" --file path/to/traces.txt If you visit the Langsmith UI, you should now see all the specified traces have been removed."
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#view-your-corrections-dataset,Create few-shot evaluators,"Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary
overhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows
you to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#view-your-corrections-dataset,Create your evaluator,"tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators
are only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,
we will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}
template variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name
as your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,
few_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend
to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#view-your-corrections-dataset,Make corrections,"Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will
begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.
The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#view-your-corrections-dataset,View your corrections dataset,"In order to view your corrections dataset, go to your rule and click ""Edit Rule"" (or ""Edit Evaluator"" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says ""View few-shot dataset"". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"
https://docs.smith.langchain.com/how_to_guides/human_feedback/attach_user_feedback,Capture user feedback from your application to traces,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and feedbackReference guide on feedback data format In many applications, but even more so for LLM applications, it is important to collect user feedback to understand how your application is performing in real-world scenarios.
The ability to observe user feedback along with trace data can be very powerful to drill down into the most interesting datapoints, then send those datapoints for further review, automatic evaluation, or even datasets.
To learn more about how to filter traces based on various attributes, including user feedback, see this guide LangSmith makes it easy to attach user feedback to traces.
It's often helpful to expose a simple mechanism (such as a thumbs-up, thumbs-down button) to collect user feedback for your application responses. You can then use the LangSmith SDK or API to send feedback for a trace. To get the run_id of a logged run, see this guide. noteYou can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.
This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline. PythonTypeScriptfrom langsmith import Clientclient = Client()# ... Run your application and get the run_id...# This information can be the result of a user-facing feedback formclient.create_feedback(    run_id,    key=""feedback-key"",    score=1.0,    comment=""comment"",)import { Client } from ""langsmith"";const client = new Client();// ... Run your application and get the run_id...// This information can be the result of a user-facing feedback formawait client.createFeedback(    runId,    ""feedback-key"",    {        score: 1.0,        comment: ""comment"",    });"
https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse,Connect to an external ClickHouse database,"ClickHouse is a high-performance, column-oriented database system. It allows for fast ingestion of data and is optimized for analytical queries. LangSmith uses ClickHouse as the primary data store for traces and feedback. By default, self-hosted LangSmith will use an internal ClickHouse database that is bundled with the LangSmith instance. This is run as a stateful set in the same Kubernetes cluster as the LangSmith application or as a Docker container on the same host as the LangSmith application. However, you can configure LangSmith to use an external ClickHouse database for easier management and scaling.
By configuring an external ClickHouse database, you can manage backups, scaling, and other operational tasks for your database.
While Clickhouse is not yet a native service in Azure, AWS, or Google Cloud, you can run LangSmith with an external ClickHouse database in the following ways: LangSmith-managed ClickHouse (beta)Provision a ClickHouse Cloud either directly or through a cloud provider marketplace:Azure MarketplaceGoogle Cloud MarketplaceAWS MarketplaceOn a VM in your cloud provider noteUsing the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC.
However, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).Additionally, sensitive information can be configured to be not stored in Clickhouse. Please reach out to support@langchain.dev for more information."
https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse,Requirements,"A provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).A user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.We only support standalone ClickHouse (not clustered or replicated) or ClickHouse Cloud.We only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later. See the LangSmith release notes for more information."
https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse,Parameters,You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include: Host: The hostname or IP address of the ClickHouse databaseHTTP Port: The port that the ClickHouse database listens on for HTTP connectionsNative Port: The port that the ClickHouse database listens on for native connectionsDatabase: The name of the ClickHouse database that LangSmith should useUsername: The username to use to connect to the ClickHouse databasePassword: The password to use to connect to the ClickHouse database
https://docs.smith.langchain.com/self_hosting/configuration/external_clickhouse,Configuration,"With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the config.yaml file for your LangSmith Helm Chart installation or the .env file for your Docker installation. HelmDocker  clickhouse:    external:      enabled: true      host: ""host""      port: ""http port""      nativePort: ""native port""      user: ""default""      password: ""password""      database: ""default""      tls: false    # In your .env fileCLICKHOUSE_HOST=langchain-clickhouse # Change to your Clickhouse host if using external Clickhouse. Otherwise, leave it as isCLICKHOUSE_USER=default # Change to your Clickhouse user if neededCLICKHOUSE_DB=default # Change to your Clickhouse database if neededCLICKHOUSE_PORT=8123 # Change to your Clickhouse port if neededCLICKHOUSE_TLS=false # Change to true if you are using TLS to connect to Clickhouse. Otherwise, leave it as isCLICKHOUSE_PASSWORD=password # Change to your Clickhouse password if neededCLICKHOUSE_NATIVE_PORT=9000 # Change to your Clickhouse native port if needed Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-datasets,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#customize-run-name,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-runs-table,Audit evaluator scores,"LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-runs-table,In the comparison view,"In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the ""edit"" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under ""Make correction"".
If you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples
in place of the few_shot_explanation prompt variable."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-runs-table,In the runs table,"In the runs table, find the ""Feedback"" column and click on the feedback tag to bring up the feedback details. Again, click the ""edit"" icon on the right to bring up the corrections view."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-runs-table,In the SDK,"Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      ""score"": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#slice-data-by-metadata-or-tag,Use monitoring charts,LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#slice-data-by-metadata-or-tag,Change the time period,"You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#slice-data-by-metadata-or-tag,Slice data by metadata or tag,"By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.
This can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.
After that, you can click the Tag or Metadata tab at the top to group runs accordingly."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#slice-data-by-metadata-or-tag,Drill down into specific subsets,"Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#create-a-dataset,Run evals with the REST API,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.
However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#create-a-dataset,Create a dataset,"Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries - API Example""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#create-a-dataset,Run a single experiment,"First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { ""dataset"": dataset_id }resp = requests.get(    ""https://api.smith.langchain.com/api/v1/examples"",    params=params,    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[""OPENAI_API_KEY""] = ""sk-...""def run_completion_on_example(example, model_name, experiment_id):    """"""Run completions on a list of examples.""""""    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        """"""Function to post a new run to the API.""""""        data = {            ""id"": run_id.hex,            ""name"": name,            ""run_type"": run_type,            ""inputs"": inputs,            ""start_time"": datetime.utcnow().isoformat(),            ""reference_example_id"": example[""id""],            ""session_id"": experiment_id,        }        if parent_id:            data[""parent_run_id""] = parent_id.hex        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/runs"", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        """"""Function to patch a run with outputs.""""""        resp = requests.patch(            f""https://api.smith.langchain.com/api/v1/runs/{run_id}"",            json={                ""outputs"": outputs,                ""end_time"": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    text = example[""inputs""][""text""]    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, ""LLM Pipeline"", ""chain"", {""text"": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, ""OpenAI Call"", ""llm"", {""messages"": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {""label"": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (""gpt-3.5-turbo"", ""gpt-4o-mini"")experiment_ids = []for model_name in model_names:    resp = requests.post(        ""https://api.smith.langchain.com/api/v1/sessions"",        json={            ""start_time"": datetime.utcnow().isoformat(),            ""reference_dataset_id"": str(dataset_id),            ""description"": ""An optional description for the experiment"",            ""name"": f""Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}"",  # A name for the experiment            ""extra"": {                ""metadata"": {""foo"": ""bar""},  # Optional metadata            },        },        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )    experiment = resp.json()    experiment_ids.append(experiment[""id""])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[""id""])    # Issue a patch request to ""end"" the experiment by updating the end_time    requests.patch(        f""https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}"",        json={""end_time"": datetime.utcnow().isoformat()},        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#create-a-dataset,Run a pairwise experiment,"Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.
For more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/comparative"",    json={        ""experiment_ids"": experiment_ids,        ""name"": ""Toxicity detection - API Example - Comparative - "" + str(uuid4())[0:8],        ""description"": ""An optional description for the comparative experiment"",        ""extra"": {            ""metadata"": {""foo"": ""bar""},  # Optional metadata        },        ""reference_dataset_id"": str(dataset_id),    },    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[""id""]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f""https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative"",    params={""id"": comparative_experiment_id},    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()[0]experiment_ids = [info[""id""] for info in comparative_experiment[""experiments_info""]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f""https://api.smith.langchain.com/api/v1/runs/query"",    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]},    json={        ""session"": experiment_ids,        ""is_root"": True, # Only fetch root runs (spans) which contain the end outputs        ""select"": [""id"", ""reference_example_id"", ""outputs""],    }).json()runs = runs[""runs""]for run in runs:    example_id = run[""reference_example_id""]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f""Example ID: {example_id}"")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f""Run ID: {run['id']}"")        feedback = {            ""score"": 1 if i == 0 else 0,            ""run_id"": str(run[""id""]),            ""key"": ""ranked_preference"",            ""feedback_group_id"": str(feedback_group_id),            ""comparative_experiment_id"": comparative_experiment_id,        }        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/feedback"",            json=feedback,            headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}        )        resp.raise_for_status()"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#list_delete_and_like_prompts,Manage prompts programmatically,"You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.
All functionality going forward will live in the langsmith package."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#list_delete_and_like_prompts,Install packages,PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#list_delete_and_like_prompts,Configure environment variables,"If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=""lsv2_..."" TerminologyWhat we refer to as ""prompts"" used to be called ""repos"", so any references to ""repo"" in the code are referring to a prompt."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#list_delete_and_like_prompts,Push a prompt,"To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = client.push_prompt(""joke-generator"", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = prompts.push(""joke-generator"", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");url = prompts.push(""joke-generator"", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.
This is useful for storing the model configuration you want to use with this prompt.
The provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelclient.push_prompt(""joke-generator-with-model"", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelurl = prompts.push(""joke-generator-with-model"", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { ChatOpenAI } from ""langchain-openai"";const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");const chain = prompt.pipe(model);prompts.push(""joke-generator-with-model"", chain);"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#list_delete_and_like_prompts,Pull a prompt,"To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { ChatOpenAI } from ""langchain-openai"";const prompt = prompts.pull(""joke-generator"");const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const chain = prompt.pipe(model);chain.invoke({""topic"": ""cats""}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.
Just specify include_model when pulling the prompt.
If the stored prompt includes a model, it will be returned as a RunnableSequence.
Make sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})from langchain import hub as promptschain = prompts.pull(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { Runnable } from ""@langchain/core/runnables"";const chain = prompts.pull<Runnable>(""joke-generator-with-model"", { includeModel: true });chain.invoke({""topic"": ""cats""}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#list_delete_and_like_prompts,Use a prompt without LangChain,"If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.
These convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(""joke-generator"")prompt_value = prompt.invoke({""topic"": ""cats""})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#list_delete_and_like_prompts,"List, delete, and like prompts","You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.
See the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include ""joke""prompts = client.list_prompts(query=""joke"", is_public=False)# Delete a promptclient.delete_prompt(""joke-generator"")# Like a promptclient.like_prompt(""efriis/my-first-prompt"")# Unlike a promptclient.unlike_prompt(""efriis/my-first-prompt"")// List all prompts in my workspaceimport Client from ""langsmith"";const client = new Client({ apiKey: ""lsv2_..."" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include ""joke""const private_joke_prompts = client.listPrompts({ query: ""joke"", isPublic: false});// Delete a promptclient.deletePrompt(""joke-generator"");// Like a promptclient.likePrompt(""efriis/my-first-prompt"");// Unlike a promptclient.unlikePrompt(""efriis/my-first-prompt""); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.
However, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields,Use LangChain off-the-shelf evaluators (Python only),"Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.
These evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Ankush"", ""Hello Ankush""),    (""Harrison"", ""Hello Harrison""),]dataset_name = ""Hello Set""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": input}, {""expected"": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields,Use question and answer (correctness) evaluators,"Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!
Three QA evaluators you can load are: ""qa"", ""context_qa"", ""cot_qa"". Based on our meta-evals, we recommend using ""cot_qa"", or Chain of Thought QA. Here is a trivial example that uses a ""cot_qa"" evaluator to evaluate a simple pipeline that prefixes the input with ""Hello"": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(""cot_qa"")client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields,Use criteria evaluators,"If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the ""criteria"" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The ""criteria"" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    ""criteria"",    config={        ""criteria"": {            ""says_hello"": ""Does the submission say hello?"",        }    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.
To specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {""creativity"": ""Is this submission creative, imaginative, or novel?""}criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={""criteria"": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent ""direction"" (i.e., higher is not necessarily better).
The direction of the score depends on the criteria being evaluated. For example, a score of 1 for ""helpfulness"" means that the prediction was deemed to be helpful by the model.
However, a score of 1 for ""maliciousness"" means that the prediction contains malicious content, which, of course, is ""bad""."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields,Use labeled criteria evaluators,"If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the ""labeled_criteria"" or ""labeled_score_string"" evaluators. The ""labeled_criteria"" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe ""labeled_score_string"" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    ""labeled_score_string"",    config={        ""criteria"": {            ""accuracy"": ""How accurate is this prediction compared to the reference on a scale of 1-10?""        },        ""normalize_by"": 10,    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields,Use string or embedding distance metrics,"To measure the similarity between a predicted string and a reference, you can use string distance metrics: The ""string_distance"" evaluator computes a normalized string edit distance between the prediction and referenceThe ""embedding_distance"" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    ""string_distance"",    config={""distance"": ""levenshtein"", ""normalize_score"": True})embedding_distance_evaluator = LangChainStringEvaluator(    ""embedding_distance"",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # ""embeddings"": HuggingFaceEmbeddings(model=""distilbert-base-uncased""),      # Can also choose ""euclidean"", ""chebyshev"", ""hamming"", and ""manhattan""        ""distance_metric"": ""cosine"",      })evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields,Use a custom LLM in off-the-shelf evaluators,"You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=""gpt-3.5-turbo"")cot_qa_evaluator = LangChainStringEvaluator(""cot_qa"", config={""llm"": eval_llm})evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#handle-multiple-input-or-output-fields,Handle multiple input or output fields,"LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.
These map the keys ""prediction"", ""reference"", and ""input"" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: ""greeting"" and ""foo"". We want to evaluate the ""greeting"" field against the ""expected"" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    },    prepare_data=lambda run, example: {        ""prediction"": run.outputs[""greeting""],        ""reference"": example.outputs[""expected""],        ""input"": example.inputs[""input""],    })client = Client()evaluate(    lambda input: {""greeting"": ""Hello "" + input[""input""], ""foo"": ""bar""},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing,Test LLM applications (Python only),"LangSmith functional tests are assertions and expectations designed to quickly identify obvious bugs and regressions in your AI system. Relative to evaluations, tests typically are designed to be fast and cheap to run, focusing on specific functionality and edge cases.
We recommend using LangSmith to track any unit tests, end-to-end integration tests, or other specific assertions that touch an LLM or other non-deterministic part of your AI system.
These should run on every commit in your CI pipeline to catch regressions early. note@test currently requires langsmith python version >=0.1.74 (named @unit for versions >=0.1.42). If you are interested in unit testing functionality in TypeScript or other languages, please let us know at support@langchain.dev.  "
https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing,Write a @test,"To write a LangSmith functional test, decorate your test function with @test.
If you want to track the full nested trace of the system or component being tested, you can mark those functions with @traceable. For example: # my_app/main.pyfrom langsmith import traceable@traceable # Optionaldef generate_sql(user_query):    # Replace with your SQL generation logic    # e.g., my_llm(my_prompt.format(user_query))    return ""SELECT * FROM customers"" Then define your test: # tests/test_my_app.pyfrom langsmith import testfrom my_app.main import generate_sql@testdef test_sql_generation_select_all():    user_query = ""Get all users from the customers table""    sql = generate_sql(user_query)    # LangSmith logs any exception raised by `assert` / `pytest.fail` / `raise` / etc.    # as a test failure    assert sql == ""SELECT * FROM customers"""
https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing,Run tests,"You can use a standard testing framework such as pytest (docs) to run. For example: pytest tests/ Each time you run this test suite, LangSmith collects the pass/fail rate and other traces as a new TestSuiteResult, logging the pass rate (1 for pass, 0 for fail) over all the applicable tests. The test suite syncs to a corresponding dataset named after your package or github repository."
https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing,Going further,"@test is designed to stay out of your way and works well with familiar pytest features. For example: Defining inputs as fixtures Pytest fixtures let you define functions that serve as reusable inputs for your tests. LangSmith automatically syncs any test case inputs defined as fixtures. For example: import pytest@pytest.fixturedef user_query():    return ""Get all users from the customers table""@pytest.fixturedef expected_sql():    return ""SELECT * FROM customers""# output_keys indicate which test arguments to save as 'outputs' in the dataset (Optional)# Otherwise, all arguments are saved as 'inputs'@test(output_keys=[""expected_sql""])def test_sql_generation_with_fixture(user_query, expected_sql):    sql = generate_sql(user_query)    assert sql == expected_sql Parametrizing tests Parametrizing tests lets you run the same assertions across multiple sets of inputs. Use pytest's parametrize decorator to achieve this. For example: @test@pytest.mark.parametrize(    ""user_query, expected_sql"",    [        (""Get all users from the customers table"", ""SELECT * FROM customers""),        (""Get all users from the orders table"", ""SELECT * FROM orders""),    ],)def test_sql_generation_parametrized(user_query, expected_sql):    sql = generate_sql(user_query)    assert sql == expected_sql Note: as the parametrized list grows, you may consider using evaluate() instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset. Expectations LangSmith provides an expect utility to help define expectations about your LLM output. For example: from langsmith import expect@testdef test_sql_generation_select_all():    user_query = ""Get all users from the customers table""    sql = generate_sql(user_query)    expect(sql).to_contain(""customers"") This will log the binary ""expectation"" score to the experiment results, additionally asserting that the expectation is met possibly triggering a test failure. expect also provides ""fuzzy match"" methods. For example: @test@pytest.mark.parametrize(    ""query, expectation"",    [       (""what's the capital of France?"", ""Paris""),    ],)def test_embedding_similarity(query, expectation):    prediction = my_chatbot(query)    expect.embedding_distance(        # This step logs the distance as feedback for this run        prediction=prediction, expectation=expectation    # Adding a matcher (in this case, 'to_be_*""), logs 'expectation' feedback    ).to_be_less_than(0.5) # Optional predicate to assert against    expect.edit_distance(        # This computes the normalized Damerau-Levenshtein distance between the two strings        prediction=prediction, expectation=expectation    # If no predicate is provided below, 'assert' isn't called, but the score is still logged    ) This test case will be assigned 4 scores: The embedding_distance between the prediction and the expectationThe binary expectation score (1 if cosine distance is less than 0.5, 0 if not)The edit_distance between the prediction and the expectationThe overall test pass/fail score (binary) The expect utility is modeled off of Jest's expect API, with some off-the-shelf functionality to make it easier to grade your LLMs. Dry-run mode If you want to run the tests without syncing the results to LangSmith, you can set LANGCHAIN_TEST_TRACKING=false in your environment. LANGCHAIN_TEST_TRACKING=false pytest tests/ The tests will run as normal, but the experiment logs will not be sent to LangSmith. Caching LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache results to disk. Any identical inputs will be loaded from the cache so you don't have to call out to your LLM provider unless there are changes to the model, prompt, or retrieved data. To enable caching, run with LANGCHAIN_TEST_CACHE=/my/cache/path. For example: LANGCHAIN_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests All requests will be cached to tests/cassettes and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well. Using watch mode With caching enabled, you can iterate quickly on your tests using watch mode without worrying about unnecessarily hitting your LLM provider. For example, using pytest-watch: pip install pytest-watchLANGCHAIN_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests"
https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing,Explanations,"The @test test decorator converts any test into a parametrized LangSmith example. By default, all tests within a given file will be grouped as a single ""test suite"" with a corresponding dataset. You can configure which test suite a test belongs to by passing the test_suite_name parameter to @test. The following metrics are available off-the-shelf: FeedbackDescriptionExamplepassBinary pass/fail score, 1 for pass, 0 for failassert False # FailsexpectationBinary expectation score, 1 if expectation is met, 0 if notexpect(prediction).against(lambda x: re.search(r""\b[a-f\d]{8}-[a-f\d]{4}-[a-f\d]{4}-[a-f\d]{4}-[a-f\d]{12}\b"", x) )embedding_distanceCosine distance between two embeddingsexpect.embedding_distance(prediction=prediction, expectation=expectation)edit_distanceEdit distance between two stringsexpect.edit_distance(prediction=prediction, expectation=expectation) You can also log any arbitrary feeback within a unit test manually using the client. from langsmith import unit, Clientfrom langsmith.run_helpers import get_current_run_treeclient = Client()@testdef test_foo():    run_tree = get_current_run_tree()    client.create_feedback(run_id=run_tree.id, key=""my_custom_feedback"", score=1)"
https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing,expect,"expect makes it easy to make approximate assertions on test results and log scores to LangSmith.
Off-the-shelf, it allows you to compute and compare embedding distances, edit distances, and make custom assertions on values. expect.embedding_distance(prediction, reference, *, config=None) Compute the embedding distance between the prediction and reference. This logs the embedding distance to LangSmith and returns a Matcher instance for making assertions on the distance value. By default, this uses the OpenAI API for computing embeddings. Parameters prediction (str): The predicted string to compare.reference (str): The reference string to compare against.config (Optional[EmbeddingConfig]): Optional configuration for the embedding distance evaluator. Supported options:encoder: A custom encoder function to encode the list of input strings to embeddings. Defaults to the OpenAI API.metric: The distance metric to use for comparison. Supported values: ""cosine"", ""euclidean"", ""manhattan"", ""chebyshev"", ""hamming"". Returns A Matcher instance for the embedding distance value. expect.edit_distance(prediction, reference, *, config=None) Compute the string distance between the prediction and reference. This logs the string distance (Damerau-Levenshtein) to LangSmith and returns a Matcher instance for making assertions on the distance value. This depends on the rapidfuzz package for string distance computation. Parameters prediction (str): The predicted string to compare.reference (str): The reference string to compare against.config (Optional[EditDistanceConfig]): Optional configuration for the string distance evaluator. Supported options:metric: The distance metric to use for comparison. Supported values: ""damerau_levenshtein"", ""levenshtein"", ""jaro"", ""jaro_winkler"", ""hamming"", ""indel"".normalize_score: Whether to normalize the score between 0 and 1. Returns A Matcher instance for the string distance value. expect.value(value) Create a Matcher instance for making assertions on the given value. Parameters value (Any): The value to make assertions on. Returns A Matcher instance for the given value. Matcher A class for making assertions on expectation values. to_be_less_than(value) Assert that the expectation value is less than the given value. to_be_greater_than(value)  Assert that the expectation value is greater than the given value. to_be_between(min_value, max_value) Assert that the expectation value is between the given min and max values. to_be_approximately(value, precision=2) Assert that the expectation value is approximately equal to the given value. to_equal(value) Assert that the expectation value equals the given value. to_contain(value) Assert that the expectation value contains the given value. against(func) Assert the expectation value against a custom function."
https://docs.smith.langchain.com/how_to_guides/evaluation/unit_testing,testAPI,"The @test decorator is used to mark a function as a test case for LangSmith. It ensures that the necessary example data is created and associated with the test function. The decorated function will be executed as a test case, and the results will be recorded and reported by LangSmith. @test(id=None, output_keys=None, client=None, test_suite_name=None) Create a test case in LangSmith. Parameters id (Optional[uuid.UUID]): A unique identifier for the test case. If not provided, an ID will be generated based on the test function's module and name.output_keys (Optional[Sequence[str]]): A list of keys to be considered as the output keys for the test case. These keys will be extracted from the test function's inputs and stored as the expected outputs.client (Optional[ls_client.Client]): An instance of the LangSmith client to be used for communication with the LangSmith service. If not provided, a default client will be used.test_suite_name (Optional[str]): The name of the test suite to which the test case belongs. If not provided, the test suite name will be determined based on the environment or the package name. Environment Variables LANGSMITH_TEST_TRACKING: Set this variable to the path of a directory to enable caching of test results. This is useful for re-running tests without re-executing the code. Requires the 'langsmith[vcr]' package."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#export-a-dataset,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-with-repetitions,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#manually-specify-a-raw-query-in-langsmith-query-language,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-selectively,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#manually-provide-token-counts,Log custom LLM traces,"noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.
In order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#manually-provide-token-counts,Chat-style models,"For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg ""openai"", ""anthropic"", etc.ls_model_name: The name of the model, eg ""gpt-3.5-turbo"", ""claude-3-opus-20240307"", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ]}# Can also use one of:# output = {#     ""message"": {#         ""role"": ""assistant"",#         ""content"": ""Sure, what time would you like to book the table for?""#     }# }## output = {#     ""role"": ""assistant"",#     ""content"": ""Sure, what time would you like to book the table for?""# }## output = [""assistant"", ""Sure, what time would you like to book the table for?""]@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" }];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?""      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: ""assistant"",//     content: ""Sure, what time would you like to book the table for?""//   }// };//// const output = {//   role: ""assistant"",//   content: ""Sure, what time would you like to book the table for?""// };//// const output = [""assistant"", ""Sure, what time would you like to book the table for?""];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#manually-provide-token-counts,Stream outputs,"For streaming, you can ""reduce"" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = """".join([chunk[""choices""][0][""message""][""content""] for chunk in chunks])    return {""choices"": [{""message"": {""content"": all_text, ""role"": ""assistant""}}]}@traceable(    run_type=""llm"",    reduce_fn=_reduce_chunks,    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def my_streaming_chat_model(messages: list):    for chunk in [""Hello, "" + messages[1][""content""]]:        yield {            ""choices"": [                {                    ""message"": {                        ""content"": chunk,                        ""role"": ""assistant"",                    }                }            ]        }list(    my_streaming_chat_model(        [            {""role"": ""system"", ""content"": ""You are a helpful assistant. Please greet the user.""},            {""role"": ""user"", ""content"": ""polly the parrot""},        ],    ))"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#manually-provide-token-counts,Manually provide token counts,"Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.
Many models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.
If token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.
If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ],    ""usage_metadata"": {        ""input_tokens"": 27,        ""output_tokens"": 13,        ""total_tokens"": 40,    },}@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" },];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?"",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages });"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#manually-provide-token-counts,Instruct-style models,"For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.
The same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def hello_llm(prompt: str):    return {        ""choices"": [            {""text"": ""Hello, "" + prompt}        ],        ""usage_metadata"": {            ""input_tokens"": 4,            ""output_tokens"": 5,            ""total_tokens"": 9,        },    }hello_llm(""polly the parrot\n"")import { traceable } from ""langsmith/traceable"";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: ""Hello, "" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: ""llm"", name: ""hello_llm"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await helloLLM({ prompt: ""polly the parrot\n"" }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#instruct-style-models,Log custom LLM traces,"noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.
In order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#instruct-style-models,Chat-style models,"For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg ""openai"", ""anthropic"", etc.ls_model_name: The name of the model, eg ""gpt-3.5-turbo"", ""claude-3-opus-20240307"", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ]}# Can also use one of:# output = {#     ""message"": {#         ""role"": ""assistant"",#         ""content"": ""Sure, what time would you like to book the table for?""#     }# }## output = {#     ""role"": ""assistant"",#     ""content"": ""Sure, what time would you like to book the table for?""# }## output = [""assistant"", ""Sure, what time would you like to book the table for?""]@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" }];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?""      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: ""assistant"",//     content: ""Sure, what time would you like to book the table for?""//   }// };//// const output = {//   role: ""assistant"",//   content: ""Sure, what time would you like to book the table for?""// };//// const output = [""assistant"", ""Sure, what time would you like to book the table for?""];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#instruct-style-models,Stream outputs,"For streaming, you can ""reduce"" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = """".join([chunk[""choices""][0][""message""][""content""] for chunk in chunks])    return {""choices"": [{""message"": {""content"": all_text, ""role"": ""assistant""}}]}@traceable(    run_type=""llm"",    reduce_fn=_reduce_chunks,    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def my_streaming_chat_model(messages: list):    for chunk in [""Hello, "" + messages[1][""content""]]:        yield {            ""choices"": [                {                    ""message"": {                        ""content"": chunk,                        ""role"": ""assistant"",                    }                }            ]        }list(    my_streaming_chat_model(        [            {""role"": ""system"", ""content"": ""You are a helpful assistant. Please greet the user.""},            {""role"": ""user"", ""content"": ""polly the parrot""},        ],    ))"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#instruct-style-models,Manually provide token counts,"Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.
Many models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.
If token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.
If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ],    ""usage_metadata"": {        ""input_tokens"": 27,        ""output_tokens"": 13,        ""total_tokens"": 40,    },}@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" },];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?"",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages });"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#instruct-style-models,Instruct-style models,"For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.
The same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def hello_llm(prompt: str):    return {        ""choices"": [            {""text"": ""Hello, "" + prompt}        ],        ""usage_metadata"": {            ""input_tokens"": 4,            ""output_tokens"": 5,            ""total_tokens"": 9,        },    }hello_llm(""polly the parrot\n"")import { traceable } from ""langsmith/traceable"";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: ""Hello, "" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: ""llm"", name: ""hello_llm"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await helloLLM({ prompt: ""polly the parrot\n"" }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_multimodal_traces,Log multimodal traces,"LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs. In order to log images, use wrap_openai/ wrapOpenAI in Python or TypeScript respectively and pass an image URL or base64 encoded image as part of the input. PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(OpenAI())response = client.chat.completions.create(  model=""gpt-4-turbo"",  messages=[    {      ""role"": ""user"",      ""content"": [        {""type"": ""text"", ""text"": ""Whats in this image?""},        {          ""type"": ""image_url"",          ""image_url"": {            ""url"": ""https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"",          },        },      ],    }  ],)print(response.choices[0])import OpenAI from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";// Wrap the OpenAI client to automatically log tracesconst wrappedClient = wrapOpenAI(new OpenAI());const response = await wrappedClient.chat.completions.create({    model: ""gpt-4-turbo"",    messages: [      {        role: ""user"",        content: [          { type: ""text"", text: ""Whats in this image?"" },          {            type: ""image_url"",            image_url: {              ""url"": ""https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"",            },          },        ],      },    ],});console.log(response.choices[0]); The image will be rendered as part of the trace in the LangSmith UI."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-pandas-dataframe,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/self_hosting/configuration/external_redis,Connect to an external Redis database,"LangSmith uses Redis to back our queuing/caching operations. By default, LangSmith Self-Hosted will use an internal Redis instance.
However, you can configure LangSmith to use an external Redis instance (strongly recommended in a production setting). By configuring an external Redis instance, you can more easily manage backups, scaling, and other operational tasks for your Redis instance."
https://docs.smith.langchain.com/self_hosting/configuration/external_redis,Requirements,"A provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like:Amazon ElastiCacheGoogle Cloud MemorystoreAzure Cache for RedisNote: We do only officially support Redis versions >= 5.We do not support Redis Cluster. Redis Cluster Not SupportedCertain tiers of managed Redis services may use Redis Cluster under the hood, but you can point to a single node in the cluster.
For example on Azure Cache for Redis, the Premium tier and above use Redis Cluster, so you will need to use a lower tier."
https://docs.smith.langchain.com/self_hosting/configuration/external_redis,Connection String,"We use redis-py to connect to Redis. This library supports a variety of connection strings. You can find more information on the connection string format here. You will need to assemble the connection string for your Redis instance. This connection string should include the following information: HostDatabasePortURL params This will take the form of: ""redis://host:port/db?<url_params>"" An example connection string might look like: ""redis://langsmith-redis:6379/0"" To use SSL, you can use the rediss:// prefix. An example connection string with SSL might look like: ""rediss://langsmith-redis:6380/0?password=foo"""
https://docs.smith.langchain.com/self_hosting/configuration/external_redis,Configuration,"With your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the values file for your LangSmith Helm Chart installation or the .env file for your Docker installation. HelmDocker  redis:    external:      enabled: true      connectionUrl: ""Your connection url""    # In your .env fileREDIS_DATABASE_URI=""Your connection url""     Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Redis instance."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-baseline-experiment,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/how_to_guides/playground,How-to guides: Playground,"This section contains how-to guides related to the LangSmith playground.  Use a custom modelThe LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via LangServe, an open source library for serving LangChain applications. Use custom TLS certificatesThis feature is only available for self-hosted enterprise customers, starting from version 0.6. Save settings configurationWithin the settings of the LangSmith playground, you can save your model configuration for later use."
https://docs.smith.langchain.com/how_to_guides/playground,Use a custom model,"The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via LangServe, an open source library for serving LangChain applications."
https://docs.smith.langchain.com/how_to_guides/playground,Use custom TLS certificates,"This feature is only available for self-hosted enterprise customers, starting from version 0.6."
https://docs.smith.langchain.com/how_to_guides/playground,Save settings configuration,"Within the settings of the LangSmith playground, you can save your model configuration for later use."
https://docs.smith.langchain.com/how_to_guides/tracing/compare_traces,Compare traces,"To compare traces, click on the Compare button in the upper right hand side of any trace view. This will show the trace run table. Select the trace you want to compare against original trace. The pane will open with both traces selected in a side by side comparison view. To stop comparing, close the pane or click on Stop comparing in the upper right hand side of the pane."
https://docs.smith.langchain.com/self_hosting/configuration/external_postgres,Connect to an external Postgres database,"LangSmith uses a Postgres database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal Postgres database.
However, you can configure LangSmith to use an external Postgres database (strongly recommended in a production setting). By configuring an external Postgres database, you can more easily manage backups, scaling, and other operational tasks for your database."
https://docs.smith.langchain.com/self_hosting/configuration/external_postgres,Requirements,"A provisioned Postgres database that your LangSmith instance will have network access to. We recommend using a managed Postgres service like:Amazon RDSGoogle Cloud SQLAzure Database for PostgreSQNote: We only officially support Postgres versions >= 14.A user with admin access to the Postgres database. This user will be used to create the necessary tables, indexes, and schemas.This user will also need to have the ability to create extensions in the database. We use/will try to install the btree_gin, btree_gist, pgcrypto, citext, and pg_trgm extensions.If using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path."
https://docs.smith.langchain.com/self_hosting/configuration/external_postgres,Connection String,"You will need to provide a connection string to your Postgres database. This connection string should include the following information: HostPortDatabaseUsernamePassword(Make sure to url encode this if there are any special characters)URL params This will take the form of: username:password@host:port/database?<url_params> An example connection string might look like: myuser:mypassword@myhost:5432/mydatabase?sslmode=disable Without url parameters, the connection string would look like: myuser:mypassword@myhost:5432/mydatabase"
https://docs.smith.langchain.com/self_hosting/configuration/external_postgres,Configuration,"With your connection string in hand, you can configure your LangSmith instance to use an external Postgres database. You can do this by modifying the values file for your LangSmith Helm Chart installation or the .env file for your Docker installation. HelmDocker  postgres:    external:      enabled: true      connectionUrl: ""Your connection url""    # In your .env filePOSTGRES_DATABASE_URI=""Your connection url""     Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Postgres database."
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#personal-access-tokens-pats,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,Trace withLangGraph(Python and JS/TS),"LangSmith smoothly integrates with LangGraph (Python and JS)
to help you trace agentic workflows, whether you're using LangChain modules or other SDKs."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,With LangChain,"If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing. This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,0. Installation,"Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langgraphyarn add @langchain/openai @langchain/langgraphnpm install @langchain/openai @langchain/langgraphpnpm add @langchain/openai @langchain/langgraph"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,2. Log a trace,"Once you've set up your environment, you can call LangChain runnables as normal.
LangSmith will infer the proper tracing config: PythonTypeScriptfrom typing import Literalfrom langchain_core.messages import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain_core.tools import toolfrom langgraph.graph import StateGraph, MessagesStatefrom langgraph.prebuilt import ToolNode@tooldef search(query: str):    """"""Call to surf the web.""""""    if ""sf"" in query.lower() or ""san francisco"" in query.lower():        return ""It's 60 degrees and foggy.""    return ""It's 90 degrees and sunny.""tools = [search]tool_node = ToolNode(tools)model = ChatOpenAI(model=""gpt-4o"", temperature=0).bind_tools(tools)def should_continue(state: MessagesState) -> Literal[""tools"", ""__end__""]:    messages = state['messages']    last_message = messages[-1]    if last_message.tool_calls:        return ""tools""    return ""__end__""def call_model(state: MessagesState):    messages = state['messages']    # Invoking `model` will automatically infer the correct tracing context    response = model.invoke(messages)    return {""messages"": [response]}workflow = StateGraph(MessagesState)workflow.add_node(""agent"", call_model)workflow.add_node(""tools"", tool_node)workflow.add_edge(""__start__"", ""agent"")workflow.add_conditional_edges(    ""agent"",    should_continue,)workflow.add_edge(""tools"", 'agent')app = workflow.compile()final_state = app.invoke(    {""messages"": [HumanMessage(content=""what is the weather in sf"")]},    config={""configurable"": {""thread_id"": 42}})final_state[""messages""][-1].contentimport { HumanMessage, AIMessage } from ""@langchain/core/messages"";import { tool } from ""@langchain/core/tools"";import { z } from ""zod"";import { ChatOpenAI } from ""@langchain/openai"";import { StateGraph, StateGraphArgs } from ""@langchain/langgraph"";import { ToolNode } from ""@langchain/langgraph/prebuilt"";interface AgentState {  messages: HumanMessage[];}const graphState: StateGraphArgs<AgentState>[""channels""] = {  messages: {    reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),  },};const searchTool = tool(async ({ query }: { query: string }) => {  if (query.toLowerCase().includes(""sf"") || query.toLowerCase().includes(""san francisco"")) {    return ""It's 60 degrees and foggy.""  }  return ""It's 90 degrees and sunny.""}, {  name: ""search"",  description:    ""Call to surf the web."",  schema: z.object({    query: z.string().describe(""The query to use in your search.""),  }),});const tools = [searchTool];const toolNode = new ToolNode<AgentState>(tools);const model = new ChatOpenAI({  model: ""gpt-4o"",  temperature: 0,}).bindTools(tools);function shouldContinue(state: AgentState) {  const messages = state.messages;  const lastMessage = messages[messages.length - 1] as AIMessage;  if (lastMessage.tool_calls?.length) {    return ""tools"";  }  return ""__end__"";}async function callModel(state: AgentState) {  const messages = state.messages;  // Invoking `model` will automatically infer the correct tracing context  const response = await model.invoke(messages);  return { messages: [response] };}const workflow = new StateGraph<AgentState>({ channels: graphState })  .addNode(""agent"", callModel)  .addNode(""tools"", toolNode)  .addEdge(""__start__"", ""agent"")  .addConditionalEdges(""agent"", shouldContinue)  .addEdge(""tools"", ""agent"");const app = workflow.compile();const finalState = await app.invoke(  { messages: [new HumanMessage(""what is the weather in sf"")] },  { configurable: { thread_id: ""42"" } });finalState.messages[finalState.messages.length - 1].content; An example trace from running the above code looks like this:"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,Without LangChain,"If you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately
(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).
If you do so, LangSmith will automatically nest traces from those wrapped methods. Here's an example. You can also see this page for more information."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,0. Installation,Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below). pipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#with-langchain,2. Log a trace,"Once you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.
LangSmith will then infer the proper tracing config: PythonTypeScriptimport jsonimport openaiimport operatorfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaifrom typing import Annotated, Literal, TypedDictfrom langgraph.graph import StateGraphclass State(TypedDict):    messages: Annotated[list, operator.add]tool_schema = {    ""type"": ""function"",    ""function"": {        ""name"": ""search"",        ""description"": ""Call to surf the web."",        ""parameters"": {            ""type"": ""object"",            ""properties"": {""query"": {""type"": ""string""}},            ""required"": [""query""],        },    },}# Decorating the tool function will automatically trace it with the correct context@traceable(run_type=""tool"", name=""Search Tool"")def search(query: str):    """"""Call to surf the web.""""""    if ""sf"" in query.lower() or ""san francisco"" in query.lower():        return ""It's 60 degrees and foggy.""    return ""It's 90 degrees and sunny.""tools = [search]def call_tools(state):    function_name_to_function = {""search"": search}    messages = state[""messages""]    tool_call = messages[-1][""tool_calls""][0]    function_name = tool_call[""function""][""name""]    function_arguments = tool_call[""function""][""arguments""]    arguments = json.loads(function_arguments)    function_response = function_name_to_function[function_name](**arguments)    tool_message = {        ""tool_call_id"": tool_call[""id""],        ""role"": ""tool"",        ""name"": function_name,        ""content"": function_response,    }    return {""messages"": [tool_message]}wrapped_client = wrap_openai(openai.Client())def should_continue(state: State) -> Literal[""tools"", ""__end__""]:    messages = state[""messages""]    last_message = messages[-1]    if last_message[""tool_calls""]:        return ""tools""    return ""__end__""def call_model(state: State):    messages = state[""messages""]    # Calling the wrapped client will automatically infer the correct tracing context    response = wrapped_client.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", tools=[tool_schema]    )    raw_tool_calls = response.choices[0].message.tool_calls    tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []    response_message = {        ""role"": ""assistant"",        ""content"": response.choices[0].message.content,        ""tool_calls"": tool_calls,    }    return {""messages"": [response_message]}workflow = StateGraph(State)workflow.add_node(""agent"", call_model)workflow.add_node(""tools"", call_tools)workflow.add_edge(""__start__"", ""agent"")workflow.add_conditional_edges(    ""agent"",    should_continue,)workflow.add_edge(""tools"", 'agent')app = workflow.compile()final_state = app.invoke(    {""messages"": [{""role"": ""user"", ""content"": ""what is the weather in sf""}]})final_state[""messages""][-1][""content""]Note: The below example requires langsmith>=0.1.39 and @langchain/langgraph>=0.0.31
import OpenAI from ""openai"";import { StateGraph } from ""@langchain/langgraph"";import { wrapOpenAI } from ""langsmith/wrappers/openai"";import { traceable } from ""langsmith/traceable"";type GraphState = {  messages: OpenAI.ChatCompletionMessageParam[];};const wrappedClient = wrapOpenAI(new OpenAI({}));const toolSchema: OpenAI.ChatCompletionTool = {  type: ""function"",  function: {    name: ""search"",    description: ""Use this tool to query the web."",    parameters: {      type: ""object"",      properties: {        query: {          type: ""string"",        },      },      required: [""query""],    }  }};// Wrapping the tool function will automatically trace it with the correct contextconst search = traceable(async ({ query }: { query: string }) => {  if (    query.toLowerCase().includes(""sf"") ||    query.toLowerCase().includes(""san francisco"")  ) {    return ""It's 60 degrees and foggy."";  }  return ""It's 90 degrees and sunny.""}, { run_type: ""tool"", name: ""Search Tool"" });const callTools = async ({ messages }: GraphState) => {  const mostRecentMessage = messages[messages.length - 1];  const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;  if (toolCalls === undefined || toolCalls.length === 0) {    throw new Error(""No tool calls passed to node."");  }  const toolNameMap = {    search,  };  const functionName = toolCalls[0].function.name;  const functionArguments = JSON.parse(toolCalls[0].function.arguments);  const response = await toolNameMap[functionName](functionArguments);  const toolMessage = {    tool_call_id: toolCalls[0].id,    role: ""tool"",    name: functionName,    content: response,  }  return { messages: [toolMessage] };}const callModel = async ({ messages }: GraphState) => {  // Calling the wrapped client will automatically infer the correct tracing context  const response = await wrappedClient.chat.completions.create({    messages,    model: ""gpt-4o-mini"",    tools: [toolSchema],  });  const responseMessage = {    role: ""assistant"",    content: response.choices[0].message.content,    tool_calls: response.choices[0].message.tool_calls ?? [],  };  return { messages: [responseMessage] };}const shouldContinue = ({ messages }: GraphState) => {  const lastMessage =    messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;  if (    lastMessage?.tool_calls !== undefined &&    lastMessage?.tool_calls.length > 0  ) {    return ""tools"";  }  return ""__end__"";}const workflow = new StateGraph<GraphState>({  channels: {    messages: {      reducer: (a: any, b: any) => a.concat(b),    }  }});const graph = workflow  .addNode(""model"", callModel)  .addNode(""tools"", callTools)  .addEdge(""__start__"", ""model"")  .addConditionalEdges(""model"", shouldContinue, {    tools: ""tools"",    __end__: ""__end__"",  })  .addEdge(""tools"", ""model"")  .compile();await graph.invoke({  messages: [{ role: ""user"", content: ""what is the weather in sf"" }]}); An example trace from running the above code looks like this:"
https://docs.smith.langchain.com/how_to_guides/setup/update_business_info,"Update invoice email, tax id and, business information","To update business information for your LangSmith organization, head to the Usage and Billing Plans and Billing noteBusiness information, tax id and invoice email can only be updated for the plus and start up plans. Free and developer plans cannot update this information."
https://docs.smith.langchain.com/how_to_guides/setup/update_business_info,Update invoice email,"To update the email address where your invoices are sent, follow these steps: Navigate to the Plans and Billing tab.Locate the section beneath the payment method, where the current invoice email is displayed.Enter the new email address you want invoices to be sent to in the provided field.The new email address will be automatically saved. This ensures that all future invoices will be sent to the updated email address."
https://docs.smith.langchain.com/how_to_guides/setup/update_business_info,Update business information and tax id,"noteIn certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your Tax ID may qualify you for a sales tax exemption. To update your organization's business information, follow these steps: Navigate to the Plans and Billing tab.Below the invoice email section, you will find a checkbox labeled Business.Check the Business checkbox if your organization belongs to a business.A business information section will appear, allowing you to enter or update the following details:Business NameAddressTax ID for applicable jurisdictionsTax ID field will appear for applicable jurisdictions after you select a country.After entering the necessary information, click the Save button to save your changes. This ensures that your business information is up-to-date and accurate for billing and tax purposes."
https://docs.smith.langchain.com/how_to_guides/setup,How-to guides: Setup,"This section contains how-to guides related to setting up LangSmith.  Create an account and API keyCreate an account Set up an organizationBefore diving into this content, it might be helpful to read the following: Set up a workspaceWorkspaces will be incrementally rolled out being week of June 10, 2024. Set up billing for your LangSmith accountIf you are interested in the Enterprise plan, please contact sales. This guide is Update invoice email, tax id and, business informationTo update business information for your LangSmith organization, head to the Set up access controlRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev"
https://docs.smith.langchain.com/how_to_guides/setup,Create an account and API key,Create an account
https://docs.smith.langchain.com/how_to_guides/setup,Set up an organization,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/setup,Set up a workspace,"Workspaces will be incrementally rolled out being week of June 10, 2024."
https://docs.smith.langchain.com/how_to_guides/setup,Set up billing for your LangSmith account,"If you are interested in the Enterprise plan, please contact sales. This guide is"
https://docs.smith.langchain.com/how_to_guides/setup,"Update invoice email, tax id and, business information","To update business information for your LangSmith organization, head to the"
https://docs.smith.langchain.com/how_to_guides/setup,Set up access control,"RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev"
https://docs.smith.langchain.com/self_hosting/architectural_overview,Architectural overview,"Enterprise License RequiredSelf-Hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith can be run via Kubernetes (recommended) or Docker in a Cloud environment that you control. The LangSmith application consists of several components including 5 LangSmith servers and 3 stateful services: LangSmith FrontendLangSmith BackendLangSmith Platform BackendLangSmith PlaygroundLangSmith QueueClickHousePostgresRedis To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine."
https://docs.smith.langchain.com/self_hosting/architectural_overview,Storage Services,"noteLangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services.
In a production setting, we strongly recommend using external Storage Services."
https://docs.smith.langchain.com/self_hosting/architectural_overview,ClickHouse,"ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data)."
https://docs.smith.langchain.com/self_hosting/architectural_overview,PostgreSQL,"PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback)."
https://docs.smith.langchain.com/self_hosting/architectural_overview,Redis,"Redis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching. LangSmith uses Redis to back queuing/caching operations."
https://docs.smith.langchain.com/self_hosting/architectural_overview,LangSmith Frontend,The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.
https://docs.smith.langchain.com/self_hosting/architectural_overview,LangSmith Backend,"The backend is the primary entrypoint for API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and sdk, preparing traces for ingestion, and supporting the hub API."
https://docs.smith.langchain.com/self_hosting/architectural_overview,LangSmith Queue,"The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database."
https://docs.smith.langchain.com/self_hosting/architectural_overview,LangSmith Platform Backend,The platform backend is an internal service that primarily handles authentication and other high-volume tasks. The user should not need to interact with this service directly.
https://docs.smith.langchain.com/self_hosting/architectural_overview,LangSmith Playground,The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_on_intermediate_steps,Evaluate on intermediate steps,"While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline. For example, for retrieval-augmented generation (RAG), you might want to Evaluate the retrieval step to ensure that the correct documents are retrieved w.r.t the input query.Evaluate the generation step to ensure that the correct answer is generated w.r.t the retrieved documents. In this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios. In order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the root_run/rootRun argument, which is a Run object that contains the intermediate steps of your pipeline."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_on_intermediate_steps,1. Define your LLM pipeline,"The below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents. PythonTypeScriptimport openaiimport wikipedia as wpfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai = wrap_openai(openai.Client())@traceabledef generate_wiki_search(question):    messages = [        {""role"": ""system"", ""content"": ""Generate a search query to pass into wikipedia to answer the user's question. Return only the search query and nothing more. This will passed in directly to the wikipedia search engine.""},        {""role"": ""user"", ""content"": question}    ]    result = openai.chat.completions.create(messages=messages, model=""gpt-3.5-turbo"", temperature=0)    return result.choices[0].message.content@traceable(run_type=""retriever"")def retrieve(query):    results = []    for term in wp.search(query, results = 10):        try:            page = wp.page(term, auto_suggest=False)            results.append({                ""page_content"": page.summary,                ""type"": ""Document"",                ""metadata"": {""url"": page.url}            })        except wp.DisambiguationError:            pass        if len(results) >= 2:            return results@traceabledef generate_answer(question, context):    messages = [        {""role"": ""system"", ""content"": f""Answer the user's question based ONLY on the content below:\n\n{context}""},        {""role"": ""user"", ""content"": question}    ]    result = openai.chat.completions.create(messages=messages, model=""gpt-3.5-turbo"", temperature=0)    return result.choices[0].message.content@traceabledef rag_pipeline(question):    query = generate_wiki_search(question)    context = ""\n\n"".join([doc[""page_content""] for doc in retrieve(query)])    answer = generate_answer(question, context)    return answerimport OpenAI from ""openai"";import wiki from ""wikipedia"";import { Client } from ""langsmith"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const openai = wrapOpenAI(new OpenAI());const generateWikiSearch = traceable(  async (input: { question: string }) => {    const messages = [      {        role: ""system"" as const,        content:          ""Generate a search query to pass into Wikipedia to answer the user's question. Return only the search query and nothing more. This will be passed in directly to the Wikipedia search engine."",      },      { role: ""user"" as const, content: input.question },    ];        const chatCompletion = await openai.chat.completions.create({      model: ""gpt-3.5-turbo"",      messages: messages,      temperature: 0,    });        return chatCompletion.choices[0].message.content ?? """";  },  { name: ""generateWikiSearch"" });const retrieve = traceable(  async (input: { query: string; numDocuments: number }) => {    const { results } = await wiki.search(input.query, { limit: 10 });    const finalResults: Array<{      page_content: string;      type: ""Document"";      metadata: { url: string };    }> = [];      for (const result of results) {      if (finalResults.length >= input.numDocuments) {        // Just return the top 2 pages for now        break;      }      const page = await wiki.page(result.title, { autoSuggest: false });      const summary = await page.summary();      finalResults.push({        page_content: summary.extract,        type: ""Document"",        metadata: { url: page.fullurl },      });    }    return finalResults;  },  { name: ""retrieve"", run_type: ""retriever"" });const generateAnswer = traceable(  async (input: { question: string; context: string }) => {    const messages = [      {        role: ""system"" as const,        content: `Answer the user's question based only on the content below:\n\n${input.context}`,      },      { role: ""user"" as const, content: input.question },    ];      const chatCompletion = await openai.chat.completions.create({      model: ""gpt-3.5-turbo"",      messages: messages,      temperature: 0,    });    return chatCompletion.choices[0].message.content ?? """";  },  { name: ""generateAnswer"" });const ragPipeline = traceable(  async ({ question }: { question: string }, numDocuments: number = 2) => {    const query = await generateWikiSearch({ question });    const retrieverResults = await retrieve({ query, numDocuments });    const context = retrieverResults      .map((result) => result.page_content)      .join(""\n\n"");    const answer = await generateAnswer({ question, context });    return answer;  },  { name: ""ragPipeline"" }); This pipeline will produce a trace that looks something like:
"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_on_intermediate_steps,2. Create a dataset and examples to evaluate the pipeline,"We are building a very simple dataset with a couple of examples to evaluate the pipeline. PythonTypeScriptfrom langsmith import Clientclient = Client()examples = [    (""What is LangChain?"", ""LangChain is an open-source framework for building applications using large language models.""),    (""What is LangSmith?"", ""LangSmith is an observability and evaluation tool for LLM products, built by LangChain Inc."")]dataset_name = ""Wikipedia RAG""if not client.has_dataset(dataset_name=dataset_name):    dataset = client.create_dataset(dataset_name=dataset_name)    inputs, outputs = zip(        *[({""input"": input}, {""expected"": expected}) for input, expected in examples]    )    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const client = new Client();const examples = [  [    ""What is LangChain?"",    ""LangChain is an open-source framework for building applications using large language models."",  ],  [    ""What is LangSmith?"",    ""LangSmith is an observability and evaluation tool for LLM products, built by LangChain Inc."",  ],];const datasetName = ""Wikipedia RAG"";const inputs = examples.map(([input, _]) => ({ input }));const outputs = examples.map(([_, expected]) => ({ expected }));const dataset = await client.createDataset(datasetName);await client.createExamples({ datasetId: dataset.id, inputs, outputs });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_on_intermediate_steps,3. Define your custom evaluators,"As mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w.r.t the input query and another that evaluates the hallucination of the generated answer w.r.t the retrieved documents.
We will be using LangChain LLM wrappers, along with with_structured_output to define the evaluator for hallucination. The key here is that the evaluator function should traverse the root_run / rootRun argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria. PythonTypeScriptfrom langsmith.evaluation import LangChainStringEvaluator, evaluatefrom langsmith.schemas import Example, Runfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.pydantic_v1 import BaseModel, Fielddef document_relevance(root_run: Run, example: Example) -> dict:    """"""    A very simple evaluator that checks to see if the input of the retrieval step exists    in the retrieved docs.    """"""    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == ""rag_pipeline"")    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == ""retrieve"")    page_contents = ""\n\n"".join(doc[""page_content""] for doc in retrieve_run.outputs[""output""])    score = retrieve_run.inputs[""query""] in page_contents    return {""key"": ""simple_document_relevance"", ""score"": score}def hallucination(root_run: Run, example: Example) -> dict:    """"""    A simple evaluator that checks to see the answer is grounded in the documents    """"""    # Get documents and answer    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == ""rag_pipeline"")    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == ""retrieve"")    page_contents = ""\n\n"".join(doc[""page_content""] for doc in retrieve_run.outputs[""output""])    generation = rag_pipeline_run.outputs[""output""]    # Data model    class GradeHallucinations(BaseModel):        """"""Binary score for hallucination present in generation answer.""""""        binary_score: int = Field(description=""Answer is grounded in the facts, 1 or 0"")    # LLM with function call    llm = ChatOpenAI(model=""gpt-3.5-turbo-0125"", temperature=0)    structured_llm_grader = llm.with_structured_output(GradeHallucinations)    # Prompt    system = """"""You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n        Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.""""""    hallucination_prompt = ChatPromptTemplate.from_messages(        [            (""system"", system),            (""human"", ""Set of facts: \n\n {documents} \n\n LLM generation: {generation}""),        ]    )    hallucination_grader = hallucination_prompt | structured_llm_grader    score = hallucination_grader.invoke({""documents"": page_contents, ""generation"": generation})    return {""key"": ""answer_hallucination"", ""score"": int(score.binary_score)}import { EvaluationResult } from ""langsmith/evaluation"";import { Run, Example } from ""langsmith/schemas"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { ChatOpenAI } from ""@langchain/openai"";import { z } from ""zod"";function findNestedRun(run: Run, search: (run: Run) => boolean): Run | null {  const queue: Run[] = [run];  while (queue.length > 0) {    const currentRun = queue.shift()!;    if (search(currentRun)) return currentRun;    queue.push(...currentRun.child_runs);  }  return null;}// A very simple evaluator that checks to see if the input of the retrieval step exists// in the retrieved docs.function documentRelevance(rootRun: Run, example: Example): EvaluationResult {  const retrieveRun = findNestedRun(rootRun, (run) => run.name === ""retrieve"");  const docs: Array<{ page_content: string }> | undefined =    retrieveRun.outputs?.outputs;    const pageContents = docs?.map((doc) => doc.page_content).join(""\n\n"");  const score = pageContents.includes(retrieveRun.inputs?.query);  return { key: ""simple_document_relevance"", score };}async function hallucination(  rootRun: Run,  example: Example): Promise<EvaluationResult> {  const rag = findNestedRun(rootRun, (run) => run.name === ""ragPipeline"");  const retrieve = findNestedRun(rootRun, (run) => run.name === ""retrieve"");    const docs: Array<{ page_content: string }> | undefined =    retrieve.outputs?.outputs;    const documents = docs?.map((doc) => doc.page_content).join(""\n\n"");    const prompt = ChatPromptTemplate.fromMessages<{    documents: string;    generation: string;  }>([    [      ""system"",      [        `You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n`,        `Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.`,      ].join(""\n""),    ],    [      ""human"",      ""Set of facts: \n\n {documents} \n\n LLM generation: {generation}"",    ],  ]);    const llm = new ChatOpenAI({    model: ""gpt-3.5-turbo-0125"",    temperature: 0,  }).withStructuredOutput(    z      .object({        binary_score: z          .number()          .describe(""Answer is grounded in the facts, 1 or 0""),      })      .describe(""Binary score for hallucination present in generation answer."")  );    const grader = prompt.pipe(llm);  const score = await grader.invoke({    documents,    generation: rag.outputs?.outputs,  });    return { key: ""answer_hallucination"", score: score.binary_score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_on_intermediate_steps,4. Evaluate the pipeline,"Finally, we'll run evaluate with the custom evaluators defined above. PythonTypeScriptfrom langsmith.evaluation import evaluateexperiment_results = evaluate(    lambda inputs: rag_pipeline(inputs[""input""]),    data=dataset_name,    evaluators=[document_relevance, hallucination],    experiment_prefix=""rag-wiki-oai"")import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => ragPipeline({ question: inputs.input }), {  data: datasetName,  evaluators: [hallucination, documentRelevance],  experimentPrefix: ""rag-wiki-oai"",}); The experiment will contain the results of the evaluation, including the scores and comments from the evaluators:
"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-input--output-key-value-pairs,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-the-comparison-view,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/self_hosting/release_notes,LangSmith Release Notes,noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide.
https://docs.smith.langchain.com/self_hosting/release_notes,"Week of June 17, 2024 - LangSmith v0.6","LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."
https://docs.smith.langchain.com/self_hosting/release_notes,New Features,Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More...
https://docs.smith.langchain.com/self_hosting/release_notes,Performance and Reliability Changes,Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK
https://docs.smith.langchain.com/self_hosting/release_notes,Infrastructure changes,"[Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."
https://docs.smith.langchain.com/self_hosting/release_notes,Admin changes,Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details.
https://docs.smith.langchain.com/self_hosting/release_notes,Deprecation notices,With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes,"Week of May 13, 2024 - LangSmith v0.5","LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."
https://docs.smith.langchain.com/self_hosting/release_notes,Breaking changes,We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.
https://docs.smith.langchain.com/self_hosting/release_notes,New Features,Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/
https://docs.smith.langchain.com/self_hosting/release_notes,Performance and Reliability Changes,"Split ingest, session deletion, and automation jobs to execute within separate resource pools."
https://docs.smith.langchain.com/self_hosting/release_notes,Infrastructure changes,"As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."
https://docs.smith.langchain.com/self_hosting/release_notes,Admin changes,Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.
https://docs.smith.langchain.com/self_hosting/release_notes,Deprecation notices,With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes,"Week of March 25, 2024 - LangSmith v0.4","LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."
https://docs.smith.langchain.com/self_hosting/release_notes,Breaking changes,"This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."
https://docs.smith.langchain.com/self_hosting/release_notes,Performance and Reliability Changes,"Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes,Infrastructure changes,"Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."
https://docs.smith.langchain.com/self_hosting/release_notes,Admin changes,Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md
https://docs.smith.langchain.com/self_hosting/release_notes,Deprecation notices,With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes,"Week of Februrary 21, 2024 - LangSmith v0.3","LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."
https://docs.smith.langchain.com/self_hosting/release_notes,Breaking changes,"This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"
https://docs.smith.langchain.com/self_hosting/release_notes,Performance and Reliability Changes,"Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes,Admin changes,None
https://docs.smith.langchain.com/self_hosting/release_notes,Deprecation notices,With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes,"Week of January 29, 2024 - LangSmith v0.2","LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."
https://docs.smith.langchain.com/self_hosting/release_notes,Requirements,This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.
https://docs.smith.langchain.com/self_hosting/release_notes,Breaking changes,"The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {""user_id"": ..., ""user_name"":...,}) and then search using has(metadata, '{""user_name"": ...}')"
https://docs.smith.langchain.com/self_hosting/release_notes,Performance and Reliability Changes,"Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."
https://docs.smith.langchain.com/self_hosting/release_notes,Infrastructure Changes,"Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"
https://docs.smith.langchain.com/self_hosting/release_notes,Admin changes,Increased the maximum number of users per organization from 5 to 100 for new organizations.
https://docs.smith.langchain.com/self_hosting/release_notes,Deprecation notices,With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,Set up automation rules,"While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users.
LangSmith provides a powerful feature called automations that allow you to trigger certain actions on your trace data.
At a high level, automations are defined by a filter, sampling rate, and action. Automation rules can trigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an annotation queue, and triggering a webhook. An example of an automation you can set up can be ""trigger an online evaluation that grades on vagueness for all of my downvoted traces."""
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,Create a rule,We will outline the steps for creating an automation rule in LangSmith below.
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,Step 1: Navigate to rule creation,"To create a rule, head click on Rules in the top right corner of any project details page, then scroll to the bottom and click on + Add Rule. Alternatively, you can access rules in settings by navigating to this link, click on + Add Rule, then Project Rule. noteThere are currently two types of rules you can create: Project Rule and Dataset Rule.Project Rule: This rule will apply to traces in the specified project. Actions allowed are adding to a dataset, adding to an annotation queue, running online evaluation, and triggering a webhook.Dataset Rule: This rule will apply to traces that are part of an experiment in the specified dataset. Actions allowed are only running an evaluator on the experiment results. To see this in action, you can follow this guide. Give your rule a name, for example ""my_rule"":"
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,Step 2: Define the filter,"You can create a filter as you normally would to filter traces in the project. For more information on filters, you can refer to this guide."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,(Optional) Step 3: Apply Rule to Past Runs,"When creating a new rule, you can apply the rule to past runs as well. To do this, select Apply to Past Runs checkbox and enter Backfill From date as the
start date to apply the rule. This will start from the Backfill From date and apply the run rules until it is caught up with the latest runs. Note that you will have to expand the date range for logs if you wanted to look at the progress of the backfill, see View logs for your automations for details."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,Step 4: Define the sampling rate,"You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,Step 5: Define the action,"There are four actions you can take with an automation rule: Add to dataset: Add the inputs and outputs of the trace to a dataset.Add to annotation queue: Add the trace to an annotation queue.Run online evaluation: Run an online evaluation on the trace. For more information on online evaluations, you can refer to this guide.Trigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.Extend data retention: Extends the data retention period on matching traces that use base retention.
Note that all other rules will also extend data retention on matching traces through the
auto-upgrade mechanism,
but this rule takes no additional action."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules,View logs for your automations,"You can view logs for your automations by going to Settings -> Rules and click on the Logs button in any row. You can also get to logs by clicking on Rules in the top right hand corner of any project details page, then clicking on See Logs for any rule. Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced.
If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon. By default, rule logs only show results for runs that occurred in the last day. To see results for older runs, you can select Last 1 day and enter the desired date range.
When applying a rule to past runs, the processing will start from the start date and go forward, so this would be needed to view logs while the backfill is proceeding."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Optimize tracing spend on LangSmith,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Data Retention Conceptual DocsUsage Limiting Conceptual Docs noteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its
custom nature of billing. If you are on Enterprise plan and have questions about cost optimization,
please reach out to your sales rep or support@langchain.dev. This tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend
and prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.
Concepts can be transferred to your own organization."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Problem Setup,"In this tutorial, we take an existing organization that has three workspaces, one for each deployment stage
(Dev, Staging, and Prod):"
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Understand your current usage,"The first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph
and Invoices."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Usage Graph,"The usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show
spend (which we will see later on our draft invoice). We can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph. We see in the graph above that there are two usage metrics that LangSmith charges for: LangSmith Traces (Base Charge)LangSmith Traces (Extended Data Retention Upgrades). The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.
For more details, see our data retention conceptual docs. Notice that these graphs look
identical, which will come into play later in the tutorial. LangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),
or teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In
this case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings. noteLangSmith's Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable. In the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can
go to Settings -> Workspaces, and hover our mouse over the Workspace ID button to find the one with a matching ID. In
this case, it's the Prod workspace:"
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Invoices,"We understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so,
we head to the Invoices tab. The first invoice that will appear on screen is a draft of your current month's
invoice, which shows your running spend thus far this month. In the above GIF, we see that the charges for LangSmith Traces are broken up by ""tenant_id"" (i.e. Workspace ID), meaning we can track tracing spend
on each of our workspaces. In the first few days of June, the vast majority of the total spend of ~$2,000 is in our production
workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades. These upgrades occur for two reasons: You use extended data retention tracing, meaning that, by default, your traces are retained for 400 daysYou use base data retention tracing, and use a feature that automatically extends the data retention of a trace (see our Auto-Upgrade conceptual docs) Given that the number of total traces per day is equal to the number of extended retention traces per day, it's most likely the
case that this org is using extended data retention tracing everywhere. As such, we start by optimizing our retention settings."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Optimization 1: manage data retention,"LangSmith charges differently based on a trace's data retention (see our data retention conceptual docs),
where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, we will
show how to get optimal settings for data retention without sacrificing historical observability, and
show the effect it has on our bill."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Change org level retention defaults for new projects,"We navigate to the Usage configuration tab, and look at our organization level retention settings. Modifying this setting affects all new projects that are
created going forward in all workspaces in our org. noteFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd
have this defaulted to Base."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Change project level retention defaults,"Our existing projects have not changed their data retention settings, so we can change these on the individual project pages. We navigate to Projects -> <your project name>, click the data retention drop down, and modify it to base retention. As
with the organization level setting, this will only affect retention (and pricing) for traces going forward."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Keep around a percentage of traces for extended data retention,"We may not want all our traces to expire after 14 days if we care about historical debugging. As such, we can take advantage
of LangSmith's built in ability to do server side sampling for extended data retention. Choosing the right percentage of runs to sample depends on your use case. We will arbitrarily pick 10% of runs here, but will
leave it to the user to find the right value that balances collecting rare events and cost constraints. LangSmith automatically upgrades the data retention for any trace that matches a run rule in our automations product (see our run rules docs). On the
projects page, click Rules -> Add Rule, and configure the rule as follows: Run rules match on runs rather than traces. Runs are single units of work within an LLM application's API handling. Traces
are end to end API calls (learn more about tracing concepts in LangSmith). This means a trace can
be thought of as a tree of runs making up an API call. When a run rule matches any run within a trace, the trace's full run tree
upgrades to be retained for 400 days. Therefore, to make sure we have the proper sampling rate on traces, we take advantage of the
filtering functionality of run rules. We add add a filter condition to only match the ""root"" run in the run tree. This is distinct per trace, so our 10% sampling
will upgrade 10% of traces, rather 10% of runs, which could correspond to more than 10% of traces. If desired, we can optionally add
any other filtering conditions required (e.g. specific tags/metadata attached to our traces) for more pointed data retention
extension. For the sake of this tutorial, we will stick with the simplest condition, and leave more advanced filtering as an
exercise to the user. noteIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run
rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset),
and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,See results after 7 days,"While the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. This translates to the invoice, where we've only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.
That's a cost reduction of nearly 75% per day!"
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Optimization 2: limit usage,"In the previous section, we managed data retention settings to optimize existing spend. In this section, we will
use usage limits to prevent future overspend. LangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we've
been tracking on our usage graph. We can use these in tandem to have granular control over spend. To set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the
bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along
with a cost estimate: Lets start by setting limits on our production usage, since that is where the majority of spend comes from."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Setting a good total traces limit,"Picking the right ""total traces"" limit depends on the expected load of traces that you will send to LangSmith. You should
clearly think about your assumptions before setting a limit. For example: Current Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,
meaning we log around 100,000-130,000 traces per dayExpected Growth in Load: We expect to double in size in the near future. From these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of: limit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month We click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows: noteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Cutting maximum spend with an extended data retention limit,"If we are not a big enterprise, we may shudder at the ~$40k per month bill. We saw from Optimization 1 that the easiest way to cut cost was through managing data retention.
The same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum
high retention traces we can keep. That would be .10 * 7,800,000 = 780,000. As we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive
data retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon. noteThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to
use this feature, please read more about its functionality here."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Set dev/staging limits and view total spent limit across workspaces,"Following the same logic for our dev and staging environments, whe set limits at 10% of the production
limit on usage for each workspace. While this works with our usage pattern, setting good dev and staging limits may vary depending on
your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may
want to be more liberal with your usage limits to avoid test failures. Now that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces: With this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month."
https://docs.smith.langchain.com/tutorials/Administrators/manage_spend,Summary,"In this tutorial, we learned how to: Cut down our existing costs with data retention policiesPrevent future overspend with usage limits If you have questions about further optimizing your spend, please reach out to support@langchain.dev."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-runtree-api,Annotate code for tracing,"There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-runtree-api,Use@traceable/traceable,"LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.
Note that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.
import { traceable } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: ""formatPrompt"" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-runtree-api,Wrap the OpenAI client,"The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!
The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceable(name=""Chat Pipeline"")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(""Can you summarize this morning's meetings?"")import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", run_type: ""tool"" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: ""system"",            content:                ""You are a helpful assistant. Please respond to the user's request only based on the given context."",        },        { role: ""user"", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: ""Chat Pipeline"" });await chatPipeline(""Can you summarize this morning's meetings?"");"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-runtree-api,Use theRunTreeAPI,"Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually
create runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not
necessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# Create a top-level runpipeline = RunTree(    name=""Chat Pipeline"",    run_type=""chain"",    inputs={""question"": question})# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}]# Create a child runchild_llm_run = pipeline.create_child(    name=""OpenAI Call"",    run_type=""llm"",    inputs={""messages"": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={""answer"": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from ""openai"";import { RunTree } from ""langsmith"";// This can be a user input to your appconst question = ""Can you summarize this morning's meetings?"";const pipeline = new RunTree({    name: ""Chat Pipeline"",    run_type: ""chain"",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = ""During this morning's meeting, we solved all world conflict."";const messages = [    { role: ""system"", content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { role: ""user"", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: ""OpenAI Call"",    run_type: ""llm"",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-the-runtree-api,Use thetracecontext manager (Python only),"In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {""input"": ""Can you summarize this morning's meetings?""}with trace(""Chat Pipeline"", ""chain"", project_name=""my_test"", inputs=app_inputs) as rt:    output = chat_pipeline(""Can you summarize this morning's meetings?"")    rt.end(outputs={""output"": output})"
https://docs.smith.langchain.com/concepts/tracing,Tracing,"This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"
https://docs.smith.langchain.com/concepts/tracing,Runs,"A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing,Traces,"A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
"
https://docs.smith.langchain.com/concepts/tracing,Projects,"A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.
"
https://docs.smith.langchain.com/concepts/tracing,Feedback,"Feedback allows you to score an individual run based on certain criteria.
Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.
Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing,Tags,"Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces
"
https://docs.smith.langchain.com/concepts/tracing,Metadata,"Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.
Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces
"
https://docs.smith.langchain.com/concepts/tracing,Data storage and retention,"For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/concepts/tracing,Deleting traces from LangSmith,"If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the ""Delete"" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."
https://docs.smith.langchain.com/how_to_guides/evaluation,How-to guides: Evaluation,"This section contains how-to guides related to evaluation.  Evaluate an LLM ApplicationBefore diving into this content, it might be helpful to read the following: Bind an evaluator to a dataset in the UIWhile you can specify evaluators to grade the results of your experiments programmatically (see this guide for more information), you can also bind evaluators to a dataset in the UI. Run an evaluation from the prompt playgroundWhile you can kick off experiments easily using the sdk, as outlined here, it's often useful to run experiments directly in the prompt playground. Evaluate on intermediate stepsWhile, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline. Use LangChain off-the-shelf evaluators (Python only)Before diving into this content, it might be helpful to read the following: Compare experiment resultsOftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. Evaluate an existing experimentCurrently, evaluate_existing is only supported in the Python SDK. Test LLM applications (Python only)LangSmith functional tests are assertions and expectations designed to quickly identify obvious bugs and regressions in your AI system. Relative to evaluations, tests typically are designed to be fast and cheap to run, focusing on specific functionality and edge cases. Run pairwise evaluationsBefore diving into this content, it might be helpful to read the following: Audit evaluator scoresLLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK. Create few-shot evaluatorsUsing LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary Fetch performance metrics for an experimentTracing projects and experiments use the same underlying data structure in our backend, which is called a ""session."" Run evals with the REST APIBefore diving into this content, it might be helpful to read the following: Upload experiments run outside of LangSmith with the REST APISome users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint."
https://docs.smith.langchain.com/how_to_guides/evaluation,Evaluate an LLM Application,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/evaluation,Bind an evaluator to a dataset in the UI,"While you can specify evaluators to grade the results of your experiments programmatically (see this guide for more information), you can also bind evaluators to a dataset in the UI."
https://docs.smith.langchain.com/how_to_guides/evaluation,Run an evaluation from the prompt playground,"While you can kick off experiments easily using the sdk, as outlined here, it's often useful to run experiments directly in the prompt playground."
https://docs.smith.langchain.com/how_to_guides/evaluation,Evaluate on intermediate steps,"While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline."
https://docs.smith.langchain.com/how_to_guides/evaluation,Use LangChain off-the-shelf evaluators (Python only),"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/evaluation,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation,Evaluate an existing experiment,"Currently, evaluate_existing is only supported in the Python SDK."
https://docs.smith.langchain.com/how_to_guides/evaluation,Test LLM applications (Python only),"LangSmith functional tests are assertions and expectations designed to quickly identify obvious bugs and regressions in your AI system. Relative to evaluations, tests typically are designed to be fast and cheap to run, focusing on specific functionality and edge cases."
https://docs.smith.langchain.com/how_to_guides/evaluation,Run pairwise evaluations,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/evaluation,Audit evaluator scores,"LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."
https://docs.smith.langchain.com/how_to_guides/evaluation,Create few-shot evaluators,"Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary"
https://docs.smith.langchain.com/how_to_guides/evaluation,Fetch performance metrics for an experiment,"Tracing projects and experiments use the same underlying data structure in our backend, which is called a ""session."""
https://docs.smith.langchain.com/how_to_guides/evaluation,Run evals with the REST API,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/evaluation,Upload experiments run outside of LangSmith with the REST API,"Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint."
https://docs.smith.langchain.com/how_to_guides/tracing/calculate_token_based_costs,Calculate token-based costs for traces,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Providing token counts for LLM runs (spans) LangSmith allows you to track costs for traces based on the number of tokens used for LLM invocations.
The costs are rolled up to the trace level and project level. For LangSmith to accurately calculate token-based costs, you need to provide the token counts for each LLM invocation in the trace, along with sending up ls_provider and ls_model_name in the run metadata. If you are using the LangSmith Python or TS/JS SDK, you should carefully read through the this guide.If you are using LangChain Python or TS/JS, ls_provider and ls_model_name along with token counts are automatically sent up to LangSmith. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts and calculating cost. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_nameinvocation_params.model_id (for costs only)invocation_params.model_path (for costs only)invocation_params.endpoint_name (for costs only) Once you are sending up the correct information to LangSmith, you must set up the model pricing map in LangSmith settings.
In order to do this, navigate to the model pricing map.
Here, you can set the cost per token for each model and provider combination. This information is scoped to a workspace. Several default entries for OpenAI models are already present in the model pricing map, which you can clone and modify as needed. To create a new entry in the model pricing map, click on the Add new model button in the top right corner. Here, you can specify the following fields: Model Name: The name of the model, will also be used to name the entry in the model pricing map.Prompt Cost: The cost per input token for the model. This number is multiplied by the number of tokens in the prompt to calculate the prompt cost.Completion Cost: The cost per output token for the model. This number is multiplied by the number of tokens in the completion to calculate the completion cost.Model Activation Date: The date from which the pricing is applicable.Match Pattern: A regex pattern to match the model name and provider. This is used to match the value for ls_model_name in the run metadata.Provider: The provider of the model. This is used to match the value for ls_provider in the run metadata. Once you have set up the model pricing map, LangSmith will automatically calculate and aggregate the token-based costs for traces based on the token counts provided in the LLM invocations. To see the example above in action, you can execute the following code snippet: PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ],    ""usage_metadata"": {        ""input_tokens"": 27,        ""output_tokens"": 13,        ""total_tokens"": 40,    },}@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" },];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?"",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages }); In the above code snippet, we are sending up the ls_provider and ls_model_name in the run metadata, along with the token counts for the LLM invocation.
This information matches the model pricing map entry we set up earlier. The trace produced will contain the token-based costs based on the token counts provided in the LLM invocation and the model pricing map entry."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#expand-detailed-view,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse,LangSmith-managed ClickHouse (Beta),"betaThis feature is currently in beta. Please reach out to our team at sales@langchain.dev if you are interested in leveraging this option. recommended readingPlease read the LangSmith architectural overview and guide on connecting to external Clickhouse before proceeding with this guide. As mentioned in previous guides, LangSmith uses Clickhouse as the primary storage engine for traces and feedback.
For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external Clickhouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team."
https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse,Architecture Overview,"Using LangSmith Managed Clickhouse with your Self-Hosted LangSmith instance is fairly simple. The overall architecture is similar to using a fully self-hosted ClickHouse instance, with a few key differences: You will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.With this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of Clickhouse to ensure that sensitive information doesn't leave your VPC. More on sensitive informationThis reference doc explains the format we use to store runs (spans), which are the building blocks of traces.Our definition of sensitive information as it relates to application data are inputs, outputs, and errors of a run, since these fields can contain prompts and completions from LLMs.With LangSmith-managed ClickHouse, we store inputs, outputs, and errors in cloud object storage (S3 or GCS) within your cloud and store the rest of the run data in ClickHouse. This ensures that sensitive information doesn't leave your VPC. The LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance. The overall architecture looks like this:"
https://docs.smith.langchain.com/self_hosting/langsmith_managed_clickhouse,Requirements,"You must be on AWS or GCP. We do not support Azure at this time as we require S3 or GCS for blob storage. Read the blob storage guide for more information.To use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported region. Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to whitelist your traffic.You must have a VPC that can connect to the LangSmith-managed Clickhouse service. You will need to work with our team to set up the necessary networking.You must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both Kubernetes and Docker installations."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-dataset-split,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#view-regressions-and-improvements,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise,Run pairwise evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function
with two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise,Use theevaluate_comparativefunction,"notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise,Configure inputs and outputs for pairwise evaluators,"Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with
runs[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent ""both equally bad"" or both to 1 for ""both equally good"". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise,Compare two experiments with LLM-based pairwise evaluators,"The following example uses a prompt
which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.
The prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(""langchain-ai/pairwise-evaluation-2"")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=""gpt-4"")        runnable = prompt | model    response = runnable.invoke({        ""question"": example.inputs[""question""],        ""answer_a"": runs[0].outputs[""output""] if runs[0].outputs is not None else ""N/A"",        ""answer_b"": runs[1].outputs[""output""] if runs[1].outputs is not None else ""N/A"",    })    score = response[""Preference""]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {""key"": ""ranked_preference"", ""scores"": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [""my-experiment-name-1"", ""my-experiment-name-2""],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.
import type { Run, Example } from ""langsmith"";import { evaluateComparative } from ""langsmith/evaluation"";import { wrapOpenAI } from ""langsmith/wrappers"";import OpenAI from ""openai"";const openai = wrapOpenAI(new OpenAI());import { z } from ""zod"";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(""Expected at least two runs"");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? ""N/A"",    answer_b: runB?.outputs?.output ?? ""N/A"",  };    const output = await openai.chat.completions.create({    model: ""gpt-4-turbo"",    messages: [      {        role: ""system"",        content: [          ""Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below."",          ""You should choose the assistant that follows the user's instructions and answers the user's question better."",          ""Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses."",          ""Begin your evaluation by comparing the two responses and provide a short explanation."",          ""Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision."",          ""Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible."",        ].join("" ""),      },      {        role: ""user"",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(""\n\n""),      },    ],    tool_choice: {      type: ""function"",      function: { name: ""Score"" },    },    tools: [      {        type: ""function"",        function: {          name: ""Score"",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output ""1"" if Assistant A answer is better based upon the factors above.`,            `Output ""2"" if Assistant B answer is better based upon the factors above.`,            `Output ""0"" if it is a tie.`,          ].join("" ""),          parameters: {            type: ""object"",            properties: {              Preference: {                type: ""integer"",                description: ""Which assistant answer is preferred?"",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: ""ranked_preference"", scores };}await evaluateComparative([""earnest-name-40"", ""reflecting-pump-91""], {  evaluators: [evaluatePairwise],});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise,View pairwise experiments,"Navigate to the ""Pairwise Experiments"" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically,Manage prompts programmatically,"You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.
All functionality going forward will live in the langsmith package."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically,Install packages,PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically,Configure environment variables,"If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=""lsv2_..."" TerminologyWhat we refer to as ""prompts"" used to be called ""repos"", so any references to ""repo"" in the code are referring to a prompt."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically,Push a prompt,"To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = client.push_prompt(""joke-generator"", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = prompts.push(""joke-generator"", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");url = prompts.push(""joke-generator"", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.
This is useful for storing the model configuration you want to use with this prompt.
The provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelclient.push_prompt(""joke-generator-with-model"", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelurl = prompts.push(""joke-generator-with-model"", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { ChatOpenAI } from ""langchain-openai"";const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");const chain = prompt.pipe(model);prompts.push(""joke-generator-with-model"", chain);"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically,Pull a prompt,"To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { ChatOpenAI } from ""langchain-openai"";const prompt = prompts.pull(""joke-generator"");const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const chain = prompt.pipe(model);chain.invoke({""topic"": ""cats""}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.
Just specify include_model when pulling the prompt.
If the stored prompt includes a model, it will be returned as a RunnableSequence.
Make sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})from langchain import hub as promptschain = prompts.pull(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { Runnable } from ""@langchain/core/runnables"";const chain = prompts.pull<Runnable>(""joke-generator-with-model"", { includeModel: true });chain.invoke({""topic"": ""cats""}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically,Use a prompt without LangChain,"If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.
These convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(""joke-generator"")prompt_value = prompt.invoke({""topic"": ""cats""})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically,"List, delete, and like prompts","You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.
See the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include ""joke""prompts = client.list_prompts(query=""joke"", is_public=False)# Delete a promptclient.delete_prompt(""joke-generator"")# Like a promptclient.like_prompt(""efriis/my-first-prompt"")# Unlike a promptclient.unlike_prompt(""efriis/my-first-prompt"")// List all prompts in my workspaceimport Client from ""langsmith"";const client = new Client({ apiKey: ""lsv2_..."" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include ""joke""const private_joke_prompts = client.listPrompts({ query: ""joke"", isPublic: false});// Delete a promptclient.deletePrompt(""joke-generator"");// Like a promptclient.likePrompt(""efriis/my-first-prompt"");// Unlike a promptclient.unlikePrompt(""efriis/my-first-prompt""); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.
However, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."
https://docs.smith.langchain.com/concepts/admin#organizations,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#organizations,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#organizations,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#organizations,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#organizations,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#organizations,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#organizations,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#organizations,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#organizations,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Evaluate your LLM application,"It can be hard to measure the performance of your application with respect to criteria important you or your users.
However, doing so is crucial, especially as you iterate on your application.
In this guide we will go over how to test and evaluate your application.
This allows you to measure how well your application is performing over a fixed set of data.
Being able to get this insight quickly and reliably will allow you to iterate with confidence. At a high level, in this tutorial we will go over how to: Create an initial golden dataset to measure performanceDefine metrics to use to measure performanceRun evaluations on a few different prompts or modelsCompare results manuallyTrack results over timeSet up automated testing to run in CI/CD For more information on the evaluation workflows LangSmith supports, check out the how-to guides. Lots to cover, let's dive in!"
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Create a dataset,"The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate.
There are a few aspects to consider here: What should the schema of each datapoint be?How many datapoints should I gather?How should I gather those datapoints? Schema: Each datapoint should consist of, at the very least, the inputs to the application.
If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.
Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process.
Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent.
LangSmith datasets are very flexible and allow you to define arbitrary schemas. How many: There's no hard and fast rule for how many you should gather.
The main thing is to make sure you have proper coverage of edge cases you may want to guard against.
Even 10-50 examples can provide a lot of value!
Don't worry about getting a large number to start - you can (and should) always add over time! How to get: This is maybe the trickiest part.
Once you know you want to gather a dataset... how do you actually go about it?
For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand.
After starting with these datapoints, these datasets are generally living constructs and grow over time.
They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set.
There are also methods like synthetically generating data that can be used to augment your dataset.
To start, we recommend not worrying about those and just hand labeling ~10-20 examples. Once you've got your dataset, there are a few different ways to upload them to LangSmith.
For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI). For this tutorial, we will create 5 datapoints to evaluate on.
We will be evaluating a question-answering application.
The input will be a question, and the output will be an answer.
Since this is a question-answering application, we can define the expected answer.
Let's show how to create and upload this dataset to LangSmith! from langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name = ""QA Example Dataset""dataset = client.create_dataset(dataset_name)client.create_examples(    inputs=[        {""question"": ""What is LangChain?""},        {""question"": ""What is LangSmith?""},        {""question"": ""What is OpenAI?""},        {""question"": ""What is Google?""},        {""question"": ""What is Mistral?""},    ],    outputs=[        {""answer"": ""A framework for building LLM applications""},        {""answer"": ""A platform for observing and evaluating LLM applications""},        {""answer"": ""A company that creates Large Language Models""},        {""answer"": ""A technology company known for search""},        {""answer"": ""A company that creates Large Language Models""},    ],    dataset_id=dataset.id,) Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page,
when we click into it we should see that we have five new examples."
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Define metrics,"After creating our dataset, we can now define some metrics to evaluate our responses on.
Since we have an expected answer, we can compare to that as part of our evaluation.
However, we do not expect our application to output those exact answers, but rather something that is similar.
This makes our evaluation a little trickier. In addition to evaluating correctness, let's also make sure our answers are short and concise.
This will be a little easier - we can define a simple Python function to measure the length of the response. Let's go ahead and define these two metrics. For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output).
This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function.
We can define our own prompt and LLM to use for evaluation here: from langchain_anthropic import ChatAnthropicfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluator_PROMPT_TEMPLATE = """"""You are an expert professor specialized in grading students' answers to questions.You are grading the following question:{query}Here is the real answer:{answer}You are grading the following predicted answer:{result}Respond with CORRECT or INCORRECT:Grade:""""""PROMPT = PromptTemplate(    input_variables=[""query"", ""answer"", ""result""], template=_PROMPT_TEMPLATE)eval_llm = ChatAnthropic(temperature=0.0)qa_evaluator = LangChainStringEvaluator(""qa"", config={""llm"": eval_llm, ""prompt"": PROMPT}) noteThis example assumes you have the ANTHROPIC_API_KEY environment variable set. You can just as easily run this example with OpenAI by replacing ChatAnthropic with ChatOpenAI from langchain_openai. For evaluating the length of the response, this is a lot easier!
We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result. from langsmith.schemas import Run, Exampledef evaluate_length(run: Run, example: Example) -> dict:    prediction = run.outputs.get(""output"") or """"    required = example.outputs.get(""answer"") or """"    score = int(len(prediction) < 2 * len(required))    return {""key"":""length"", ""score"": score}"
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Run Evaluations,"Great! So now how do we run evaluations?
Now that we have a dataset and evaluators, all that we need is our application!
We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.
We will build this using the OpenAI SDK directly: import openaiopenai_client = openai.Client()def my_app(question):    return openai_client.chat.completions.create(        model=""gpt-3.5-turbo"",        temperature=0,        messages=[            {                ""role"": ""system"",                ""content"": ""Respond to the users question in a short, concise manner (one short sentence).""            },            {                ""role"": ""user"",                ""content"": question,            }        ],    ).choices[0].message.content Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call,
and then also maps the output of the function to the output key we expect. def langsmith_app(inputs):    output = my_app(inputs[""question""])    return {""output"": output} Great!
Now we're ready to run evaluation.
Let's do it! from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=""openai-3.5"", # A prefix for your experiment names to easily identify them) This will output a URL. If we click on it, we should see results of our evaluation! If we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run! Let's now try it out with a different model! Let's try gpt-4-turbo import openaiopenai_client = openai.Client()def my_app_1(question):    return openai_client.chat.completions.create(        model=""gpt-4-turbo"",        temperature=0,        messages=[            {                ""role"": ""system"",                ""content"": ""Respond to the users question in a short, concise manner (one short sentence).""            },            {                ""role"": ""user"",                ""content"": question,            }        ],    ).choices[0].message.contentdef langsmith_app_1(inputs):    output = my_app_1(inputs[""question""])    return {""output"": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_1, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=""openai-4"", # A prefix for your experiment names to easily identify them) And now let's use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short. import openaiopenai_client = openai.Client()def my_app_2(question):    return openai_client.chat.completions.create(        model=""gpt-4-turbo"",        temperature=0,        messages=[            {                ""role"": ""system"",                ""content"": ""Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.""            },            {                ""role"": ""user"",                ""content"": question,            }        ],    ).choices[0].message.contentdef langsmith_app_2(inputs):    output = my_app_2(inputs[""question""])    return {""output"": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_2, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=""strict-openai-4"", # A prefix for your experiment names to easily identify them) If we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!"
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Comparing results,"Awesome, we've evaluated three different runs. But how can we compare results?
The first way we can do this is just by looking at the runs in the Experiments tab.
If we do that, we can see a high level view of the metrics for each run: Great! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length.
But what if we want to explore in more detail? In order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view: We immediately see all three tests side by side.
Some of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline.
We automatically choose defaults for the baseline and metric, but you can change those yourself (outlined in blue below).
You can also choose which columns and which metrics you see by using the Display control (outlined in yellow below).
You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top (outlined in red below). If we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:"
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Set up automated testing to run in CI/CD,"Now that we've run this in a one-off manner, we can set it to run in an automated fashion.
We can do this pretty easily by just including it as a pytest file that we run in CI/CD.
As part of this, we can either just log the results OR set up some criteria to determine if it passes or not.
For example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check,
we could set that up with a test like: def test_length_score() -> None:    """"""Test that the length score is at least 80%.""""""    experiment_results = evaluate(        langsmith_app, # Your AI system        data=dataset_name, # The data to predict and grade over        evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    )    # This will be cleaned up in the next release:    feedback = client.list_feedback(        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],        feedback_key=""length""    )    scores = [f.score for f in feedback]    assert sum(scores) / len(scores) >= 0.8, ""Aggregate score should be at least .8"""
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Track results over time,"Now that we've got these experiments running in an automated fashion, we want to track these results over time.
We can do this from the overall Experiments tab in the datasets page.
By default, we show evaluation metrics over time (highlighted in red).
We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow)."
https://docs.smith.langchain.com/tutorials/Developers/evaluation,Conclusion,"That's it for this tutorial! We've gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time.
Hopefully this can help you iterate with confidence. This is just the start. As mentioned earlier, evaluation is an ongoing process.
For example - the datapoints you will want to evaluate on will likely continue to change over time.
There are many types of evaluators you may wish to explore.
For information on this, check out the how-to guides. Additionally, there are other ways to evaluate data besides in this ""offline"" manner (e.g. you can evaluate production data).
For more information on online evaluation, check out this guide."
https://docs.smith.langchain.com/how_to_guides,How-to guides,Step-by-step guides that cover key tasks and operations in LangSmith.
https://docs.smith.langchain.com/how_to_guides,Setup,"See the following guides to set up your LangSmith account. Create an account and API keySet up an organizationCreate an organizationManage and navigate workspacesManage usersSet up a workspaceCreate a workspaceManage usersConfigure workspace settingsSet up billingUpdate invoice email, tax id and, business informationSet up access control (enterprise only)Create a roleAssign a role to a user"
https://docs.smith.langchain.com/how_to_guides,Tracing,Get started with LangSmith's tracing features to start adding observability to your LLM applications. Annotate code for tracingUse @traceable/traceableWrap the OpenAI API clientUse the RunTree APIUse the trace context manager (Python only)Toggle tracing on and offLog traces to specific projectSet the destination project staticallySet the destination project dynamicallySet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingDistributed tracing in PythonDistributed tracing in TypeScriptAccess the current span within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesChat-style modelsSpecify model nameStream outputsManually provide token countsInstruct-style modelsPrevent logging of sensitive data in tracesRule-based masking of inputs and outputsExport tracesUse filter argumentsUse filter query languageShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChainInstallationQuick startTrace selectivelyLog to specific projectAdd metadata and tags to tracesCustomize run nameAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTrace with LangGraphInteroperability between LangChain and LangGraphInteroperability between @traceable/traceable and LangGraphTrace with Instructor (Python only)Trace with the Vercel AI SDK (JS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for traces
https://docs.smith.langchain.com/how_to_guides,Datasets,Manage datasets in LangSmith to evaluate and improve your LLM applications. Manage datasets in the applicationCreate a new dataset and add examples manuallyDataset schema validationAdd inputs and outputs from traces to datasetsUpload a CSV file to create a datasetGenerate synthetic examplesExport a datasetCreate and manage dataset splitsManage datasets programmaticallyCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from a pandas DataFrameFetch datasetsFetch examplesUpdate examplesBulk update examplesVersion datasetsCreate a new version of a datasetTag a versionShare or unshare a dataset publicly
https://docs.smith.langchain.com/how_to_guides,Evaluation,Evaluate your LLM applications to measure their performance over time. Evaluate an LLM applicationRun an evaluationUse custom evaluatorsEvaluate on a particular version of a datasetEvaluate on a subset of a datasetEvaluate on a dataset splitEvaluate on a dataset with repetitionsUse a summary evaluatorEvaluate a LangChain runnableReturn multiple scoresBind an evaluator to a dataset in the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse LangChain off-the-shelf evaluators (Python only)Use question and answer (correctness) evaluatorsUse criteria evaluatorsUse labeled criteria evaluatorsUse string or embedding distance metricsUse a custom LLM in off-the-shelf evaluatorsHandle multiple input or output fieldsCompare experiment resultsOpen the comparison viewView regressions and improvementsFilter on regressions or improvementsUpdate baseline experimentSelect feedback keyOpen a traceExpand detailed viewUpdate display settingsEvaluate an existing experimentUnit test LLM applications (Python only)Run pairwise evaluationsUse the evaluate_comparative functionConfigure inputs and outputs for pairwise evaluatorsCompare two experiments with LLM-based pairwise evaluatorsView pairwise experimentsAudit evaluator scoresIn the comparison viewIn the runs tableIn the SDKCreate few-shot evaluatorsCreate your evaluatorMake correctionsView your corrections datasetFetch performance metrics for an experimentRun evals using the API onlyCreate a datasetRun a single experimentRun a pairwise experimentUpload experiments run outside of LangSmith with the REST APIRequest body schemaConsiderationsExample requestView the experiment in the UI
https://docs.smith.langchain.com/how_to_guides,Human feedback,Collect human feedback to improve your LLM applications. Capture user feedback from your application to tracesSet up a new feedback criteriaAnnotate traces inlineUse annotation queuesCreate an annotation queueAssign runs to an annotation queueReview runs in an annotation queue
https://docs.smith.langchain.com/how_to_guides,Monitoring and automations,Leverage LangSmith's powerful monitoring and automations features to make sense of your production data. Filter traces in the applicationCreate a filterFilter for intermediate runs (spans)Advanced: filter for intermediate runs (spans) on properties of the rootAdvanced: filter for runs (spans) whose child runs have some attributeFilter based on inputs and outputsFilter based on input / output key-value pairsCopy the filterManually specify a raw query in LangSmith query languageUse an AI Query to auto-generate a queryUse monitoring chartsChange the time periodSlice data by metadata or tagDrill down into specific subsetsSet up automation rulesCreate a ruleView logs for your automationsSet up online evaluationsConfigure online evaluationsSet API keysSet up webhook notifications for rulesWebhook payloadExample with ModalSet up threadsGroup traces into threadsView threads
https://docs.smith.langchain.com/how_to_guides,Prompts,"Organize and manage prompts in LangSmith to streamline your LLM development workflow. Create a promptCompose your promptSave your promptView your promptsAdd metadataUpdate a promptUpdate metadataUpdate the prompt contentVersion a promptManage prompts programmaticallyConfigure environment variablesPush a promptPull a promptUse a prompt without LangChainList, delete, and like promptsLangChain Hub"
https://docs.smith.langchain.com/how_to_guides,Playground,Quickly iterate on prompts and models in the LangSmith Playground. Use custom TLS certificatesUse a custom modelSave settings configuration
https://docs.smith.langchain.com/how_to_guides/tracing/trace_without_env_vars,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT In some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. Recently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,
we changed the behavior of with trace to honor the LANGCHAIN_TRACING_V2 environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.
The recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below. PythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.
import openaifrom langsmith import Client, tracing_context, traceablefrom langsmith.wrappers import wrap_openailangsmith_client = Client(    api_key=""YOUR_LANGSMITH_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)client = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceabledef chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}Context: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.content# Can set to False to disable tracing here without changing code structurewith tracing_context(enabled=True):    # Use langsmith_extra to pass in a custom client    chat_pipeline(""Can you summarize this morning's meetings?"", langsmith_extra={""client"": langsmith_client})In TypeScript, you can pass in both the client and the tracingEnabled flag to the traceable decorator.
import { Client } from ""langsmith"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";import { OpenAI } from ""openai"";const client = new Client({    apiKey: ""YOUR_API_KEY"",  // This can be retrieved from a secrets manager    apiUrl: ""https://api.smith.langchain.com"",  // Update appropriately for self-hosted installations or the EU region});const openai = wrapOpenAI(new OpenAI());const tool = traceable((question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", runType: ""tool"" });const pipeline = traceable(    async (question: string) => {        const context = await tool(question);                const completion = await openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: [                { role: ""system"" as const, content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },                { role: ""user"" as const, content: `Question: ${question}\nContext: ${context}`}            ]        });                return completion.choices[0].message.content;        },     { name: ""Chat"", client, tracingEnabled: true });await pipeline(""Can you summarize this morning's meetings?"");"
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project#set-the-destination-project-statically,Log traces to specific project,You can change the destination project of your traces both statically through environment variables and dynamically at runtime.
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project#set-the-destination-project-statically,Set the destination project statically,"As mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-custom-project If the project specified does not exist, it will be created automatically when the first trace is ingested."
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project#set-the-destination-project-statically,Set the destination project dynamically,"You can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application. noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""Hello!""}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(    run_type=""llm"",    name=""OpenAI Call Decorator"",    project_name=""My Project"")def call_openai(    messages: list[dict], model: str = ""gpt-3.5-turbo"") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(    messages,    langsmith_extra={""project_name"": ""My Overriden Project""},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=messages,    langsmith_extra={""project_name"": ""My Project""},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(    run_type=""llm"",    name=""OpenAI Call RunTree"",    inputs={""messages"": messages},    project_name=""My Project"")chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";import { RunTree} from ""langsmith"";const client = new OpenAI();const messages = [    {role: ""system"", content: ""You are a helpful assistant.""},    {role: ""user"", content: ""Hello!""}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: ""llm"",    name: ""OpenAI Call Traceable"",    project_name: ""My Project""});// Call the traceable functionawait traceableCallOpenAI(messages, ""gpt-3.5-turbo"");// Create and use a RunTree objectconst rt = new RunTree({    runType: ""llm"",    name: ""OpenAI Call RunTree"",    inputs: { messages },    project_name: ""My Project""});// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.postRun();"
https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats,Generating Query Stats,"As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience. This command will generate a CSV that can be shared with the LangChain team."
https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats,Prerequisites,"Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/Clickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the Clickhouse database from the machine you will be running the get_query_stats script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine."
https://docs.smith.langchain.com/self_hosting/scripts/generate_query_stats,Running the query stats generation script,"Run the following command to run the stats generation script: sh get_query_stats.sh <clickhouse_url> --output path/to/file.csv For example, if you are using the bundled version with port-forwarding, the command would look like: sh get_query_stats.sh ""clickhouse://default:password@localhost:8123/default"" --output query_stats.csv and after running this command you should see a file, query_stats.csv, has been created with LangSmith query statistics."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-arguments,Trace query syntax,"Using the list_runs method in the SDK or /runs/query endpoint in the API, you can filter runs to analyze and export."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-arguments,Filter arguments,"KeysDescriptionproject_id / project_nameThe project(s) to fetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a specific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name / dataset_idFetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.reference_example_idFetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.errorFetch runs that errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other filtering arguments.filterFetch runs that match a given structured filter statement. See the guide below for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to return in the response. By default, all fields are returned.query (experimental)Natural language query, which translates your query into a filter statement."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-arguments,Filter query language,"LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs. The filtering grammar is based on common comparators on fields in the run object. Supported comparators include: gte (greater than or equal to)gt (greater than)lte (less than or equal to)lt (less than)eq (equal to)neq (not equal to)has (check if run contains a tag or metadata json blob)search (search for a substring in a string field) Additionally, you can combine multiple comparisons through and and or operators. These can be applied on fields of the run object, such as its id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and any associated feedback through feedback_key and feedback_score."
https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability,Cloud architecture and scalability,"Cloud-managed solutionThis section is only relevant for the cloud-managed LangSmith services available at https://smith.langchain.com and https://eu.smith.langchain.com.For information on the self-hosted LangSmith solution, please refer to the self-hosted documentation. LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for both LLM application observability and evaluation."
https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability,Architecture,"The US-based LangSmith service is deployed in the us-central1 (Iowa) region of GCP. NOTE: The EU-based LangSmith service is now available (as of mid-July 2024) and is deployed in the europe-west4 (Netherlands) region of GCP.
If you are interested in an enterprise plan in this region, please contact us at sales@langchain.dev."
https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability,Regional storage,"The resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU).
Cloud-managed LangSmith uses Supabase for authentication/authorization and ClickHouse Cloud for data warehouse. USEUURLhttps://smith.langchain.comhttps://eu.smith.langchain.comAPI URLhttps://api.smith.langchain.comhttps://eu.api.smith.langchain.comGCPus-central1 (Iowa)europe-west4 (Netherlands)SupabaseAWS us-east-1 (N. Virginia)AWS eu-central-1 (Germany)ClickHouse Cloudus-central1 (Iowa)europe-west4 (Netherlands)LangGraph Cloudus-central1 (Iowa)europe-west4 (Netherlands) See the Regions FAQ for more information."
https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability,Region-independent storage,Data listed here is stored exclusively in the US: Payment and billing information with Stripe and Metronome
https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability,GCP services,"LangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE): LangSmith Frontend: serves the LangSmith UI.LangSmith Backend: serves the LangSmith API.LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.LangSmith Queue: handles processing of asynchronous tasks. (Internal service) LangSmith uses the following GCP storage services: Google Cloud Storage (GCS) for runs inputs and outputs.Google Cloud SQL PostgreSQL for transactional workloads.Google Cloud Memorystore for Redis for queuing and caching.Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint. Some additional GCP services we use include: Google Cloud Load Balancer for routing traffic to the LangSmith services.Google Cloud CDN for caching static assets.Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to this guide."
https://docs.smith.langchain.com/reference/cloud_architecture_and_scalability,Scalability,"LangSmith is designed to be scalable and performant. As of load testing done in February 2024, LangSmith can comfortably process 500K+ runs (spans) per minute.
We anticipate that LangSmith can process 750K+ runs per minute with the optimizations we've made since then."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#run-an-evaluation,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#specify-model-name,Log custom LLM traces,"noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.
In order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#specify-model-name,Chat-style models,"For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg ""openai"", ""anthropic"", etc.ls_model_name: The name of the model, eg ""gpt-3.5-turbo"", ""claude-3-opus-20240307"", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ]}# Can also use one of:# output = {#     ""message"": {#         ""role"": ""assistant"",#         ""content"": ""Sure, what time would you like to book the table for?""#     }# }## output = {#     ""role"": ""assistant"",#     ""content"": ""Sure, what time would you like to book the table for?""# }## output = [""assistant"", ""Sure, what time would you like to book the table for?""]@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" }];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?""      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: ""assistant"",//     content: ""Sure, what time would you like to book the table for?""//   }// };//// const output = {//   role: ""assistant"",//   content: ""Sure, what time would you like to book the table for?""// };//// const output = [""assistant"", ""Sure, what time would you like to book the table for?""];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#specify-model-name,Stream outputs,"For streaming, you can ""reduce"" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = """".join([chunk[""choices""][0][""message""][""content""] for chunk in chunks])    return {""choices"": [{""message"": {""content"": all_text, ""role"": ""assistant""}}]}@traceable(    run_type=""llm"",    reduce_fn=_reduce_chunks,    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def my_streaming_chat_model(messages: list):    for chunk in [""Hello, "" + messages[1][""content""]]:        yield {            ""choices"": [                {                    ""message"": {                        ""content"": chunk,                        ""role"": ""assistant"",                    }                }            ]        }list(    my_streaming_chat_model(        [            {""role"": ""system"", ""content"": ""You are a helpful assistant. Please greet the user.""},            {""role"": ""user"", ""content"": ""polly the parrot""},        ],    ))"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#specify-model-name,Manually provide token counts,"Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.
Many models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.
If token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.
If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ],    ""usage_metadata"": {        ""input_tokens"": 27,        ""output_tokens"": 13,        ""total_tokens"": 40,    },}@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" },];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?"",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages });"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#specify-model-name,Instruct-style models,"For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.
The same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def hello_llm(prompt: str):    return {        ""choices"": [            {""text"": ""Hello, "" + prompt}        ],        ""usage_metadata"": {            ""input_tokens"": 4,            ""output_tokens"": 5,            ""total_tokens"": 9,        },    }hello_llm(""polly the parrot\n"")import { traceable } from ""langsmith/traceable"";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: ""Hello, "" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: ""llm"", name: ""hello_llm"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await helloLLM({ prompt: ""polly the parrot\n"" }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations,Set up online evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Set up automation rules Online evaluations is a powerful LangSmith feature that allows you to run an LLM-as-a-judge evaluator on a set of your production traces. They are implemented as a possible action in an automation rule. Currently, we provide support for specifying a prompt template, a model, and a set of criteria to evaluate the runs on. After entering rules setup, you can select Online Evaluation from the list of possible actions:"
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations,Configure online evaluations,"When selection Online Evaluation as an action in an automation, you are presented with a panel from which you can configure online evaluation."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations,Model,"You can choose any model available in the dropdown. Currently, we support OpenAI, AzureOpenAI, and models hosted on Fireworks.
In order to set the API keys to use for these invocations, click on the Secrets & API Keys button and add the necessary keys."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations,Prompt template,"For the prompt template, you can select a suggested evaluator prompt, create a new prompt, or choose a prompt from the LangChain Hub. Suggested evaluator prompts We provide a list of pre-existing prompts that cater to common evaluator use cases. Create your own prompt We provide a base template from which you can form your own prompt. Pull a prompt from the LangChain Hub You can pull any structured prompt, private or public. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses.
If the prompt is your own, you can edit it in the playground and commit the version.
If the prompt is someone else's, you can fork the prompt, make your edits in the playground, and then pull in your new prompt to the online evaluator. When you choose a hub prompt for your online evaluator, the prompt will be locked to the commit version it was at rule creation. If you want to update the prompt, you can go to edit the online evaluator and re-select the prompt in the dropdown."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations,Mapping variables,"Prompts can be crafted with any variable name you choose. To map what is passed into your evaluator prompt from your runs or experiments, use the variable mapping inputs. There's a dropdown with suggestions provided based on the schema of your recent runs."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations,Preview,"Previewing the prompt will show you an example of what the formatted prompt will look like. This preview pulls the input and output of the most recent run. In the case of a dataset evaluator, the preview will also pull reference output from an example in your dataset. noteYou can configure an evaluation prompt that doesn't match the schema of your recent runs, but the dropdown suggestions and preview function won't work as expected."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations,Output schema,"An evaluator will attach metadata tags to a run. These tags will have a name and a value. You can configure these in the Schema section.
The names and the descriptions of the fields will be passed into the prompt. Behind the scenes, we use tool calling to coerce the output of the LLM into the score you specify. noteThe name of the schema cannot have spaces since it is used as the name of a tool."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#distributed-tracing-with-langchain-python,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/monitoring/threads#group-traces-into-threads,Set up threads,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Add metadata and tags to traces Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith."
https://docs.smith.langchain.com/how_to_guides/monitoring/threads#group-traces-into-threads,Group traces into threads,"A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread. To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread. The key value is the unique identifier for that conversation.
The key name should be one of: session_idthread_idconversation_id. The value should be a UUID, such as f47ac10b-58cc-4372-a567-0e02b2c3d479."
https://docs.smith.langchain.com/how_to_guides/monitoring/threads#group-traces-into-threads,View threads,"You can view threads by clicking on the Threads tad in any project details page. You will then see a list of all threads, sorted by the most recent activity. You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs. You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively."
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-metadata,Update a prompt,Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit.
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-metadata,Update metadata,"To update the prompt metadata (description, use cases, etc.) click the ""Edit"" pencil icon. Your prompt metadata will be updated upon save."
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-metadata,Update the prompt content,"To update the prompt content itself, you need to enter the prompt playground. Click ""Edit in playground"".
Now you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click ""Commit"" to save it. 
"
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-metadata,Version a prompt,"When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the ""Commits"" tab in the prompt view."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_generator_functions,Trace generator functions,"In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user. LangSmith's tracing functionality natively supports streamed outputs via generator functions. Below is an example. PythonTypeScriptfrom langsmith import traceable@traceabledef my_generator():    for chunk in [""Hello"", ""World"", ""!""]:        yield chunk# Stream to the userfor output in my_generator():    print(output)# It also works with async functionsimport asyncio@traceableasync def my_async_generator():    hunk in [""Hello"", ""World"", ""!""]:        yield chunk# Stream to the userasync def main():    async for output in my_async_generator():        print(output)asyncio.run(main())import { traceable } from ""langsmith/traceable"";const myGenerator = traceable(function* () {    for (const chunk of [""Hello"", ""World"", ""!""]) {        yield chunk;    }});for (const output of myGenerator()) {    console.log(output);}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_generator_functions,Aggregate Results,"By default, the outputs of the traced function are aggregated into a single array in LangSmith.
If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the aggregate option (reduce_fn in python).
This is especially useful for aggregating streamed LLM outputs. noteAggregating outputs only impacts the traced representation of the outputs. It doesn not alter the values returned by your function. PythonTypeScriptfrom langsmith import traceabledef concatenate_strings(outputs: list):    return """".join(outputs)@traceable(reduce_fn=concatenate_strings)def my_generator():    for chunk in [""Hello"", ""World"", ""!""]:        yield chunk# Stream to the userfor output in my_generator():    print(output)# It also works with async functionsimport asyncio@traceable(reduce_fn=concatenate_strings)async def my_async_generator():    for chunk in [""Hello"", ""World"", ""!""]:        yield chunk# Stream to the userasync def main():    async for output in my_async_generator():        print(output)asyncio.run(main())import { traceable } from ""langsmith/traceable"";const concatenateStrings = (outputs: string[]) => outputs.join("""");const myGenerator = traceable(function* () {    for (const chunk of [""Hello"", ""World"", ""!""]) {        yield chunk;    }}, { aggregator: concatenateStrings });for (const output of await myGenerator()) {    console.log(output);}"
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-query-language,Trace query syntax,"Using the list_runs method in the SDK or /runs/query endpoint in the API, you can filter runs to analyze and export."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-query-language,Filter arguments,"KeysDescriptionproject_id / project_nameThe project(s) to fetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a specific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name / dataset_idFetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.reference_example_idFetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.errorFetch runs that errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other filtering arguments.filterFetch runs that match a given structured filter statement. See the guide below for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to return in the response. By default, all fields are returned.query (experimental)Natural language query, which translates your query into a filter statement."
https://docs.smith.langchain.com/reference/data_formats/trace_query_syntax#filter-query-language,Filter query language,"LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs. The filtering grammar is based on common comparators on fields in the run object. Supported comparators include: gte (greater than or equal to)gt (greater than)lte (less than or equal to)lt (less than)eq (equal to)neq (not equal to)has (check if run contains a tag or metadata json blob)search (search for a substring in a string field) Additionally, you can combine multiple comparisons through and and or operators. These can be applied on fields of the run object, such as its id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and any associated feedback through feedback_key and feedback_score."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,LangSmith Release Notes,noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,"Week of June 17, 2024 - LangSmith v0.6","LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,New Features,Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More...
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Performance and Reliability Changes,Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Infrastructure changes,"[Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Admin changes,Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Deprecation notices,With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,"Week of May 13, 2024 - LangSmith v0.5","LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Breaking changes,We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,New Features,Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Performance and Reliability Changes,"Split ingest, session deletion, and automation jobs to execute within separate resource pools."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Infrastructure changes,"As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Admin changes,Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Deprecation notices,With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,"Week of March 25, 2024 - LangSmith v0.4","LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Breaking changes,"This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Performance and Reliability Changes,"Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Infrastructure changes,"Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Admin changes,Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Deprecation notices,With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,"Week of Februrary 21, 2024 - LangSmith v0.3","LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Breaking changes,"This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Performance and Reliability Changes,"Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Admin changes,None
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Deprecation notices,With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,"Week of January 29, 2024 - LangSmith v0.2","LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Requirements,This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Breaking changes,"The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {""user_id"": ..., ""user_name"":...,}) and then search using has(metadata, '{""user_name"": ...}')"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Performance and Reliability Changes,"Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Infrastructure Changes,"Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Admin changes,Increased the maximum number of users per organization from 5 to 100 for new organizations.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-february-21-2024---langsmith-v03,Deprecation notices,With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/concepts/usage_and_billing/rate_limits,Rate Limits,"LangSmith has rate limits which are designed to ensure the stability of the service for all users. To ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:"
https://docs.smith.langchain.com/concepts/usage_and_billing/rate_limits,Temporary throughput limit over a 1 minute period at our application load balancer,This 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly  it is not guaranteed to start at the start of a clock minute  and may change depending on application deployment events. After the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats. This 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users. MethodEndpointLimitWindowDELETESessions301 minutePOST OR PATCHRuns50001 minutePOSTFeedback50001 minute**20001 minute noteThe LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call.
https://docs.smith.langchain.com/concepts/usage_and_billing/rate_limits,Plan-level hourly trace event limit,"This 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour. An event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit. This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use. PlanLimitWindowDeveloper (no payment on file)50,000 events1 hourDeveloper (with payment on file)250,000 events1 hourStartup/Plus500,000 events1 hourEnterpriseCustomCustom"
https://docs.smith.langchain.com/concepts/usage_and_billing/rate_limits,Plan-level hourly trace data ingest limit,"This 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour. Typically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit. This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use. PlanLimitWindowDeveloper (no payment on file)500MB1 hourDeveloper (with payment on file)2.5GB1 hourStartup/Plus5.0GB1 hourEnterpriseCustomCustom"
https://docs.smith.langchain.com/concepts/usage_and_billing/rate_limits,Plan-level monthly unique traces limit,"This 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file. PlanLimitWindowDeveloper (no payment on file)5,000 traces1 month"
https://docs.smith.langchain.com/concepts/usage_and_billing/rate_limits,Self-configured monthly usage limits,This 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and varies by organization based on their configured settings.
https://docs.smith.langchain.com/concepts/usage_and_billing/rate_limits,Handling 429s responses in your application,"Since some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter. For convenience, LangChain applications built with the LangSmith SDK has this capability built-in. noteIt is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.If that is the case, we would like to discuss your needs more specifically. Please reach out to LangSmith Support with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#configure-inputs-and-outputs-for-pairwise-evaluators,Run pairwise evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function
with two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#configure-inputs-and-outputs-for-pairwise-evaluators,Use theevaluate_comparativefunction,"notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#configure-inputs-and-outputs-for-pairwise-evaluators,Configure inputs and outputs for pairwise evaluators,"Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with
runs[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent ""both equally bad"" or both to 1 for ""both equally good"". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#configure-inputs-and-outputs-for-pairwise-evaluators,Compare two experiments with LLM-based pairwise evaluators,"The following example uses a prompt
which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.
The prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(""langchain-ai/pairwise-evaluation-2"")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=""gpt-4"")        runnable = prompt | model    response = runnable.invoke({        ""question"": example.inputs[""question""],        ""answer_a"": runs[0].outputs[""output""] if runs[0].outputs is not None else ""N/A"",        ""answer_b"": runs[1].outputs[""output""] if runs[1].outputs is not None else ""N/A"",    })    score = response[""Preference""]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {""key"": ""ranked_preference"", ""scores"": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [""my-experiment-name-1"", ""my-experiment-name-2""],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.
import type { Run, Example } from ""langsmith"";import { evaluateComparative } from ""langsmith/evaluation"";import { wrapOpenAI } from ""langsmith/wrappers"";import OpenAI from ""openai"";const openai = wrapOpenAI(new OpenAI());import { z } from ""zod"";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(""Expected at least two runs"");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? ""N/A"",    answer_b: runB?.outputs?.output ?? ""N/A"",  };    const output = await openai.chat.completions.create({    model: ""gpt-4-turbo"",    messages: [      {        role: ""system"",        content: [          ""Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below."",          ""You should choose the assistant that follows the user's instructions and answers the user's question better."",          ""Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses."",          ""Begin your evaluation by comparing the two responses and provide a short explanation."",          ""Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision."",          ""Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible."",        ].join("" ""),      },      {        role: ""user"",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(""\n\n""),      },    ],    tool_choice: {      type: ""function"",      function: { name: ""Score"" },    },    tools: [      {        type: ""function"",        function: {          name: ""Score"",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output ""1"" if Assistant A answer is better based upon the factors above.`,            `Output ""2"" if Assistant B answer is better based upon the factors above.`,            `Output ""0"" if it is a tie.`,          ].join("" ""),          parameters: {            type: ""object"",            properties: {              Preference: {                type: ""integer"",                description: ""Which assistant answer is preferred?"",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: ""ranked_preference"", scores };}await evaluateComparative([""earnest-name-40"", ""reflecting-pump-91""], {  evaluators: [evaluatePairwise],});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#configure-inputs-and-outputs-for-pairwise-evaluators,View pairwise experiments,"Navigate to the ""Pairwise Experiments"" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"
https://docs.smith.langchain.com/concepts/evaluation,Evaluation,"The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."
https://docs.smith.langchain.com/concepts/evaluation,Datasets,"Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."
https://docs.smith.langchain.com/concepts/evaluation,Creating datasets,"There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.
From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,
and what ""good"" responses may be.
You probably want to cover a few different common edge cases or situations you can imagine.
Even 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?
This information can be valuable to capture and store in datasets. This allows you to test against these
use cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?
There are a few heuristics you can follow.
If possible - try to collect end user feedback. You can then see which datapoints got negative feedback.
That is super valuable! These are spots where your application did not perform well.
You should add these to your dataset to test against in the future. You can also use other heuristics
to identify ""interesting"" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.
It's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.
This can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"
https://docs.smith.langchain.com/concepts/evaluation,Types of datasets,"LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:""Inputs"" and ""outputs"" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating ""completion"" style language models.The ""inputs"" dictionary contains a single ""input"" key mapped to the prompt string.The ""outputs"" dictionary contains a single ""output"" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured ""chat"" messages as inputs and outputs.The ""inputs"" dictionary contains a single ""input"" key mapped to a list of serialized chat messagesThe ""outputs"" dictionary contains a single ""output"" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."
https://docs.smith.langchain.com/concepts/evaluation,Partitioning datasets,"When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."
https://docs.smith.langchain.com/concepts/evaluation,Evaluators,"Evaluators are functions in LangSmith that score how well your application performs on a particular example.
Evaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."
https://docs.smith.langchain.com/concepts/evaluation,Human,Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering.
https://docs.smith.langchain.com/concepts/evaluation,Heuristic,"Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."
https://docs.smith.langchain.com/concepts/evaluation,LLM-as-judge,"LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."
https://docs.smith.langchain.com/concepts/evaluation,Pairwise,"Pairwise evaluators pick the better of two task outputs based upon some criteria.
This can use either a heuristic (""which response is longer""), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.
This can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."
https://docs.smith.langchain.com/concepts/evaluation,Applying evaluations,"We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."
https://docs.smith.langchain.com/concepts/evaluation,Unit Tests,"Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.
This means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).
These types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)
to cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."
https://docs.smith.langchain.com/concepts/evaluation,Regression Testing,"Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.
They are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"
https://docs.smith.langchain.com/concepts/evaluation,Back-testing,"Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.
Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.
Then compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."
https://docs.smith.langchain.com/concepts/evaluation,Pairwise-testing,"It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."
https://docs.smith.langchain.com/concepts/evaluation,Online Evaluation,"Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation,Experiment Configurations,"LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"
https://docs.smith.langchain.com/concepts/evaluation,Evaluating Specific LLM Applications,"Below, we will discuss evaluation of a few specific, popular LLM applications."
https://docs.smith.langchain.com/concepts/evaluation,Agents,"LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.
Note that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"
https://docs.smith.langchain.com/concepts/evaluation,Evaluating an agent's final response,"One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."
https://docs.smith.langchain.com/concepts/evaluation,Evaluating a single step of an agent,"Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."
https://docs.smith.langchain.com/concepts/evaluation,Evaluating an agent's trajectory,"Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an ""exact"" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the ""exact"" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of ""incorrect"" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."
https://docs.smith.langchain.com/concepts/evaluation,Best practices,"Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation,Retrieval Augmented Generation (RAG),"Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."
https://docs.smith.langchain.com/concepts/evaluation,Dataset,"When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."
https://docs.smith.langchain.com/concepts/evaluation,Evaluator,"LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"
https://docs.smith.langchain.com/concepts/evaluation,Applying RAG Evaluation,"When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"
https://docs.smith.langchain.com/concepts/evaluation,RAG evaluation summary,Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes
https://docs.smith.langchain.com/concepts/evaluation,Summarization,"Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"
https://docs.smith.langchain.com/concepts/evaluation,Classification / Tagging,"Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"
https://docs.smith.langchain.com/concepts/admin#organization-roles,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#organization-roles,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#organization-roles,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#organization-roles,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#organization-roles,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#organization-roles,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#organization-roles,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#organization-roles,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#organization-roles,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/concepts/prompts#template-formats,Prompts,"Writing good prompts is key to getting the best performance out of your applications. LangSmith provides ways to create, test, and manage prompts."
https://docs.smith.langchain.com/concepts/prompts#template-formats,Prompt types,"We support three types of prompt templates: StringPromptTemplateChatPromptTemplateStructuredPromptTemplate For detailed information about these templates, please refer to the LangChain documentation. In LangSmith, you can create prompts using the Playground. From the prompt view in the Playground, you can select either ""Chat-style prompt"" or ""Instruct-style prompt"" to get started. Chat-style prompt Chat prompts are used for chat-style models that accept a list of messages as an input and respond with an assistant message. A chat-style prompt is represented in LangSmith as a ChatPromptTemplate, which can contain multiple messages, each with prompt variables. You can also specify an output schema which is represented in LangSmith as a StructuredPromptTemplate. Instruct-style prompt An instruct-style prompt is represented as a StringPromptTemplate that gets formatted to a single string input for your model."
https://docs.smith.langchain.com/concepts/prompts#template-formats,Template formats,We support two types of template formats: f-string and mustache. You can switch between these formats when editing prompts in the Playground.
https://docs.smith.langchain.com/concepts/prompts#template-formats,F-string,"F-strings are a Python-specific string formatting method that allows you to embed expressions inside string literals, using curly braces {}.
Here's an example of an f-string template: Hello, {name}!"
https://docs.smith.langchain.com/concepts/prompts#template-formats,Mustache,"Mustache is a template syntax that allows you to embed variables inside double curly braces {{}}. NotePlease see the Mustache documentation for more detailed information on how to use Mustache templates. Here's an example of a mustache template: Hello, {{name}}! Mustache is more robust than f-strings since it supports more complex logic. You can use conditionals, loops, and access nested keys in mustache templates. Conditional Hello, {{#name}}{{name}}{{/name}}{{^name}}world{{/name}}! The template will output ""Hello, Bob!"" if ""Bob"" is provided as the name variable, and ""Hello, world!"" if the name variable is not provided. Loop {{#names}}{{name}}{{/names}} input: {  ""names"": [{ ""name"": ""Alice"" }, { ""name"": ""Bob"" }, { ""name"": ""Charlie"" }]} output: ""AliceBobCharlie"" Loop with nesting {{#people}}{{name}} is {{age}} years old. {{/people}} input: {  ""people"": [    { ""name"": ""Alice"", ""age"": 30 },    { ""name"": ""Bob"", ""age"": 25 },    { ""name"": ""Charlie"", ""age"": 35 }  ]} output: ""Alice is 30 years old. Bob is 25 years old. Charlie is 35 years old."" Dot notation {{person.name}} is {{person.age}} years old. input: {  ""person"": {    ""name"": ""Alice"",    ""age"": 30  }} output: ""Alice is 30 years old."""
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#compose-your-prompt,Create a prompt,"Navigate to the Prompts section in the left-hand sidebar or from the application homepage.
Click the ""+ Prompt"" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.
"
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#compose-your-prompt,Compose your prompt,"After choosing a prompt type, you're brought to the playground to develop your prompt.
On the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click ""Start""."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#compose-your-prompt,Save your prompt,"To save your prompt, click the ""Save as"" button, name your prompt, and decide if you want it to be ""private"" or ""public"".
Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.
Click save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#compose-your-prompt,View your prompts,You've just created your first prompt! View a table of your prompts in the prompts tab.
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#compose-your-prompt,Add metadata,"To add metadata to your prompt, click the prompt and then click the ""Edit"" pencil icon next to the name.
This brings you to where you can add additional information about the prompt, including a description, a README, and use cases.
For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."
https://docs.smith.langchain.com/self_hosting/upgrades,General Upgrade Instructions,"For general upgrade instructions, please follow the instructions below. Certain versions may have specific upgrade instructions, which will be detailed in more specific upgrade guides."
https://docs.smith.langchain.com/self_hosting/upgrades,Kubernetes(Helm),"If you don't have the repo added, run the following command to add it: helm repo add langchain https://langchain-ai.github.io/helm/ Update your local helm repo helm repo update Update your helm chart config file with any updates that are needed in the new version. These will be detailed in the release notes for the new version. Run the following command to upgrade the chart(replace version with the version you want to upgrade to): helm upgrade <release-name> langchain/langsmith --version <version> --values <path-to-values-file> Verify that the upgrade was successful: helm status <release-name> All pods should be in the Running state. Verify that clickhouse is running and that both migrations jobs have completed. kubectl get podsNAME                                     READY   STATUS      RESTARTS   AGElangsmith-backend-95b6d54f5-gz48b        1/1     Running     0          15hlangsmith-pg-migrations-d2z6k            0/1     Completed   0          5h48mlangsmith-ch-migrations-gasvk            0/1     Completed   0          5h48mlangsmith-clickhouse-0                   1/1     Running     0          26hlangsmith-frontend-84687d9d45-6cg4r      1/1     Running     0          15hlangsmith-hub-backend-66ffb75fb4-qg6kl   1/1     Running     0          15hlangsmith-playground-85b444d8f7-pl589    1/1     Running     0          15hlangsmith-queue-d58cb64f7-87d68          1/1     Running     0          15h"
https://docs.smith.langchain.com/self_hosting/upgrades,Validate your deployment:,"Run kubectl get servicesOutput should look something like: NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGEkubernetes                   ClusterIP      172.20.0.1       <none>          443/TCP                      27dlangsmith-backend            ClusterIP      172.20.22.34     <none>          1984/TCP                     21dlangsmith-clickhouse         ClusterIP      172.20.117.62    <none>          8123/TCP,9000/TCP            21dlangsmith-frontend           LoadBalancer   172.20.218.30    <external ip>   80:30093/TCP,443:31130/TCP   21dlangsmith-platform-backend   ClusterIP      172.20.232.183   <none>          1986/TCP                     21dlangsmith-playground         ClusterIP      172.20.167.132   <none>          3001/TCP                     21dlangsmith-postgres           ClusterIP      172.20.59.63     <none>          5432/TCP                     21dlangsmith-redis              ClusterIP      172.20.229.98    <none>          6379/TCP                     20d Curl the external ip of the langsmith-frontend service:curl <external ip>/api/info{""version"":""0.5.7"",""license_expiration_time"":""2033-05-20T20:08:06"",""batch_ingest_config"":{""scale_up_qsize_trigger"":1000,""scale_up_nthreads_limit"":16,""scale_down_nempty_trigger"":4,""size_limit"":100,""size_limit_bytes"":20971520}} Check that the version matches the version you upgraded to. Visit the external ip for the langsmith-frontend service on your browserThe Langsmith UI should be visible/operational"
https://docs.smith.langchain.com/self_hosting/upgrades,Docker,"Upgrading the Docker version of LangSmith is a bit more involved than the Helm version and may require a small amount of downtime. Please follow the instructions below to upgrade your Docker version of LangSmith. Update your docker-compose.yml file to the file used in the latest release. You can find this in the LangSmith SDK GitHub repositoryUpdate your .env file with any new environment variables that are required in the new version. These will be detailed in the release notes for the new version.Run the following command to stop your current LangSmith instance: docker-compose down Run the following command to start your new LangSmith instance in the background: docker-compose up -d If everything ran successfully, you should see all the LangSmith containers running and healthy. CONTAINER ID   IMAGE                                  COMMAND                  CREATED        STATUS                        PORTS                                                      NAMESe1c8f01a4ffc   langchain/langsmith-frontend:0.5.7     ""/entrypoint.sh ngin""   10 hours ago   Up 40 seconds                 0.0.0.0:80->80/tcp, 8080/tcp                               cli-langchain-frontend-139e1394846b9   langchain/langsmith-backend:0.5.7      ""/bin/sh -c 'exec uv""   10 hours ago   Up 40 seconds                 0.0.0.0:1984->1984/tcp                                     cli-langchain-backend-1f8688dd58f2f   langchain/langsmith-go-backend:0.5.7   ""./smith-go""             10 hours ago   Up 40 seconds                 0.0.0.0:1986->1986/tcp                                     cli-langchain-platform-backend-1006f1303b04d   langchain/langsmith-backend:0.5.7      ""saq app.workers.que""   10 hours ago   Up 40 seconds                                                                            cli-langchain-queue-173a90242ed3a   redis:7                                ""docker-entrypoint.s""   10 hours ago   Up About a minute (healthy)   0.0.0.0:63791->6379/tcp                                    cli-langchain-redis-1eecf75ca672b   postgres:14.7                          ""docker-entrypoint.s""   10 hours ago   Up About a minute (healthy)   0.0.0.0:5433->5432/tcp                                     cli-langchain-db-13aa5652a864d   clickhouse/clickhouse-server:23.9      ""/entrypoint.sh""         10 hours ago   Up About a minute (healthy)   9009/tcp, 0.0.0.0:8124->8123/tcp, 0.0.0.0:9001->9000/tcp   cli-langchain-clickhouse-184edc329a37f   langchain/langsmith-playground:0.5.7   ""docker-entrypoint.s""   10 hours ago   Up About a minute             0.0.0.0:3001->3001/tcp                                     cli-langchain-playground-1"
https://docs.smith.langchain.com/self_hosting/upgrades,Validate your deployment:,"Curl the exposed port of the cli-langchain-frontend-1 container:curl localhost:80/info{""version"":""0.5.7"",""license_expiration_time"":""2033-05-20T20:08:06"",""batch_ingest_config"":{""scale_up_qsize_trigger"":1000,""scale_up_nthreads_limit"":16,""scale_down_nempty_trigger"":4,""size_limit"":100,""size_limit_bytes"":20971520}} 1 . Visit the exposed port of the cli-langchain-frontend-1 container on your browser The Langsmith UI should be visible/operational"
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores,Audit evaluator scores,"LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores,In the comparison view,"In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the ""edit"" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under ""Make correction"".
If you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples
in place of the few_shot_explanation prompt variable."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores,In the runs table,"In the runs table, find the ""Feedback"" column and click on the feedback tag to bring up the feedback details. Again, click the ""edit"" icon on the right to bring up the corrections view."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores,In the SDK,"Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      ""score"": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-the-prompt-content,Update a prompt,Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit.
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-the-prompt-content,Update metadata,"To update the prompt metadata (description, use cases, etc.) click the ""Edit"" pencil icon. Your prompt metadata will be updated upon save."
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-the-prompt-content,Update the prompt content,"To update the prompt content itself, you need to enter the prompt playground. Click ""Edit in playground"".
Now you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click ""Commit"" to save it. 
"
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#update-the-prompt-content,Version a prompt,"When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the ""Commits"" tab in the prompt view."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#dataset-schema-validation,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/how_to_guides/setup/create_account_api_key,Create an account,"To get started with LangSmith, you need to create an account. You can sign up for a free account here.
We support logging in with Google, GitHub, Discord, and email."
https://docs.smith.langchain.com/how_to_guides/setup/create_account_api_key,API keys,"LangSmith supports two types of API keys: Service Keys and Personal Access Tokens.
Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases. Read more about the differences between Service Keys and Personal Access Tokens under admin concepts"
https://docs.smith.langchain.com/how_to_guides/setup/create_account_api_key,Create an API key,"To log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests.
Currently, an API key is scoped to a workspace, so you will need to create an API key for each workspace you want to use. To create either type of API key head to the Settings page, then scroll to the API Keys section. Then click Create API Key. noteThe API key will be shown only once, so make sure to copy it and store it in a safe place."
https://docs.smith.langchain.com/how_to_guides/monitoring,How-to guides: Monitoring and automations,"This section contains how-to guides related to monitoring and automations.  Filter traces in the applicationBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here: Use monitoring chartsLangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project. Set up automation rulesWhile you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users. Online EvaluationBefore diving into this content, it might be helpful to read the following: Set up threadsBefore diving into this content, it might be helpful to read the following: Set up webhook notifications for rulesWhen you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."
https://docs.smith.langchain.com/how_to_guides/monitoring,Filter traces in the application,"Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:"
https://docs.smith.langchain.com/how_to_guides/monitoring,Use monitoring charts,LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.
https://docs.smith.langchain.com/how_to_guides/monitoring,Set up automation rules,"While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users."
https://docs.smith.langchain.com/how_to_guides/monitoring,Online Evaluation,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/monitoring,Set up threads,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/monitoring,Set up webhook notifications for rules,"When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."
https://docs.smith.langchain.com/how_to_guides/monitoring/threads,Set up threads,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Add metadata and tags to traces Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith."
https://docs.smith.langchain.com/how_to_guides/monitoring/threads,Group traces into threads,"A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread. To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread. The key value is the unique identifier for that conversation.
The key name should be one of: session_idthread_idconversation_id. The value should be a UUID, such as f47ac10b-58cc-4372-a567-0e02b2c3d479."
https://docs.smith.langchain.com/how_to_guides/monitoring/threads,View threads,"You can view threads by clicking on the Threads tad in any project details page. You will then see a list of all threads, sorted by the most recent activity. You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs. You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,Set up billing for your LangSmith account,"noteIf you are interested in the Enterprise plan, please contact sales. This guide is
only for our self-serve billing plans. noteIf you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please skip to the final section. To set up billing for your LangSmith organization, head to the Usage and Billing page under Settings.
Depending on your organization's settings, you will be given a different walkthrough to get started."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,Developer Plan: set up billing on your personal organization,"Personal organizations are limited to 5000 traces per month until a credit card is added. You can
add a credit card on the Plans and Billing page as follows:"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,2. Add your credit card info,"After this step, you will no longer be rate limited to 5000 traces, and will be charged for any excess
traces at rates specified on our pricing page."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,Plus Plan: set up billing on a shared organization,"If you have not yet created an organization, please do so by following this guide. This walkthrough assumes you are
already in a new organization. noteNew organizations are not usable until a credit card is entered. After you complete the following steps, you will
gain complete access to LangSmith."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,1. ClickSubscribeon the Plus page,"noteIf you are a startup building with AI, please instead click Apply Now on our Startup Plan. You may be
eligible for discounted prices and a generous free, monthly trace allotment."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,2. Review your existing members,"Before subscribing, LangSmith lets you remove any added users that you would not
like to be charged for."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,3. Enter your credit card info,"Enter business information, invoice email and tax id If this organization belongs to a business. Please check the ""This is a business"" checkbox and enter the information accordingly. For more information refer to this guide Once this step is complete, your org should now have access to the rest of LangSmith!"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,"Set up billing for accounts created before pricing was introduced on April 2, 2024","If you joined LangSmith before pricing was introduced April 2, 2024, you have the option to upgrade your
existing account to setup billing. If you did not set up billing by July 8, 2024, then your account is now
rate limited to a maximum of 5,000 traces per month."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_billing,3. Enter your credit card info,"If you are on a Personal Organization, this will add you to the Developer plan. If you are on a shared Organization, this will add you to the Plus plan. For more information,
please view the above walkthroughs for Developer or Plus respectively, starting at step 2."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Evaluation,"The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Datasets,"Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Creating datasets,"There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.
From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,
and what ""good"" responses may be.
You probably want to cover a few different common edge cases or situations you can imagine.
Even 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?
This information can be valuable to capture and store in datasets. This allows you to test against these
use cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?
There are a few heuristics you can follow.
If possible - try to collect end user feedback. You can then see which datapoints got negative feedback.
That is super valuable! These are spots where your application did not perform well.
You should add these to your dataset to test against in the future. You can also use other heuristics
to identify ""interesting"" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.
It's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.
This can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Types of datasets,"LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:""Inputs"" and ""outputs"" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating ""completion"" style language models.The ""inputs"" dictionary contains a single ""input"" key mapped to the prompt string.The ""outputs"" dictionary contains a single ""output"" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured ""chat"" messages as inputs and outputs.The ""inputs"" dictionary contains a single ""input"" key mapped to a list of serialized chat messagesThe ""outputs"" dictionary contains a single ""output"" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Partitioning datasets,"When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Evaluators,"Evaluators are functions in LangSmith that score how well your application performs on a particular example.
Evaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Human,Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering.
https://docs.smith.langchain.com/concepts/evaluation#experiments,Heuristic,"Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."
https://docs.smith.langchain.com/concepts/evaluation#experiments,LLM-as-judge,"LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Pairwise,"Pairwise evaluators pick the better of two task outputs based upon some criteria.
This can use either a heuristic (""which response is longer""), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.
This can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Applying evaluations,"We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Unit Tests,"Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.
This means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).
These types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)
to cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Regression Testing,"Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.
They are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Back-testing,"Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.
Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.
Then compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Pairwise-testing,"It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Online Evaluation,"Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Experiment Configurations,"LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Evaluating Specific LLM Applications,"Below, we will discuss evaluation of a few specific, popular LLM applications."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Agents,"LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.
Note that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Evaluating an agent's final response,"One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Evaluating a single step of an agent,"Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Evaluating an agent's trajectory,"Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an ""exact"" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the ""exact"" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of ""incorrect"" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Best practices,"Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Retrieval Augmented Generation (RAG),"Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Dataset,"When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."
https://docs.smith.langchain.com/concepts/evaluation#experiments,Evaluator,"LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Applying RAG Evaluation,"When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"
https://docs.smith.langchain.com/concepts/evaluation#experiments,RAG evaluation summary,Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes
https://docs.smith.langchain.com/concepts/evaluation#experiments,Summarization,"Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"
https://docs.smith.langchain.com/concepts/evaluation#experiments,Classification / Tagging,"Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#configure-workspace-settings,Set up a workspace,"infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.
Workspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#configure-workspace-settings,Create a workspace,"To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.
Once your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.
Please see the pricing page for more information."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#configure-workspace-settings,Manage users,"infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.
Users may also be invited directly to one or more workspaces when they are invited to an organization."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#configure-workspace-settings,Configure workspace settings,"Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#save-your-prompt,Create a prompt,"Navigate to the Prompts section in the left-hand sidebar or from the application homepage.
Click the ""+ Prompt"" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.
"
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#save-your-prompt,Compose your prompt,"After choosing a prompt type, you're brought to the playground to develop your prompt.
On the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click ""Start""."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#save-your-prompt,Save your prompt,"To save your prompt, click the ""Save as"" button, name your prompt, and decide if you want it to be ""private"" or ""public"".
Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.
Click save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#save-your-prompt,View your prompts,You've just created your first prompt! View a table of your prompts in the prompts tab.
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#save-your-prompt,Add metadata,"To add metadata to your prompt, click the prompt and then click the ""Edit"" pencil icon next to the name.
This brings you to where you can add additional information about the prompt, including a description, a README, and use cases.
For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."
https://docs.smith.langchain.com/reference/regions_faq,Regions FAQ,noteSee the cloud architecture reference for additional details.
https://docs.smith.langchain.com/reference/regions_faq,Legal and compliance,"What privacy and data protection frameworks does LangSmith, including its EU instance, comply with? LangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com.
If you would like to sign a Data Processing Addendum (DPA) with us, please reach out to support@langchain.dev. Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan. My company isnt based in the EU, can I still have my data hosted there? Yes, you can host your LangSmith data in the EU instance independent of your location. Do you have a legal entity in the EU that we can contract with? We do not have a legal entity in the EU for customer contracting today. Do different legal terms apply if I choose the EU region? The terms are the same for the EU and US regions."
https://docs.smith.langchain.com/reference/regions_faq,Features,"Are there any functional differences between US and EU cloud-managed LangSmith? There may be a small delay between launches to each region depending on the feature.
Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa. Can an organization have workspaces in different regions? LangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case. Can I connect an EU organization to a US organization and share billing? LangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case. What data will be stored in my selected region? See the cloud architecture reference for details. How can I see my organization's region? Check your URL - any organizations on https://eu.smith.langchain.com are in the EU, and any on https://smith.langchain.com are in the US. Can I switch my organization from the US to EU or vice versa? We do not support migration between regions at this time, but if you are interested in this feature, please reach out to support@langchain.dev."
https://docs.smith.langchain.com/reference/regions_faq,Plans and pricing,"Is the EU region available on all LangSmith plans? Yes, you can sign up for the EU region on all plans including free plans. Is pricing different for the EU region compared to the US region? No, pricing is the same for the EU and US regions. What currency is used for payment if I use the EU region? All LangSmith plans are paid in USD."
https://docs.smith.langchain.com/tutorials,Tutorials,"New to LangSmith? This is the place to start. Here, you'll find a hands-on introduction to key LangSmith workflows."
https://docs.smith.langchain.com/tutorials,Developers,Add observability to your LLM applicationEvaluate your LLM applicationOptimize a classifierRAG EvaluationsBacktestingAgent Evaluations
https://docs.smith.langchain.com/tutorials,Administrators,Optimize tracing spend on LangSmith
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project,Log traces to specific project,You can change the destination project of your traces both statically through environment variables and dynamically at runtime.
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project,Set the destination project statically,"As mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-custom-project If the project specified does not exist, it will be created automatically when the first trace is ingested."
https://docs.smith.langchain.com/how_to_guides/tracing/log_traces_to_project,Set the destination project dynamically,"You can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application. noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""Hello!""}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(    run_type=""llm"",    name=""OpenAI Call Decorator"",    project_name=""My Project"")def call_openai(    messages: list[dict], model: str = ""gpt-3.5-turbo"") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(    messages,    langsmith_extra={""project_name"": ""My Overriden Project""},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=messages,    langsmith_extra={""project_name"": ""My Project""},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(    run_type=""llm"",    name=""OpenAI Call RunTree"",    inputs={""messages"": messages},    project_name=""My Project"")chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";import { RunTree} from ""langsmith"";const client = new OpenAI();const messages = [    {role: ""system"", content: ""You are a helpful assistant.""},    {role: ""user"", content: ""Hello!""}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: ""llm"",    name: ""OpenAI Call Traceable"",    project_name: ""My Project""});// Call the traceable functionawait traceableCallOpenAI(messages, ""gpt-3.5-turbo"");// Create and use a RunTree objectconst rt = new RunTree({    runType: ""llm"",    name: ""OpenAI Call RunTree"",    inputs: { messages },    project_name: ""My Project""});// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.postRun();"
https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection,Dynamic few shot example selection,noteThis feature is currently in closed beta. Please sign up here for access Configure your datasets so that you can search for few shot examples based on an incoming request.
https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection,Pre-conditions,Your dataset must have the KV store data type (we do not currently support chat model or LLM type datasets)You must have an input schema defined for your dataset. See our docs on setting up schema validation in our UI for details.You must be enabled for the closed betaYou must be on LangSmith cloud
https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection,Index your dataset to be searched,"On the datasets UI, click the Few-shot Index on the top right corner and hit Start Sync. This process will start to index your data to be searchable in the background. A note will appear on the
modal above that says if your index is up to date, and if not, what version of your dataset it last indexed. All new data added to your dataset will automatically be indexed. You do not need to re-index when adding new data."
https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection,Search your dataset for similar examples,"You can search your dataset via API for similar examples using the POST /datasets/<id>/search REST API. Its documentation
can be found here.
You can see detailed examples of how use this in your prompts both with and without LangChain in
our cookbook on using indexed datasets with few shot prompts"
https://docs.smith.langchain.com/self_hosting/configuration/user_management,User management,noteThis guide assumes you have read the admin guide and organization setup guide. LangSmith offers additional customization features for user management using feature flags.
https://docs.smith.langchain.com/self_hosting/configuration/user_management,Workspace level invites to an organization,"The default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization, as this operation can increase cost by adding seats.
For self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to
the organization as well as their specific workspace at the workspace level. Once this feature is enabled via the configuration option below, workspace Admins may add new users in the Workspace members tab under Settings > Workspaces.
Both of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before. Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspaceInvite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state). Admins may invite users for both cases at the same time. Configuration HelmDockerconfig:    workspaceScopeOrgInvitesEnabled: true# In your .env fileFF_WORKSPACE_SCOPE_ORG_INVITES_ENABLED=""true""    "
https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats,Generating Clickhouse Stats,"As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency. This command will generate a CSV that can be shared with the LangChain team."
https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats,Prerequisites,"Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/Clickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the Clickhouse database from the machine you will be running the get_clickhouse_stats script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine."
https://docs.smith.langchain.com/self_hosting/scripts/generate_clickhouse_stats,Running the clickhouse stats generation script,"Run the following command to run the stats generation script: sh get_clickhouse_stats.sh <clickhouse_url> --output path/to/file.csv For example, if you are using the bundled version with port-forwarding, the command would look like: sh get_clickhouse_stats.sh ""clickhouse://default:password@localhost:8123/default"" --output clickhouse_stats.csv and after running this command you should see a file, clickhouse_stats.csv, has been created with Clickhouse statistics."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Set up webhook notifications for rules,"When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Webhook payload,"The payload we send to your webhook endpoint contains ""rule_id"" this is the ID of the automation that sent this payload""start_time"" and ""end_time"" these are the time boundaries where we found matching runs""runs"" this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.""feedback_stats"" this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below. ""feedback_stats"": {    ""about_langchain"": {        ""n"": 1,        ""avg"": 0.0,        ""show_feedback_arrow"": true,        ""values"": {}    },    ""category"": {        ""n"": 0,        ""avg"": null,        ""show_feedback_arrow"": true,        ""values"": {            ""CONCEPTUAL"": 1        }    },    ""user_score"": {        ""n"": 2,        ""avg"": 0.0,        ""show_feedback_arrow"": false,        ""values"": {}    },    ""vagueness"": {        ""n"": 1,        ""avg"": 0.0,        ""show_feedback_arrow"": true,        ""values"": {}    }}, fetching from S3 URLsDepending on how recent your runs are, the inputs_s3_urls and outputs_s3_urls fields may contain S3 URLs to the actual data instead of the data itself.The inputs and outputs can be fetched by the ROOT.presigned_url provided in inputs_s3_urls and outputs_s3_urls respectively. This is an example of the entire payload we send to your webhook endpoint: {  ""rule_id"": ""d75d7417-0c57-4655-88fe-1db3cda3a47a"",  ""start_time"": ""2024-04-05T01:28:54.734491+00:00"",  ""end_time"": ""2024-04-05T01:28:56.492563+00:00"",  ""runs"": [    {      ""status"": ""success"",      ""is_root"": true,      ""trace_id"": ""6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""dotted_order"": ""20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""run_type"": ""tool"",      ""modified_at"": ""2024-04-05T01:28:54.145062"",      ""tenant_id"": ""2ebda79f-2946-4491-a9ad-d642f49e0815"",      ""end_time"": ""2024-04-05T01:28:54.085649"",      ""name"": ""Search"",      ""start_time"": ""2024-04-05T01:28:54.085646"",      ""id"": ""6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""session_id"": ""6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5"",      ""parent_run_ids"": [],      ""child_run_ids"": null,      ""direct_child_run_ids"": null,      ""total_tokens"": 0,      ""completion_tokens"": 0,      ""prompt_tokens"": 0,      ""total_cost"": null,      ""completion_cost"": null,      ""prompt_cost"": null,      ""first_token_time"": null,      ""app_path"": ""/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809"",      ""in_dataset"": false,      ""last_queued_at"": null,      ""inputs"": null,      ""inputs_s3_urls"": null,      ""outputs"": null,      ""outputs_s3_urls"": null,      ""extra"": null,      ""events"": null,      ""feedback_stats"": null,      ""serialized"": null,      ""share_token"": null    }  ]}"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Webhook Security,"We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications. An example would be https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Webhook custom HTTP headers,"If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the Headers option next to the URL field and add your headers. noteHeaders are stored in encrypted format."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Webhook Delivery,"When delivering events to your webhook endpoint we follow these guidelines If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.Anything your endpoint returns in the body will be ignored"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Setup,"For an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here. First, create a Modal account. Then, locally install the Modal SDK: pip install modal To finish setting up your account, run the command: modal setup and follow the instructions"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Secrets,"Next, you will need to set up some secrets in Modal. First, LangSmith will need to authenticate to Modal by passing in a secret.
The easiest way to do this is to pass in a secret in the query parameters.
To validate this secret, we will need to add a secret in Modal to validate it.
We will do that by creating a Modal secret.
You can see instructions for secrets here.
For this purpose, let's call our secret ls-webhook and have it set an environment variable with the name LS_WEBHOOK. We can also set up a LangSmith secret - luckily there is already an integration template for this!"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Service,"After that, you can create a Python file that will serve as your endpoint.
An example is below, with comments explaining what is going on: from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub(""auth-example"", image=Image.debian_slim().pip_install(""langsmith""))@stub.function(    secrets=[Secret.from_name(""ls-webhook""), Secret.from_name(""my-langsmith-secret"")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method=""POST"")# We set up a `secret` query parameterdef f(data: dict, secret: str = Query(...)):    # You can import dependencies you don't have locally inside Modal funxtions    from langsmith import Client    # First, we validate the secret key we pass    import os    if secret != os.environ[""LS_WEBHOOK""]:        raise HTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=""Incorrect bearer token"",            headers={""WWW-Authenticate"": ""Bearer""},        )    # This is where we put the logic for what should happen inside this webhook    ls_client = Client()    runs = data[""runs""]    ids = [r[""id""] for r in runs]    feedback = list(ls_client.list_feedback(run_ids=ids))    for r, f in zip(runs, feedback):        try:            ls_client.create_example(                inputs=r[""inputs""],                outputs={""output"": f.correction},                dataset_name=""classifier-github-issues"",            )        except Exception:            raise ValueError(f""{r} and {f}"")    # Function body    return ""success!"" We can now deploy this easily with modal deploy ... (see docs here). You should now get something like:  Created objects.  Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py  Created mount PythonPackage:langsmith  Created f => https://hwchase17--auth-example-f.modal.run App deployed! View Deployment: https://modal.com/apps/hwchase17/auth-example The important thing to remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.
NOTE: this is NOT the final deployment URL, make sure not to accidentally use that."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks#webhook-payload,Hooking it up,"We can now take the function URL we create above and add it as a webhook.
We have to remember to also pass in the secret key as a query parameter.
Putting it all together, it should look something like: https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET} Replace {SECRET} with the secret key you created to access the Modal service."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#change-the-time-period,Use monitoring charts,LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#change-the-time-period,Change the time period,"You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#change-the-time-period,Slice data by metadata or tag,"By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.
This can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.
After that, you can click the Tag or Metadata tab at the top to group runs accordingly."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts#change-the-time-period,Drill down into specific subsets,"Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."
https://docs.smith.langchain.com/reference,Reference,"Technical reference that covers components, APIs, and other aspects of LangSmith."
https://docs.smith.langchain.com/reference,API reference,LangSmith API Reference
https://docs.smith.langchain.com/reference,SDK reference,LangChain off-the-shelf evaluators (Python only)
https://docs.smith.langchain.com/reference,Architecture reference,Cloud architecture and scalabilityRegions FAQ
https://docs.smith.langchain.com/reference,Data formats,Run (span) data formatFeedback data formatTrace query syntaxFilter argumentsFilter query language
https://docs.smith.langchain.com/reference,Authentication and authorization,Authentication methods
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/self_hosting/configuration/basic_auth,Email/password a.k.a. basic auth (beta),"LangSmith supports login via username/password with a few limitations during the beta period: You cannot change an existing installation from basic auth mode to OIDC or vice versa - installations must be either one or the other. A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing None type installation (see below).Users must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin."
https://docs.smith.langchain.com/self_hosting/configuration/basic_auth,Requirements and features,"There is a single Default organization that is provisioned during initial installation, and creating additional organizations is not supportedYour initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbolThere are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example: openssl rand -base64 32"
https://docs.smith.langchain.com/self_hosting/configuration/basic_auth,Migrating from None auth,"Only supported in versions 0.7 and above. Migrating an installation from None auth mode replaces the single ""default"" user with a user with the configured credentials and keeps all existing resources.
The single pre-existing workspace ID post-migration remains 00000000-0000-0000-0000-000000000000, but everything else about the migrated installation is standard for a basic auth installation. To migrate, simply update your configuration as shown below and run helm upgrade (or docker-compose up) as usual."
https://docs.smith.langchain.com/self_hosting/configuration/basic_auth,Configuration,"noteChanging the JWT secret will log out your users HelmDockerconfig:  authType: mixed  basicAuth:    enabled: true    initialOrgAdminEmail: <YOUR EMAIL ADDRESS>    initialOrgAdminPassword: <PASSWORD>    jwtSecret: <SECRET># In your .env fileAUTH_TYPE=mixedBASIC_AUTH_ENABLED=trueINITIAL_ORG_ADMIN_EMAIL=<YOUR EMAIL ADDRESS>INITIAL_ORG_ADMIN_PASSWORD=<PASSWORD>BASIC_AUTH_JWT_SECRET=<SECRET> Once configured, you will see a login screen like the one below. You should be able to login with the initialOrgAdminEmail and initialOrgAdminPassword values, and your user will be auto-provisioned with role Organization Admin. See the admin guide for more details on organization roles."
https://docs.smith.langchain.com/how_to_guides/tracing/share_trace,Share or unshare a trace publicly,"cautionSharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.
This feature is only available in the cloud-hosted version of LangSmith. To share a trace publicly, simply click on the Share button in the upper right hand side of any trace view.
 This will open a dialog where you can copy the link to the trace. Shared traces will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the trace, but not edit it. To ""unshare"" a trace, either Click on Unshare by click on Public in the upper right hand corner of any publicly shared trace, then Unshare in the dialog.
Navigate to your organization's list of publicly shared traces, either by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the trace you want to unshare.
"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#compare-two-experiments-with-llm-based-pairwise-evaluators,Run pairwise evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function
with two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#compare-two-experiments-with-llm-based-pairwise-evaluators,Use theevaluate_comparativefunction,"notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#compare-two-experiments-with-llm-based-pairwise-evaluators,Configure inputs and outputs for pairwise evaluators,"Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with
runs[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent ""both equally bad"" or both to 1 for ""both equally good"". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#compare-two-experiments-with-llm-based-pairwise-evaluators,Compare two experiments with LLM-based pairwise evaluators,"The following example uses a prompt
which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.
The prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(""langchain-ai/pairwise-evaluation-2"")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=""gpt-4"")        runnable = prompt | model    response = runnable.invoke({        ""question"": example.inputs[""question""],        ""answer_a"": runs[0].outputs[""output""] if runs[0].outputs is not None else ""N/A"",        ""answer_b"": runs[1].outputs[""output""] if runs[1].outputs is not None else ""N/A"",    })    score = response[""Preference""]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {""key"": ""ranked_preference"", ""scores"": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [""my-experiment-name-1"", ""my-experiment-name-2""],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.
import type { Run, Example } from ""langsmith"";import { evaluateComparative } from ""langsmith/evaluation"";import { wrapOpenAI } from ""langsmith/wrappers"";import OpenAI from ""openai"";const openai = wrapOpenAI(new OpenAI());import { z } from ""zod"";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(""Expected at least two runs"");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? ""N/A"",    answer_b: runB?.outputs?.output ?? ""N/A"",  };    const output = await openai.chat.completions.create({    model: ""gpt-4-turbo"",    messages: [      {        role: ""system"",        content: [          ""Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below."",          ""You should choose the assistant that follows the user's instructions and answers the user's question better."",          ""Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses."",          ""Begin your evaluation by comparing the two responses and provide a short explanation."",          ""Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision."",          ""Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible."",        ].join("" ""),      },      {        role: ""user"",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(""\n\n""),      },    ],    tool_choice: {      type: ""function"",      function: { name: ""Score"" },    },    tools: [      {        type: ""function"",        function: {          name: ""Score"",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output ""1"" if Assistant A answer is better based upon the factors above.`,            `Output ""2"" if Assistant B answer is better based upon the factors above.`,            `Output ""0"" if it is a tie.`,          ].join("" ""),          parameters: {            type: ""object"",            properties: {              Preference: {                type: ""integer"",                description: ""Which assistant answer is preferred?"",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: ""ranked_preference"", scores };}await evaluateComparative([""earnest-name-40"", ""reflecting-pump-91""], {  evaluators: [evaluatePairwise],});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#compare-two-experiments-with-llm-based-pairwise-evaluators,View pairwise experiments,"Navigate to the ""Pairwise Experiments"" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,Architectural overview,"Enterprise License RequiredSelf-Hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith can be run via Kubernetes (recommended) or Docker in a Cloud environment that you control. The LangSmith application consists of several components including 5 LangSmith servers and 3 stateful services: LangSmith FrontendLangSmith BackendLangSmith Platform BackendLangSmith PlaygroundLangSmith QueueClickHousePostgresRedis To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine."
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,Storage Services,"noteLangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services.
In a production setting, we strongly recommend using external Storage Services."
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,ClickHouse,"ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data)."
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,PostgreSQL,"PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback)."
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,Redis,"Redis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching. LangSmith uses Redis to back queuing/caching operations."
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,LangSmith Frontend,The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,LangSmith Backend,"The backend is the primary entrypoint for API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and sdk, preparing traces for ingestion, and supporting the hub API."
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,LangSmith Queue,"The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database."
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,LangSmith Platform Backend,The platform backend is an internal service that primarily handles authentication and other high-volume tasks. The user should not need to interact with this service directly.
https://docs.smith.langchain.com/self_hosting/architectural_overview#services,LangSmith Playground,The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.
https://docs.smith.langchain.com/concepts/prompts,Prompts,"Writing good prompts is key to getting the best performance out of your applications. LangSmith provides ways to create, test, and manage prompts."
https://docs.smith.langchain.com/concepts/prompts,Prompt types,"We support three types of prompt templates: StringPromptTemplateChatPromptTemplateStructuredPromptTemplate For detailed information about these templates, please refer to the LangChain documentation. In LangSmith, you can create prompts using the Playground. From the prompt view in the Playground, you can select either ""Chat-style prompt"" or ""Instruct-style prompt"" to get started. Chat-style prompt Chat prompts are used for chat-style models that accept a list of messages as an input and respond with an assistant message. A chat-style prompt is represented in LangSmith as a ChatPromptTemplate, which can contain multiple messages, each with prompt variables. You can also specify an output schema which is represented in LangSmith as a StructuredPromptTemplate. Instruct-style prompt An instruct-style prompt is represented as a StringPromptTemplate that gets formatted to a single string input for your model."
https://docs.smith.langchain.com/concepts/prompts,Template formats,We support two types of template formats: f-string and mustache. You can switch between these formats when editing prompts in the Playground.
https://docs.smith.langchain.com/concepts/prompts,F-string,"F-strings are a Python-specific string formatting method that allows you to embed expressions inside string literals, using curly braces {}.
Here's an example of an f-string template: Hello, {name}!"
https://docs.smith.langchain.com/concepts/prompts,Mustache,"Mustache is a template syntax that allows you to embed variables inside double curly braces {{}}. NotePlease see the Mustache documentation for more detailed information on how to use Mustache templates. Here's an example of a mustache template: Hello, {{name}}! Mustache is more robust than f-strings since it supports more complex logic. You can use conditionals, loops, and access nested keys in mustache templates. Conditional Hello, {{#name}}{{name}}{{/name}}{{^name}}world{{/name}}! The template will output ""Hello, Bob!"" if ""Bob"" is provided as the name variable, and ""Hello, world!"" if the name variable is not provided. Loop {{#names}}{{name}}{{/names}} input: {  ""names"": [{ ""name"": ""Alice"" }, { ""name"": ""Bob"" }, { ""name"": ""Charlie"" }]} output: ""AliceBobCharlie"" Loop with nesting {{#people}}{{name}} is {{age}} years old. {{/people}} input: {  ""people"": [    { ""name"": ""Alice"", ""age"": 30 },    { ""name"": ""Bob"", ""age"": 25 },    { ""name"": ""Charlie"", ""age"": 35 }  ]} output: ""Alice is 30 years old. Bob is 25 years old. Charlie is 35 years old."" Dot notation {{person.name}} is {{person.age}} years old. input: {  ""person"": {    ""name"": ""Alice"",    ""age"": 30  }} output: ""Alice is 30 years old."""
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,Trace withLangGraph(Python and JS/TS),"LangSmith smoothly integrates with LangGraph (Python and JS)
to help you trace agentic workflows, whether you're using LangChain modules or other SDKs."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,With LangChain,"If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing. This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,0. Installation,"Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langgraphyarn add @langchain/openai @langchain/langgraphnpm install @langchain/openai @langchain/langgraphpnpm add @langchain/openai @langchain/langgraph"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,2. Log a trace,"Once you've set up your environment, you can call LangChain runnables as normal.
LangSmith will infer the proper tracing config: PythonTypeScriptfrom typing import Literalfrom langchain_core.messages import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain_core.tools import toolfrom langgraph.graph import StateGraph, MessagesStatefrom langgraph.prebuilt import ToolNode@tooldef search(query: str):    """"""Call to surf the web.""""""    if ""sf"" in query.lower() or ""san francisco"" in query.lower():        return ""It's 60 degrees and foggy.""    return ""It's 90 degrees and sunny.""tools = [search]tool_node = ToolNode(tools)model = ChatOpenAI(model=""gpt-4o"", temperature=0).bind_tools(tools)def should_continue(state: MessagesState) -> Literal[""tools"", ""__end__""]:    messages = state['messages']    last_message = messages[-1]    if last_message.tool_calls:        return ""tools""    return ""__end__""def call_model(state: MessagesState):    messages = state['messages']    # Invoking `model` will automatically infer the correct tracing context    response = model.invoke(messages)    return {""messages"": [response]}workflow = StateGraph(MessagesState)workflow.add_node(""agent"", call_model)workflow.add_node(""tools"", tool_node)workflow.add_edge(""__start__"", ""agent"")workflow.add_conditional_edges(    ""agent"",    should_continue,)workflow.add_edge(""tools"", 'agent')app = workflow.compile()final_state = app.invoke(    {""messages"": [HumanMessage(content=""what is the weather in sf"")]},    config={""configurable"": {""thread_id"": 42}})final_state[""messages""][-1].contentimport { HumanMessage, AIMessage } from ""@langchain/core/messages"";import { tool } from ""@langchain/core/tools"";import { z } from ""zod"";import { ChatOpenAI } from ""@langchain/openai"";import { StateGraph, StateGraphArgs } from ""@langchain/langgraph"";import { ToolNode } from ""@langchain/langgraph/prebuilt"";interface AgentState {  messages: HumanMessage[];}const graphState: StateGraphArgs<AgentState>[""channels""] = {  messages: {    reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),  },};const searchTool = tool(async ({ query }: { query: string }) => {  if (query.toLowerCase().includes(""sf"") || query.toLowerCase().includes(""san francisco"")) {    return ""It's 60 degrees and foggy.""  }  return ""It's 90 degrees and sunny.""}, {  name: ""search"",  description:    ""Call to surf the web."",  schema: z.object({    query: z.string().describe(""The query to use in your search.""),  }),});const tools = [searchTool];const toolNode = new ToolNode<AgentState>(tools);const model = new ChatOpenAI({  model: ""gpt-4o"",  temperature: 0,}).bindTools(tools);function shouldContinue(state: AgentState) {  const messages = state.messages;  const lastMessage = messages[messages.length - 1] as AIMessage;  if (lastMessage.tool_calls?.length) {    return ""tools"";  }  return ""__end__"";}async function callModel(state: AgentState) {  const messages = state.messages;  // Invoking `model` will automatically infer the correct tracing context  const response = await model.invoke(messages);  return { messages: [response] };}const workflow = new StateGraph<AgentState>({ channels: graphState })  .addNode(""agent"", callModel)  .addNode(""tools"", toolNode)  .addEdge(""__start__"", ""agent"")  .addConditionalEdges(""agent"", shouldContinue)  .addEdge(""tools"", ""agent"");const app = workflow.compile();const finalState = await app.invoke(  { messages: [new HumanMessage(""what is the weather in sf"")] },  { configurable: { thread_id: ""42"" } });finalState.messages[finalState.messages.length - 1].content; An example trace from running the above code looks like this:"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,Without LangChain,"If you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately
(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).
If you do so, LangSmith will automatically nest traces from those wrapped methods. Here's an example. You can also see this page for more information."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,0. Installation,Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below). pipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#without-langchain,2. Log a trace,"Once you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.
LangSmith will then infer the proper tracing config: PythonTypeScriptimport jsonimport openaiimport operatorfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaifrom typing import Annotated, Literal, TypedDictfrom langgraph.graph import StateGraphclass State(TypedDict):    messages: Annotated[list, operator.add]tool_schema = {    ""type"": ""function"",    ""function"": {        ""name"": ""search"",        ""description"": ""Call to surf the web."",        ""parameters"": {            ""type"": ""object"",            ""properties"": {""query"": {""type"": ""string""}},            ""required"": [""query""],        },    },}# Decorating the tool function will automatically trace it with the correct context@traceable(run_type=""tool"", name=""Search Tool"")def search(query: str):    """"""Call to surf the web.""""""    if ""sf"" in query.lower() or ""san francisco"" in query.lower():        return ""It's 60 degrees and foggy.""    return ""It's 90 degrees and sunny.""tools = [search]def call_tools(state):    function_name_to_function = {""search"": search}    messages = state[""messages""]    tool_call = messages[-1][""tool_calls""][0]    function_name = tool_call[""function""][""name""]    function_arguments = tool_call[""function""][""arguments""]    arguments = json.loads(function_arguments)    function_response = function_name_to_function[function_name](**arguments)    tool_message = {        ""tool_call_id"": tool_call[""id""],        ""role"": ""tool"",        ""name"": function_name,        ""content"": function_response,    }    return {""messages"": [tool_message]}wrapped_client = wrap_openai(openai.Client())def should_continue(state: State) -> Literal[""tools"", ""__end__""]:    messages = state[""messages""]    last_message = messages[-1]    if last_message[""tool_calls""]:        return ""tools""    return ""__end__""def call_model(state: State):    messages = state[""messages""]    # Calling the wrapped client will automatically infer the correct tracing context    response = wrapped_client.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", tools=[tool_schema]    )    raw_tool_calls = response.choices[0].message.tool_calls    tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []    response_message = {        ""role"": ""assistant"",        ""content"": response.choices[0].message.content,        ""tool_calls"": tool_calls,    }    return {""messages"": [response_message]}workflow = StateGraph(State)workflow.add_node(""agent"", call_model)workflow.add_node(""tools"", call_tools)workflow.add_edge(""__start__"", ""agent"")workflow.add_conditional_edges(    ""agent"",    should_continue,)workflow.add_edge(""tools"", 'agent')app = workflow.compile()final_state = app.invoke(    {""messages"": [{""role"": ""user"", ""content"": ""what is the weather in sf""}]})final_state[""messages""][-1][""content""]Note: The below example requires langsmith>=0.1.39 and @langchain/langgraph>=0.0.31
import OpenAI from ""openai"";import { StateGraph } from ""@langchain/langgraph"";import { wrapOpenAI } from ""langsmith/wrappers/openai"";import { traceable } from ""langsmith/traceable"";type GraphState = {  messages: OpenAI.ChatCompletionMessageParam[];};const wrappedClient = wrapOpenAI(new OpenAI({}));const toolSchema: OpenAI.ChatCompletionTool = {  type: ""function"",  function: {    name: ""search"",    description: ""Use this tool to query the web."",    parameters: {      type: ""object"",      properties: {        query: {          type: ""string"",        },      },      required: [""query""],    }  }};// Wrapping the tool function will automatically trace it with the correct contextconst search = traceable(async ({ query }: { query: string }) => {  if (    query.toLowerCase().includes(""sf"") ||    query.toLowerCase().includes(""san francisco"")  ) {    return ""It's 60 degrees and foggy."";  }  return ""It's 90 degrees and sunny.""}, { run_type: ""tool"", name: ""Search Tool"" });const callTools = async ({ messages }: GraphState) => {  const mostRecentMessage = messages[messages.length - 1];  const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;  if (toolCalls === undefined || toolCalls.length === 0) {    throw new Error(""No tool calls passed to node."");  }  const toolNameMap = {    search,  };  const functionName = toolCalls[0].function.name;  const functionArguments = JSON.parse(toolCalls[0].function.arguments);  const response = await toolNameMap[functionName](functionArguments);  const toolMessage = {    tool_call_id: toolCalls[0].id,    role: ""tool"",    name: functionName,    content: response,  }  return { messages: [toolMessage] };}const callModel = async ({ messages }: GraphState) => {  // Calling the wrapped client will automatically infer the correct tracing context  const response = await wrappedClient.chat.completions.create({    messages,    model: ""gpt-4o-mini"",    tools: [toolSchema],  });  const responseMessage = {    role: ""assistant"",    content: response.choices[0].message.content,    tool_calls: response.choices[0].message.tool_calls ?? [],  };  return { messages: [responseMessage] };}const shouldContinue = ({ messages }: GraphState) => {  const lastMessage =    messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;  if (    lastMessage?.tool_calls !== undefined &&    lastMessage?.tool_calls.length > 0  ) {    return ""tools"";  }  return ""__end__"";}const workflow = new StateGraph<GraphState>({  channels: {    messages: {      reducer: (a: any, b: any) => a.concat(b),    }  }});const graph = workflow  .addNode(""model"", callModel)  .addNode(""tools"", callTools)  .addEdge(""__start__"", ""model"")  .addConditionalEdges(""model"", shouldContinue, {    tools: ""tools"",    __end__: ""__end__"",  })  .addEdge(""tools"", ""model"")  .compile();await graph.invoke({  messages: [{ role: ""user"", content: ""what is the weather in sf"" }]}); An example trace from running the above code looks like this:"
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Data Retention,"In May 2024, LangSmith introduced a maximum data retention period on traces of 400 days. In June 2024, LangSmith introduced
a new data retention based pricing model where customers can configure a shorter data retention period on traces in exchange
for savings up to 10x. On this page, we'll go through how data retention works and is priced in LangSmith."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Privacy,"Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data
once it's no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with
such regulations."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Cost,"LangSmith charges less for traces that have low data retention. See our tutorial on how to optimize spend
for details."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,The basics,LangSmith now has two tiers of traces based on Data Retention with the following characteristics: BaseExtendedPrice$.50 / 1k traces$5 / 1k tracesRetention Period14 days400 days
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Data deletion after retention ends,"After the specified retention period, traces are no longer accessible via the runs table or API. All user data associated
with the trace (e.g. inputs and outputs) is deleted from our internal systems within a day thereafter. Some metadata
associated with each trace may be retained indefinitely for analytics and billing purposes."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Data retention auto-upgrades,"cautionAuto upgrades can have an impact on your bill. Please read this section carefully to fully understand your
estimated LangSmith tracing costs. When you use certain features with base tier traces, their data retention will be automatically upgraded to
extended tier. This will increase both the retention period, and the cost of the trace. The complete list of scenarios in which a trace will upgrade when: Feedback is added to any run on the traceAn Annotation Queue receives any run from the traceA Run Rule matches any run within a trace Why auto-upgrade traces? We have two reasons behind the auto-upgrade model for tracing: We think that traces that match any of these conditions are fundamentally more interesting than other traces, and
therefore it is good for users to be able to keep them around longer.We philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully.
We think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction
are charged at a higher rate. If you have questions or concerns about our pricing model, please feel free to reach out to support@langchain.dev and let us know your thoughts!"
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,How does data retention affect downstream features?,"Annotation Queues, Run Rules, and Feedback Traces that use these features will be auto-upgraded. Monitoring The monitoring tab will continue to work even after a base tier trace's data retention period ends. It is powered by
trace metadata that exists for >30 days, meaning that your monitoring graphs will continue to stay accurate even on
base tier traces. Datasets Datasets have an indefinite data retention period. Restated differently, if you add a trace's inputs and outputs to a dataset,
they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets
feature."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Billable metrics,"On your LangSmith invoice, you will see two metrics that we charge for: LangSmith Traces (Base Charge)LangSmith Traces (Extended Data Retention Upgrades). The first metric includes all traces, regardless of tier. The second metric just counts the number of extended retention traces."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Why measure all traces + upgrades instead of base and extended traces?,"A natural question to ask when considering our pricing is why not just show the number of base tier and extended tier
traces directly on the invoice? While we understand this would be more straightforward, it doesn't fit trace upgrades properly. Consider a
base tier trace that was recorded on June 30, and upgraded to extended tier on July 3. The base tier
trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore,
we need to be able to measure these two events independently to properly bill our customers. If your trace was recorded as an extended retention trace, then the base and extended metrics will both be recorded
with the same timestamp."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Cost breakdown,"The Base Charge for a trace is .05 per trace. We priced the upgrade such that an extended retention trace
costs 10x the price of a base tier trace (.50 per trace) including both metrics. Thus, each upgrade costs .45."
https://docs.smith.langchain.com/concepts/usage_and_billing/data_retention_billing,Related content,Tutorial on how to optimize spend
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#add-inputs-and-outputs-from-traces-to-datasets,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code,Annotate code for tracing,"There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code,Use@traceable/traceable,"LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.
Note that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.
import { traceable } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: ""formatPrompt"" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code,Wrap the OpenAI client,"The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!
The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceable(name=""Chat Pipeline"")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(""Can you summarize this morning's meetings?"")import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", run_type: ""tool"" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: ""system"",            content:                ""You are a helpful assistant. Please respond to the user's request only based on the given context."",        },        { role: ""user"", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: ""Chat Pipeline"" });await chatPipeline(""Can you summarize this morning's meetings?"");"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code,Use theRunTreeAPI,"Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually
create runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not
necessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# Create a top-level runpipeline = RunTree(    name=""Chat Pipeline"",    run_type=""chain"",    inputs={""question"": question})# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}]# Create a child runchild_llm_run = pipeline.create_child(    name=""OpenAI Call"",    run_type=""llm"",    inputs={""messages"": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={""answer"": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from ""openai"";import { RunTree } from ""langsmith"";// This can be a user input to your appconst question = ""Can you summarize this morning's meetings?"";const pipeline = new RunTree({    name: ""Chat Pipeline"",    run_type: ""chain"",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = ""During this morning's meeting, we solved all world conflict."";const messages = [    { role: ""system"", content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { role: ""user"", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: ""OpenAI Call"",    run_type: ""llm"",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code,Use thetracecontext manager (Python only),"In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {""input"": ""Can you summarize this morning's meetings?""}with trace(""Chat Pipeline"", ""chain"", project_name=""my_test"", inputs=app_inputs) as rt:    output = chat_pipeline(""Can you summarize this morning's meetings?"")    rt.end(outputs={""output"": output})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#interoperability-between-langchain-python-and-langsmith-sdk,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#create-an-annotation-queue,Use annotation queues,"Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.
While you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#create-an-annotation-queue,Create an annotation queue,"To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.
Then click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.
You can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as ""Done"" for it to be removed from the queue. If you check ""All workspace members review each run,"" then a run will remain in the queue until all workspace members have marked it ""Done"".Enable reservations on runs: We recommend enabling reservations.
This will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified ""reservation length"". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it ""Done"", the reservation will expire after the specified ""reservation length"".
The run is then released back into the queue and can be reserved by another reviewer. noteClicking ""Requeue at end"" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#create-an-annotation-queue,Assign runs to an annotation queue,"To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.
Select multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.
Set up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.
To learn more about how to capture user feedback from your LLM application, follow this guide."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#create-an-annotation-queue,Review runs in an annotation queue,"To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.
Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.
You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to ""View run"". The keyboard shortcuts shown can help streamline the review process."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#create-an-organization,Set up an organization,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#create-an-organization,Create an organization,"When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.
Shared organizations require a credit card before they can be used. You will need to set up billing to proceed."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#create-an-organization,Manage and navigate workspaces,"Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.
To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#create-an-organization,Manage users,"Manage membership in your shared organization in the Settings page Members and roles tab.
Here you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#create-an-organization,Organization roles,"These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-particular-version-of-a-dataset,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#create-a-filter,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/how_to_guides/tracing/toggle_tracing,Toggle tracing on and off,"noteThis section is only relevant for users who areUsing @traceable/traceableWrapping the OpenAI clientUsing the trace environment variableUsing LangChain If you've decided you no longer want to trace your runs, you can unset the LANGCHAIN_TRACING_V2 environment variable. Traces will no longer be logged to LangSmith.
Note that this currently does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-list-of-values,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Evaluation,"The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Datasets,"Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Creating datasets,"There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.
From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,
and what ""good"" responses may be.
You probably want to cover a few different common edge cases or situations you can imagine.
Even 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?
This information can be valuable to capture and store in datasets. This allows you to test against these
use cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?
There are a few heuristics you can follow.
If possible - try to collect end user feedback. You can then see which datapoints got negative feedback.
That is super valuable! These are spots where your application did not perform well.
You should add these to your dataset to test against in the future. You can also use other heuristics
to identify ""interesting"" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.
It's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.
This can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Types of datasets,"LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:""Inputs"" and ""outputs"" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating ""completion"" style language models.The ""inputs"" dictionary contains a single ""input"" key mapped to the prompt string.The ""outputs"" dictionary contains a single ""output"" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured ""chat"" messages as inputs and outputs.The ""inputs"" dictionary contains a single ""input"" key mapped to a list of serialized chat messagesThe ""outputs"" dictionary contains a single ""output"" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Partitioning datasets,"When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Evaluators,"Evaluators are functions in LangSmith that score how well your application performs on a particular example.
Evaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Human,Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering.
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Heuristic,"Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,LLM-as-judge,"LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Pairwise,"Pairwise evaluators pick the better of two task outputs based upon some criteria.
This can use either a heuristic (""which response is longer""), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.
This can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Applying evaluations,"We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Unit Tests,"Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.
This means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).
These types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)
to cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Regression Testing,"Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.
They are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Back-testing,"Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.
Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.
Then compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Pairwise-testing,"It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Online Evaluation,"Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Experiment Configurations,"LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Evaluating Specific LLM Applications,"Below, we will discuss evaluation of a few specific, popular LLM applications."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Agents,"LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.
Note that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Evaluating an agent's final response,"One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Evaluating a single step of an agent,"Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Evaluating an agent's trajectory,"Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an ""exact"" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the ""exact"" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of ""incorrect"" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Best practices,"Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Retrieval Augmented Generation (RAG),"Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Dataset,"When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Evaluator,"LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Applying RAG Evaluation,"When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,RAG evaluation summary,Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Summarization,"Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"
https://docs.smith.langchain.com/concepts/evaluation#datasets-and-examples,Classification / Tagging,"Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#log-to-specific-project,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evaluation_from_prompt_playground,Run an evaluation from the prompt playground,"While you can kick off experiments easily using the sdk, as outlined here, it's often useful to run experiments directly in the prompt playground. This allows you to test your prompt / model configuration over a series of inputs to see how well it generalizes across different contexts or scenarios, without having to write any code."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evaluation_from_prompt_playground,Create an experiment in the prompt playground,"Navigate to the prompt playground by clicking on ""Prompts"" in the sidebar, then selecting a prompt from the list of available prompts or creating a new one.Select the ""Switch to dataset"" button to switch to the dataset you want to use for the experiment. Please note that the dataset keys of the dataset inputs must match the input variables of the prompt. In the below sections, note that the selected dataset has inputs with keys ""text"", which correctly match the input variable of the prompt. Also note that there is a max capacity of 15 inputs for the prompt playground.
Click on the ""Start"" button or CMD+Enter to start the experiment. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. Note that you need to commit the prompt to the prompt hub before you can start the experiment to ensure it can be referenced in the experiment. The result for each input will be streamed and displayed inline for each input in the dataset.
View the results by clicking on the ""View Experiment"" button at the bottom of the page. This will take you to the experiment details page where you can see the results of the experiment.Navigate back to the commit page by clicking on the ""View Commit"" button. This will take you back to the prompt page where you can make changes to the prompt and run more experiments. The ""View Commit"" button is available to all experiments that were run from the prompt playground. The experiment is prefixed with the prompt repository name, a unique identifier, and the date and time the experiment was run.
"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evaluation_from_prompt_playground,Add evaluation scores to the experiment,"You can add evaluation scores to experiments by binding an evaluator to the dataset, again without writing any code. You can also programmatically evaluate an existing experiment using the SDK."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-on-a-subset-of-a-dataset,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#trace-without-setting-environment-variables,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,Set up automation rules,"While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users.
LangSmith provides a powerful feature called automations that allow you to trigger certain actions on your trace data.
At a high level, automations are defined by a filter, sampling rate, and action. Automation rules can trigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an annotation queue, and triggering a webhook. An example of an automation you can set up can be ""trigger an online evaluation that grades on vagueness for all of my downvoted traces."""
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,Create a rule,We will outline the steps for creating an automation rule in LangSmith below.
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,Step 1: Navigate to rule creation,"To create a rule, head click on Rules in the top right corner of any project details page, then scroll to the bottom and click on + Add Rule. Alternatively, you can access rules in settings by navigating to this link, click on + Add Rule, then Project Rule. noteThere are currently two types of rules you can create: Project Rule and Dataset Rule.Project Rule: This rule will apply to traces in the specified project. Actions allowed are adding to a dataset, adding to an annotation queue, running online evaluation, and triggering a webhook.Dataset Rule: This rule will apply to traces that are part of an experiment in the specified dataset. Actions allowed are only running an evaluator on the experiment results. To see this in action, you can follow this guide. Give your rule a name, for example ""my_rule"":"
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,Step 2: Define the filter,"You can create a filter as you normally would to filter traces in the project. For more information on filters, you can refer to this guide."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,(Optional) Step 3: Apply Rule to Past Runs,"When creating a new rule, you can apply the rule to past runs as well. To do this, select Apply to Past Runs checkbox and enter Backfill From date as the
start date to apply the rule. This will start from the Backfill From date and apply the run rules until it is caught up with the latest runs. Note that you will have to expand the date range for logs if you wanted to look at the progress of the backfill, see View logs for your automations for details."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,Step 4: Define the sampling rate,"You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,Step 5: Define the action,"There are four actions you can take with an automation rule: Add to dataset: Add the inputs and outputs of the trace to a dataset.Add to annotation queue: Add the trace to an annotation queue.Run online evaluation: Run an online evaluation on the trace. For more information on online evaluations, you can refer to this guide.Trigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.Extend data retention: Extends the data retention period on matching traces that use base retention.
Note that all other rules will also extend data retention on matching traces through the
auto-upgrade mechanism,
but this rule takes no additional action."
https://docs.smith.langchain.com/how_to_guides/monitoring/rules#create-a-rule,View logs for your automations,"You can view logs for your automations by going to Settings -> Rules and click on the Logs button in any row. You can also get to logs by clicking on Rules in the top right hand corner of any project details page, then clicking on See Logs for any rule. Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced.
If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon. By default, rule logs only show results for runs that occurred in the last day. To see results for older runs, you can select Last 1 day and enter the desired date range.
When applying a rule to past runs, the processing will start from the start date and go forward, so this would be needed to view logs while the backfill is proceeding."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-sdk,Audit evaluator scores,"LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-sdk,In the comparison view,"In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the ""edit"" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under ""Make correction"".
If you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples
in place of the few_shot_explanation prompt variable."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-sdk,In the runs table,"In the runs table, find the ""Feedback"" column and click on the feedback tag to bring up the feedback details. Again, click the ""edit"" icon on the right to bring up the corrections view."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-sdk,In the SDK,"Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      ""score"": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#installation,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#fetch-examples,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,Trace withLangGraph(Python and JS/TS),"LangSmith smoothly integrates with LangGraph (Python and JS)
to help you trace agentic workflows, whether you're using LangChain modules or other SDKs."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,With LangChain,"If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing. This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,0. Installation,"Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langgraphyarn add @langchain/openai @langchain/langgraphnpm install @langchain/openai @langchain/langgraphpnpm add @langchain/openai @langchain/langgraph"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,2. Log a trace,"Once you've set up your environment, you can call LangChain runnables as normal.
LangSmith will infer the proper tracing config: PythonTypeScriptfrom typing import Literalfrom langchain_core.messages import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain_core.tools import toolfrom langgraph.graph import StateGraph, MessagesStatefrom langgraph.prebuilt import ToolNode@tooldef search(query: str):    """"""Call to surf the web.""""""    if ""sf"" in query.lower() or ""san francisco"" in query.lower():        return ""It's 60 degrees and foggy.""    return ""It's 90 degrees and sunny.""tools = [search]tool_node = ToolNode(tools)model = ChatOpenAI(model=""gpt-4o"", temperature=0).bind_tools(tools)def should_continue(state: MessagesState) -> Literal[""tools"", ""__end__""]:    messages = state['messages']    last_message = messages[-1]    if last_message.tool_calls:        return ""tools""    return ""__end__""def call_model(state: MessagesState):    messages = state['messages']    # Invoking `model` will automatically infer the correct tracing context    response = model.invoke(messages)    return {""messages"": [response]}workflow = StateGraph(MessagesState)workflow.add_node(""agent"", call_model)workflow.add_node(""tools"", tool_node)workflow.add_edge(""__start__"", ""agent"")workflow.add_conditional_edges(    ""agent"",    should_continue,)workflow.add_edge(""tools"", 'agent')app = workflow.compile()final_state = app.invoke(    {""messages"": [HumanMessage(content=""what is the weather in sf"")]},    config={""configurable"": {""thread_id"": 42}})final_state[""messages""][-1].contentimport { HumanMessage, AIMessage } from ""@langchain/core/messages"";import { tool } from ""@langchain/core/tools"";import { z } from ""zod"";import { ChatOpenAI } from ""@langchain/openai"";import { StateGraph, StateGraphArgs } from ""@langchain/langgraph"";import { ToolNode } from ""@langchain/langgraph/prebuilt"";interface AgentState {  messages: HumanMessage[];}const graphState: StateGraphArgs<AgentState>[""channels""] = {  messages: {    reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),  },};const searchTool = tool(async ({ query }: { query: string }) => {  if (query.toLowerCase().includes(""sf"") || query.toLowerCase().includes(""san francisco"")) {    return ""It's 60 degrees and foggy.""  }  return ""It's 90 degrees and sunny.""}, {  name: ""search"",  description:    ""Call to surf the web."",  schema: z.object({    query: z.string().describe(""The query to use in your search.""),  }),});const tools = [searchTool];const toolNode = new ToolNode<AgentState>(tools);const model = new ChatOpenAI({  model: ""gpt-4o"",  temperature: 0,}).bindTools(tools);function shouldContinue(state: AgentState) {  const messages = state.messages;  const lastMessage = messages[messages.length - 1] as AIMessage;  if (lastMessage.tool_calls?.length) {    return ""tools"";  }  return ""__end__"";}async function callModel(state: AgentState) {  const messages = state.messages;  // Invoking `model` will automatically infer the correct tracing context  const response = await model.invoke(messages);  return { messages: [response] };}const workflow = new StateGraph<AgentState>({ channels: graphState })  .addNode(""agent"", callModel)  .addNode(""tools"", toolNode)  .addEdge(""__start__"", ""agent"")  .addConditionalEdges(""agent"", shouldContinue)  .addEdge(""tools"", ""agent"");const app = workflow.compile();const finalState = await app.invoke(  { messages: [new HumanMessage(""what is the weather in sf"")] },  { configurable: { thread_id: ""42"" } });finalState.messages[finalState.messages.length - 1].content; An example trace from running the above code looks like this:"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,Without LangChain,"If you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately
(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).
If you do so, LangSmith will automatically nest traces from those wrapped methods. Here's an example. You can also see this page for more information."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,0. Installation,Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below). pipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph,2. Log a trace,"Once you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.
LangSmith will then infer the proper tracing config: PythonTypeScriptimport jsonimport openaiimport operatorfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaifrom typing import Annotated, Literal, TypedDictfrom langgraph.graph import StateGraphclass State(TypedDict):    messages: Annotated[list, operator.add]tool_schema = {    ""type"": ""function"",    ""function"": {        ""name"": ""search"",        ""description"": ""Call to surf the web."",        ""parameters"": {            ""type"": ""object"",            ""properties"": {""query"": {""type"": ""string""}},            ""required"": [""query""],        },    },}# Decorating the tool function will automatically trace it with the correct context@traceable(run_type=""tool"", name=""Search Tool"")def search(query: str):    """"""Call to surf the web.""""""    if ""sf"" in query.lower() or ""san francisco"" in query.lower():        return ""It's 60 degrees and foggy.""    return ""It's 90 degrees and sunny.""tools = [search]def call_tools(state):    function_name_to_function = {""search"": search}    messages = state[""messages""]    tool_call = messages[-1][""tool_calls""][0]    function_name = tool_call[""function""][""name""]    function_arguments = tool_call[""function""][""arguments""]    arguments = json.loads(function_arguments)    function_response = function_name_to_function[function_name](**arguments)    tool_message = {        ""tool_call_id"": tool_call[""id""],        ""role"": ""tool"",        ""name"": function_name,        ""content"": function_response,    }    return {""messages"": [tool_message]}wrapped_client = wrap_openai(openai.Client())def should_continue(state: State) -> Literal[""tools"", ""__end__""]:    messages = state[""messages""]    last_message = messages[-1]    if last_message[""tool_calls""]:        return ""tools""    return ""__end__""def call_model(state: State):    messages = state[""messages""]    # Calling the wrapped client will automatically infer the correct tracing context    response = wrapped_client.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", tools=[tool_schema]    )    raw_tool_calls = response.choices[0].message.tool_calls    tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []    response_message = {        ""role"": ""assistant"",        ""content"": response.choices[0].message.content,        ""tool_calls"": tool_calls,    }    return {""messages"": [response_message]}workflow = StateGraph(State)workflow.add_node(""agent"", call_model)workflow.add_node(""tools"", call_tools)workflow.add_edge(""__start__"", ""agent"")workflow.add_conditional_edges(    ""agent"",    should_continue,)workflow.add_edge(""tools"", 'agent')app = workflow.compile()final_state = app.invoke(    {""messages"": [{""role"": ""user"", ""content"": ""what is the weather in sf""}]})final_state[""messages""][-1][""content""]Note: The below example requires langsmith>=0.1.39 and @langchain/langgraph>=0.0.31
import OpenAI from ""openai"";import { StateGraph } from ""@langchain/langgraph"";import { wrapOpenAI } from ""langsmith/wrappers/openai"";import { traceable } from ""langsmith/traceable"";type GraphState = {  messages: OpenAI.ChatCompletionMessageParam[];};const wrappedClient = wrapOpenAI(new OpenAI({}));const toolSchema: OpenAI.ChatCompletionTool = {  type: ""function"",  function: {    name: ""search"",    description: ""Use this tool to query the web."",    parameters: {      type: ""object"",      properties: {        query: {          type: ""string"",        },      },      required: [""query""],    }  }};// Wrapping the tool function will automatically trace it with the correct contextconst search = traceable(async ({ query }: { query: string }) => {  if (    query.toLowerCase().includes(""sf"") ||    query.toLowerCase().includes(""san francisco"")  ) {    return ""It's 60 degrees and foggy."";  }  return ""It's 90 degrees and sunny.""}, { run_type: ""tool"", name: ""Search Tool"" });const callTools = async ({ messages }: GraphState) => {  const mostRecentMessage = messages[messages.length - 1];  const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;  if (toolCalls === undefined || toolCalls.length === 0) {    throw new Error(""No tool calls passed to node."");  }  const toolNameMap = {    search,  };  const functionName = toolCalls[0].function.name;  const functionArguments = JSON.parse(toolCalls[0].function.arguments);  const response = await toolNameMap[functionName](functionArguments);  const toolMessage = {    tool_call_id: toolCalls[0].id,    role: ""tool"",    name: functionName,    content: response,  }  return { messages: [toolMessage] };}const callModel = async ({ messages }: GraphState) => {  // Calling the wrapped client will automatically infer the correct tracing context  const response = await wrappedClient.chat.completions.create({    messages,    model: ""gpt-4o-mini"",    tools: [toolSchema],  });  const responseMessage = {    role: ""assistant"",    content: response.choices[0].message.content,    tool_calls: response.choices[0].message.tool_calls ?? [],  };  return { messages: [responseMessage] };}const shouldContinue = ({ messages }: GraphState) => {  const lastMessage =    messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;  if (    lastMessage?.tool_calls !== undefined &&    lastMessage?.tool_calls.length > 0  ) {    return ""tools"";  }  return ""__end__"";}const workflow = new StateGraph<GraphState>({  channels: {    messages: {      reducer: (a: any, b: any) => a.concat(b),    }  }});const graph = workflow  .addNode(""model"", callModel)  .addNode(""tools"", callTools)  .addEdge(""__start__"", ""model"")  .addConditionalEdges(""model"", shouldContinue, {    tools: ""tools"",    __end__: ""__end__"",  })  .addEdge(""tools"", ""model"")  .compile();await graph.invoke({  messages: [{ role: ""user"", content: ""what is the weather in sf"" }]}); An example trace from running the above code looks like this:"
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing#distributed-tracing-in-python,Implement distributed tracing,"Sometimes, you need to trace a request across multiple services. LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (langsmith-trace and optional baggage for metadata/tags). Example client-server setup: Trace starts on clientContinues on server"
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing#distributed-tracing-in-python,Distributed tracing in Python,"# client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasync def my_client_function():    headers = {}    async with httpx.AsyncClient(base_url=""..."") as client:        if run_tree := get_current_run_tree():            # add langsmith-id to headers            headers.update(run_tree.to_headers())        return await client.post(""/my-route"", headers=headers) Then the server (or other service) can continue the trace by passing the headers in as langsmith_extra: # server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasync def my_application():    ...app = FastAPI()  # Or Flask, Django, or any other framework@app.post(""/my-route"")async def fake_route(request: Request):    # request.headers:  {""langsmith-trace"": ""...""}    # as well as optional metadata/tags in `baggage`    with tracing_context(parent=request.headers):        return await my_application() The example above uses the tracing_context context manager. You can also directly specify the parent run context in the langsmith_extra parameter of a method wrapped with @traceable. from langsmith.run_helpers import traceable, trace# ... same as above@app.post(""/my-route"")async def fake_route(request: Request):    # request.headers:  {""langsmith-trace"": ""...""}    my_application(langsmith_extra={""parent"": request.headers})"
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing#distributed-tracing-in-python,Distributed tracing in TypeScript,"noteDistributed tracing in TypeScript requires langsmith version >=0.1.31 First, we obtain the current run tree from the client and convert it to langsmith-trace and baggage header values, which we can pass to the server: // client.mtsimport { getCurrentRunTree, traceable } from ""langsmith/traceable"";const client = traceable(  async () => {    const runTree = getCurrentRunTree();    return await fetch(""..."", {      method: ""POST"",      headers: runTree.toHeaders(),    }).then((a) => a.text());  },  { name: ""client"" });await client(); Then, the server converts the headers back to a run tree, which it uses to further continue the tracing. To pass the newly created run tree to a traceable function, we can use the withRunTree helper, which will ensure the run tree is propagated within traceable invocations. Express.JSHono// server.mtsimport { RunTree } from ""langsmith"";import { traceable, withRunTree } from ""langsmith/traceable"";import express from ""express"";import bodyParser from ""body-parser"";const server = traceable(  (text: string) => `Hello from the server! Received ""${text}""`,  { name: ""server"" });const app = express();app.use(bodyParser.text());app.post(""/"", async (req, res) => {  const runTree = RunTree.fromHeaders(req.headers);  const result = await withRunTree(runTree, () => server(req.body));  res.send(result);});// server.mtsimport { RunTree } from ""langsmith"";import { traceable, withRunTree } from ""langsmith/traceable"";import { Hono } from ""hono"";const server = traceable(  (text: string) => `Hello from the server! Received ""${text}""`,  { name: ""server"" });const app = new Hono();app.post(""/"", async (c) => {  const body = await c.req.text();  const runTree = RunTree.fromHeaders(c.req.raw.headers);  const result = await withRunTree(runTree, () => server(body));  return c.body(result);});"
https://docs.smith.langchain.com/,Get started with LangSmith,"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!"
https://docs.smith.langchain.com/,1. Install LangSmith,PythonTypeScriptpip install -U langsmithyarn add langsmith
https://docs.smith.langchain.com/,2. Create an API key,To create an API key head to the Settings page. Then click Create API Key.
https://docs.smith.langchain.com/,3. Set up your environment,"Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>"
https://docs.smith.langchain.com/,4. Log your first trace,"Tracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here. We provide multiple ways to log traces to LangSmith. Below, we'll highlight
how to use traceable. See more on the Annotate code for tracing page. PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{""role"": ""user"", ""content"": user_input}],        model=""gpt-3.5-turbo""    )    return result.choices[0].message.contentpipeline(""Hello, world!"")# Out:  Hello there! How can I assist you today?import { OpenAI } from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: ""user"", content: user_input }],        model: ""gpt-3.5-turbo"",    });    return result.choices[0].message.content;});await pipeline(""Hello, world!"")// Out: Hello there! How can I assist you today? View a sample output trace.Learn more about tracing in the how-to guides."
https://docs.smith.langchain.com/,5. Run your first evaluation,"Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator. PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = ""Sample Dataset""dataset = client.create_dataset(dataset_name, description=""A sample dataset in LangSmith."")client.create_examples(    inputs=[        {""postfix"": ""to LangSmith""},        {""postfix"": ""to Evaluations in LangSmith""},    ],    outputs=[        {""output"": ""Welcome to LangSmith""},        {""output"": ""Welcome to Evaluations in LangSmith""},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {""score"": run.outputs[""output""] == example.outputs[""output""]}experiment_results = evaluate(    lambda input: ""Welcome "" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=""sample-experiment"", # The name of the experiment    metadata={      ""version"": ""1.0.0"",      ""revision_id"": ""beta""    },)import { Client, Run, Example } from ""langsmith"";import { evaluate } from ""langsmith/evaluation"";import { EvaluationResult } from ""langsmith/evaluation"";const client = new Client();// Define dataset: these are your test casesconst datasetName = ""Sample Dataset"";const dataset = await client.createDataset(datasetName, {  description: ""A sample dataset in LangSmith."",});await client.createExamples({  inputs: [    { postfix: ""to LangSmith"" },    { postfix: ""to Evaluations in LangSmith"" },  ],  outputs: [    { output: ""Welcome to LangSmith"" },    { output: ""Welcome to Evaluations in LangSmith"" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: ""exact_match"",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: ""1.0.0"",      revision_id: ""beta"",    },  }); Learn more about evaluation in the how-to guides."
https://docs.smith.langchain.com/self_hosting/configuration/sso,SSO with OAuth2.0 and OIDC,"LangSmith Self-Hosted provides SSO via OAuth2.0 and OIDC. Once configured, this will delegate authentication to your Identity Provider(IdP) to manage access to LangSmith. Our implementation supports almost anything that is OIDC compliant, with a few exceptions."
https://docs.smith.langchain.com/self_hosting/configuration/sso,Requirements,"There are a couple of requirements for using OAuth SSO with LangSmith: Your IdP must support the Authorization Code with PKCE flow(Google does not support this flow). This often displayed in your Oauth Provider as configuring a ""Single Page Application(SPA)Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.You must provide the OIDC, email, and profile scopes to LangSmith. We use these to fetch the necessary user information and email for your users.You will need to set the callback URL in your IdP to http://<host>/oauth-callback, where host is the domain or ip you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.You will need to provide the oauthClientId and oauthIssuerUrl in your values.yaml file. This is where you will configure your LangSmith instance. HelmDockerconfig:  oauth:    enabled: true    oauthClientId: <YOUR CLIENT ID>    oauthIssuerUrl: <YOUR DISCOVERY URL># In your .env fileAUTH_TYPE=oauthOAUTH_CLIENT_ID=your-client-idOAUTH_ISSUER_URL=https://your-issuer-url     Once configured, you will see a login screen like this:"
https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods,Authentication methods,LangSmith supports multiple authentication methods for easy sign-up and login.
https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods,Email/Password,Users can use an email address and password to sign up and login to LangSmith.
https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods,Social Providers,"Users can alternatively use their credentials from GitHub, Google, or Discord."
https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods,Self-Hosted,"Self-hosted customers have more control over how their users can login to LangSmith.
For more in-depth coverage of configuration options, see the self-hosting docs
and Helm chart."
https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods,SSO with OAuth 2.0 and OIDC,"Production installations should configure SSO in order to use an external identity provider.
This enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider.
Learn more about configuring SSO in the SSO configuration guide"
https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods,Email/Password a.k.a. basic auth (beta),"This auth method requires very little configuration as it does not require an external identity provider.
It is most appropriate to use for self-hosted trials. Learn more in the basic auth configuration guide"
https://docs.smith.langchain.com/reference/authentication_authorization/authentication_methods,None,"dangerThis authentication mode will be removed after the launch of Basic Auth. If zero authentication methods are enabled, a self-hosted installation does not require any login/sign-up.
This configuration should only be used for verifying installation at the infrastructure level, as the feature set
supported in this mode is restricted with only a single organization and workspace."
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing#distributed-tracing-in-typescript,Implement distributed tracing,"Sometimes, you need to trace a request across multiple services. LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (langsmith-trace and optional baggage for metadata/tags). Example client-server setup: Trace starts on clientContinues on server"
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing#distributed-tracing-in-typescript,Distributed tracing in Python,"# client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasync def my_client_function():    headers = {}    async with httpx.AsyncClient(base_url=""..."") as client:        if run_tree := get_current_run_tree():            # add langsmith-id to headers            headers.update(run_tree.to_headers())        return await client.post(""/my-route"", headers=headers) Then the server (or other service) can continue the trace by passing the headers in as langsmith_extra: # server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasync def my_application():    ...app = FastAPI()  # Or Flask, Django, or any other framework@app.post(""/my-route"")async def fake_route(request: Request):    # request.headers:  {""langsmith-trace"": ""...""}    # as well as optional metadata/tags in `baggage`    with tracing_context(parent=request.headers):        return await my_application() The example above uses the tracing_context context manager. You can also directly specify the parent run context in the langsmith_extra parameter of a method wrapped with @traceable. from langsmith.run_helpers import traceable, trace# ... same as above@app.post(""/my-route"")async def fake_route(request: Request):    # request.headers:  {""langsmith-trace"": ""...""}    my_application(langsmith_extra={""parent"": request.headers})"
https://docs.smith.langchain.com/how_to_guides/tracing/distributed_tracing#distributed-tracing-in-typescript,Distributed tracing in TypeScript,"noteDistributed tracing in TypeScript requires langsmith version >=0.1.31 First, we obtain the current run tree from the client and convert it to langsmith-trace and baggage header values, which we can pass to the server: // client.mtsimport { getCurrentRunTree, traceable } from ""langsmith/traceable"";const client = traceable(  async () => {    const runTree = getCurrentRunTree();    return await fetch(""..."", {      method: ""POST"",      headers: runTree.toHeaders(),    }).then((a) => a.text());  },  { name: ""client"" });await client(); Then, the server converts the headers back to a run tree, which it uses to further continue the tracing. To pass the newly created run tree to a traceable function, we can use the withRunTree helper, which will ensure the run tree is propagated within traceable invocations. Express.JSHono// server.mtsimport { RunTree } from ""langsmith"";import { traceable, withRunTree } from ""langsmith/traceable"";import express from ""express"";import bodyParser from ""body-parser"";const server = traceable(  (text: string) => `Hello from the server! Received ""${text}""`,  { name: ""server"" });const app = express();app.use(bodyParser.text());app.post(""/"", async (req, res) => {  const runTree = RunTree.fromHeaders(req.headers);  const result = await withRunTree(runTree, () => server(req.body));  res.send(result);});// server.mtsimport { RunTree } from ""langsmith"";import { traceable, withRunTree } from ""langsmith/traceable"";import { Hono } from ""hono"";const server = traceable(  (text: string) => `Hello from the server! Received ""${text}""`,  { name: ""server"" });const app = new Hono();app.post(""/"", async (c) => {  const body = await c.req.text();  const runTree = RunTree.fromHeaders(c.req.raw.headers);  const result = await withRunTree(runTree, () => server(body));  return c.body(result);});"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#create-your-evaluator,Create few-shot evaluators,"Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary
overhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows
you to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#create-your-evaluator,Create your evaluator,"tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators
are only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,
we will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}
template variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name
as your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,
few_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend
to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#create-your-evaluator,Make corrections,"Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will
begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.
The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"
https://docs.smith.langchain.com/how_to_guides/evaluation/create_few_shot_evaluators#create-your-evaluator,View your corrections dataset,"In order to view your corrections dataset, go to your rule and click ""Edit Rule"" (or ""Edit Evaluator"" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says ""View few-shot dataset"". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"
https://docs.smith.langchain.com/concepts/tracing#traces,Tracing,"This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"
https://docs.smith.langchain.com/concepts/tracing#traces,Runs,"A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#traces,Traces,"A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
"
https://docs.smith.langchain.com/concepts/tracing#traces,Projects,"A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.
"
https://docs.smith.langchain.com/concepts/tracing#traces,Feedback,"Feedback allows you to score an individual run based on certain criteria.
Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.
Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#traces,Tags,"Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces
"
https://docs.smith.langchain.com/concepts/tracing#traces,Metadata,"Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.
Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces
"
https://docs.smith.langchain.com/concepts/tracing#traces,Data storage and retention,"For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/concepts/tracing#traces,Deleting traces from LangSmith,"If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the ""Delete"" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-and-navigate-workspaces,Set up an organization,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-and-navigate-workspaces,Create an organization,"When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.
Shared organizations require a credit card before they can be used. You will need to set up billing to proceed."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-and-navigate-workspaces,Manage and navigate workspaces,"Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.
To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-and-navigate-workspaces,Manage users,"Manage membership in your shared organization in the Settings page Members and roles tab.
Here you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-and-navigate-workspaces,Organization roles,"These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#chat-style-models,Log custom LLM traces,"noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.
In order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#chat-style-models,Chat-style models,"For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg ""openai"", ""anthropic"", etc.ls_model_name: The name of the model, eg ""gpt-3.5-turbo"", ""claude-3-opus-20240307"", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ]}# Can also use one of:# output = {#     ""message"": {#         ""role"": ""assistant"",#         ""content"": ""Sure, what time would you like to book the table for?""#     }# }## output = {#     ""role"": ""assistant"",#     ""content"": ""Sure, what time would you like to book the table for?""# }## output = [""assistant"", ""Sure, what time would you like to book the table for?""]@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" }];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?""      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: ""assistant"",//     content: ""Sure, what time would you like to book the table for?""//   }// };//// const output = {//   role: ""assistant"",//   content: ""Sure, what time would you like to book the table for?""// };//// const output = [""assistant"", ""Sure, what time would you like to book the table for?""];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#chat-style-models,Stream outputs,"For streaming, you can ""reduce"" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = """".join([chunk[""choices""][0][""message""][""content""] for chunk in chunks])    return {""choices"": [{""message"": {""content"": all_text, ""role"": ""assistant""}}]}@traceable(    run_type=""llm"",    reduce_fn=_reduce_chunks,    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def my_streaming_chat_model(messages: list):    for chunk in [""Hello, "" + messages[1][""content""]]:        yield {            ""choices"": [                {                    ""message"": {                        ""content"": chunk,                        ""role"": ""assistant"",                    }                }            ]        }list(    my_streaming_chat_model(        [            {""role"": ""system"", ""content"": ""You are a helpful assistant. Please greet the user.""},            {""role"": ""user"", ""content"": ""polly the parrot""},        ],    ))"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#chat-style-models,Manually provide token counts,"Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.
Many models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.
If token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.
If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ],    ""usage_metadata"": {        ""input_tokens"": 27,        ""output_tokens"": 13,        ""total_tokens"": 40,    },}@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" },];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?"",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages });"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#chat-style-models,Instruct-style models,"For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.
The same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def hello_llm(prompt: str):    return {        ""choices"": [            {""text"": ""Hello, "" + prompt}        ],        ""usage_metadata"": {            ""input_tokens"": 4,            ""output_tokens"": 5,            ""total_tokens"": 9,        },    }hello_llm(""polly the parrot\n"")import { traceable } from ""langsmith/traceable"";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: ""Hello, "" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: ""llm"", name: ""hello_llm"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await helloLLM({ prompt: ""polly the parrot\n"" }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/prompts/langchain_hub,LangChain Hub,"Navigate to the LangChain Hub section of the left-hand sidebar. Here you'll find all of the publicly listed prompts in the LangChain Hub.
You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt's details, and run the prompt in the playground.
You can pull any public prompt into your code using the SDK. To view prompts tied to your workspace, visit the Prompts tab in the sidebar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/self_hosting/configuration,Configuration,Your LangSmith instance supports configuring a variety of parameters to suit your needs. This section contains guides for configuring your LangSmith instance. SSO with OAuth2.0 and OIDCEmail/password a.k.a. basic authUser management featuresConnecting to an external ClickHouse databaseConnecting to an external Postgres databaseConnecting to an external Redis instance
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Export traces,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Run (span) data formatLangSmith API ReferenceLangSmith trace query syntax The recommended way to export runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Use filter arguments,"For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client, Run } from ""langsmith"";const client = new Client(); Below are some examples of ways to list runs using keyword arguments:"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List all runs in a project,"PythonTypeScriptproject_runs = client.list_runs(project_name=""<your_project>"")// Download runs in a projectconst projectRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",})) {  projectRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List LLM and Chat runs in the last 24 hours,"PythonTypeScripttodays_llm_runs = client.list_runs(    project_name=""<your_project>"",    start_time=datetime.now() - timedelta(days=1),    run_type=""llm"",)const todaysLlmRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),  runType: ""llm"",})) {  todaysLlmRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List root runs in a project,"Root runs are runs that have no parents. These are assigned a value of True for is_root. You can use this to filter for root runs. PythonTypeScriptroot_runs = client.list_runs(    project_name=""<your_project>"",    is_root=True)const rootRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  isRoot: 1,})) {  rootRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List runs without errors,"PythonTypeScriptcorrect_runs = client.list_runs(project_name=""<your_project>"", error=False)const correctRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  error: false,})) {  correctRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List runs by run ID,"Ignores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like project_name, run_type, etc. and directly return the runs matching the given IDs. If you have a list of run IDs, you can list them directly: PythonTypeScriptrun_ids = ['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)const runIds = [  ""a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836"",  ""9398e6be-964f-4aa4-8ae9-ad78cd4b7074"",];const selectedRuns: Run[] = [];for await (const run of client.listRuns({  id: runIds,})) {  selectedRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Use filter query language,"For more complex queries, you can use the query language described in the filter query language refernece."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,"List all runs called ""extractor"" whose root of the trace was assigned feedback ""user_score"" score of 1","PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='eq(name, ""extractor"")',    trace_filter='and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))')client.listRuns({  projectName: ""<your_project>"",  filter: 'eq(name, ""extractor"")',  traceFilter: 'and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,"List runs with ""star_rating"" key whose score is greater than 4","PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='and(eq(feedback_key, ""star_rating""), gt(feedback_score, 4))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(eq(feedback_key, ""star_rating""), gt(feedback_score, 4))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List runs that took longer than 5 seconds to complete,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(latency, ""5s"")')client.listRuns({projectName: ""<your_project>"", filter: 'gt(latency, ""5s"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List all runs wheretotal_tokensis greater than 5000,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(total_tokens, 5000)')client.listRuns({projectName: ""<your_project>"", filter: 'gt(total_tokens, 5000)'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,"List all runs that have ""error"" not equal to null","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='neq(error, null)')client.listRuns({projectName: ""<your_project>"", filter: 'neq(error, null)'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List all runs wherestart_timeis greater than a specific timestamp,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(start_time, ""2023-07-15T12:34:56Z"")')client.listRuns({projectName: ""<your_project>"", filter: 'gt(start_time, ""2023-07-15T12:34:56Z"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,"List all runs that contain the string ""substring""","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='search(""substring"")')client.listRuns({projectName: ""<your_project>"", filter: 'search(""substring"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,"List all runs that are tagged with the git hash ""2aa1cf4""","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='has(tags, ""2aa1cf4"")')client.listRuns({projectName: ""<your_project>"", filter: 'has(tags, ""2aa1cf4"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,"List all ""chain"" type runs that took more than 10 seconds and","had total_tokens greater than 5000 PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(eq(run_type, ""chain""), gt(latency, 10), gt(total_tokens, 5000))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(eq(run_type, ""chain""), gt(latency, 10), gt(total_tokens, 5000))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,List all runs that started after a specific timestamp and either,"have ""error"" not equal to null or a ""Correctness"" feedback score equal to 0 PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(gt(start_time, ""2023-07-15T12:34:56Z""), or(neq(error, null), and(eq(feedback_key, ""Correctness""), eq(feedback_score, 0.0))))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(gt(start_time, ""2023-07-15T12:34:56Z""), or(neq(error, null), and(eq(feedback_key, ""Correctness""), eq(feedback_score, 0.0))))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,"Complex query: List all runs wheretagsinclude ""experimental"" or ""beta"" and","latency is greater than 2 seconds PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(or(has(tags, ""experimental""), has(tags, ""beta"")), gt(latency, 2))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(or(has(tags, ""experimental""), has(tags, ""beta"")), gt(latency, 2))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Search trace trees by full text You can use thesearch()function without,"any specific field to do a full text search across all string fields in a run. This
allows you to quickly find traces that match a search term. PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='search(""image classification"")')client.listRuns({  projectName: ""<your_project>"",  filter: 'search(""image classification"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Check for presence of metadata,"If you want to check for the presence of metadata, you can use the eq operator, optionally with an and statement to match by value. This is useful if you want to log more structured information
about your runs. PythonTypeScriptto_search = {    ""user_id"": """"}# Check for any run with the ""user_id"" metadata keyclient.list_runs(  project_name=""default"",  filter=""eq(metadata_key, 'user_id')"")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))"")// Check for any run with the ""user_id"" metadata keyclient.listRuns({  projectName: 'default',  filter: `eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Check for environment details in metadata.,"A common pattern is to add environment
information to your traces via metadata. If you want to filter for runs containing
environment metadata, you can use the same pattern as above: PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Check for conversation ID in metadata,"Another common way to associate traces
in the same conversation is by using a shared conversation ID. If you want to filter
runs based on a conversation ID in this way, you can search for that ID in the metadata. PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Combine multiple filters,"If you want to combine multiple conditions to refine your search, you can use the and operator along with other
filtering functions. Here's how you can search for runs named ""ChatOpenAI"" that also
have a specific conversation_id in their metadata: PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Tree Filter,"List all runs named ""RetrieveDocs"" whose root run has a ""user_score"" feedback of 1 and any run in the full trace is named ""ExpandQuery"". This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace. PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='eq(name, ""RetrieveDocs"")',    trace_filter='and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))',    tree_filter='eq(name, ""ExpandQuery"")')client.listRuns({  projectName: ""<your_project>"",  filter: 'eq(name, ""RetrieveDocs"")',  traceFilter: 'and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))',  treeFilter: 'eq(name, ""ExpandQuery"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Advanced: export flattened trace view with child tool usage,"The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.
This can be used to analyze the behavior of your agents across multiple traces. This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information. To optimize the query, the example: Selects only the necessary fields when querying tool runs to reduce query time.Fetches root runs in batches while processing tool runs concurrently. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = ""my-project""num_days = 30# List all tool runstool_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=""tool"",    # We don't need to fetch inputs, outputs, and other values that # may increase the query time    select=[""trace_id"", ""name"", ""run_type""],)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group tool runs by parent run ID    for run in tqdm(tool_runs):        # Collect all tools invoked within a given trace        tool_runs_by_parent[run.trace_id][""tools_involved""].add(run.name)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the tool runs        if len(tool_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(tool_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[""name"", ""inputs"", ""outputs"", ""run_type""],                    )                )    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[""name"", ""inputs"", ""outputs"", ""run_type""],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = tool_runs_by_parent[root_run.id]        data.append(            {                ""run_id"": root_run.id,                ""run_name"": root_run.name,                ""run_type"": root_run.run_type,                ""inputs"": root_run.inputs,                ""outputs"": root_run.outputs,                ""tools_involved"": list(root_data[""tools_involved""]),            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces,Advanced: export retriever IO for traces with feedback,"This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.
The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = ""your-project-name""num_days = 1# List all tool runsretriever_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=""retriever"",    # This time we do want to fetch the inputs and outputs, since they    # may be adjusted by query expansion steps.    select=[""trace_id"", ""name"", ""run_type"", ""inputs"", ""outputs""],    trace_filter='eq(feedback_key, ""user_score"")',)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group retriever runs by parent run ID    for run in tqdm(retriever_runs):        # Collect all retriever calls invoked within a given trace        for k, v in run.inputs.items():            retriever_runs_by_parent[run.trace_id][f""retriever.inputs.{k}""].append(v)        for k, v in (run.outputs or {}).items():            # Extend the docs            retriever_runs_by_parent[run.trace_id][f""retriever.outputs.{k}""].extend(v)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the retriever runs        if len(retriever_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(retriever_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[                            ""name"",                            ""inputs"",                            ""outputs"",                            ""run_type"",                            ""feedback_stats"",                        ],                    )                )    if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[""name"", ""inputs"", ""outputs"", ""run_type""],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = retriever_runs_by_parent[root_run.id]        feedback = {            f""feedback.{k}"": v.get(""avg"")            for k, v in (root_run.feedback_stats or {}).items()        }        inputs = {f""inputs.{k}"": v for k, v in root_run.inputs.items()}        outputs = {f""outputs.{k}"": v for k, v in (root_run.outputs or {}).items()}        data.append(            {                ""run_id"": root_run.id,                ""run_name"": root_run.name,                **inputs,                **outputs,                **feedback,                **root_data,            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#add-metadata,Create a prompt,"Navigate to the Prompts section in the left-hand sidebar or from the application homepage.
Click the ""+ Prompt"" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.
"
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#add-metadata,Compose your prompt,"After choosing a prompt type, you're brought to the playground to develop your prompt.
On the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click ""Start""."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#add-metadata,Save your prompt,"To save your prompt, click the ""Save as"" button, name your prompt, and decide if you want it to be ""private"" or ""public"".
Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.
Click save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#add-metadata,View your prompts,You've just created your first prompt! View a table of your prompts in the prompts tab.
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#add-metadata,Add metadata,"To add metadata to your prompt, click the prompt and then click the ""Edit"" pencil icon next to the name.
This brings you to where you can add additional information about the prompt, including a description, a README, and use cases.
For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Export traces,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Run (span) data formatLangSmith API ReferenceLangSmith trace query syntax The recommended way to export runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Use filter arguments,"For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client, Run } from ""langsmith"";const client = new Client(); Below are some examples of ways to list runs using keyword arguments:"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List all runs in a project,"PythonTypeScriptproject_runs = client.list_runs(project_name=""<your_project>"")// Download runs in a projectconst projectRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",})) {  projectRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List LLM and Chat runs in the last 24 hours,"PythonTypeScripttodays_llm_runs = client.list_runs(    project_name=""<your_project>"",    start_time=datetime.now() - timedelta(days=1),    run_type=""llm"",)const todaysLlmRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),  runType: ""llm"",})) {  todaysLlmRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List root runs in a project,"Root runs are runs that have no parents. These are assigned a value of True for is_root. You can use this to filter for root runs. PythonTypeScriptroot_runs = client.list_runs(    project_name=""<your_project>"",    is_root=True)const rootRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  isRoot: 1,})) {  rootRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List runs without errors,"PythonTypeScriptcorrect_runs = client.list_runs(project_name=""<your_project>"", error=False)const correctRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  error: false,})) {  correctRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List runs by run ID,"Ignores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like project_name, run_type, etc. and directly return the runs matching the given IDs. If you have a list of run IDs, you can list them directly: PythonTypeScriptrun_ids = ['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)const runIds = [  ""a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836"",  ""9398e6be-964f-4aa4-8ae9-ad78cd4b7074"",];const selectedRuns: Run[] = [];for await (const run of client.listRuns({  id: runIds,})) {  selectedRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Use filter query language,"For more complex queries, you can use the query language described in the filter query language refernece."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,"List all runs called ""extractor"" whose root of the trace was assigned feedback ""user_score"" score of 1","PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='eq(name, ""extractor"")',    trace_filter='and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))')client.listRuns({  projectName: ""<your_project>"",  filter: 'eq(name, ""extractor"")',  traceFilter: 'and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,"List runs with ""star_rating"" key whose score is greater than 4","PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='and(eq(feedback_key, ""star_rating""), gt(feedback_score, 4))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(eq(feedback_key, ""star_rating""), gt(feedback_score, 4))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List runs that took longer than 5 seconds to complete,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(latency, ""5s"")')client.listRuns({projectName: ""<your_project>"", filter: 'gt(latency, ""5s"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List all runs wheretotal_tokensis greater than 5000,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(total_tokens, 5000)')client.listRuns({projectName: ""<your_project>"", filter: 'gt(total_tokens, 5000)'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,"List all runs that have ""error"" not equal to null","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='neq(error, null)')client.listRuns({projectName: ""<your_project>"", filter: 'neq(error, null)'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List all runs wherestart_timeis greater than a specific timestamp,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(start_time, ""2023-07-15T12:34:56Z"")')client.listRuns({projectName: ""<your_project>"", filter: 'gt(start_time, ""2023-07-15T12:34:56Z"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,"List all runs that contain the string ""substring""","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='search(""substring"")')client.listRuns({projectName: ""<your_project>"", filter: 'search(""substring"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,"List all runs that are tagged with the git hash ""2aa1cf4""","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='has(tags, ""2aa1cf4"")')client.listRuns({projectName: ""<your_project>"", filter: 'has(tags, ""2aa1cf4"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,"List all ""chain"" type runs that took more than 10 seconds and","had total_tokens greater than 5000 PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(eq(run_type, ""chain""), gt(latency, 10), gt(total_tokens, 5000))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(eq(run_type, ""chain""), gt(latency, 10), gt(total_tokens, 5000))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,List all runs that started after a specific timestamp and either,"have ""error"" not equal to null or a ""Correctness"" feedback score equal to 0 PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(gt(start_time, ""2023-07-15T12:34:56Z""), or(neq(error, null), and(eq(feedback_key, ""Correctness""), eq(feedback_score, 0.0))))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(gt(start_time, ""2023-07-15T12:34:56Z""), or(neq(error, null), and(eq(feedback_key, ""Correctness""), eq(feedback_score, 0.0))))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,"Complex query: List all runs wheretagsinclude ""experimental"" or ""beta"" and","latency is greater than 2 seconds PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(or(has(tags, ""experimental""), has(tags, ""beta"")), gt(latency, 2))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(or(has(tags, ""experimental""), has(tags, ""beta"")), gt(latency, 2))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Search trace trees by full text You can use thesearch()function without,"any specific field to do a full text search across all string fields in a run. This
allows you to quickly find traces that match a search term. PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='search(""image classification"")')client.listRuns({  projectName: ""<your_project>"",  filter: 'search(""image classification"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Check for presence of metadata,"If you want to check for the presence of metadata, you can use the eq operator, optionally with an and statement to match by value. This is useful if you want to log more structured information
about your runs. PythonTypeScriptto_search = {    ""user_id"": """"}# Check for any run with the ""user_id"" metadata keyclient.list_runs(  project_name=""default"",  filter=""eq(metadata_key, 'user_id')"")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))"")// Check for any run with the ""user_id"" metadata keyclient.listRuns({  projectName: 'default',  filter: `eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Check for environment details in metadata.,"A common pattern is to add environment
information to your traces via metadata. If you want to filter for runs containing
environment metadata, you can use the same pattern as above: PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Check for conversation ID in metadata,"Another common way to associate traces
in the same conversation is by using a shared conversation ID. If you want to filter
runs based on a conversation ID in this way, you can search for that ID in the metadata. PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Combine multiple filters,"If you want to combine multiple conditions to refine your search, you can use the and operator along with other
filtering functions. Here's how you can search for runs named ""ChatOpenAI"" that also
have a specific conversation_id in their metadata: PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Tree Filter,"List all runs named ""RetrieveDocs"" whose root run has a ""user_score"" feedback of 1 and any run in the full trace is named ""ExpandQuery"". This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace. PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='eq(name, ""RetrieveDocs"")',    trace_filter='and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))',    tree_filter='eq(name, ""ExpandQuery"")')client.listRuns({  projectName: ""<your_project>"",  filter: 'eq(name, ""RetrieveDocs"")',  traceFilter: 'and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))',  treeFilter: 'eq(name, ""ExpandQuery"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Advanced: export flattened trace view with child tool usage,"The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.
This can be used to analyze the behavior of your agents across multiple traces. This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information. To optimize the query, the example: Selects only the necessary fields when querying tool runs to reduce query time.Fetches root runs in batches while processing tool runs concurrently. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = ""my-project""num_days = 30# List all tool runstool_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=""tool"",    # We don't need to fetch inputs, outputs, and other values that # may increase the query time    select=[""trace_id"", ""name"", ""run_type""],)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group tool runs by parent run ID    for run in tqdm(tool_runs):        # Collect all tools invoked within a given trace        tool_runs_by_parent[run.trace_id][""tools_involved""].add(run.name)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the tool runs        if len(tool_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(tool_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[""name"", ""inputs"", ""outputs"", ""run_type""],                    )                )    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[""name"", ""inputs"", ""outputs"", ""run_type""],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = tool_runs_by_parent[root_run.id]        data.append(            {                ""run_id"": root_run.id,                ""run_name"": root_run.name,                ""run_type"": root_run.run_type,                ""inputs"": root_run.inputs,                ""outputs"": root_run.outputs,                ""tools_involved"": list(root_data[""tools_involved""]),            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-query-language,Advanced: export retriever IO for traces with feedback,"This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.
The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = ""your-project-name""num_days = 1# List all tool runsretriever_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=""retriever"",    # This time we do want to fetch the inputs and outputs, since they    # may be adjusted by query expansion steps.    select=[""trace_id"", ""name"", ""run_type"", ""inputs"", ""outputs""],    trace_filter='eq(feedback_key, ""user_score"")',)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group retriever runs by parent run ID    for run in tqdm(retriever_runs):        # Collect all retriever calls invoked within a given trace        for k, v in run.inputs.items():            retriever_runs_by_parent[run.trace_id][f""retriever.inputs.{k}""].append(v)        for k, v in (run.outputs or {}).items():            # Extend the docs            retriever_runs_by_parent[run.trace_id][f""retriever.outputs.{k}""].extend(v)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the retriever runs        if len(retriever_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(retriever_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[                            ""name"",                            ""inputs"",                            ""outputs"",                            ""run_type"",                            ""feedback_stats"",                        ],                    )                )    if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[""name"", ""inputs"", ""outputs"", ""run_type""],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = retriever_runs_by_parent[root_run.id]        feedback = {            f""feedback.{k}"": v.get(""avg"")            for k, v in (root_run.feedback_stats or {}).items()        }        inputs = {f""inputs.{k}"": v for k, v in root_run.inputs.items()}        outputs = {f""outputs.{k}"": v for k, v in (root_run.outputs or {}).items()}        data.append(            {                ""run_id"": root_run.id,                ""run_name"": root_run.name,                **inputs,                **outputs,                **feedback,                **root_data,            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"
https://docs.smith.langchain.com/reference/data_formats/feedback_data_format,Feedback data format,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and feedback Feedback is LangSmith's way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span).
Feedback can be produced from a variety of ways, such as: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator Feedback is stored in a simple format with the following fields: Field NameTypeDescriptionidUUIDUnique identifier for the record itselfcreated_atdatetimeTimestamp when the record was createdmodified_atdatetimeTimestamp when the record was last modifiedsession_idUUIDUnique identifier for the experiment or tracing project the run was a part ofrun_idUUIDUnique identifier for a specific run within a sessionkeystringA key describing the criteria of the feedback, eg ""correctness""scorenumberNumerical score associated with the feedback keyvaluestringReserved for storing a value associated with the score. Useful for categorical feedback.commentstringAny comment or annotation associated with the record. This can be a justification for the score given.correctionobjectReserved for storing correction details, if anyfeedback_sourceobjectObject containing information about the feedback sourcefeedback_source.typestringThe type of source where the feedback originated, eg ""api"", ""app"", ""evaluator""feedback_source.metadataobjectReserved for additional metadata, currentlyfeedback_source.user_idUUIDUnique identifier for the user providing feedback Here is an example JSON representation of a feedback record in the above format: {  ""created_at"": ""2024-05-05T23:23:11.077838"",  ""modified_at"": ""2024-05-05T23:23:11.232962"",  ""session_id"": ""c919298b-0af2-4517-97a2-0f98ed4a48f8"",  ""run_id"": ""e26174e5-2190-4566-b970-7c3d9a621baa"",  ""key"": ""correctness"",  ""score"": 1.0,  ""value"": null,  ""comment"": ""I gave this score because the answer was correct."",  ""correction"": null,  ""id"": ""62104630-c7f5-41dc-8ee2-0acee5c14224"",  ""feedback_source"": {    ""type"": ""app"",    ""metadata"": null,    ""user_id"": ""ad52b092-1346-42f4-a934-6e5521562fab""  }}"
https://docs.smith.langchain.com/how_to_guides/tracing/access_current_span,Access the current run (span) within a traced function,"In some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run. You can access the current run by calling the get_current_run_tree/getCurrentRunTree function in the Python or TypeScript SDK, respectively. For a full list of available properties on the RunTree object, see this reference. PythonTypeScriptfrom langsmith import traceablefrom langsmith.run_helpers import get_current_run_treefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    run = get_current_run_tree()    print(f""format_prompt Run Id: {run.id}"")    print(f""format_prompt Trace Id: {run.trace_id}"")    print(f""format_prompt Parent Run Id: {run.parent_run.id}"")    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    run = get_current_run_tree()    print(f""invoke_llm Run Id: {run.id}"")    print(f""invoke_llm Trace Id: {run.trace_id}"")    print(f""invoke_llm Parent Run Id: {run.parent_run.id}"")    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    run = get_current_run_tree()    print(f""parse_output Run Id: {run.id}"")    print(f""parse_output Trace Id: {run.trace_id}"")    print(f""parse_output Parent Run Id: {run.parent_run.id}"")    return response.choices[0].message.content@traceabledef run_pipeline():    run = get_current_run_tree()    print(f""run_pipeline Run Id: {run.id}"")    print(f""run_pipeline Trace Id: {run.trace_id}"")    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()import { traceable, getCurrentRunTree } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    const run = getCurrentRunTree();    console.log(""formatPrompt Run ID"", run.id)    console.log(""formatPrompt Trace ID"", run.trace_id)    console.log(""formatPrompt Parent Run ID"", run.parent_run.id)    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,      },    ];  },  { name: ""formatPrompt"" });const invokeLLM = traceable(    async (messages: { role: string; content: string }[]) => {        const run = getCurrentRunTree();        console.log(""invokeLLM Run ID"", run.id)        console.log(""invokeLLM Trace ID"", run.trace_id)        console.log(""invokeLLM Parent Run ID"", run.parent_run.id)        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        const run = getCurrentRunTree();        console.log(""parseOutput Run ID"", run.id)        console.log(""parseOutput Trace ID"", run.trace_id)        console.log(""parseOutput Parent Run ID"", run.parent_run.id)        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const run = getCurrentRunTree();        console.log(""runPipline Run ID"", run.id)        console.log(""runPipline Trace ID"", run.trace_id)        console.log(""runPipline Parent Run ID"", run.parent_run?.id)        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM(messages);        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only,Run evals with the REST API,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.
However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only,Create a dataset,"Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries - API Example""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only,Run a single experiment,"First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { ""dataset"": dataset_id }resp = requests.get(    ""https://api.smith.langchain.com/api/v1/examples"",    params=params,    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[""OPENAI_API_KEY""] = ""sk-...""def run_completion_on_example(example, model_name, experiment_id):    """"""Run completions on a list of examples.""""""    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        """"""Function to post a new run to the API.""""""        data = {            ""id"": run_id.hex,            ""name"": name,            ""run_type"": run_type,            ""inputs"": inputs,            ""start_time"": datetime.utcnow().isoformat(),            ""reference_example_id"": example[""id""],            ""session_id"": experiment_id,        }        if parent_id:            data[""parent_run_id""] = parent_id.hex        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/runs"", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        """"""Function to patch a run with outputs.""""""        resp = requests.patch(            f""https://api.smith.langchain.com/api/v1/runs/{run_id}"",            json={                ""outputs"": outputs,                ""end_time"": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    text = example[""inputs""][""text""]    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, ""LLM Pipeline"", ""chain"", {""text"": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, ""OpenAI Call"", ""llm"", {""messages"": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {""label"": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (""gpt-3.5-turbo"", ""gpt-4o-mini"")experiment_ids = []for model_name in model_names:    resp = requests.post(        ""https://api.smith.langchain.com/api/v1/sessions"",        json={            ""start_time"": datetime.utcnow().isoformat(),            ""reference_dataset_id"": str(dataset_id),            ""description"": ""An optional description for the experiment"",            ""name"": f""Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}"",  # A name for the experiment            ""extra"": {                ""metadata"": {""foo"": ""bar""},  # Optional metadata            },        },        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )    experiment = resp.json()    experiment_ids.append(experiment[""id""])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[""id""])    # Issue a patch request to ""end"" the experiment by updating the end_time    requests.patch(        f""https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}"",        json={""end_time"": datetime.utcnow().isoformat()},        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only,Run a pairwise experiment,"Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.
For more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/comparative"",    json={        ""experiment_ids"": experiment_ids,        ""name"": ""Toxicity detection - API Example - Comparative - "" + str(uuid4())[0:8],        ""description"": ""An optional description for the comparative experiment"",        ""extra"": {            ""metadata"": {""foo"": ""bar""},  # Optional metadata        },        ""reference_dataset_id"": str(dataset_id),    },    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[""id""]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f""https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative"",    params={""id"": comparative_experiment_id},    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()[0]experiment_ids = [info[""id""] for info in comparative_experiment[""experiments_info""]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f""https://api.smith.langchain.com/api/v1/runs/query"",    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]},    json={        ""session"": experiment_ids,        ""is_root"": True, # Only fetch root runs (spans) which contain the end outputs        ""select"": [""id"", ""reference_example_id"", ""outputs""],    }).json()runs = runs[""runs""]for run in runs:    example_id = run[""reference_example_id""]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f""Example ID: {example_id}"")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f""Run ID: {run['id']}"")        feedback = {            ""score"": 1 if i == 0 else 0,            ""run_id"": str(run[""id""]),            ""key"": ""ranked_preference"",            ""feedback_group_id"": str(feedback_group_id),            ""comparative_experiment_id"": comparative_experiment_id,        }        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/feedback"",            json=feedback,            headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}        )        resp.raise_for_status()"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics,Use LangChain off-the-shelf evaluators (Python only),"Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.
These evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Ankush"", ""Hello Ankush""),    (""Harrison"", ""Hello Harrison""),]dataset_name = ""Hello Set""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": input}, {""expected"": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics,Use question and answer (correctness) evaluators,"Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!
Three QA evaluators you can load are: ""qa"", ""context_qa"", ""cot_qa"". Based on our meta-evals, we recommend using ""cot_qa"", or Chain of Thought QA. Here is a trivial example that uses a ""cot_qa"" evaluator to evaluate a simple pipeline that prefixes the input with ""Hello"": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(""cot_qa"")client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics,Use criteria evaluators,"If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the ""criteria"" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The ""criteria"" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    ""criteria"",    config={        ""criteria"": {            ""says_hello"": ""Does the submission say hello?"",        }    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.
To specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {""creativity"": ""Is this submission creative, imaginative, or novel?""}criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={""criteria"": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent ""direction"" (i.e., higher is not necessarily better).
The direction of the score depends on the criteria being evaluated. For example, a score of 1 for ""helpfulness"" means that the prediction was deemed to be helpful by the model.
However, a score of 1 for ""maliciousness"" means that the prediction contains malicious content, which, of course, is ""bad""."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics,Use labeled criteria evaluators,"If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the ""labeled_criteria"" or ""labeled_score_string"" evaluators. The ""labeled_criteria"" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe ""labeled_score_string"" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    ""labeled_score_string"",    config={        ""criteria"": {            ""accuracy"": ""How accurate is this prediction compared to the reference on a scale of 1-10?""        },        ""normalize_by"": 10,    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics,Use string or embedding distance metrics,"To measure the similarity between a predicted string and a reference, you can use string distance metrics: The ""string_distance"" evaluator computes a normalized string edit distance between the prediction and referenceThe ""embedding_distance"" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    ""string_distance"",    config={""distance"": ""levenshtein"", ""normalize_score"": True})embedding_distance_evaluator = LangChainStringEvaluator(    ""embedding_distance"",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # ""embeddings"": HuggingFaceEmbeddings(model=""distilbert-base-uncased""),      # Can also choose ""euclidean"", ""chebyshev"", ""hamming"", and ""manhattan""        ""distance_metric"": ""cosine"",      })evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics,Use a custom LLM in off-the-shelf evaluators,"You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=""gpt-3.5-turbo"")cot_qa_evaluator = LangChainStringEvaluator(""cot_qa"", config={""llm"": eval_llm})evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-string-or-embedding-distance-metrics,Handle multiple input or output fields,"LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.
These map the keys ""prediction"", ""reference"", and ""input"" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: ""greeting"" and ""foo"". We want to evaluate the ""greeting"" field against the ""expected"" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    },    prepare_data=lambda run, example: {        ""prediction"": run.outputs[""greeting""],        ""reference"": example.outputs[""expected""],        ""input"": example.inputs[""input""],    })client = Client()evaluate(    lambda input: {""greeting"": ""Hello "" + input[""input""], ""foo"": ""bar""},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization,Set up an organization,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization,Create an organization,"When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.
Shared organizations require a credit card before they can be used. You will need to set up billing to proceed."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization,Manage and navigate workspaces,"Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.
To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization,Manage users,"Manage membership in your shared organization in the Settings page Members and roles tab.
Here you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization,Organization roles,"These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_vercel_ai_sdk,Trace with the Vercel AI SDK (JS/TS only),"betaThis feature is currently in beta while Vercel rolls out official telemetry support. You can use LangSmith to trace runs from the Vercel AI SDK using
the special wrapAISDKModel method. The important detail is that you must wrap the Vercel model wrapper
rather than of the top-level generateText or generateStream calls. This guide will walk through an example. noteThe wrapAISDKModel method is only available in langsmith JS SDK version >=0.1.40."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_vercel_ai_sdk,0. Installation,"Install the Vercel AI SDK. We use their OpenAI integration for the code snippets below, but you can use any of their
other options as well. yarnnpmpnpmyarn add ai @ai-sdk/openainpm install ai @ai-sdk/openaipnpm add ai @ai-sdk/openai"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_vercel_ai_sdk,1. Configure your environment,"Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_vercel_ai_sdk,2. Log a trace,"Once you've set up your environment, wrap an initialized Vercel model as shown below: import { wrapAISDKModel } from ""langsmith/wrappers/vercel"";import { openai } from ""@ai-sdk/openai"";import { generateText } from ""ai"";const vercelModel = openai(""gpt-4o-mini"");const modelWithTracing = wrapAISDKModel(vercelModel);const { text } = await generateText({  model: modelWithTracing,  prompt: ""Write a vegetarian lasagna recipe for 4 people."",});console.log(text); An example trace from running the above code looks like this: If you are using a streaming method, LangSmith will trace chunks as a single aggregated response: import { wrapAISDKModel } from ""langsmith/wrappers/vercel"";import { openai } from ""@ai-sdk/openai"";import { streamText } from ""ai"";const vercelModel = openai(""gpt-4o-mini"");const modelWithTracing = wrapAISDKModel(vercelModel);const { textStream } = await streamText({  model: modelWithTracing,  prompt: ""Write a vegetarian lasagna recipe for 4 people."",});for await (const chunk of textStream) {  ...} An example trace from running the above code will look the same as the above one from generateText."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#return-multiple-scores,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#use_a_prompt_without_langchain,Manage prompts programmatically,"You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.
All functionality going forward will live in the langsmith package."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#use_a_prompt_without_langchain,Install packages,PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#use_a_prompt_without_langchain,Configure environment variables,"If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=""lsv2_..."" TerminologyWhat we refer to as ""prompts"" used to be called ""repos"", so any references to ""repo"" in the code are referring to a prompt."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#use_a_prompt_without_langchain,Push a prompt,"To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = client.push_prompt(""joke-generator"", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = prompts.push(""joke-generator"", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");url = prompts.push(""joke-generator"", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.
This is useful for storing the model configuration you want to use with this prompt.
The provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelclient.push_prompt(""joke-generator-with-model"", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelurl = prompts.push(""joke-generator-with-model"", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { ChatOpenAI } from ""langchain-openai"";const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");const chain = prompt.pipe(model);prompts.push(""joke-generator-with-model"", chain);"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#use_a_prompt_without_langchain,Pull a prompt,"To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { ChatOpenAI } from ""langchain-openai"";const prompt = prompts.pull(""joke-generator"");const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const chain = prompt.pipe(model);chain.invoke({""topic"": ""cats""}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.
Just specify include_model when pulling the prompt.
If the stored prompt includes a model, it will be returned as a RunnableSequence.
Make sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})from langchain import hub as promptschain = prompts.pull(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { Runnable } from ""@langchain/core/runnables"";const chain = prompts.pull<Runnable>(""joke-generator-with-model"", { includeModel: true });chain.invoke({""topic"": ""cats""}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#use_a_prompt_without_langchain,Use a prompt without LangChain,"If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.
These convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(""joke-generator"")prompt_value = prompt.invoke({""topic"": ""cats""})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#use_a_prompt_without_langchain,"List, delete, and like prompts","You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.
See the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include ""joke""prompts = client.list_prompts(query=""joke"", is_public=False)# Delete a promptclient.delete_prompt(""joke-generator"")# Like a promptclient.like_prompt(""efriis/my-first-prompt"")# Unlike a promptclient.unlike_prompt(""efriis/my-first-prompt"")// List all prompts in my workspaceimport Client from ""langsmith"";const client = new Client({ apiKey: ""lsv2_..."" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include ""joke""const private_joke_prompts = client.listPrompts({ query: ""joke"", isPublic: false});// Delete a promptclient.deletePrompt(""joke-generator"");// Like a promptclient.likePrompt(""efriis/my-first-prompt"");// Unlike a promptclient.unlikePrompt(""efriis/my-first-prompt""); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.
However, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."
https://docs.smith.langchain.com/langgraph_cloud,LangGraph Cloud,"LangGraph Cloud is a managed service for deploying and hosting LangGraph applications. Deploying applications with LangGraph Cloud shortens the time-to-market for developers. With one click, deploy a production-ready API with built-in persistence for your LangGraph application. LangGraph Cloud APIs are horizontally scalable and deployed with durable storage. LangGraph Cloud is seamlessly integrated with LangSmith and is accessible from the Deployments section in the left-hand navigation bar. See the official LangGraph Cloud documentation for more details."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#view-pairwise-experiments,Run pairwise evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function
with two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#view-pairwise-experiments,Use theevaluate_comparativefunction,"notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#view-pairwise-experiments,Configure inputs and outputs for pairwise evaluators,"Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with
runs[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent ""both equally bad"" or both to 1 for ""both equally good"". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#view-pairwise-experiments,Compare two experiments with LLM-based pairwise evaluators,"The following example uses a prompt
which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.
The prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(""langchain-ai/pairwise-evaluation-2"")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=""gpt-4"")        runnable = prompt | model    response = runnable.invoke({        ""question"": example.inputs[""question""],        ""answer_a"": runs[0].outputs[""output""] if runs[0].outputs is not None else ""N/A"",        ""answer_b"": runs[1].outputs[""output""] if runs[1].outputs is not None else ""N/A"",    })    score = response[""Preference""]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {""key"": ""ranked_preference"", ""scores"": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [""my-experiment-name-1"", ""my-experiment-name-2""],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.
import type { Run, Example } from ""langsmith"";import { evaluateComparative } from ""langsmith/evaluation"";import { wrapOpenAI } from ""langsmith/wrappers"";import OpenAI from ""openai"";const openai = wrapOpenAI(new OpenAI());import { z } from ""zod"";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(""Expected at least two runs"");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? ""N/A"",    answer_b: runB?.outputs?.output ?? ""N/A"",  };    const output = await openai.chat.completions.create({    model: ""gpt-4-turbo"",    messages: [      {        role: ""system"",        content: [          ""Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below."",          ""You should choose the assistant that follows the user's instructions and answers the user's question better."",          ""Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses."",          ""Begin your evaluation by comparing the two responses and provide a short explanation."",          ""Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision."",          ""Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible."",        ].join("" ""),      },      {        role: ""user"",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(""\n\n""),      },    ],    tool_choice: {      type: ""function"",      function: { name: ""Score"" },    },    tools: [      {        type: ""function"",        function: {          name: ""Score"",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output ""1"" if Assistant A answer is better based upon the factors above.`,            `Output ""2"" if Assistant B answer is better based upon the factors above.`,            `Output ""0"" if it is a tie.`,          ].join("" ""),          parameters: {            type: ""object"",            properties: {              Preference: {                type: ""integer"",                description: ""Which assistant answer is preferred?"",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: ""ranked_preference"", scores };}await evaluateComparative([""earnest-name-40"", ""reflecting-pump-91""], {  evaluators: [evaluatePairwise],});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#view-pairwise-experiments,View pairwise experiments,"Navigate to the ""Pairwise Experiments"" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#assign-runs-to-an-annotation-queue,Use annotation queues,"Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.
While you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#assign-runs-to-an-annotation-queue,Create an annotation queue,"To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.
Then click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.
You can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as ""Done"" for it to be removed from the queue. If you check ""All workspace members review each run,"" then a run will remain in the queue until all workspace members have marked it ""Done"".Enable reservations on runs: We recommend enabling reservations.
This will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified ""reservation length"". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it ""Done"", the reservation will expire after the specified ""reservation length"".
The run is then released back into the queue and can be reserved by another reviewer. noteClicking ""Requeue at end"" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#assign-runs-to-an-annotation-queue,Assign runs to an annotation queue,"To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.
Select multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.
Set up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.
To learn more about how to capture user feedback from your LLM application, follow this guide."
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotation_queues#assign-runs-to-an-annotation-queue,Review runs in an annotation queue,"To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.
Then click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.
You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to ""View run"". The keyboard shortcuts shown can help streamline the review process."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace,Log custom LLM traces,"noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.
In order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace,Chat-style models,"For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg ""openai"", ""anthropic"", etc.ls_model_name: The name of the model, eg ""gpt-3.5-turbo"", ""claude-3-opus-20240307"", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ]}# Can also use one of:# output = {#     ""message"": {#         ""role"": ""assistant"",#         ""content"": ""Sure, what time would you like to book the table for?""#     }# }## output = {#     ""role"": ""assistant"",#     ""content"": ""Sure, what time would you like to book the table for?""# }## output = [""assistant"", ""Sure, what time would you like to book the table for?""]@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" }];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?""      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: ""assistant"",//     content: ""Sure, what time would you like to book the table for?""//   }// };//// const output = {//   role: ""assistant"",//   content: ""Sure, what time would you like to book the table for?""// };//// const output = [""assistant"", ""Sure, what time would you like to book the table for?""];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace,Stream outputs,"For streaming, you can ""reduce"" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = """".join([chunk[""choices""][0][""message""][""content""] for chunk in chunks])    return {""choices"": [{""message"": {""content"": all_text, ""role"": ""assistant""}}]}@traceable(    run_type=""llm"",    reduce_fn=_reduce_chunks,    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def my_streaming_chat_model(messages: list):    for chunk in [""Hello, "" + messages[1][""content""]]:        yield {            ""choices"": [                {                    ""message"": {                        ""content"": chunk,                        ""role"": ""assistant"",                    }                }            ]        }list(    my_streaming_chat_model(        [            {""role"": ""system"", ""content"": ""You are a helpful assistant. Please greet the user.""},            {""role"": ""user"", ""content"": ""polly the parrot""},        ],    ))"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace,Manually provide token counts,"Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.
Many models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.
If token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.
If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ],    ""usage_metadata"": {        ""input_tokens"": 27,        ""output_tokens"": 13,        ""total_tokens"": 40,    },}@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" },];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?"",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages });"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace,Instruct-style models,"For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.
The same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def hello_llm(prompt: str):    return {        ""choices"": [            {""text"": ""Hello, "" + prompt}        ],        ""usage_metadata"": {            ""input_tokens"": 4,            ""output_tokens"": 5,            ""total_tokens"": 9,        },    }hello_llm(""polly the parrot\n"")import { traceable } from ""langsmith/traceable"";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: ""Hello, "" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: ""llm"", name: ""hello_llm"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await helloLLM({ prompt: ""polly the parrot\n"" }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments,Upload experiments run outside of LangSmith with the REST API,"Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments,Request body schema,"Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within
the experiment. Each object in the results represents a ""row"" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name
refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset
in LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  ""experiment_name"": ""string (required)"",  ""experiment_description"": ""string (optional)"",  ""experiment_start_time"": ""datetime (required)"",  ""experiment_end_time"": ""datetime (required)"",  ""dataset_id"": ""uuid (optional - an external dataset id, used to group experiments together)"",  ""dataset_name"": ""string (optional - must provide either dataset_id or dataset_name)"",  ""dataset_description"": ""string (optional)"",  ""experiment_metadata"": { // Object (any shape - optional)    ""key"": ""value""  },  ""summary_experiment_scores"": [ // List of summary feedback objects (optional)    {      ""key"": ""string (required)"",      ""score"": ""number (optional)"",      ""value"": ""string (optional)"",      ""comment"": ""string (optional)"",      ""feedback_source"": { // Object (optional)        ""type"": ""string (required)""      },      ""feedback_config"": { // Object (optional)        ""type"": ""string enum: continuous, categorical, or freeform"",        ""min"": ""number (optional)"",        ""max"": ""number (optional)"",        ""categories"": [ // List of feedback category objects (optional)            ""value"": ""number (required)"",            ""label"": ""string (optional)""        ]      },      ""created_at"": ""datetime (optional - defaults to now)"",      ""modified_at"": ""datetime (optional - defaults to now)"",      ""correction"": ""Object or string (optional)""    }  ],  ""results"": [ // List of experiment row objects (required)    {      ""row_id"": ""uuid (required)"",      ""inputs"": {     // Object (required - any shape). This will        ""key"": ""val""  // be the input to both the run and the dataset example.      },      ""expected_outputs"": { // Object (optional - any shape).        ""key"": ""val""        // These will be the outputs of the dataset examples.      },      ""actual_outputs"": { // Object (optional - any shape).        ""key"": ""val       // These will be the outputs of the runs.      },      ""evaluation_scores"": [ // List of feedback objects for the run (optional)        {            ""key"": ""string (required)"",            ""score"": ""number (optional)"",            ""value"": ""string (optional)"",            ""comment"": ""string (optional)"",            ""feedback_source"": { // Object (optional)                ""type"": ""string (required)""            },            ""feedback_config"": { // Object (optional)                ""type"": ""string enum: continuous, categorical, or freeform"",                ""min"": ""number (optional)"",                ""max"": ""number (optional)"",                ""categories"": [ // List of feedback category objects (optional)                    ""value"": ""number (required)"",                    ""label"": ""string (optional)""                ]            },            ""created_at"": ""datetime (optional - defaults to now)"",            ""modified_at"": ""datetime (optional - defaults to now)"",            ""correction"": ""Object or string (optional)""        }      ],      ""start_time"": ""datetime (required)"", // The start/end times for the runs will be used to      ""end_time"": ""datetime (required)"",   // calculate latency. They must all fall between the      ""run_name"": ""string (optional)"",     // start and end times for the experiment.      ""error"": ""string (optional)"",      ""run_metadata"": { // Object (any shape - optional)        ""key"": ""value""      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments,Considerations,"You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together
under a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if
you only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments,Example request,"Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  ""experiment_name"": ""My external experiment"",  ""experiment_description"": ""An experiment uploaded to LangSmith"",  ""dataset_name"": ""my-external-dataset"",  ""summary_experiment_scores"": [    {      ""key"": ""summary_accuracy"",      ""score"": 0.9,      ""comment"": ""Great job!""    }  ],  ""results"": [    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the weather in San Francisco today?""      },      ""expected_outputs"": {        ""output"": ""Sorry, I am unable to provide information about the current weather.""      },      ""actual_outputs"": {        ""output"": ""The weather is partly cloudy with a high of 65.""      },      ""evaluation_scores"": [        {          ""key"": ""hallucination"",          ""score"": 1,          ""comment"": ""The chatbot made up the weather instead of identifying that ""                     ""they don't have enough info to answer the question. This is ""                     ""a hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:39"",      ""end_time"": ""2024-08-03T00:12:41"",      ""run_name"": ""Chatbot""    },    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the square root of 49?""      },      ""expected_outputs"": {        ""output"": ""The square root of 49 is 7.""      },      ""actual_outputs"": {        ""output"": ""7.""      },      ""evaluation_scores"": [       {          ""key"": ""hallucination"",          ""score"": 0,          ""comment"": ""The chatbot correctly identified the answer. This is not a ""                     ""hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:40"",      ""end_time"": ""2024-08-03T00:12:42"",      ""run_name"": ""Chatbot""    }  ],  ""experiment_start_time"": ""2024-08-03T00:12:38"",  ""experiment_end_time"": ""2024-08-03T00:12:43""}resp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/upload-experiment"",    json=body,    headers={""x-api-key"": os.environ[""LANGCHAIN_API_KEY""]})print(resp.json()) Below is the response received: {  ""dataset"": {    ""name"": ""my-external-dataset"",    ""description"": null,    ""created_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""data_type"": ""kv"",    ""inputs_schema_definition"": null,    ""outputs_schema_definition"": null,    ""externally_managed"": true,    ""id"": ""<<uuid>>"",    ""tenant_id"": ""<<uuid>>"",    ""example_count"": 0,    ""session_count"": 0,    ""modified_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""last_session_start_time"": null  },  ""experiment"": {    ""start_time"": ""2024-08-03T00:12:38"",    ""end_time"": ""2024-08-03T00:12:43+00:00"",    ""extra"": null,    ""name"": ""My external experiment"",    ""description"": ""An experiment uploaded to LangSmith"",    ""default_dataset_id"": null,    ""reference_dataset_id"": ""<<uuid>>"",    ""trace_tier"": ""longlived"",    ""id"": ""<<uuid>>"",    ""run_count"": null,    ""latency_p50"": null,    ""latency_p99"": null,    ""first_token_p50"": null,    ""first_token_p99"": null,    ""total_tokens"": null,    ""prompt_tokens"": null,    ""completion_tokens"": null,    ""total_cost"": null,    ""prompt_cost"": null,    ""completion_cost"": null,    ""tenant_id"": ""<<uuid>>"",    ""last_run_start_time"": null,    ""last_run_start_time_live"": null,    ""feedback_stats"": null,    ""session_feedback_stats"": null,    ""run_facets"": null,    ""error_rate"": null,    ""streaming_rate"": null,    ""test_run_number"": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.
If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this
information in the request body)."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments,View the experiment in the UI,"Now, login to the UI and click on your newly-created dataset! You should see a single experiment:
 Your examples will have been uploaded:
 Clicking on your experiment will bring you to the comparison view:
 As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,LangSmith Release Notes,noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,"Week of June 17, 2024 - LangSmith v0.6","LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,New Features,Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More...
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Performance and Reliability Changes,Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Infrastructure changes,"[Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Admin changes,Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Deprecation notices,With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,"Week of May 13, 2024 - LangSmith v0.5","LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Breaking changes,We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,New Features,Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Performance and Reliability Changes,"Split ingest, session deletion, and automation jobs to execute within separate resource pools."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Infrastructure changes,"As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Admin changes,Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Deprecation notices,With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,"Week of March 25, 2024 - LangSmith v0.4","LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Breaking changes,"This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Performance and Reliability Changes,"Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Infrastructure changes,"Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Admin changes,Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Deprecation notices,With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,"Week of Februrary 21, 2024 - LangSmith v0.3","LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Breaking changes,"This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Performance and Reliability Changes,"Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Admin changes,None
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Deprecation notices,With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,"Week of January 29, 2024 - LangSmith v0.2","LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Requirements,This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Breaking changes,"The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {""user_id"": ..., ""user_name"":...,}) and then search using has(metadata, '{""user_name"": ...}')"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Performance and Reliability Changes,"Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Infrastructure Changes,"Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Admin changes,Increased the maximum number of users per organization from 5 to 100 for new organizations.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-june-17-2024---langsmith-v05,Deprecation notices,With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace,Deleting Workspaces,"The LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table. This command using the Workspace ID as an argument."
https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace,Prerequisites,"Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/PostgreSQL clienthttps://www.postgresql.org/download/PostgreSQL database connection:HostPortUsernameIf using the bundled version, this is postgresPasswordIf using the bundled version, this is postgresDatabase nameIf using the bundled version, this is postgresClickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the PostgreSQL database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the postgresql service to your local machine.Run kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.Connectivity to the Clickhouse database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port 8443"
https://docs.smith.langchain.com/self_hosting/scripts/delete_a_workspace,Running the deletion script for a single workspace,"Run the following command to run the workspace removal script: sh delete_workspace.sh <postgres_url> <clickhouse_url> --workspace_id <workspace_id> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_workspace.sh ""postgres://postgres:postgres@localhost:5432/postgres"" ""clickhouse://default:password@localhost:8123/default"" --workspace_id 4ec70ec7-0808-416a-b836-7100aeec934b If you visit the Langsmith UI, you should now see workspace is deleted."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#ensure-all-traces-are-submitted-before-exiting,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#use-an-ai-query-to-auto-generate-a-query,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-traceable--traceable,Annotate code for tracing,"There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-traceable--traceable,Use@traceable/traceable,"LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.
Note that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.
import { traceable } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: ""formatPrompt"" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-traceable--traceable,Wrap the OpenAI client,"The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!
The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceable(name=""Chat Pipeline"")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(""Can you summarize this morning's meetings?"")import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", run_type: ""tool"" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: ""system"",            content:                ""You are a helpful assistant. Please respond to the user's request only based on the given context."",        },        { role: ""user"", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: ""Chat Pipeline"" });await chatPipeline(""Can you summarize this morning's meetings?"");"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-traceable--traceable,Use theRunTreeAPI,"Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually
create runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not
necessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# Create a top-level runpipeline = RunTree(    name=""Chat Pipeline"",    run_type=""chain"",    inputs={""question"": question})# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}]# Create a child runchild_llm_run = pipeline.create_child(    name=""OpenAI Call"",    run_type=""llm"",    inputs={""messages"": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={""answer"": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from ""openai"";import { RunTree } from ""langsmith"";// This can be a user input to your appconst question = ""Can you summarize this morning's meetings?"";const pipeline = new RunTree({    name: ""Chat Pipeline"",    run_type: ""chain"",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = ""During this morning's meeting, we solved all world conflict."";const messages = [    { role: ""system"", content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { role: ""user"", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: ""OpenAI Call"",    run_type: ""llm"",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#use-traceable--traceable,Use thetracecontext manager (Python only),"In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {""input"": ""Can you summarize this morning's meetings?""}with trace(""Chat Pipeline"", ""chain"", project_name=""my_test"", inputs=app_inputs) as rt:    output = chat_pipeline(""Can you summarize this morning's meetings?"")    rt.end(outputs={""output"": output})"
https://docs.smith.langchain.com/tutorials/Developers/backtesting,Backtesting,"Deploying your app into production is just one step in a longer journey continuous improvement. You'll likely want to develop other candidate systems that improve on your production model using improved prompts, llms, indexing strategies, and other techniques. While you may have a set of offline datasets already created by this point, it's often useful to compare system performance on more recent production data. This notebook shows how to do this in LangSmith. The basic steps are: Sample runs to test against from your production tracing project.Convert runs to dataset + initial experiment.Run new system against the dataset to compare. You will then have a new dataset of representative inputs you can you can version and backtest your models against. Note: In most cases, you won't have ""ground truth"" answers in this case, but you can manually compare and label or use reference-free evaluators to score the outputs.(If your application DOES permit capturing ground-truth labels, then we obviously recommend you use those."
https://docs.smith.langchain.com/tutorials/Developers/backtesting,Prerequisites,"Install + set environment variables. This requires langsmith>=0.1.29 to use the beta utilities. %%capture --no-stderr%pip install -U --quiet langsmith langchain_anthropic langchainhub langchain import os# Set the project name to whichever project you'd like to be testing againstproject_name = ""Tweet Critic""os.environ[""LANGCHAIN_API_KEY""] = ""YOUR API KEY""os.environ[""LANGCHAIN_TRACING_V2""] = ""true""os.environ[""ANTHROPIC_API_KEY""] = ""YOUR ANTHROPIC API KEY""os.environ[""LANGCHAIN_PROJECT""] = project_name (Preliminary) Production Deployment You likely have a project already and can skip this step. We'll simulate one here so no one reading this notebook gets left out.
Our example app is a ""tweet critic"" that revises tweets we put out. from langchain import hubfrom langchain_anthropic import ChatAnthropicfrom langchain_core.messages import HumanMessagefrom langchain_core.output_parsers import StrOutputParserprompt = hub.pull(""wfh/tweet-critic:7e4f539e"")llm = ChatAnthropic(model=""claude-3-haiku-20240307"")system = prompt | llm | StrOutputParser()inputs = [    """"""RAG From Scratch: Our RAG From Scratch video series covers some important RAG concepts in short, focused videos with code. This is the 10th video and it discusses query routing. Problem: We sometimes have multiple datastores (e.g., different vector DBs, SQL DBs, etc) and prompts to choose from based on a user query. Idea: Logical routing can use an LLM to decide which datastore is more appropriate. Semantic routing embeds the query and prompts, then chooses the best prompt based on similarity. Video: https://youtu.be/pfpIndq7Fi8 Code: https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb"""""",    """"""@Voyage_AI_ Embedding Integration Package Use the same custom embeddings that power Chat LangChain via the new langchain-voyageai package! Voyage AI builds custom embedding models that can improve retrieval quality. ChatLangChain: https://chat.langchain.com Python Docs: https://python.langchain.com/docs/integrations/providers/voyageai"""""",    """"""Implementing RAG: How to Write a Graph Retrieval Query in LangChain Our friends at @neo4j have a nice guide on combining LLMs and graph databases. Blog:"""""",    """"""Text-to-PowerPoint with LangGraph.js You can now generate PowerPoint presentations from text! @TheGreatBonnie wrote a guide showing how to use LangGraph.js, @tavilyai, and @CopilotKit to build a Next.js app for this. Tutorial: https://dev.to/copilotkit/how-to-build-an-ai-powered-powerpoint-generator-langchain-copilotkit-openai-nextjs-4c76 Repo: https://github.com/TheGreatBonnie/aipoweredpowerpointapp"""""",    """"""Build an Answer Engine Using Groq, Mixtral, Langchain, Brave & OpenAI in 10 Min Our friends at @Dev__Digest have a tutorial on building an answer engine over the internet. Code: https://github.com/developersdigest/llm-answer-engine YouTube: https://youtube.com/watch?v=43ZCeBTcsS8&t=96s"""""",    """"""Building a RAG Pipeline with LangChain and Amazon Bedrock Amazon Bedrock has great models for building LLM apps. This guide covers how to get started with them to build a RAG pipeline. https://gettingstarted.ai/langchain-bedrock/"""""",    """"""SF Meetup on March 27! Join our meetup to hear from LangChain and Pulumi experts and learn about building AI-enabled capabilities. Sign up: https://meetup.com/san-francisco-pulumi-user-group/events/299491923/?utm_campaign=FY2024Q3_Meetup_PUG%20SF&utm_content=286236214&utm_medium=social&utm_source=twitter&hss_channel=tw-837770064870817792"""""",    """"""Chat model response metadata @LangChainAI chat model invocations now include metadata like logprobs directly in the output. Upgrade your version of `langchain-core` to try it. PY: https://python.langchain.com/docs/modules/model_io/chat/logprobs JS: https://js.langchain.com/docs/integrations/chat/openai#generation-metadata"""""",    """"""Benchmarking Query Analysis in High Cardinality Situations Handling high-cardinality categorical values can be challenging. This blog explores 6 different approaches you can take in these situations. Blog: https://blog.langchain.dev/high-cardinality"""""",    """"""Building Google's Dramatron with LangGraph.js & Claude 3 We just released a long YouTube video (1.5 hours!) on building Dramatron using LangGraphJS and @AnthropicAI's Claude 3 ""Haiku"" model. It's a perfect fit for LangGraph.js and Haiku's speed. Check out the tutorial: https://youtube.com/watch?v=alHnQjyn7hg"""""",    """"""Document Loading Webinar with @AirbyteHQ Join a webinar on document loading with PyAirbyte and LangChain on 3/14 at 10am PDT. Features our founding engineer @eyfriis and the @aaronsteers and Bindi Pankhudi team. Register: https://airbyte.com/session/airbyte-monthly-ai-demo"""""",]_ = system.batch(    [{""messages"": [HumanMessage(content=content)]} for content in inputs],    {""max_concurrency"": 3},)"
https://docs.smith.langchain.com/tutorials/Developers/backtesting,Convert Prod Runs to Experiment,"The first step is to generate a dataset based on the production inputs.
Then copy over all the traces to serve as a baseline run. convert_runs_to_test is a function which takes some runs and does the following: The inputs, and optionally the outputs, are saved to a dataset as Examples.The inputs and outputs are stored as an experiment, as if you had run the evaluate
function and received those outputs. from datetime import datetime, timedelta, timezonefrom langsmith import Clientfrom langsmith.beta import convert_runs_to_test# How we are sampling runs to include in our datasetend_time = datetime.now(tz=timezone.utc)start_time = end_time - timedelta(days=1)run_filter = f'and(gt(start_time, ""{start_time.isoformat()}""), lt(end_time, ""{end_time.isoformat()}""))'# Fetch the runs we want to convert to a dataset/experimentclient = Client()prod_runs = list(    client.list_runs(        project_name=project_name,        execution_order=1,        filter=run_filter,    ))# Name of the dataset we want to createdataset_name = f'{project_name}-backtesting {start_time.strftime(""%Y-%m-%d"")}-{end_time.strftime(""%Y-%m-%d"")}'# This converts the runs to a dataset + experiment# It does not actually invoke your modelconvert_runs_to_test(    prod_runs,    # Name of the resulting dataset    dataset_name=dataset_name,    # Whether to include the run outputs as reference/ground truth    include_outputs=False,    # Whether to include the full traces in the resulting experiment    # (default is to just include the root run)    load_child_runs=True,)"
https://docs.smith.langchain.com/tutorials/Developers/backtesting,Benchmark new system,"Now we have the dataset and prod runs saved as an experiment. Let's run inference on our new system to compare. from langsmith.evaluation import evaluatedef predict(example_input: dict):    # The dataset includes serialized messages that we    # must convert to a format accepted by our system.    messages = {        ""messages"": [            (message[""type""], message[""content""])            for message in example_input[""messages""]        ]    }    return system.invoke(messages)# Use an updated version of the promptprompt = hub.pull(""wfh/tweet-critic:34c57e4f"")llm = ChatAnthropic(model=""claude-3-haiku-20240307"")system = prompt | llm | StrOutputParser()test_results = evaluate(    predict, data=dataset_name, experiment_prefix=""HaikuBenchmark"", max_concurrency=3)"
https://docs.smith.langchain.com/tutorials/Developers/backtesting,Review runs,You can now compare the outputs in the UI.
https://docs.smith.langchain.com/tutorials/Developers/backtesting,Conclusion,"Congrats! You've sampled production runs and started benchmarking other systems against them.
In this exercise, we chose not to apply any evaluators to simplify things (since we lack ground-truth answers for this task).
You can manually review the results in LangSmith and/or apply a reference-free evaluator to the results to generate metrics instead."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-single-experiment,Run evals with the REST API,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.
However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-single-experiment,Create a dataset,"Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries - API Example""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-single-experiment,Run a single experiment,"First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { ""dataset"": dataset_id }resp = requests.get(    ""https://api.smith.langchain.com/api/v1/examples"",    params=params,    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[""OPENAI_API_KEY""] = ""sk-...""def run_completion_on_example(example, model_name, experiment_id):    """"""Run completions on a list of examples.""""""    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        """"""Function to post a new run to the API.""""""        data = {            ""id"": run_id.hex,            ""name"": name,            ""run_type"": run_type,            ""inputs"": inputs,            ""start_time"": datetime.utcnow().isoformat(),            ""reference_example_id"": example[""id""],            ""session_id"": experiment_id,        }        if parent_id:            data[""parent_run_id""] = parent_id.hex        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/runs"", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        """"""Function to patch a run with outputs.""""""        resp = requests.patch(            f""https://api.smith.langchain.com/api/v1/runs/{run_id}"",            json={                ""outputs"": outputs,                ""end_time"": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    text = example[""inputs""][""text""]    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, ""LLM Pipeline"", ""chain"", {""text"": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, ""OpenAI Call"", ""llm"", {""messages"": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {""label"": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (""gpt-3.5-turbo"", ""gpt-4o-mini"")experiment_ids = []for model_name in model_names:    resp = requests.post(        ""https://api.smith.langchain.com/api/v1/sessions"",        json={            ""start_time"": datetime.utcnow().isoformat(),            ""reference_dataset_id"": str(dataset_id),            ""description"": ""An optional description for the experiment"",            ""name"": f""Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}"",  # A name for the experiment            ""extra"": {                ""metadata"": {""foo"": ""bar""},  # Optional metadata            },        },        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )    experiment = resp.json()    experiment_ids.append(experiment[""id""])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[""id""])    # Issue a patch request to ""end"" the experiment by updating the end_time    requests.patch(        f""https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}"",        json={""end_time"": datetime.utcnow().isoformat()},        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-single-experiment,Run a pairwise experiment,"Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.
For more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/comparative"",    json={        ""experiment_ids"": experiment_ids,        ""name"": ""Toxicity detection - API Example - Comparative - "" + str(uuid4())[0:8],        ""description"": ""An optional description for the comparative experiment"",        ""extra"": {            ""metadata"": {""foo"": ""bar""},  # Optional metadata        },        ""reference_dataset_id"": str(dataset_id),    },    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[""id""]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f""https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative"",    params={""id"": comparative_experiment_id},    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()[0]experiment_ids = [info[""id""] for info in comparative_experiment[""experiments_info""]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f""https://api.smith.langchain.com/api/v1/runs/query"",    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]},    json={        ""session"": experiment_ids,        ""is_root"": True, # Only fetch root runs (spans) which contain the end outputs        ""select"": [""id"", ""reference_example_id"", ""outputs""],    }).json()runs = runs[""runs""]for run in runs:    example_id = run[""reference_example_id""]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f""Example ID: {example_id}"")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f""Run ID: {run['id']}"")        feedback = {            ""score"": 1 if i == 0 else 0,            ""run_id"": str(run[""id""]),            ""key"": ""ranked_preference"",            ""feedback_group_id"": str(feedback_group_id),            ""comparative_experiment_id"": comparative_experiment_id,        }        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/feedback"",            json=feedback,            headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}        )        resp.raise_for_status()"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-and-manage-dataset-splits,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Export traces,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Run (span) data formatLangSmith API ReferenceLangSmith trace query syntax The recommended way to export runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Use filter arguments,"For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client, Run } from ""langsmith"";const client = new Client(); Below are some examples of ways to list runs using keyword arguments:"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List all runs in a project,"PythonTypeScriptproject_runs = client.list_runs(project_name=""<your_project>"")// Download runs in a projectconst projectRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",})) {  projectRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List LLM and Chat runs in the last 24 hours,"PythonTypeScripttodays_llm_runs = client.list_runs(    project_name=""<your_project>"",    start_time=datetime.now() - timedelta(days=1),    run_type=""llm"",)const todaysLlmRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),  runType: ""llm"",})) {  todaysLlmRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List root runs in a project,"Root runs are runs that have no parents. These are assigned a value of True for is_root. You can use this to filter for root runs. PythonTypeScriptroot_runs = client.list_runs(    project_name=""<your_project>"",    is_root=True)const rootRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  isRoot: 1,})) {  rootRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List runs without errors,"PythonTypeScriptcorrect_runs = client.list_runs(project_name=""<your_project>"", error=False)const correctRuns: Run[] = [];for await (const run of client.listRuns({  projectName: ""<your_project>"",  error: false,})) {  correctRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List runs by run ID,"Ignores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like project_name, run_type, etc. and directly return the runs matching the given IDs. If you have a list of run IDs, you can list them directly: PythonTypeScriptrun_ids = ['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)const runIds = [  ""a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836"",  ""9398e6be-964f-4aa4-8ae9-ad78cd4b7074"",];const selectedRuns: Run[] = [];for await (const run of client.listRuns({  id: runIds,})) {  selectedRuns.push(run);};"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Use filter query language,"For more complex queries, you can use the query language described in the filter query language refernece."
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,"List all runs called ""extractor"" whose root of the trace was assigned feedback ""user_score"" score of 1","PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='eq(name, ""extractor"")',    trace_filter='and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))')client.listRuns({  projectName: ""<your_project>"",  filter: 'eq(name, ""extractor"")',  traceFilter: 'and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,"List runs with ""star_rating"" key whose score is greater than 4","PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='and(eq(feedback_key, ""star_rating""), gt(feedback_score, 4))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(eq(feedback_key, ""star_rating""), gt(feedback_score, 4))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List runs that took longer than 5 seconds to complete,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(latency, ""5s"")')client.listRuns({projectName: ""<your_project>"", filter: 'gt(latency, ""5s"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List all runs wheretotal_tokensis greater than 5000,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(total_tokens, 5000)')client.listRuns({projectName: ""<your_project>"", filter: 'gt(total_tokens, 5000)'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,"List all runs that have ""error"" not equal to null","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='neq(error, null)')client.listRuns({projectName: ""<your_project>"", filter: 'neq(error, null)'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List all runs wherestart_timeis greater than a specific timestamp,"PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='gt(start_time, ""2023-07-15T12:34:56Z"")')client.listRuns({projectName: ""<your_project>"", filter: 'gt(start_time, ""2023-07-15T12:34:56Z"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,"List all runs that contain the string ""substring""","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='search(""substring"")')client.listRuns({projectName: ""<your_project>"", filter: 'search(""substring"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,"List all runs that are tagged with the git hash ""2aa1cf4""","PythonTypeScriptclient.list_runs(project_name=""<your_project>"", filter='has(tags, ""2aa1cf4"")')client.listRuns({projectName: ""<your_project>"", filter: 'has(tags, ""2aa1cf4"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,"List all ""chain"" type runs that took more than 10 seconds and","had total_tokens greater than 5000 PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(eq(run_type, ""chain""), gt(latency, 10), gt(total_tokens, 5000))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(eq(run_type, ""chain""), gt(latency, 10), gt(total_tokens, 5000))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,List all runs that started after a specific timestamp and either,"have ""error"" not equal to null or a ""Correctness"" feedback score equal to 0 PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(gt(start_time, ""2023-07-15T12:34:56Z""), or(neq(error, null), and(eq(feedback_key, ""Correctness""), eq(feedback_score, 0.0))))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(gt(start_time, ""2023-07-15T12:34:56Z""), or(neq(error, null), and(eq(feedback_key, ""Correctness""), eq(feedback_score, 0.0))))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,"Complex query: List all runs wheretagsinclude ""experimental"" or ""beta"" and","latency is greater than 2 seconds PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='and(or(has(tags, ""experimental""), has(tags, ""beta"")), gt(latency, 2))')client.listRuns({  projectName: ""<your_project>"",  filter: 'and(or(has(tags, ""experimental""), has(tags, ""beta"")), gt(latency, 2))'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Search trace trees by full text You can use thesearch()function without,"any specific field to do a full text search across all string fields in a run. This
allows you to quickly find traces that match a search term. PythonTypeScriptclient.list_runs(  project_name=""<your_project>"",  filter='search(""image classification"")')client.listRuns({  projectName: ""<your_project>"",  filter: 'search(""image classification"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Check for presence of metadata,"If you want to check for the presence of metadata, you can use the eq operator, optionally with an and statement to match by value. This is useful if you want to log more structured information
about your runs. PythonTypeScriptto_search = {    ""user_id"": """"}# Check for any run with the ""user_id"" metadata keyclient.list_runs(  project_name=""default"",  filter=""eq(metadata_key, 'user_id')"")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))"")// Check for any run with the ""user_id"" metadata keyclient.listRuns({  projectName: 'default',  filter: `eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Check for environment details in metadata.,"A common pattern is to add environment
information to your traces via metadata. If you want to filter for runs containing
environment metadata, you can use the same pattern as above: PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Check for conversation ID in metadata,"Another common way to associate traces
in the same conversation is by using a shared conversation ID. If you want to filter
runs based on a conversation ID in this way, you can search for that ID in the metadata. PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Combine multiple filters,"If you want to combine multiple conditions to refine your search, you can use the and operator along with other
filtering functions. Here's how you can search for runs named ""ChatOpenAI"" that also
have a specific conversation_id in their metadata: PythonTypeScriptclient.list_runs(  project_name=""default"",  filter=""and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))"")client.listRuns({  projectName: 'default',  filter: `and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Tree Filter,"List all runs named ""RetrieveDocs"" whose root run has a ""user_score"" feedback of 1 and any run in the full trace is named ""ExpandQuery"". This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace. PythonTypeScriptclient.list_runs(    project_name=""<your_project>"",    filter='eq(name, ""RetrieveDocs"")',    trace_filter='and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))',    tree_filter='eq(name, ""ExpandQuery"")')client.listRuns({  projectName: ""<your_project>"",  filter: 'eq(name, ""RetrieveDocs"")',  traceFilter: 'and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))',  treeFilter: 'eq(name, ""ExpandQuery"")'})"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Advanced: export flattened trace view with child tool usage,"The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.
This can be used to analyze the behavior of your agents across multiple traces. This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information. To optimize the query, the example: Selects only the necessary fields when querying tool runs to reduce query time.Fetches root runs in batches while processing tool runs concurrently. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = ""my-project""num_days = 30# List all tool runstool_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=""tool"",    # We don't need to fetch inputs, outputs, and other values that # may increase the query time    select=[""trace_id"", ""name"", ""run_type""],)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group tool runs by parent run ID    for run in tqdm(tool_runs):        # Collect all tools invoked within a given trace        tool_runs_by_parent[run.trace_id][""tools_involved""].add(run.name)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the tool runs        if len(tool_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(tool_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[""name"", ""inputs"", ""outputs"", ""run_type""],                    )                )    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[""name"", ""inputs"", ""outputs"", ""run_type""],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = tool_runs_by_parent[root_run.id]        data.append(            {                ""run_id"": root_run.id,                ""run_name"": root_run.name,                ""run_type"": root_run.run_type,                ""inputs"": root_run.inputs,                ""outputs"": root_run.outputs,                ""tools_involved"": list(root_data[""tools_involved""]),            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"
https://docs.smith.langchain.com/how_to_guides/tracing/export_traces#use-filter-arguments,Advanced: export retriever IO for traces with feedback,"This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.
The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = ""your-project-name""num_days = 1# List all tool runsretriever_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=""retriever"",    # This time we do want to fetch the inputs and outputs, since they    # may be adjusted by query expansion steps.    select=[""trace_id"", ""name"", ""run_type"", ""inputs"", ""outputs""],    trace_filter='eq(feedback_key, ""user_score"")',)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group retriever runs by parent run ID    for run in tqdm(retriever_runs):        # Collect all retriever calls invoked within a given trace        for k, v in run.inputs.items():            retriever_runs_by_parent[run.trace_id][f""retriever.inputs.{k}""].append(v)        for k, v in (run.outputs or {}).items():            # Extend the docs            retriever_runs_by_parent[run.trace_id][f""retriever.outputs.{k}""].extend(v)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the retriever runs        if len(retriever_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(retriever_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[                            ""name"",                            ""inputs"",                            ""outputs"",                            ""run_type"",                            ""feedback_stats"",                        ],                    )                )    if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[""name"", ""inputs"", ""outputs"", ""run_type""],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = retriever_runs_by_parent[root_run.id]        feedback = {            f""feedback.{k}"": v.get(""avg"")            for k, v in (root_run.feedback_stats or {}).items()        }        inputs = {f""inputs.{k}"": v for k, v in root_run.inputs.items()}        outputs = {f""outputs.{k}"": v for k, v in (root_run.outputs or {}).items()}        data.append(            {                ""run_id"": root_run.id,                ""run_name"": root_run.name,                **inputs,                **outputs,                **feedback,                **root_data,            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#set-api-keys,Set up online evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Set up automation rules Online evaluations is a powerful LangSmith feature that allows you to run an LLM-as-a-judge evaluator on a set of your production traces. They are implemented as a possible action in an automation rule. Currently, we provide support for specifying a prompt template, a model, and a set of criteria to evaluate the runs on. After entering rules setup, you can select Online Evaluation from the list of possible actions:"
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#set-api-keys,Configure online evaluations,"When selection Online Evaluation as an action in an automation, you are presented with a panel from which you can configure online evaluation."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#set-api-keys,Model,"You can choose any model available in the dropdown. Currently, we support OpenAI, AzureOpenAI, and models hosted on Fireworks.
In order to set the API keys to use for these invocations, click on the Secrets & API Keys button and add the necessary keys."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#set-api-keys,Prompt template,"For the prompt template, you can select a suggested evaluator prompt, create a new prompt, or choose a prompt from the LangChain Hub. Suggested evaluator prompts We provide a list of pre-existing prompts that cater to common evaluator use cases. Create your own prompt We provide a base template from which you can form your own prompt. Pull a prompt from the LangChain Hub You can pull any structured prompt, private or public. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses.
If the prompt is your own, you can edit it in the playground and commit the version.
If the prompt is someone else's, you can fork the prompt, make your edits in the playground, and then pull in your new prompt to the online evaluator. When you choose a hub prompt for your online evaluator, the prompt will be locked to the commit version it was at rule creation. If you want to update the prompt, you can go to edit the online evaluator and re-select the prompt in the dropdown."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#set-api-keys,Mapping variables,"Prompts can be crafted with any variable name you choose. To map what is passed into your evaluator prompt from your runs or experiments, use the variable mapping inputs. There's a dropdown with suggestions provided based on the schema of your recent runs."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#set-api-keys,Preview,"Previewing the prompt will show you an example of what the formatted prompt will look like. This preview pulls the input and output of the most recent run. In the case of a dataset evaluator, the preview will also pull reference output from an example in your dataset. noteYou can configure an evaluation prompt that doesn't match the schema of your recent runs, but the dropdown suggestions and preview function won't work as expected."
https://docs.smith.langchain.com/how_to_guides/monitoring/online_evaluations#set-api-keys,Output schema,"An evaluator will attach metadata tags to a run. These tags will have a name and a value. You can configure these in the Schema section.
The names and the descriptions of the fields will be passed into the prompt. Behind the scenes, we use tool calling to coerce the output of the LLM into the score you specify. noteThe name of the schema cannot have spaces since it is used as the name of a tool."
https://docs.smith.langchain.com/self_hosting/usage,Using your self-hosted instance of LangSmith,"This guide will walk you through the process of using your self-hosted instance of LangSmith. Self-Hosted LangSmith Instance RequiredThis guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the kubernetes deployment guide or the docker deployment guide."
https://docs.smith.langchain.com/self_hosting/usage,Configuring the application you want to use with LangSmith,"LangSmith has two APIs: One for interacting with the LangChain Hub/prompts and one for interacting with the backend of the LangSmith application. Each exists at its own URL and in a self-hosted environment are set via the LANGCHAIN_HUB_API_URL and LANGCHAIN_ENDPOINT environment variables, respectively, and have their own separate SDKs. Once you have deployed your instance, you can access the LangSmith UI at http://<host>.The backend API will be available at http://<host>/apiThe hub/prompts API will be available at http://<host>/api-hub. To use the LangSmith API, you will need to set the following environment variables in your application: LANGCHAIN_ENDPOINT=http://<host>/apiLANGCHAIN_HUB_API_URL=http://<host>/api-hubLANGCHAIN_API_KEY=foo # Set to a legitimate API key if using OAuth You can also configure these variables directly in the LangSmith SDK client: import langsmithlangsmith_client = langsmith.Client(    api_key='<api_key>',    api_url='http://<host>/api',)import langchainhublangchainhub.Client(    api_key='<api_key>',    api_url='http://<host>/api-hub') After setting the above, you should be able to run your code and see the results in your self-hosted instance.
We recommend running through the quickstart guide to get a feel for how to use LangSmith."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-users,Set up an organization,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-users,Create an organization,"When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.
Shared organizations require a credit card before they can be used. You will need to set up billing to proceed."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-users,Manage and navigate workspaces,"Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.
To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-users,Manage users,"Manage membership in your shared organization in the Settings page Members and roles tab.
Here you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_organization#manage-users,Organization roles,"These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."
https://docs.smith.langchain.com/how_to_guides/tracing,How-to guides: Tracing,"This section contains how-to guides related to tracing.  Annotate code for tracingThere are several ways to log traces to LangSmith. Toggle tracing on and offThis section is only relevant for users who are Log traces to specific projectYou can change the destination project of your traces both statically through environment variables and dynamically at runtime. Set a sampling rate for tracesThis section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API. Add metadata and tags to tracesLangSmith supports sending arbitrary metadata and tags along with traces. Implement distributed tracingSometimes, you need to trace a request across multiple services. Access the current run (span) within a traced functionIn some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run. Log multimodal tracesLangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs. Log retriever tracesNothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps. Log custom LLM tracesNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. Prevent logging of sensitive data in tracesIn some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend. Export tracesBefore diving into this content, it might be helpful to read the following: Share or unshare a trace publiclySharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information. Compare tracesTo compare traces, click on the Compare button in the upper right hand side of any trace view. Trace generator functionsIn most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user. Trace with LangChain (Python and JS/TS)LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications. Trace with LangGraph (Python and JS/TS)LangSmith smoothly integrates with LangGraph (Python and JS) Trace with Instructor (Python only)We provide a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs. Trace with the Vercel AI SDK (JS/TS only)This feature is currently in beta while Vercel rolls out official telemetry support. Trace without setting environment variablesAs mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: Trace using the LangSmith REST APIIt is HIGHLY recommended to use our Python or TypeScript SDKs to send traces to LangSmith. Calculate token-based costs for tracesBefore diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/tracing,Annotate code for tracing,There are several ways to log traces to LangSmith.
https://docs.smith.langchain.com/how_to_guides/tracing,Toggle tracing on and off,This section is only relevant for users who are
https://docs.smith.langchain.com/how_to_guides/tracing,Log traces to specific project,You can change the destination project of your traces both statically through environment variables and dynamically at runtime.
https://docs.smith.langchain.com/how_to_guides/tracing,Set a sampling rate for traces,"This section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API."
https://docs.smith.langchain.com/how_to_guides/tracing,Add metadata and tags to traces,LangSmith supports sending arbitrary metadata and tags along with traces.
https://docs.smith.langchain.com/how_to_guides/tracing,Implement distributed tracing,"Sometimes, you need to trace a request across multiple services."
https://docs.smith.langchain.com/how_to_guides/tracing,Access the current run (span) within a traced function,"In some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run."
https://docs.smith.langchain.com/how_to_guides/tracing,Log multimodal traces,LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs.
https://docs.smith.langchain.com/how_to_guides/tracing,Log retriever traces,"Nothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps."
https://docs.smith.langchain.com/how_to_guides/tracing,Log custom LLM traces,"Nothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs."
https://docs.smith.langchain.com/how_to_guides/tracing,Prevent logging of sensitive data in traces,"In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend."
https://docs.smith.langchain.com/how_to_guides/tracing,Export traces,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/tracing,Share or unshare a trace publicly,Sharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.
https://docs.smith.langchain.com/how_to_guides/tracing,Compare traces,"To compare traces, click on the Compare button in the upper right hand side of any trace view."
https://docs.smith.langchain.com/how_to_guides/tracing,Trace generator functions,"In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user."
https://docs.smith.langchain.com/how_to_guides/tracing,Trace with LangChain (Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing,Trace with LangGraph (Python and JS/TS),LangSmith smoothly integrates with LangGraph (Python and JS)
https://docs.smith.langchain.com/how_to_guides/tracing,Trace with Instructor (Python only),"We provide a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs."
https://docs.smith.langchain.com/how_to_guides/tracing,Trace with the Vercel AI SDK (JS/TS only),This feature is currently in beta while Vercel rolls out official telemetry support.
https://docs.smith.langchain.com/how_to_guides/tracing,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:"
https://docs.smith.langchain.com/how_to_guides/tracing,Trace using the LangSmith REST API,It is HIGHLY recommended to use our Python or TypeScript SDKs to send traces to LangSmith.
https://docs.smith.langchain.com/how_to_guides/tracing,Calculate token-based costs for traces,"Before diving into this content, it might be helpful to read the following:"
https://docs.smith.langchain.com/how_to_guides/tracing/sample_traces,Set a sampling rate for traces,"noteThis section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API. By default, all traces are logged to LangSmith.
To down-sample the number of traces logged to LangSmith, set the LANGCHAIN_TRACING_SAMPLING_RATE environment variable to
any float between 0 (no traces) and 1 (all traces).
For instance, setting the following environment variable will log 75% of the traces. export LANGCHAIN_TRACING_SAMPLING_RATE=0.75 This works for the traceable decorator and RunTree objects."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Evaluation,"The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Datasets,"Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Creating datasets,"There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.
From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,
and what ""good"" responses may be.
You probably want to cover a few different common edge cases or situations you can imagine.
Even 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?
This information can be valuable to capture and store in datasets. This allows you to test against these
use cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?
There are a few heuristics you can follow.
If possible - try to collect end user feedback. You can then see which datapoints got negative feedback.
That is super valuable! These are spots where your application did not perform well.
You should add these to your dataset to test against in the future. You can also use other heuristics
to identify ""interesting"" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.
It's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.
This can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Types of datasets,"LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:""Inputs"" and ""outputs"" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating ""completion"" style language models.The ""inputs"" dictionary contains a single ""input"" key mapped to the prompt string.The ""outputs"" dictionary contains a single ""output"" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured ""chat"" messages as inputs and outputs.The ""inputs"" dictionary contains a single ""input"" key mapped to a list of serialized chat messagesThe ""outputs"" dictionary contains a single ""output"" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Partitioning datasets,"When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Evaluators,"Evaluators are functions in LangSmith that score how well your application performs on a particular example.
Evaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Human,Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering.
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Heuristic,"Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,LLM-as-judge,"LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Pairwise,"Pairwise evaluators pick the better of two task outputs based upon some criteria.
This can use either a heuristic (""which response is longer""), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.
This can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Applying evaluations,"We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Unit Tests,"Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.
This means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).
These types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)
to cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Regression Testing,"Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.
They are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Back-testing,"Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.
Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.
Then compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Pairwise-testing,"It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Online Evaluation,"Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Experiment Configurations,"LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Evaluating Specific LLM Applications,"Below, we will discuss evaluation of a few specific, popular LLM applications."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Agents,"LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.
Note that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Evaluating an agent's final response,"One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Evaluating a single step of an agent,"Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Evaluating an agent's trajectory,"Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an ""exact"" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the ""exact"" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of ""incorrect"" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Best practices,"Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Retrieval Augmented Generation (RAG),"Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Dataset,"When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Evaluator,"LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Applying RAG Evaluation,"When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,RAG evaluation summary,Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Summarization,"Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"
https://docs.smith.langchain.com/concepts/evaluation#evaluators,Classification / Tagging,"Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Manage datasets in the application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Create a new dataset and add examples manually,"To get started, you can create a new datasets by heading to the ""Datasets and Testing"" section of the application and clicking on ""+ New Dataset"". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on ""Add Example"". Here, you can enter the input and output as JSON objects."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Dataset schema validation,"If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click ""Editor"" at the bottom of the schema editor and then select ""JSON"". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Add inputs and outputs from traces to datasets,"We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.
For tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Upload a CSV file to create a dataset,"The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Generate synthetic examples,"For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the ""API Key"" link.
 After clicking ""Generate,"" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.
Each example will be validated against your specified dataset schema and tagged as ""synthetic"" in the source metadata.
"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Export a dataset,"You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click ""Export Dataset"" from the homepage.
To do so, select a dataset, click on ""Examples"", and then click the ""Export Dataset"" button at the top of the examples table. This will open a modal where you can select the format you want to export to."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Create and manage dataset splits,"Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common
in machine learning workflows to split datasets into training, validation, and test sets. This can be useful
to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation
workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate
separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want
to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits
to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas
metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).
However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for
some evaluation workflows - for example, if an example falls into multiple categories on which you may want to
evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click ""Add to Split"". From the resulting popup menu,
you can select and unselect splits for the selected examples, or create a new split."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Edit example metadata,"You can add metadata to your examples by clicking on an example and then clicking on the ""Metadata"" tab in the side pane.
From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about
your examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_in_application#create-a-new-dataset-and-add-examples-manually,Filter examples,"You can filter examples by metadata key/value or full-text search. To filter examples, click ""Filter"" in the top left of the table: Next, click ""Add filter"" and select ""Full Text"" or ""Metadata"" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the
filters will be displayed in the table."
https://docs.smith.langchain.com/concepts/admin#workspaces,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#workspaces,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#workspaces,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#workspaces,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#workspaces,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#workspaces,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#workspaces,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#workspaces,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#workspaces,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#view-your-prompts,Create a prompt,"Navigate to the Prompts section in the left-hand sidebar or from the application homepage.
Click the ""+ Prompt"" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.
"
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#view-your-prompts,Compose your prompt,"After choosing a prompt type, you're brought to the playground to develop your prompt.
On the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click ""Start""."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#view-your-prompts,Save your prompt,"To save your prompt, click the ""Save as"" button, name your prompt, and decide if you want it to be ""private"" or ""public"".
Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.
Click save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#view-your-prompts,View your prompts,You've just created your first prompt! View a table of your prompts in the prompts tab.
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt#view-your-prompts,Add metadata,"To add metadata to your prompt, click the prompt and then click the ""Edit"" pencil icon next to the name.
This brings you to where you can add additional information about the prompt, including a description, a README, and use cases.
For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."
https://docs.smith.langchain.com/self_hosting/configuration/blob_storage,Enable blob storage,"By default, LangSmith stores run inputs, outputs, and errors in ClickHouse. In addition, LangSmith will store run manifests in Postgres. If you so choose, you can instead store this information in blob storage, which has a couple of notable benefits: In high trace environments, inputs, outputs, errors, and manifests may balloon the size of your databases.If using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment.
To alleviate this, LangSmith supports storing run manifests, inputs, outputs, and errors in an external blob storage system."
https://docs.smith.langchain.com/self_hosting/configuration/blob_storage,Requirements,"Access to a valid blob storage serviceAmazon S3Google Cloud StorageCurrently, Azure Blob Storage is not supported (coming soon)A bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data.If you are using TTLs, you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs here. These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss.Credentials to permit LangSmith Services to access the bucket/directoryYou will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication section below for more information.An API url for your blob storage serviceThis will be the URL that LangSmith uses to access your blob storage systemIn the case of Amazon S3, this will be the URL of the S3 endpoint. Something like: https://s3.amazonaws.com or https://s3.us-west-1.amazonaws.com if using a regional endpoint.In the case of Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: https://storage.googleapis.com"
https://docs.smith.langchain.com/self_hosting/configuration/blob_storage,Amazon S3,"To authenticate to Amazon S3, you will need to create an IAM policy granting admin permissions on your bucket.
This will look something like the following: {  ""Version"": ""2012-10-17"",  ""Statement"": [    {      ""Effect"": ""Allow"",      ""Action"": ""s3:*"",      ""Resource"": [        ""arn:aws:s3:::your-bucket-name"",        ""arn:aws:s3:::your-bucket-name/*""      ]    }  ]} Once you have the correct policy, there are two ways to authenticate with Amazon S3: IAM Role for Service Account: You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.You will need to create an IAM role with the policy attached.You will need to allow LangSmith service accounts to assume the role. The langsmith-queue and langsmith-backend service accounts will need to be able to assume the role.Service Account NamesThe service account names will be different if you are using a custom release name. You can find the service account names by running kubectl get serviceaccounts in your cluster.You will need to provide the role ARN to LangSmith. You can do this by adding the eks.amazonaws.com/role-arn: ""<role_arn>"" annotation to the langsmith-queue and langsmith-backend services in your Helm Chart installation.Access Key and Secret Key: You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user."
https://docs.smith.langchain.com/self_hosting/configuration/blob_storage,Google Cloud Storage,"To authenticate with Google Cloud Storage, you will need to create a service account with the necessary permissions to access your bucket. Your service account will need the Storage Admin role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using. Once you have a provisioned service account, you will need to generate a HMAC key for that service account. This key and secret will be used to authenticate with Google Cloud Storage."
https://docs.smith.langchain.com/self_hosting/configuration/blob_storage,Configuration,"After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system. Helm  config:    blobStorage:      enabled: true      bucketName: ""your-bucket-name""      apiURL: ""Your connection url""      accessKey: ""Your access key"" # Optional. Only required if using access key and secret key      accessKeySecret: ""Your access key secret"" # Optional. Only required if using access key and secret key    backend: # Optional, only required if using IAM role for service account     serviceAccount:       annotations:         eks.amazonaws.com/role-arn: ""<role_arn>""    queue: # Optional, only required if using IAM role for service account      serviceAccount:        annotations:          eks.amazonaws.com/role-arn: ""<role_arn>""       Using an existing secretIf using an access key and secret, you can also provide an existing kubernetes secret that contains the access key and secret key.
This is recommended over providing the access key and secret key directly in your config."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#request-body-schema,Upload experiments run outside of LangSmith with the REST API,"Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#request-body-schema,Request body schema,"Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within
the experiment. Each object in the results represents a ""row"" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name
refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset
in LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  ""experiment_name"": ""string (required)"",  ""experiment_description"": ""string (optional)"",  ""experiment_start_time"": ""datetime (required)"",  ""experiment_end_time"": ""datetime (required)"",  ""dataset_id"": ""uuid (optional - an external dataset id, used to group experiments together)"",  ""dataset_name"": ""string (optional - must provide either dataset_id or dataset_name)"",  ""dataset_description"": ""string (optional)"",  ""experiment_metadata"": { // Object (any shape - optional)    ""key"": ""value""  },  ""summary_experiment_scores"": [ // List of summary feedback objects (optional)    {      ""key"": ""string (required)"",      ""score"": ""number (optional)"",      ""value"": ""string (optional)"",      ""comment"": ""string (optional)"",      ""feedback_source"": { // Object (optional)        ""type"": ""string (required)""      },      ""feedback_config"": { // Object (optional)        ""type"": ""string enum: continuous, categorical, or freeform"",        ""min"": ""number (optional)"",        ""max"": ""number (optional)"",        ""categories"": [ // List of feedback category objects (optional)            ""value"": ""number (required)"",            ""label"": ""string (optional)""        ]      },      ""created_at"": ""datetime (optional - defaults to now)"",      ""modified_at"": ""datetime (optional - defaults to now)"",      ""correction"": ""Object or string (optional)""    }  ],  ""results"": [ // List of experiment row objects (required)    {      ""row_id"": ""uuid (required)"",      ""inputs"": {     // Object (required - any shape). This will        ""key"": ""val""  // be the input to both the run and the dataset example.      },      ""expected_outputs"": { // Object (optional - any shape).        ""key"": ""val""        // These will be the outputs of the dataset examples.      },      ""actual_outputs"": { // Object (optional - any shape).        ""key"": ""val       // These will be the outputs of the runs.      },      ""evaluation_scores"": [ // List of feedback objects for the run (optional)        {            ""key"": ""string (required)"",            ""score"": ""number (optional)"",            ""value"": ""string (optional)"",            ""comment"": ""string (optional)"",            ""feedback_source"": { // Object (optional)                ""type"": ""string (required)""            },            ""feedback_config"": { // Object (optional)                ""type"": ""string enum: continuous, categorical, or freeform"",                ""min"": ""number (optional)"",                ""max"": ""number (optional)"",                ""categories"": [ // List of feedback category objects (optional)                    ""value"": ""number (required)"",                    ""label"": ""string (optional)""                ]            },            ""created_at"": ""datetime (optional - defaults to now)"",            ""modified_at"": ""datetime (optional - defaults to now)"",            ""correction"": ""Object or string (optional)""        }      ],      ""start_time"": ""datetime (required)"", // The start/end times for the runs will be used to      ""end_time"": ""datetime (required)"",   // calculate latency. They must all fall between the      ""run_name"": ""string (optional)"",     // start and end times for the experiment.      ""error"": ""string (optional)"",      ""run_metadata"": { // Object (any shape - optional)        ""key"": ""value""      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#request-body-schema,Considerations,"You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together
under a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if
you only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#request-body-schema,Example request,"Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  ""experiment_name"": ""My external experiment"",  ""experiment_description"": ""An experiment uploaded to LangSmith"",  ""dataset_name"": ""my-external-dataset"",  ""summary_experiment_scores"": [    {      ""key"": ""summary_accuracy"",      ""score"": 0.9,      ""comment"": ""Great job!""    }  ],  ""results"": [    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the weather in San Francisco today?""      },      ""expected_outputs"": {        ""output"": ""Sorry, I am unable to provide information about the current weather.""      },      ""actual_outputs"": {        ""output"": ""The weather is partly cloudy with a high of 65.""      },      ""evaluation_scores"": [        {          ""key"": ""hallucination"",          ""score"": 1,          ""comment"": ""The chatbot made up the weather instead of identifying that ""                     ""they don't have enough info to answer the question. This is ""                     ""a hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:39"",      ""end_time"": ""2024-08-03T00:12:41"",      ""run_name"": ""Chatbot""    },    {      ""row_id"": ""<<uuid>>"",      ""inputs"": {        ""input"": ""Hello, what is the square root of 49?""      },      ""expected_outputs"": {        ""output"": ""The square root of 49 is 7.""      },      ""actual_outputs"": {        ""output"": ""7.""      },      ""evaluation_scores"": [       {          ""key"": ""hallucination"",          ""score"": 0,          ""comment"": ""The chatbot correctly identified the answer. This is not a ""                     ""hallucination.""        }      ],      ""start_time"": ""2024-08-03T00:12:40"",      ""end_time"": ""2024-08-03T00:12:42"",      ""run_name"": ""Chatbot""    }  ],  ""experiment_start_time"": ""2024-08-03T00:12:38"",  ""experiment_end_time"": ""2024-08-03T00:12:43""}resp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/upload-experiment"",    json=body,    headers={""x-api-key"": os.environ[""LANGCHAIN_API_KEY""]})print(resp.json()) Below is the response received: {  ""dataset"": {    ""name"": ""my-external-dataset"",    ""description"": null,    ""created_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""data_type"": ""kv"",    ""inputs_schema_definition"": null,    ""outputs_schema_definition"": null,    ""externally_managed"": true,    ""id"": ""<<uuid>>"",    ""tenant_id"": ""<<uuid>>"",    ""example_count"": 0,    ""session_count"": 0,    ""modified_at"": ""2024-08-03T00:36:23.289730+00:00"",    ""last_session_start_time"": null  },  ""experiment"": {    ""start_time"": ""2024-08-03T00:12:38"",    ""end_time"": ""2024-08-03T00:12:43+00:00"",    ""extra"": null,    ""name"": ""My external experiment"",    ""description"": ""An experiment uploaded to LangSmith"",    ""default_dataset_id"": null,    ""reference_dataset_id"": ""<<uuid>>"",    ""trace_tier"": ""longlived"",    ""id"": ""<<uuid>>"",    ""run_count"": null,    ""latency_p50"": null,    ""latency_p99"": null,    ""first_token_p50"": null,    ""first_token_p99"": null,    ""total_tokens"": null,    ""prompt_tokens"": null,    ""completion_tokens"": null,    ""total_cost"": null,    ""prompt_cost"": null,    ""completion_cost"": null,    ""tenant_id"": ""<<uuid>>"",    ""last_run_start_time"": null,    ""last_run_start_time_live"": null,    ""feedback_stats"": null,    ""session_feedback_stats"": null,    ""run_facets"": null,    ""error_rate"": null,    ""streaming_rate"": null,    ""test_run_number"": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.
If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this
information in the request body)."
https://docs.smith.langchain.com/how_to_guides/evaluation/upload_existing_experiments#request-body-schema,View the experiment in the UI,"Now, login to the UI and click on your newly-created dataset! You should see a single experiment:
 Your examples will have been uploaded:
 Clicking on your experiment will bring you to the comparison view:
 As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-for-intermediate-runs-spans,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt,Create a prompt,"Navigate to the Prompts section in the left-hand sidebar or from the application homepage.
Click the ""+ Prompt"" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.
"
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt,Compose your prompt,"After choosing a prompt type, you're brought to the playground to develop your prompt.
On the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click ""Start""."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt,Save your prompt,"To save your prompt, click the ""Save as"" button, name your prompt, and decide if you want it to be ""private"" or ""public"".
Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.
Click save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt,View your prompts,You've just created your first prompt! View a table of your prompts in the prompts tab.
https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt,Add metadata,"To add metadata to your prompt, click the prompt and then click the ""Edit"" pencil icon next to the name.
This brings you to where you can add additional information about the prompt, including a description, a README, and use cases.
For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."
https://docs.smith.langchain.com/tutorials/Developers/observability,Add observability to your LLM application,"Observability is important for any software application, but especially so for LLM applications.
LLMs are non-deterministic by nature, meaning they can produce unexpected results.
This makes them trickier than normal to debug. Luckily, this is where LangSmith can help!
LangSmith has LLM-native observability, allowing you to get meaningful insights into your application. Note that observability is important throughout all stages of application development - from prototyping, to beta testing, to production.
There are different considerations at all stages, but they are all intricately tied together.
In this tutorial we walk through the natural progression. Let's assume that we're building a simple RAG application using the OpenAI SDK.
The simple application we're adding observability to looks like: PythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [""Harrison worked at Kensho""]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = """"""Answer the users question using only the provided information below:        {docs}"""""".format(docs=""\n"".join(docs))        return openai_client.chat.completions.create(        messages=[            {""role"": ""system"", ""content"": system_message},            {""role"": ""user"", ""content"": question},        ],        model=""gpt-3.5-turbo"",    )import { OpenAI } from ""openai"";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [""This is a document""];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    ""Answer the users question using only the provided information below:\n\n"" +    docs.join(""\n"");      return await openAIClient.chat.completions.create({    messages: [      { role: ""system"", content: systemMessage },      { role: ""user"", content: question },    ],    model: ""gpt-3.5-turbo"",  });}"
https://docs.smith.langchain.com/tutorials/Developers/observability,Prototyping,"Having observability set up from the start can you help iterate much more quickly than you would otherwise be able to.
It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using.
In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping."
https://docs.smith.langchain.com/tutorials/Developers/observability,Set up your environment,"First, create an API key by navigating to the settings page. Next, install the LangSmith SDK: Python SDKTypeScript SDKpip install langsmithnpm install langsmith Finally, set up the appropriate environment variables. This will log traces to the default project (though you can easily change that). export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key>export LANGCHAIN_PROJECT=default"
https://docs.smith.langchain.com/tutorials/Developers/observability,Trace your LLM calls,"The first thing you might want to trace is all your OpenAI calls.
After all, this is where the LLM is actually being called, so it is the most important part!
We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper.
All you have to do is modify your code to look something like: PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [""Harrison worked at Kensho""]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = """"""Answer the users question using only the provided information below:        {docs}"""""".format(docs=""\n"".join(docs))        return openai_client.chat.completions.create(        messages=[            {""role"": ""system"", ""content"": system_message},            {""role"": ""user"", ""content"": question},        ],        model=""gpt-3.5-turbo"",    )import { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [""This is a document""];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    ""Answer the users question using only the provided information below:\n\n"" +    docs.join(""\n"");      return await openAIClient.chat.completions.create({    messages: [      { role: ""system"", content: systemMessage },      { role: ""user"", content: question },    ],    model: ""gpt-3.5-turbo"",  });} Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client (openai_client = wrap_openai(OpenAI())). What happens if you call it in the following way? rag(""where did harrison work"") This will produce a trace of just the OpenAI call - it should look something like this"
https://docs.smith.langchain.com/tutorials/Developers/observability,Trace the whole chain,"Great - we've traced the LLM call. But it's often very informative to trace more than that.
LangSmith is built for tracing the entire LLM pipeline - so let's do that!
We can do this by modifying the code to now look something like this: PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [""Harrison worked at Kensho""]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = """"""Answer the users question using only the provided information below:        {docs}"""""".format(docs=""\n"".join(docs))        return openai_client.chat.completions.create(        messages=[            {""role"": ""system"", ""content"": system_message},            {""role"": ""user"", ""content"": question},        ],        model=""gpt-3.5-turbo"",    )import { OpenAI } from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [""This is a document""];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    ""Answer the users question using only the provided information below:\n\n"" +    docs.join(""\n"");      return await openAIClient.chat.completions.create({    messages: [      { role: ""system"", content: systemMessage },      { role: ""user"", content: question },    ],    model: ""gpt-3.5-turbo"",  });}); Notice how we import from langsmith import traceable and use it decorate the overall function (@traceable). What happens if you call it in the following way? rag(""where did harrison work"") This will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this"
https://docs.smith.langchain.com/tutorials/Developers/observability,Trace the retrieval step,"There's one last part of the application we haven't traced - the retrieval step!
Retrieval is a key part of LLM applications, and we've made it easy to log retrieval steps as well.
All we have to do is modify our code to look like: PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=""retriever"")def retriever(query: str):    results = [""Harrison worked at Kensho""]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = """"""Answer the users question using only the provided information below:        {docs}"""""".format(docs=""\n"".join(docs))        return openai_client.chat.completions.create(        messages=[            {""role"": ""system"", ""content"": system_message},            {""role"": ""user"", ""content"": question},        ],        model=""gpt-3.5-turbo"",    )import { OpenAI } from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const openAIClient = wrapOpenAI(new OpenAI());const retriever = traceable(  async function retriever(query: string) {    return [""This is a document""];  },  { run_type: ""retriever"" })const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    ""Answer the users question using only the provided information below:\n\n"" +    docs.join(""\n"");      return await openAIClient.chat.completions.create({    messages: [      { role: ""system"", content: systemMessage },      { role: ""user"", content: question },    ],    model: ""gpt-3.5-turbo"",  });}); Notice how we import from langsmith import traceable and use it decorate the overall function (@traceable(run_type=""retriever"")). What happens if you call it in the following way? rag(""where did harrison work"") This will produce a trace of the whole chain including the retrieval step - it should look something like this"
https://docs.smith.langchain.com/tutorials/Developers/observability,Beta Testing,"The next stage of LLM application development is beta testing your application.
This is when you release it to a few initial users.
Having good observability set up here is crucial as often you don't know exactly how users will actually use your application, so this allows you get insights into how they do so.
This also means that you probably want to make some changes to your tracing set up to better allow for that.
This extends the observability you set up in the previous section"
https://docs.smith.langchain.com/tutorials/Developers/observability,Collecting Feedback,"A huge part of having good observability during beta testing is collecting feedback.
What feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start.
After logging that feedback, you need to be able to easily associate it with the run that caused that.
Luckily LangSmith makes it easy to do that. First, you need to log the feedback from your app.
An easy way to do this is to keep track of a run ID for each run, and then use that to log feedback.
Keeping track of the run ID would look something like: import uuidrun_id = str(uuid.uuid4())rag(    ""where did harrison work"",    langsmith_extra={""run_id"": run_id}) Associating feedback with that run would look something like: from langsmith import Clientls_client = Client()ls_client.create_feedback(    run_id,    key=""user-score"",    score=1.0,) Once the feedback is logged, you can then see it associated with each run by clicking into the Metadata tab when inspecting the run.
It should look something like this You can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table.
You can do this by creating a filter like the following:"
https://docs.smith.langchain.com/tutorials/Developers/observability,Logging Metadata,"It is also a good idea to start logging metadata.
This allows you to start keep track of different attributes of your app.
This is important in allowing you to know what version or variant of your app was used to produce a given result. For this example, we will log the LLM used.
Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering.
In order to do that, we can add it as such: from openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=""retriever"")def retriever(query: str):    results = [""Harrison worked at Kensho""]    return results@traceable(metadata={""llm"": ""gpt-3.5-turbo""})def rag(question):    docs = retriever(question)    system_message = """"""Answer the users question using only the provided information below:    {docs}"""""".format(docs='\n'.join(docs))    return openai_client.chat.completions.create(messages = [        {""role"": ""system"", ""content"": system_message},        {""role"": ""user"", ""content"": question},    ], model=""gpt-3.5-turbo"") Notice we added @traceable(metadata={""llm"": ""gpt-3.5-turbo""}) to the rag function. Keeping track of metadata in this way assumes that it is known ahead of time.
This is fine for LLM types, but less desirable for other types of information - like a User ID.
In order to log information that, we can pass it in at run time with the run ID. import uuidrun_id = str(uuid.uuid4())rag(    ""where did harrison work"",    langsmith_extra={""run_id"": run_id, ""metadata"": {""user_id"": ""harrison""}}) Now that we've logged these two pieces of metadata, we should be able to see them both show up in the UI here. We can filter for these pieces of information by constructing a filter like the following:"
https://docs.smith.langchain.com/tutorials/Developers/observability,Production,"Great - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well.
Time to ship it to production!
What new observability do you need to add? First of all, let's note that the same observability you've already added will keep on providing value in production.
You will continue to be able to drill down into particular runs. In production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time.
Luckily, LangSmith has a set of tools to help with observability in production."
https://docs.smith.langchain.com/tutorials/Developers/observability,Monitoring,"If you click on the Monitor tab in a project, you will see a series of monitoring charts.
Here we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc.
You can view these over time across a few different time bins."
https://docs.smith.langchain.com/tutorials/Developers/observability,A/B Testing,"noteGroup-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key. You can also use this tab to perform a version of A/B Testing.
In the previous tutorial we starting tracking a few different metadata attributes - one of which was llm.
We can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time.
This allows us to experiment with different LLMs (or prompts, or other) and track their performance over time. In order to do this, we just need to click on the Metadata button at the top.
This will give us a drop down of options to choose from to group by: Once we select this, we will start to see charts grouped by this attribute:"
https://docs.smith.langchain.com/tutorials/Developers/observability,Drilldown,"One of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify
as problematic while looking at monitoring charts.
In order to do this, you can simply hover over a datapoint in the monitoring chart.
When you do this, you will be able to click the datapoint.
This will lead you back to the runs table with a filtered view:"
https://docs.smith.langchain.com/tutorials/Developers/observability,Conclusion,"In this tutorial you saw how to set up your LLM application with best-in-class observability.
No matter what stage your application is in, you will still benefit from observability. If you have more in-depth questions about observability, check out the how-to section for guides on topics like testing, prompt management, and more. Observability is not the only thing LangSmith can help with!
It can also help with evaluation, optimization, and more!
Check out the other tutorials to see how to get started with those."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Trace withLangChain(Python and JS/TS),"LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Installation,"Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,1. Configure your environment,"PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:
export LANGCHAIN_CALLBACKS_BACKGROUND=true
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,2. Log a trace,"No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""chain.invoke({""question"": question, ""context"": context})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke({ question: question, context: context });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,3. View your trace,"By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Trace selectively,"The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({""question"": ""Am I being traced?"", ""context"": ""I'm not being traced""})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer();await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Statically,"As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Dynamically,"This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=""My Project"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=""My Project""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})// You can set the project name for a specific tracer instance:import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";const tracer = new LangChainTracer({ projectName: ""My Project"" });await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback""  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces.
This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.
For information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""You are a helpful AI.""),  (""user"", ""{input}"")])# The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({""tags"": [""model-tag""], ""metadata"": {""model-key"": ""model-value""}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""config-value""}})# Tags and metadata can also be passed at runtimechain.invoke({""input"": ""What is the meaning of life?""}, {""tags"": [""invoke-tag""], ""metadata"": {""invoke-key"": ""invoke-value""}})import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful AI.""],  [""user"", ""{input}""]])// The tag ""model-tag"" and metadata {""model-key"": ""model-value""} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [""model-tag""], metadata: { ""model-key"": ""model-value"" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({""tags"": [""config-tag""], ""metadata"": {""config-key"": ""top-level-value""}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: ""What is the meaning of life?""}, {tags: [""invoke-tag""], metadata: {""invoke-key"": ""invoke-value""}})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Customize run name,"When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.
This can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({""run_name"": ""MyCustomChain""})configured_chain.invoke({""input"": ""What is the meaning of life?""})# You can also configure the run name at invocation time, like belowchain.invoke({""input"": ""What is the meaning of life?""}, {""run_name"": ""MyCustomChain""})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: ""MyCustomChain"" });await configuredChain.invoke({ input: ""What is the meaning of life?"" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: ""What is the meaning of life?"" }, {runName: ""MyCustomChain""})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Access run (span) ID for LangChain invocations,"When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\n\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = ""Can you summarize this morning's meetings?""context = ""During this morning's meeting, we solved all world conflict.""with collect_runs() as cb:  result = chain.invoke({""question"": question, ""context"": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { RunCollectorCallbackHandler } from ""@langchain/core/tracers/run_collector"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""],  [""user"", ""Question: {question\n\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = ""Can you summarize this morning's meetings?""const context = ""During this morning's meeting, we solved all world conflict.""await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Ensure all traces are submitted before exiting,"In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to ""true"". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.
Below is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(""Hello, World!"")finally:    wait_for_all_tracers()import { ChatOpenAI } from ""@langchain/openai"";import { awaitAllCallbacks } from ""@langchain/core/callbacks/promises"";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(""Hello, World!"");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Trace without setting environment variables,"As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=""YOUR_API_KEY"",  # This can be retrieved from a secrets manager    api_url=""https://api.smith.langchain.com"",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=""test-no-env"")chain.invoke({""question"": ""Am I using a callback?"", ""context"": ""I'm using a callback""}, config={""callbacks"": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=""test-no-env""):    chain.invoke({""question"": ""Am I using a context manager?"", ""context"": ""I'm using a context manager""})import { LangChainTracer } from ""@langchain/core/tracers/tracer_langchain"";import { Client } from ""langsmith"";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: ""YOUR_API_KEY"",        apiUrl: ""https://api.smith.langchain.com"",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: ""test-no-env""});await chain.invoke(  {    question: ""Am I using a callback?"",    context: ""I'm using a callback"",  },  { callbacks: [tracer] });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Distributed tracing with LangChain (Python),"LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.
The principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[""test""] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({""test"": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({""test"": 1})"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Interoperability between LangChain (Python) and LangSmith SDK,"If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (""system"", ""You are a helpful assistant. Please respond to the user's request only based on the given context.""),    (""user"", ""Question: {question}\nContext: {context}"")])model = ChatOpenAI(model=""gpt-3.5-turbo"")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[""openai"", ""chat""],    metadata={""foo"": ""bar""})def invoke_runnnable(question, context):    result = chain.invoke({""question"": question, ""context"": context})    return ""The response is: "" + resultinvoke_runnnable(""Can you summarize this morning's meetings?"", ""During this morning's meeting, we solved all world conflict."") This will produce the following trace tree:
"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Tracing LangChain objects insidetraceable(JS only),"Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";import { getLangchainCallbacks } from ""langsmith/langchain"";const prompt = ChatPromptTemplate.fromMessages([  [    ""system"",    ""You are a helpful assistant. Please respond to the user's request only based on the given context."",  ],  [""user"", ""Question: {question}\nContext: {context}""],]);const model = new ChatOpenAI({ modelName: ""gpt-3.5-turbo"" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: ""main"" });"
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#access-run-span-id-for-langchain-invocations,Tracing LangChain child runs viatraceable/ RunTree API (JS only),"noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from ""langsmith/traceable"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: ""Child Run"",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from ""langsmith/run_trees"";import { RunnableLambda } from ""@langchain/core/runnables"";import { RunnableConfig } from ""@langchain/core/runnables"";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: ""Child Run"",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#create-a-dataset-from-a-csv-file,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/how_to_guides/tracing/log_retriever_trace,Log retriever traces,"noteNothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps. Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever.
LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken. Annotate the retriever step with run_type=""retriever"".Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:page_content: The text of the document.type: This should always be ""Document"".metadata: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace. The following code snippets show how to log a retrieval steps in Python and TypeScript. PythonTypeScriptfrom langsmith import traceabledef _convert_docs(results):    return [        {            ""page_content"": r,            ""type"": ""Document"",            ""metadata"": {""foo"": ""bar""}        }        for r in results    ]@traceable(run_type=""retriever"")def retrieve_docs(query):    # Foo retriever returning hardcoded dummy documents.    # In production, this could be a real vector datatabase or other document index.    contents = [""Document contents 1"", ""Document contents 2"", ""Document contents 3""]    return _convert_docs(contents)retrieve_docs(""User query"")import { traceable } from ""langsmith/traceable"";interface Document {  page_content: string;  type: string;  metadata: { foo: string };}function convertDocs(results: string[]): Document[] {  return results.map((r) => ({    page_content: r,    type: ""Document"",    metadata: { foo: ""bar"" }  }));}const retrieveDocs = traceable(  (query: string): Document[] => {    // Foo retriever returning hardcoded dummy documents.    // In production, this could be a real vector database or other document index.    const contents = [""Document contents 1"", ""Document contents 2"", ""Document contents 3""];    return convertDocs(contents);  },  { name: ""retrieveDocs"", run_type: ""retriever"" } // Configuration for traceable);await retrieveDocs(""User query""); The following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#filter-based-on-inputs-and-outputs,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/self_hosting/scripts/running_support_queries,Running Support Queries against Postgres,"This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining trace counts for multiple organizations in a single query). This command takes a postgres connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the pg_get_trace_counts_daily.sql input file in the support_queries directory."
https://docs.smith.langchain.com/self_hosting/scripts/running_support_queries,Prerequisites,"Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/PostgreSQL clienthttps://www.postgresql.org/download/PostgreSQL database connection:HostPortUsernameIf using the bundled version, this is postgresPasswordIf using the bundled version, this is postgresDatabase nameIf using the bundled version, this is postgresConnectivity to the PostgreSQL database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the postgresql service to your local machine.Run kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine."
https://docs.smith.langchain.com/self_hosting/scripts/running_support_queries,Running the query script,"Run the following command to run the desired query: sh run_support_query_pg.sh <postgres_url> --input path/to/query.sql For example, if you are using the bundled version with port-forwarding, the command might look like: sh run_support_query_pg.sh ""postgres://postgres:postgres@localhost:5432/postgres"" --input support_queries/pg_get_trace_counts_daily.sql which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag --output path/to/file.csv"
https://docs.smith.langchain.com/how_to_guides/monitoring/threads#view-threads,Set up threads,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Add metadata and tags to traces Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith."
https://docs.smith.langchain.com/how_to_guides/monitoring/threads#view-threads,Group traces into threads,"A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread. To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread. The key value is the unique identifier for that conversation.
The key name should be one of: session_idthread_idconversation_id. The value should be a UUID, such as f47ac10b-58cc-4372-a567-0e02b2c3d479."
https://docs.smith.langchain.com/how_to_guides/monitoring/threads#view-threads,View threads,"You can view threads by clicking on the Threads tad in any project details page. You will then see a list of all threads, sorted by the most recent activity. You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs. You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control,Set up access control,"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Read more about roles under admin concepts Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces LangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it.
Only users with the workspace:manage permission can manage access control settings for a workspace."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control,Create a role,"By default, LangSmith comes with a set of system roles: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) If these do not fit your access model, Organization Admins can create custom roles to suit your needs. To create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization. Click on the Create Role button to create a new role. You should see a form like the one below: Assign permissions for the different LangSmith resources that you want to control access to."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_access_control,Assign a role to a user,"Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page Each user will have a Role dropdown that you can use to assign a role to them. You can also invite new users with a given role."
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#workspace-roles,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#workspace-roles,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-pairwise-experiment,Run evals with the REST API,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.
However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-pairwise-experiment,Create a dataset,"Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries - API Example""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-pairwise-experiment,Run a single experiment,"First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { ""dataset"": dataset_id }resp = requests.get(    ""https://api.smith.langchain.com/api/v1/examples"",    params=params,    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[""OPENAI_API_KEY""] = ""sk-...""def run_completion_on_example(example, model_name, experiment_id):    """"""Run completions on a list of examples.""""""    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        """"""Function to post a new run to the API.""""""        data = {            ""id"": run_id.hex,            ""name"": name,            ""run_type"": run_type,            ""inputs"": inputs,            ""start_time"": datetime.utcnow().isoformat(),            ""reference_example_id"": example[""id""],            ""session_id"": experiment_id,        }        if parent_id:            data[""parent_run_id""] = parent_id.hex        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/runs"", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        """"""Function to patch a run with outputs.""""""        resp = requests.patch(            f""https://api.smith.langchain.com/api/v1/runs/{run_id}"",            json={                ""outputs"": outputs,                ""end_time"": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    text = example[""inputs""][""text""]    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, ""LLM Pipeline"", ""chain"", {""text"": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, ""OpenAI Call"", ""llm"", {""messages"": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {""label"": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (""gpt-3.5-turbo"", ""gpt-4o-mini"")experiment_ids = []for model_name in model_names:    resp = requests.post(        ""https://api.smith.langchain.com/api/v1/sessions"",        json={            ""start_time"": datetime.utcnow().isoformat(),            ""reference_dataset_id"": str(dataset_id),            ""description"": ""An optional description for the experiment"",            ""name"": f""Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}"",  # A name for the experiment            ""extra"": {                ""metadata"": {""foo"": ""bar""},  # Optional metadata            },        },        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )    experiment = resp.json()    experiment_ids.append(experiment[""id""])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[""id""])    # Issue a patch request to ""end"" the experiment by updating the end_time    requests.patch(        f""https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}"",        json={""end_time"": datetime.utcnow().isoformat()},        headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}    )"
https://docs.smith.langchain.com/how_to_guides/evaluation/run_evals_api_only#run-a-pairwise-experiment,Run a pairwise experiment,"Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.
For more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    ""https://api.smith.langchain.com/api/v1/datasets/comparative"",    json={        ""experiment_ids"": experiment_ids,        ""name"": ""Toxicity detection - API Example - Comparative - "" + str(uuid4())[0:8],        ""description"": ""An optional description for the comparative experiment"",        ""extra"": {            ""metadata"": {""foo"": ""bar""},  # Optional metadata        },        ""reference_dataset_id"": str(dataset_id),    },    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[""id""]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f""https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative"",    params={""id"": comparative_experiment_id},    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]})comparative_experiment = resp.json()[0]experiment_ids = [info[""id""] for info in comparative_experiment[""experiments_info""]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f""https://api.smith.langchain.com/api/v1/runs/query"",    headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]},    json={        ""session"": experiment_ids,        ""is_root"": True, # Only fetch root runs (spans) which contain the end outputs        ""select"": [""id"", ""reference_example_id"", ""outputs""],    }).json()runs = runs[""runs""]for run in runs:    example_id = run[""reference_example_id""]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f""Example ID: {example_id}"")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f""Run ID: {run['id']}"")        feedback = {            ""score"": 1 if i == 0 else 0,            ""run_id"": str(run[""id""]),            ""key"": ""ranked_preference"",            ""feedback_group_id"": str(feedback_group_id),            ""comparative_experiment_id"": comparative_experiment_id,        }        resp = requests.post(            ""https://api.smith.langchain.com/api/v1/feedback"",            json=feedback,            headers={""x-api-key"": os.environ[""LANGSMITH_API_KEY""]}        )        resp.raise_for_status()"
https://docs.smith.langchain.com/concepts/usage_and_billing/usage_limits,Usage Limits,noteThis page assumes that you have already read our guide on data retention. Please read that page before proceeding.  
https://docs.smith.langchain.com/concepts/usage_and_billing/usage_limits,How usage limits work,"LangSmith lets you configure usage limits on tracing. Note that these are usage limits, not spend limits, which
mean they let you limit the quantity of occurrances of some event rather than the total amount you will spend. LangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide: All traces limitExtended data retention traces limit These let you limit the number of total traces, and extended data retention traces respectively."
https://docs.smith.langchain.com/concepts/usage_and_billing/usage_limits,Properties of usage limiting,"Usage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there
may be a small period of time where additional traces are processed above the limit threshold before usage limiting
begins to apply."
https://docs.smith.langchain.com/concepts/usage_and_billing/usage_limits,Side effects of extended data retention traces limit,"The extended data retention traces limit has side effects. If the limit is already reached, any feature that could
cause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause
another extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can
no longer: match run rulesadd feedback to tracesadd runs to annotation queues Each of these features may cause an auto upgrade, so we shut them off when the limit is reached."
https://docs.smith.langchain.com/concepts/usage_and_billing/usage_limits,Updating usage limits,"Usage limits can be updated from the Settings page under Usage and Billing. Limit values are cached, so it
may take a minute or two before the new limits apply."
https://docs.smith.langchain.com/concepts/usage_and_billing/usage_limits,Related content,Tutorial on how to optimize spend
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators,Use LangChain off-the-shelf evaluators (Python only),"Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.
These evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Ankush"", ""Hello Ankush""),    (""Harrison"", ""Hello Harrison""),]dataset_name = ""Hello Set""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": input}, {""expected"": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators,Use question and answer (correctness) evaluators,"Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!
Three QA evaluators you can load are: ""qa"", ""context_qa"", ""cot_qa"". Based on our meta-evals, we recommend using ""cot_qa"", or Chain of Thought QA. Here is a trivial example that uses a ""cot_qa"" evaluator to evaluate a simple pipeline that prefixes the input with ""Hello"": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(""cot_qa"")client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators,Use criteria evaluators,"If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the ""criteria"" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The ""criteria"" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    ""criteria"",    config={        ""criteria"": {            ""says_hello"": ""Does the submission say hello?"",        }    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.
To specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {""creativity"": ""Is this submission creative, imaginative, or novel?""}criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={""criteria"": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent ""direction"" (i.e., higher is not necessarily better).
The direction of the score depends on the criteria being evaluated. For example, a score of 1 for ""helpfulness"" means that the prediction was deemed to be helpful by the model.
However, a score of 1 for ""maliciousness"" means that the prediction contains malicious content, which, of course, is ""bad""."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators,Use labeled criteria evaluators,"If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the ""labeled_criteria"" or ""labeled_score_string"" evaluators. The ""labeled_criteria"" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe ""labeled_score_string"" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    ""labeled_score_string"",    config={        ""criteria"": {            ""accuracy"": ""How accurate is this prediction compared to the reference on a scale of 1-10?""        },        ""normalize_by"": 10,    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators,Use string or embedding distance metrics,"To measure the similarity between a predicted string and a reference, you can use string distance metrics: The ""string_distance"" evaluator computes a normalized string edit distance between the prediction and referenceThe ""embedding_distance"" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    ""string_distance"",    config={""distance"": ""levenshtein"", ""normalize_score"": True})embedding_distance_evaluator = LangChainStringEvaluator(    ""embedding_distance"",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # ""embeddings"": HuggingFaceEmbeddings(model=""distilbert-base-uncased""),      # Can also choose ""euclidean"", ""chebyshev"", ""hamming"", and ""manhattan""        ""distance_metric"": ""cosine"",      })evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators,Use a custom LLM in off-the-shelf evaluators,"You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=""gpt-3.5-turbo"")cot_qa_evaluator = LangChainStringEvaluator(""cot_qa"", config={""llm"": eval_llm})evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-question-and-answer-correctness-evaluators,Handle multiple input or output fields,"LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.
These map the keys ""prediction"", ""reference"", and ""input"" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: ""greeting"" and ""foo"". We want to evaluate the ""greeting"" field against the ""expected"" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    },    prepare_data=lambda run, example: {        ""prediction"": run.outputs[""greeting""],        ""reference"": example.outputs[""expected""],        ""input"": example.inputs[""input""],    })client = Client()evaluate(    lambda input: {""greeting"": ""Hello "" + input[""input""], ""foo"": ""bar""},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#stream-outputs,Log custom LLM traces,"noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.
In order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#stream-outputs,Chat-style models,"For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg ""openai"", ""anthropic"", etc.ls_model_name: The name of the model, eg ""gpt-3.5-turbo"", ""claude-3-opus-20240307"", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ]}# Can also use one of:# output = {#     ""message"": {#         ""role"": ""assistant"",#         ""content"": ""Sure, what time would you like to book the table for?""#     }# }## output = {#     ""role"": ""assistant"",#     ""content"": ""Sure, what time would you like to book the table for?""# }## output = [""assistant"", ""Sure, what time would you like to book the table for?""]@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" }];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?""      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: ""assistant"",//     content: ""Sure, what time would you like to book the table for?""//   }// };//// const output = {//   role: ""assistant"",//   content: ""Sure, what time would you like to book the table for?""// };//// const output = [""assistant"", ""Sure, what time would you like to book the table for?""];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages }); The above code will log the following trace:"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#stream-outputs,Stream outputs,"For streaming, you can ""reduce"" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = """".join([chunk[""choices""][0][""message""][""content""] for chunk in chunks])    return {""choices"": [{""message"": {""content"": all_text, ""role"": ""assistant""}}]}@traceable(    run_type=""llm"",    reduce_fn=_reduce_chunks,    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def my_streaming_chat_model(messages: list):    for chunk in [""Hello, "" + messages[1][""content""]]:        yield {            ""choices"": [                {                    ""message"": {                        ""content"": chunk,                        ""role"": ""assistant"",                    }                }            ]        }list(    my_streaming_chat_model(        [            {""role"": ""system"", ""content"": ""You are a helpful assistant. Please greet the user.""},            {""role"": ""user"", ""content"": ""polly the parrot""},        ],    ))"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#stream-outputs,Manually provide token counts,"Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.
Many models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.
If token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.
If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""I'd like to book a table for two.""},]output = {    ""choices"": [        {            ""message"": {                ""role"": ""assistant"",                ""content"": ""Sure, what time would you like to book the table for?""            }        }    ],    ""usage_metadata"": {        ""input_tokens"": 27,        ""output_tokens"": 13,        ""total_tokens"": 40,    },}@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from ""langsmith/traceable"";const messages = [  { role: ""system"", content: ""You are a helpful assistant."" },  { role: ""user"", content: ""I'd like to book a table for two."" },];const output = {  choices: [    {      message: {        role: ""assistant"",        content: ""Sure, what time would you like to book the table for?"",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: ""llm"", name: ""chat_model"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await chatModel({ messages });"
https://docs.smith.langchain.com/how_to_guides/tracing/log_llm_trace#stream-outputs,Instruct-style models,"For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.
The same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=""llm"",    metadata={""ls_provider"": ""my_provider"", ""ls_model_name"": ""my_model""})def hello_llm(prompt: str):    return {        ""choices"": [            {""text"": ""Hello, "" + prompt}        ],        ""usage_metadata"": {            ""input_tokens"": 4,            ""output_tokens"": 5,            ""total_tokens"": 9,        },    }hello_llm(""polly the parrot\n"")import { traceable } from ""langsmith/traceable"";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: ""Hello, "" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: ""llm"", name: ""hello_llm"", metadata: { ls_provider: ""my_provider"", ls_model_name: ""my_model"" } });await helloLLM({ prompt: ""polly the parrot\n"" }); The above code will log the following trace:"
https://docs.smith.langchain.com/self_hosting/installation,Installation,This section contains guides for installing LangSmith on your own infrastructure. Kubernetes: Deploy LangSmith on Kubernetes.Docker: Deploy LangSmith using Docker.
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#bulk-update-examples,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/how_to_guides/datasets/share_dataset,Share or unshare a dataset publicly,"cautionSharing a dataset publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.
This link gives viewers access to all example rows, experiments, and associated runs and feedback on this dataset.
This feature is only available in the cloud-hosted version of LangSmith. To share a dataset publicly, simply click on the Share button in the upper right hand side of any dataset details page.
 This will open a dialog where you can copy the link to the dataset. Shared datasets will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the dataset, experiments and examples, but not edit any of this information. To ""unshare"" a dataset, either Click on Unshare by click on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog.
Navigate to your organization's list of publicly shared dataset, either by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.
"
https://docs.smith.langchain.com/concepts/tracing#metadata,Tracing,"This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"
https://docs.smith.langchain.com/concepts/tracing#metadata,Runs,"A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#metadata,Traces,"A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
"
https://docs.smith.langchain.com/concepts/tracing#metadata,Projects,"A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.
"
https://docs.smith.langchain.com/concepts/tracing#metadata,Feedback,"Feedback allows you to score an individual run based on certain criteria.
Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.
Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#metadata,Tags,"Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces
"
https://docs.smith.langchain.com/concepts/tracing#metadata,Metadata,"Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.
Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces
"
https://docs.smith.langchain.com/concepts/tracing#metadata,Data storage and retention,"For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/concepts/tracing#metadata,Deleting traces from LangSmith,"If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the ""Delete"" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."
https://docs.smith.langchain.com/self_hosting/faq,"I can't create API keys or manage users in the UI, what's wrong?","You have likely deployed LangSmith without setting up SSO. LangSmith requires SSO to manage users and API keys. You can find more information on setting up SSO in the configuration section. How does load balancing/ingress work? You will need to expose the frontend container/service to your applications/users. This will handle routing to all downstream services.You will need to terminate SSL at the ingress level. We recommend using a managed service like AWS ALB, GCP Load Balancer, or Nginx. How can we authenticate to the application? Currently, our self-hosted solution supports SSO with OAuth2.0 and OIDC as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production. You can find more information on setting up SSO in the configuration section. Can I use external storage services? You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services. Check out the configuration section for more information. Does my application need egress to function properly? Our deployment only needs egress for a few things: Fetching images (If mirroring your images, this may not be needed)Talking to any LLMsTalking to any external storage services you may have configuredFetching OAuth information Your VPC can set up rules to limit any other access.
Note: We require the X-Organization-Id and X-Tenant-Id headers to be allowed to be passed through to the backend service. These are used to determine which organization and workspace (previously called ""tenant"") the request is for. Resource requirements for the application? In kubernetes, we recommend a minimum helm configuration which can be found in here. For docker, we recommend a minimum of 16GB of RAM and 4 CPUs.For Postgres, we recommend a minimum of 8GB of RAM and 2 CPUs.For Redis, we recommend 4GB of RAM and 2 CPUs.For Clickhouse, we recommend 32GB of RAM and 8 CPUs."
https://docs.smith.langchain.com/self_hosting/installation/kubernetes,Self-hosting LangSmith on Kubernetes,"Enterprise License RequiredSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. This guide will walk you through the process of deploying LangSmith to a Kubernetes cluster. We will use Helm to install LangSmith and its dependencies. We've successfully tested LangSmith on the following Kubernetes distributions: Google Kubernetes Engine (GKE)Amazon Elastic Kubernetes Service (EKS)Azure Kubernetes Service (AKS)OpenShiftMinikube and Kind (for development purposes) To review all configuration options, look at the values.yaml for the LangSmith helm chart."
https://docs.smith.langchain.com/self_hosting/installation/kubernetes,Prerequisites,"Ensure you have the following tools/items ready. Some items are marked optional: A working Kubernetes cluster that you can access via kubectl. Your cluster should have the following minimum requirements:Recommended: At least 4 vCPUs, 16GB Memory availableYou may need to tune resource requests/limits for all of our different services based off of organization size/usageValid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running:kubectl get storageclassThe output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:  NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE  gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   true                   161dnoteWe highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.Refer to the Kubernetes documentation for more information on storage classes.Helmbrew install helmLangSmith License KeyYou can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.Api Key SaltThis is a secret key that you can generate. It should be a random string of characters.You can generate this using the following command:openssl rand -base64 32ConfigurationThere are several configuration options that you can set in the langsmith_config.yaml file. You can find more information on the available configuration options in the Configuration section."
https://docs.smith.langchain.com/self_hosting/installation/kubernetes,Configure your Helm Charts:,Create a new file called langsmith_config.yaml. This should have a similar structure to the values.yaml file in the LangSmith Helm Chart repository. Only include the values you want to override to avoid having to update the file every time the chart is updated.Set the appropriate values in the langsmith_config.yaml file. You can find the available configuration options in the configuration section. You can also see some example configurations in the examples directory of the Helm Chart repository here: LangSmith helm chart examples.
https://docs.smith.langchain.com/self_hosting/installation/kubernetes,Deploying to Kubernetes:,"Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)Run kubectl get podsOutput should look something like:kubectl get pods                                                                                                                                                                      langsmith-eks-2vauP7wf 21:07:46No resources found in default namespace.Ensure you have the Langchain Helm repo added. (skip this step if you are using local charts)helm repo add langchain https://langchain-ai.github.io/helm/""langchain"" has been added to your repositoriesRun helm install langsmith langchain/langsmith --values langsmith_config.yaml --namespace <your-namespace> --version <version>Output should look something like:NAME: langsmithLAST DEPLOYED: Fri Sep 17 21:08:47 2021NAMESPACE: langsmithSTATUS: deployedREVISION: 1TEST SUITE: NoneRun kubectl get pods
Output should now look something like:langsmith-backend-6ff46c99c4-wz22d       1/1     Running   0          3h2mlangsmith-frontend-6bbb94c5df-8xrlr      1/1     Running   0          3h2mlangsmith-hub-backend-5cc68c888c-vppjj   1/1     Running   0          3h2mlangsmith-playground-6d95fd8dc6-x2d9b    1/1     Running   0          3h2mlangsmith-postgres-0                     1/1     Running   0          9hlangsmith-queue-5898b9d566-tv6q8         1/1     Running   0          3h2mlangsmith-redis-0                        1/1     Running   0          9h"
https://docs.smith.langchain.com/self_hosting/installation/kubernetes,Validate your deployment:,"Run kubectl get servicesOutput should look something like:NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGElangsmith-backend       ClusterIP      172.20.140.77    <none>                                                                    1984/TCP       35hlangsmith-frontend      LoadBalancer   172.20.253.251   <external ip>                                                             80:31591/TCP   35hlangsmith-hub-backend   ClusterIP      172.20.112.234   <none>                                                                    1985/TCP       35hlangsmith-playground    ClusterIP      172.20.153.194   <none>                                                                    3001/TCP       9hlangsmith-postgres      ClusterIP      172.20.244.82    <none>                                                                    5432/TCP       35hlangsmith-redis         ClusterIP      172.20.81.217    <none>                                                                    6379/TCP       35hCurl the external ip of the langsmith-frontend service:curl <external ip>/api/tenants[{""id"":""00000000-0000-0000-0000-000000000000"",""has_waitlist_access"":true,""created_at"":""2023-09-13T18:25:10.488407"",""display_name"":""Personal"",""config"":{""is_personal"":true,""max_identities"":1},""tenant_handle"":""default""}]%Visit the external ip for the langsmith-frontend service on your browserThe LangSmith UI should be visible/operational"
https://docs.smith.langchain.com/self_hosting/installation/kubernetes,Using LangSmith,"Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the self-hosted usage guide."
https://docs.smith.langchain.com/self_hosting/scripts,Scripts for administering LangSmith,This section contains guides for performing common administrative tasks that are not currently available via the LangSmith UI. You can find these scripts in the Helm chart repository. Delete an organization: Delete an organization in LangSmith.Delete a workspace: Delete a workspace in LangSmith.Delete a trace: Delete a trace in LangSmith.Generate Clickhouse statistics: Generate Clickhouse statistics.Generate LangSmith Query statistics: Generate LangSmith Query statistics from Clickhouse.Running Support Queries: Running other support queries supplied by the LangChain support team.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,LangSmith Release Notes,noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,"Week of June 17, 2024 - LangSmith v0.6","LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,New Features,Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More...
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Performance and Reliability Changes,Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Infrastructure changes,"[Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Admin changes,Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Deprecation notices,With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,"Week of May 13, 2024 - LangSmith v0.5","LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Breaking changes,We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,New Features,Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Performance and Reliability Changes,"Split ingest, session deletion, and automation jobs to execute within separate resource pools."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Infrastructure changes,"As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Admin changes,Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Deprecation notices,With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,"Week of March 25, 2024 - LangSmith v0.4","LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Breaking changes,"This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Performance and Reliability Changes,"Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Infrastructure changes,"Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Admin changes,Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Deprecation notices,With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,"Week of Februrary 21, 2024 - LangSmith v0.3","LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Breaking changes,"This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Performance and Reliability Changes,"Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Admin changes,None
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Deprecation notices,With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,"Week of January 29, 2024 - LangSmith v0.2","LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Requirements,This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Breaking changes,"The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {""user_id"": ..., ""user_name"":...,}) and then search using has(metadata, '{""user_name"": ...}')"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Performance and Reliability Changes,"Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Infrastructure Changes,"Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Admin changes,Increased the maximum number of users per organization from 5 to 100 for new organizations.
https://docs.smith.langchain.com/self_hosting/release_notes#week-of-may-13-2024---langsmith-v05,Deprecation notices,With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes.
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#create-a-workspace,Set up a workspace,"infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.
Workspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#create-a-workspace,Create a workspace,"To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.
Once your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.
Please see the pricing page for more information."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#create-a-workspace,Manage users,"infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.
Users may also be invited directly to one or more workspaces when they are invited to an organization."
https://docs.smith.langchain.com/how_to_guides/setup/set_up_workspace#create-a-workspace,Configure workspace settings,"Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."
https://docs.smith.langchain.com/concepts/admin#service-keys,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#service-keys,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#service-keys,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#service-keys,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#service-keys,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#service-keys,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#service-keys,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#service-keys,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#service-keys,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/concepts/tracing#projects,Tracing,"This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"
https://docs.smith.langchain.com/concepts/tracing#projects,Runs,"A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#projects,Traces,"A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
"
https://docs.smith.langchain.com/concepts/tracing#projects,Projects,"A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.
"
https://docs.smith.langchain.com/concepts/tracing#projects,Feedback,"Feedback allows you to score an individual run based on certain criteria.
Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.
Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#projects,Tags,"Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces
"
https://docs.smith.langchain.com/concepts/tracing#projects,Metadata,"Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.
Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces
"
https://docs.smith.langchain.com/concepts/tracing#projects,Data storage and retention,"For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/concepts/tracing#projects,Deleting traces from LangSmith,"If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the ""Delete"" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."
https://docs.smith.langchain.com/tutorials/Developers/rag,RAG Evaluations,We will walk through the evaluation workflow for RAG (retrieval augmented generation).
https://docs.smith.langchain.com/tutorials/Developers/rag,Overview,We will discuss each piece of the workflow below.
https://docs.smith.langchain.com/tutorials/Developers/rag,Dataset,"Here is a dataset of LCEL (LangChain Expression Language) related questions that we will use. This dataset was created using csv upload in the LangSmith UI: https://smith.langchain.com/public/730d833b-74da-43e2-a614-4e2ca2502606/d Here, we ensure that API keys for OpenAI as well as LangSmith are set. import getpassimport osdef _set_env(var: str):    if not os.environ.get(var):        os.environ[var] = getpass.getpass(f""{var}: "")_set_env(""OPENAI_API_KEY"")os.environ[""LANGCHAIN_TRACING_V2""] = ""true""os.environ[""LANGCHAIN_ENDPOINT""] = ""https://api.smith.langchain.com"" # Update appropriately for self-hosted installations or the EU region_set_env(""LANGCHAIN_API_KEY"") ### Dataset namedataset_name = ""LCEL-QA"""
https://docs.smith.langchain.com/tutorials/Developers/rag,Task,"Here is a chain that will perform RAG on LCEL (LangChain Expression Language) docs. We will be using LangChain strictly for creating the retriever and retrieving the relevant documents. The overall pipeline does not use LangChain; LangSmith works regardless of whether or not your pipeline is built with LangChain. Here, we return the retrieved documents as part of the final answer. However, below we will show that this is not required (using evaluation of intermediate steps). See our RAG-From-Scratch repo and tutorial video series for more on this. ### INDEXfrom bs4 import BeautifulSoup as Soupfrom langchain_community.vectorstores import Chromafrom langchain_openai import OpenAIEmbeddingsfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoaderfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load docsurl = ""https://python.langchain.com/v0.1/docs/expression_language/""loader = RecursiveUrlLoader(    url=url, max_depth=20, extractor=lambda x: Soup(x, ""html.parser"").text)docs = loader.load()# Split into chunkstext_splitter = RecursiveCharacterTextSplitter(chunk_size=4500, chunk_overlap=200)splits = text_splitter.split_documents(docs)# Embed and store in Chromavectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())# Indexretriever = vectorstore.as_retriever() ### RAG botimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclass RagBot:    def __init__(self, retriever, model: str = ""gpt-4-0125-preview""):        self._retriever = retriever        # Wrapping the client instruments the LLM        self._client = wrap_openai(openai.Client())        self._model = model    @traceable()    def retrieve_docs(self, question):        return self._retriever.invoke(question)    @traceable()    def invoke_llm(self, question, docs):        response = self._client.chat.completions.create(            model=self._model,            messages=[                {                    ""role"": ""system"",                    ""content"": ""You are a helpful AI code assistant with expertise in LCEL.""                    "" Use the following docs to produce a concise code solution to the user question.\n\n""                    f""## Docs\n\n{docs}"",                },                {""role"": ""user"", ""content"": question},            ],        )        # Evaluators will expect ""answer"" and ""contexts""        return {            ""answer"": response.choices[0].message.content,            ""contexts"": [str(doc) for doc in docs],        }    @traceable()    def get_answer(self, question: str):        docs = self.retrieve_docs(question)        return self.invoke_llm(question, docs)rag_bot = RagBot(retriever) response = rag_bot.get_answer(""How to build a RAG chain in LCEL?"")response[""answer""][:150] Define a function that will: Take a dataset exampleExtract the relevant key (e.g., question) from the examplePass it to the RAG chainReturn the relevant output values from the RAG chain def predict_rag_answer(example: dict):    """"""Use this for answer evaluation""""""    response = rag_bot.get_answer(example[""input_question""])    return {""answer"": response[""answer""]}def predict_rag_answer_with_context(example: dict):    """"""Use this for evaluation of retrieved documents and hallucinations""""""    response = rag_bot.get_answer(example[""input_question""])    return {""answer"": response[""answer""], ""contexts"": response[""contexts""]}"
https://docs.smith.langchain.com/tutorials/Developers/rag,Evaluator,"There are at least 4 types of RAG eval that users are typically interested in. Response vs reference answer Goal: Measure ""how similar/correct is the RAG chain answer, relative to a ground-truth answer""Mode: Uses ground truth (reference) answer supplied through a datasetJudge: Use LLM-as-judge to assess answer correctness. Response vs input Goal: Measure ""how well does the generated response address the initial user input""Mode: Reference-free, because it will compare the answer to the input questionJudge: Use LLM-as-judge to assess answer relevance, helpfulness, etc. Response vs retrieved docs Goal: Measure ""to what extent does the generated response agree with the retrieved context""Mode: Reference-free, because it will compare the answer to the retrieved contextJudge: Use LLM-as-judge to assess faithfulness, hallucinations, etc. Retrieved docs vs input Goal: Measure ""how good are my retrieved results for this query""Mode: Reference-free, because it will compare the question to the retrieved contextJudge: Use LLM-as-judge to assess relevance"
https://docs.smith.langchain.com/tutorials/Developers/rag,Response vs reference answer,"Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/lTfhw_9cJqc?feature=shared Here is our evaluator function: run is the invocation of predict_rag_answer, which has key answerexample is from our eval set, which has keys input_question and output_answerWe extract these values and pass them into our grader from langchain import hubfrom langchain_openai import ChatOpenAI# Grade promptgrade_prompt_answer_accuracy = prompt = hub.pull(""langchain-ai/rag-answer-vs-reference"")def answer_evaluator(run, example) -> dict:    """"""    A simple evaluator for RAG answer accuracy    """"""    # Get question, ground truth answer, RAG chain answer    input_question = example.inputs[""input_question""]    reference = example.outputs[""output_answer""]    prediction = run.outputs[""answer""]    # LLM grader    llm = ChatOpenAI(model=""gpt-4-turbo"", temperature=0)    # Structured prompt    answer_grader = grade_prompt_answer_accuracy | llm    # Run evaluator    score = answer_grader.invoke({""question"": input_question,                                  ""correct_answer"": reference,                                  ""student_answer"": prediction})    score = score[""Score""]    return {""key"": ""answer_v_reference_score"", ""score"": score} Now, we kick off evaluation: predict_rag_answer: Takes an example from our eval set, extracts the question, passes to our RAG chainanswer_evaluator: Passes RAG chain answer, question, and ground truth answer to an evaluator from langsmith.evaluation import evaluateexperiment_results = evaluate(    predict_rag_answer,    data=dataset_name,    evaluators=[answer_evaluator],    experiment_prefix=""rag-answer-v-reference"",    metadata={""version"": ""LCEL context, gpt-4-0125-preview""},)"
https://docs.smith.langchain.com/tutorials/Developers/rag,Response vs input,"Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-answer-helpfulness The information flow is similar to above, but we simply look at the run answer versus the example question. # Grade promptgrade_prompt_answer_helpfulness = prompt = hub.pull(""langchain-ai/rag-answer-helpfulness"")def answer_helpfulness_evaluator(run, example) -> dict:    """"""    A simple evaluator for RAG answer helpfulness    """"""    # Get question, ground truth answer, RAG chain answer    input_question = example.inputs[""input_question""]    prediction = run.outputs[""answer""]    # LLM grader    llm = ChatOpenAI(model=""gpt-4-turbo"", temperature=0)    # Structured prompt    answer_grader = grade_prompt_answer_helpfulness | llm    # Run evaluator    score = answer_grader.invoke({""question"": input_question,                                  ""student_answer"": prediction})    score = score[""Score""]    return {""key"": ""answer_helpfulness_score"", ""score"": score} experiment_results = evaluate(    predict_rag_answer,    data=dataset_name,    evaluators=[answer_helpfulness_evaluator],    experiment_prefix=""rag-answer-helpfulness"",    metadata={""version"": ""LCEL context, gpt-4-0125-preview""},)"
https://docs.smith.langchain.com/tutorials/Developers/rag,Response vs retrieved docs,"Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/IlNglM9bKLw?feature=shared # Promptgrade_prompt_hallucinations = prompt = hub.pull(""langchain-ai/rag-answer-hallucination"")def answer_hallucination_evaluator(run, example) -> dict:    """"""    A simple evaluator for generation hallucination    """"""    # RAG inputs    input_question = example.inputs[""input_question""]    contexts = run.outputs[""contexts""]    # RAG answer    prediction = run.outputs[""answer""]    # LLM grader    llm = ChatOpenAI(model=""gpt-4-turbo"", temperature=0)    # Structured prompt    answer_grader = grade_prompt_hallucinations | llm    # Get score    score = answer_grader.invoke({""documents"": contexts,                                  ""student_answer"": prediction})    score = score[""Score""]    return {""key"": ""answer_hallucination"", ""score"": score} experiment_results = evaluate(    predict_rag_answer_with_context,    data=dataset_name,    evaluators=[answer_hallucination_evaluator],    experiment_prefix=""rag-answer-hallucination"",    metadata={""version"": ""LCEL context, gpt-4-0125-preview""},)"
https://docs.smith.langchain.com/tutorials/Developers/rag,Retrieved docs vs input,"Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-document-relevance Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/Fr_7HtHjcf0?feature=shared # Grade promptgrade_prompt_doc_relevance = hub.pull(""langchain-ai/rag-document-relevance"")def docs_relevance_evaluator(run, example) -> dict:    """"""    A simple evaluator for document relevance    """"""    # RAG inputs    input_question = example.inputs[""input_question""]    contexts = run.outputs[""contexts""]    # LLM grader    llm = ChatOpenAI(model=""gpt-4-turbo"", temperature=0)    # Structured prompt    answer_grader = grade_prompt_doc_relevance | llm    # Get score    score = answer_grader.invoke({""question"":input_question,                                  ""documents"":contexts})    score = score[""Score""]    return {""key"": ""document_relevance"", ""score"": score} experiment_results = evaluate(    predict_rag_answer_with_context,    data=dataset_name,    evaluators=[docs_relevance_evaluator],    experiment_prefix=""rag-doc-relevance"",    metadata={""version"": ""LCEL context, gpt-4-0125-preview""},)"
https://docs.smith.langchain.com/tutorials/Developers/rag,Evaluating intermediate steps,"Above, we returned the retrieved documents as part of the final answer. However, we will show that this is not required. We can isolate them as intermediate chain steps. See detail on isolating intermediate chain steps here. Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/yx3JMAaNggQ?feature=shared from langsmith.schemas import Example, Runfrom langsmith.evaluation import evaluatedef document_relevance_grader(root_run: Run, example: Example) -> dict:    """"""    A simple evaluator that checks to see if retrieved documents are relevant to the question    """"""    # Get specific steps in our RAG pipeline, which are noted with @traceable decorator    rag_pipeline_run = next(        run for run in root_run.child_runs if run.name == ""get_answer""    )    retrieve_run = next(        run for run in rag_pipeline_run.child_runs if run.name == ""retrieve_docs""    )    contexts = ""\n\n"".join(doc.page_content for doc in retrieve_run.outputs[""output""])    input_question = example.inputs[""input_question""]    # LLM grader    llm = ChatOpenAI(model=""gpt-4-turbo"", temperature=0)    # Structured prompt    answer_grader = grade_prompt_doc_relevance | llm    # Get score    score = answer_grader.invoke({""question"":input_question,                                  ""documents"":contexts})    score = score[""Score""]    return {""key"": ""document_relevance"", ""score"": score}def answer_hallucination_grader(root_run: Run, example: Example) -> dict:    """"""    A simple evaluator that checks to see the answer is grounded in the documents    """"""    # RAG input    rag_pipeline_run = next(        run for run in root_run.child_runs if run.name == ""get_answer""    )    retrieve_run = next(        run for run in rag_pipeline_run.child_runs if run.name == ""retrieve_docs""    )    contexts = ""\n\n"".join(doc.page_content for doc in retrieve_run.outputs[""output""])    # RAG output    prediction = rag_pipeline_run.outputs[""answer""]    # LLM grader    llm = ChatOpenAI(model=""gpt-4-turbo"", temperature=0)    # Structured prompt    answer_grader = grade_prompt_hallucinations | llm    # Get score    score = answer_grader.invoke({""documents"": contexts,                                  ""student_answer"": prediction})    score = score[""Score""]    return {""key"": ""answer_hallucination"", ""score"": score}experiment_results = evaluate(    predict_rag_answer,    data=dataset_name,    evaluators=[document_relevance_grader, answer_hallucination_grader],    metadata={""version"": ""LCEL context, gpt-4-0125-preview""},)"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#configure_environment_variables,Manage prompts programmatically,"You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.
All functionality going forward will live in the langsmith package."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#configure_environment_variables,Install packages,PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#configure_environment_variables,Configure environment variables,"If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=""lsv2_..."" TerminologyWhat we refer to as ""prompts"" used to be called ""repos"", so any references to ""repo"" in the code are referring to a prompt."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#configure_environment_variables,Push a prompt,"To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = client.push_prompt(""joke-generator"", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = prompts.push(""joke-generator"", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");url = prompts.push(""joke-generator"", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.
This is useful for storing the model configuration you want to use with this prompt.
The provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelclient.push_prompt(""joke-generator-with-model"", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelurl = prompts.push(""joke-generator-with-model"", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { ChatOpenAI } from ""langchain-openai"";const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");const chain = prompt.pipe(model);prompts.push(""joke-generator-with-model"", chain);"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#configure_environment_variables,Pull a prompt,"To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { ChatOpenAI } from ""langchain-openai"";const prompt = prompts.pull(""joke-generator"");const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const chain = prompt.pipe(model);chain.invoke({""topic"": ""cats""}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.
Just specify include_model when pulling the prompt.
If the stored prompt includes a model, it will be returned as a RunnableSequence.
Make sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})from langchain import hub as promptschain = prompts.pull(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { Runnable } from ""@langchain/core/runnables"";const chain = prompts.pull<Runnable>(""joke-generator-with-model"", { includeModel: true });chain.invoke({""topic"": ""cats""}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#configure_environment_variables,Use a prompt without LangChain,"If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.
These convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(""joke-generator"")prompt_value = prompt.invoke({""topic"": ""cats""})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#configure_environment_variables,"List, delete, and like prompts","You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.
See the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include ""joke""prompts = client.list_prompts(query=""joke"", is_public=False)# Delete a promptclient.delete_prompt(""joke-generator"")# Like a promptclient.like_prompt(""efriis/my-first-prompt"")# Unlike a promptclient.unlike_prompt(""efriis/my-first-prompt"")// List all prompts in my workspaceimport Client from ""langsmith"";const client = new Client({ apiKey: ""lsv2_..."" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include ""joke""const private_joke_prompts = client.listPrompts({ query: ""joke"", isPublic: false});// Delete a promptclient.deletePrompt(""joke-generator"");// Like a promptclient.likePrompt(""efriis/my-first-prompt"");// Unlike a promptclient.unlikePrompt(""efriis/my-first-prompt""); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.
However, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."
https://docs.smith.langchain.com/self_hosting/installation/docker,Self-hosting LangSmith with Docker,"Enterprise License RequiredSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. This guide provides instructions for installing and setting up your environment to run LangSmith locally using Docker. You can do this either by using the LangSmith SDK or by using Docker Compose directly."
https://docs.smith.langchain.com/self_hosting/installation/docker,Prerequisites,"Ensure Docker is installed and running on your system. You can verify this by running:docker infoIf you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.Recommended: At least 4 vCPUs, 16GB Memory available on your machine.You may need to tune resource requests/limits for all of our different services based off of organization size/usageDisk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.LangSmith License KeyYou can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.Api Key SaltThis is a secret key that you can generate. It should be a random string of characters.You can generate this using the following command:openssl rand -base64 32ConfigurationThere are several configuration options that you can set in the .env file. You can find more information on the available configuration options in the Configuration section."
https://docs.smith.langchain.com/self_hosting/installation/docker,Running via Docker Compose,"The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. In production, we highly recommend using Kubernetes."
https://docs.smith.langchain.com/self_hosting/installation/docker,1. Fetch the LangSmithdocker-compose.ymlfile,You can find the docker-compose.yml file and related files in the LangSmith SDK repository here: LangSmith Docker Compose File Copy the docker-compose.yml file and all files in that directory from the LangSmith SDK to your project directory. Ensure that you copy the users.xml file as well.
https://docs.smith.langchain.com/self_hosting/installation/docker,2. Configure environment variables,"Copy the .env.example file from the LangSmith SDK to your project directory and rename it to .env. Then, set the following environment variables in the .env file:Configure the appropriate values in the .env file. You can find the available configuration options in the Configuration section. You can also set these environment variables in the docker-compose.yml file directly or export them in your terminal. We recommend setting them in the .env file."
https://docs.smith.langchain.com/self_hosting/installation/docker,2. Start server,Start the LangSmith application by executing the following command in your terminal: docker-compose up You can also run the server in the background by running: docker-compose up -d
https://docs.smith.langchain.com/self_hosting/installation/docker,Validate your deployment:,"Curl the exposed port of the cli-langchain-frontend-1 container:curl localhost:1980/info{""version"":""0.5.7"",""license_expiration_time"":""2033-05-20T20:08:06"",""batch_ingest_config"":{""scale_up_qsize_trigger"":1000,""scale_up_nthreads_limit"":16,""scale_down_nempty_trigger"":4,""size_limit"":100,""size_limit_bytes"":20971520}}Visit the exposed port of the cli-langchain-frontend-1 container on your browserThe Langsmith UI should be visible/operational at http://localhost:1980"
https://docs.smith.langchain.com/self_hosting/installation/docker,Checking the logs,"If, at any point, you want to check if the server is running and see the logs, run docker-compose logs"
https://docs.smith.langchain.com/self_hosting/installation/docker,Stopping the server,docker-compose down
https://docs.smith.langchain.com/how_to_guides/tracing/add_metadata_tags,Add metadata and tags to traces,"LangSmith supports sending arbitrary metadata and tags along with traces. Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace. Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID.
For more information on tags and metadata, see the Concepts page. For information on how to query traces and runs by metadata and tags, see the Filter traces in the application page. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},    {""role"": ""user"", ""content"": ""Hello!""}]# Use the @traceable decorator with tags and metadata# Ensure that the LANGCHAIN_TRACING_V2 environment variables are set for @traceable to work@traceable(    run_type=""llm"",    name=""OpenAI Call Decorator"",    tags=[""my-tag""],    metadata={""my-key"": ""my-value""})def call_openai(    messages: list[dict], model: str = ""gpt-3.5-turbo"") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.contentcall_openai(    messages,    # You can also provide tags and metadata at invocation time    # via the langsmith_extra parameter    langsmith_extra={""tags"": [""my-other-tag""], ""metadata"": {""my-other-key"": ""my-value""}})# Alternatively, you can create a RunTree object with tags and metadatart = RunTree(    run_type=""llm"",    name=""OpenAI Call RunTree"",    inputs={""messages"": messages},    tags=[""my-tag""],    extra={""metadata"": {""my-key"": ""my-value""}})chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";import { RunTree} from ""langsmith"";const client = new OpenAI();const messages = [    {role: ""system"", content: ""You are a helpful assistant.""},    {role: ""user"", content: ""Hello!""}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: ""llm"",    name: ""OpenAI Call Traceable"",    tags: [""my-tag""],    metadata: { ""my-key"": ""my-value"" }});// Call the traceable functionawait traceableCallOpenAI(messages, ""gpt-3.5-turbo"");// Create a RunTree objectconst rt = new RunTree({  run_type: ""llm"",  name: ""OpenAI Call RunTree"",  inputs: { messages },  tags: [""my-tag""],  extra: { metadata: { ""my-key"": ""my-value"" } },});const chatCompletion = await client.chat.completions.create({  model: ""gpt-3.5-turbo"",  messages: messages,});// End and submit the runrt.end(chatCompletion);await rt.postRun();"
https://docs.smith.langchain.com/how_to_guides/human_feedback/set_up_feedback_criteria,Set up feedback criteria,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and feedbackReference guide on feedback data format Feedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback. To set up a new feedback criteria, follow this link to view all existing tags for your workspace, then click New Tag."
https://docs.smith.langchain.com/how_to_guides/human_feedback/set_up_feedback_criteria,Continuous feedback,"For continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores."
https://docs.smith.langchain.com/how_to_guides/human_feedback/set_up_feedback_criteria,Categorical feedback,"For categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score.
Both the category label and the score will be logged as feedback in value and score fields, respectively."
https://docs.smith.langchain.com/how_to_guides/playground/save_model_configuration,Save settings configuration,"Within the settings of the LangSmith playground, you can save your model configuration for later use.
This helps you quickly apply your frequently-used settings without having to re-enter the details each time. To save the current playground configuration, click on the Save button in the top right corner of settings.
You can name the configuration and easily access it later."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#wrap-the-openai-client,Annotate code for tracing,"There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#wrap-the-openai-client,Use@traceable/traceable,"LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.
from langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            ""role"": ""system"",            ""content"": ""You are a helpful assistant."",        },        {            ""role"": ""user"",            ""content"": f""What's a good name for a store that sells {subject}?""        }    ]@traceable(run_type=""llm"")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(""colorful socks"")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.
Note that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.
import { traceable } from ""langsmith/traceable"";import OpenAI from ""openai"";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: ""system"" as const,        content: ""You are a helpful assistant."",      },      {        role: ""user"" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: ""formatPrompt"" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: ""gpt-3.5-turbo"",            messages: messages,            temperature: 0,        });    },    { run_type: ""llm"", name: ""invokeLLM"" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: ""parseOutput"" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(""colorful socks"");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: ""runPipeline"" });await runPipeline();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#wrap-the-openai-client,Wrap the OpenAI client,"The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!
The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.
To log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""@traceable(name=""Chat Pipeline"")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(""Can you summarize this morning's meetings?"")import OpenAI from ""openai"";import { traceable } from ""langsmith/traceable"";import { wrapOpenAI } from ""langsmith/wrappers"";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return ""During this morning's meeting, we solved all world conflict."";}, { name: ""Retrieve Context"", run_type: ""tool"" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: ""system"",            content:                ""You are a helpful assistant. Please respond to the user's request only based on the given context."",        },        { role: ""user"", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: ""gpt-3.5-turbo"",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: ""Chat Pipeline"" });await chatPipeline(""Can you summarize this morning's meetings?"");"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#wrap-the-openai-client,Use theRunTreeAPI,"Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually
create runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not
necessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = ""Can you summarize this morning's meetings?""# Create a top-level runpipeline = RunTree(    name=""Chat Pipeline"",    run_type=""chain"",    inputs={""question"": question})# This can be retrieved in a retrieval stepcontext = ""During this morning's meeting, we solved all world conflict.""messages = [    { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}]# Create a child runchild_llm_run = pipeline.create_child(    name=""OpenAI Call"",    run_type=""llm"",    inputs={""messages"": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=""gpt-3.5-turbo"", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={""answer"": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from ""openai"";import { RunTree } from ""langsmith"";// This can be a user input to your appconst question = ""Can you summarize this morning's meetings?"";const pipeline = new RunTree({    name: ""Chat Pipeline"",    run_type: ""chain"",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = ""During this morning's meeting, we solved all world conflict."";const messages = [    { role: ""system"", content: ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },    { role: ""user"", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: ""OpenAI Call"",    run_type: ""llm"",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: ""gpt-3.5-turbo"",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"
https://docs.smith.langchain.com/how_to_guides/tracing/annotate_code#wrap-the-openai-client,Use thetracecontext manager (Python only),"In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=""tool"", name=""Retrieve Context"")def my_tool(question: str) -> str:    return ""During this morning's meeting, we solved all world conflict.""def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { ""role"": ""system"", ""content"": ""You are a helpful assistant. Please respond to the user's request only based on the given context."" },        { ""role"": ""user"", ""content"": f""Question: {question}\nContext: {context}""}    ]    chat_completion = client.chat.completions.create(        model=""gpt-3.5-turbo"", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {""input"": ""Can you summarize this morning's meetings?""}with trace(""Chat Pipeline"", ""chain"", project_name=""my_test"", inputs=app_inputs) as rt:    output = chat_pipeline(""Can you summarize this morning's meetings?"")    rt.end(outputs={""output"": output})"
https://docs.smith.langchain.com/how_to_guides/human_feedback/annotate_traces_inline,Annotate traces and runs inline,"LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue.
You can annotate a trace either inline or by sending the trace to an annotation queue, which allows you closely inspect and log feedbacks to runs one at a time.
Feedback tags are associated with your workspace. noteYou can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.
This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline. To annotate a trace inline, click on the Annotate in the upper right corner of trace view for any particular run that is part of the trace. This will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow this guide to set up feedback tags for your workspace.
You can also set up new feedback criteria from within the pane itself. You can use the labeled keyboard shortcuts to streamline the annotation process."
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets,Version datasets,"In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets,Create a new version of a dataset,"Any time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved. By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the ""Examples"" tab, you can see the state of the dataset at that point in time. Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the ""latest"" version of the dataset. Also, by default the latest version of the dataset is shown in the ""Examples"" tab and experiments from all versions are shown in the ""Tests"" tab. In the ""Tests"" tab, you can see the results of tests run on the dataset at different versions."
https://docs.smith.langchain.com/how_to_guides/datasets/version_datasets,Tag a version,"You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history. For example, you might tag a version of your dataset as ""prod"" and use it to run tests against your LLM pipeline. Tagging can be done in the UI by clicking on ""+ Tag this version"" in the ""Examples"" tab. You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK: from langsmith import Clientfromt datetime import datetimeclient = Client()initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like ""prod""client.update_dataset_tag(    dataset_name=toxic_dataset_name, as_of=initial_time, tag=""prod"") To run an evaluation on a particular tagged version of a dataset, you can follow this guide."
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt,Update a prompt,Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit.
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt,Update metadata,"To update the prompt metadata (description, use cases, etc.) click the ""Edit"" pencil icon. Your prompt metadata will be updated upon save."
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt,Update the prompt content,"To update the prompt content itself, you need to enter the prompt playground. Click ""Edit in playground"".
Now you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click ""Commit"" to save it. 
"
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt,Version a prompt,"When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the ""Commits"" tab in the prompt view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_existing_experiment,Evaluate an existing experiment,"noteCurrently, evaluate_existing is only supported in the Python SDK. If you have already run an experiment and want to add additional evaluation metrics, you
can apply any evaluators to the experiment using the evaluate_existing method. from langsmith.evaluation import evaluate_existingdef always_half(run, example):    return {""score"": 0.5}experiment_name = ""my-experiment:abcd123"" # Replace with an actual experiment name or IDevaluate_existing(experiment_name, evaluators=[always_half])"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_existing_experiment,Example,"Suppose you are evaluating a semantic router. You may first run an experiment: from langsmith.evaluation import evaluatedef semantic_router(inputs: dict):    return {""class"": 1}def accuracy(run, example):    prediction = run.outputs[""class""]    expected = example.outputs[""label""]    return {""score"": prediction == expected}results = evaluate(semantic_router, data=""Router Classification Dataset"", evaluators=[accuracy])experiment_name = results.experiment_name Later, you realize you want to add precision and recall summary metrics. The evaluate_existing method accepts the same arguments as the evaluate method, replacing the target system with the experiment you wish to add metrics to, meaning
you can add both instance-level evaluator's and aggregate summary_evaluator's. from langsmith.evaluation import evaluate_existingdef precision(runs: list, examples: list):    true_positives = sum([1 for run, example in zip(runs, examples) if run.outputs[""class""] == example.outputs[""label""]])    false_positives = sum([1 for run, example in zip(runs, examples) if run.outputs[""class""] != example.outputs[""label""]])    return {""score"": true_positives / (true_positives + false_positives)}def recall(runs: list, examples: list):    true_positives = sum([1 for run, example in zip(runs, examples) if run.outputs[""class""] == example.outputs[""label""]])    false_negatives = sum([1 for run, example in zip(runs, examples) if run.outputs[""class""] != example.outputs[""label""]])    return {""score"": true_positives / (true_positives + false_negatives)}evaluate_existing(experiment_name, summary_evaluators=[precision, recall]) The precision and recall metrics will now be available in the LangSmith UI for the experiment_name experiment. As is the case with the evaluate function, there is an identical, asynchronous aevaluate_existing function that can be used to evaluate experiments asynchronously."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#push_a_prompt,Manage prompts programmatically,"You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.
All functionality going forward will live in the langsmith package."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#push_a_prompt,Install packages,PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#push_a_prompt,Configure environment variables,"If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=""lsv2_..."" TerminologyWhat we refer to as ""prompts"" used to be called ""repos"", so any references to ""repo"" in the code are referring to a prompt."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#push_a_prompt,Push a prompt,"To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = client.push_prompt(""joke-generator"", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")url = prompts.push(""joke-generator"", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");url = prompts.push(""joke-generator"", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.
This is useful for storing the model configuration you want to use with this prompt.
The provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelclient.push_prompt(""joke-generator-with-model"", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=""gpt-4o-mini"")prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"")chain = prompt | modelurl = prompts.push(""joke-generator-with-model"", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from ""langchain/hub"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { ChatOpenAI } from ""langchain-openai"";const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const prompt = ChatPromptTemplate.fromTemplate(""tell me a joke about {topic}"");const chain = prompt.pipe(model);prompts.push(""joke-generator-with-model"", chain);"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#push_a_prompt,Pull a prompt,"To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(""joke-generator"")model = ChatOpenAI(model=""gpt-4o-mini"")chain = prompt | modelchain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { ChatOpenAI } from ""langchain-openai"";const prompt = prompts.pull(""joke-generator"");const model = new ChatOpenAI({ model: ""gpt-4o-mini"" });const chain = prompt.pipe(model);chain.invoke({""topic"": ""cats""}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.
Just specify include_model when pulling the prompt.
If the stored prompt includes a model, it will be returned as a RunnableSequence.
Make sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})from langchain import hub as promptschain = prompts.pull(""joke-generator-with-model"", include_model=True)chain.invoke({""topic"": ""cats""})import * as prompts from ""langchain/hub"";import { Runnable } from ""@langchain/core/runnables"";const chain = prompts.pull<Runnable>(""joke-generator-with-model"", { includeModel: true });chain.invoke({""topic"": ""cats""}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"")prompt = prompts.pull(""joke-generator:12344e88"") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")prompt = prompts.pull(""efriis/my-first-prompt"")"
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#push_a_prompt,Use a prompt without LangChain,"If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.
These convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(""joke-generator"")prompt_value = prompt.invoke({""topic"": ""cats""})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."
https://docs.smith.langchain.com/how_to_guides/prompts/manage_prompts_programatically#push_a_prompt,"List, delete, and like prompts","You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.
See the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include ""joke""prompts = client.list_prompts(query=""joke"", is_public=False)# Delete a promptclient.delete_prompt(""joke-generator"")# Like a promptclient.like_prompt(""efriis/my-first-prompt"")# Unlike a promptclient.unlike_prompt(""efriis/my-first-prompt"")// List all prompts in my workspaceimport Client from ""langsmith"";const client = new Client({ apiKey: ""lsv2_..."" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include ""joke""const private_joke_prompts = client.listPrompts({ query: ""joke"", isPublic: false});// Delete a promptclient.deletePrompt(""joke-generator"");// Like a promptclient.likePrompt(""efriis/my-first-prompt"");// Unlike a promptclient.unlikePrompt(""efriis/my-first-prompt""); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.
However, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#open-a-trace,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Manage datasets programmatically,"You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Create a dataset from list of values,"The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.
If creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (""What is the largest mammal?"", ""The blue whale""),  (""What do mammals and birds have in common?"", ""They are both warm-blooded""),  (""What are reptiles known for?"", ""Having scales""),  (""What's the main characteristic of amphibians?"", ""They live both in water and on land""),]client = Client()dataset_name = ""Elementary Animal Questions""# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=""Questions and answers about animal phylogenetics."",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{""question"": input_prompt} for input_prompt, _ in example_inputs]outputs = [{""answer"": output_answer} for _, output_answer in example_inputs]metadata = [{""source"": ""Wikipedia""} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from ""langsmith"";const client = new Client();const exampleInputs: [string, string][] = [  [""What is the largest mammal?"", ""The blue whale""],  [""What do mammals and birds have in common?"", ""They are both warm-blooded""],  [""What are reptiles known for?"", ""Having scales""],  [    ""What's the main characteristic of amphibians?"",    ""They live both in water and on land"",  ],];const datasetName = ""Elementary Animal Questions"";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: ""Questions and answers about animal phylogenetics"",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: ""Wikipedia"" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Create a dataset from traces,"To create datasets from the runs (spans) of your traces, you can use the same approach.
For many more examples of how to fetch and filter runs, see the export traces guide.
Below is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = ""Example Dataset""# Filter runs to add to the datasetruns = client.list_runs(    project_name=""my_project"",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=""An example dataset"")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from ""langsmith"";const client = new Client();const datasetName = ""Example Dataset"";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: ""my_project"",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: ""An example dataset"",  dataType: ""kv"",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Create a dataset from a CSV file,"In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=""My CSV Dataset"",    description=""Dataset created from a CSV file""    data_type=""kv"")import { Client } from ""langsmith"";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: ""My CSV Dataset"",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: ""Dataset created from a CSV file"",    dataType: ""kv""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Create a dataset from pandas DataFrame (Python only),"The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=""My Parquet Dataset"",    description=""Dataset created from a parquet file"",    data_type=""kv"" # The default)"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Fetch datasets,"You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Query all datasets,PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,List datasets by name,"If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=""My Test Dataset 1"")const datasets = await client.listDatasets({datasetName: ""My Test Dataset 1""}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=""some substring"")const datasets = await client.listDatasets({datasetNameContains: ""some substring""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,List datasets by type,"You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=""chat"")const datasets = await client.listDatasets({dataType: ""chat""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Fetch examples,"You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from ""langsmith"";const client = new Client();"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,List all examples for a dataset,"You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab"")const examples = await client.listExamples({datasetId: ""c9ace0d8-a82c-4b6c-13d2-83401d68e9ab""}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=""My Test Dataset"")const examples = await client.listExamples({datasetName: ""My test Dataset""});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,List examples by id,"You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  ""734fc6a0-c187-4266-9721-90b7a025751a"",  ""d6b4c1b9-6160-4d63-9b61-b034c585074f"",  ""4d31df4e-f9c3-4a6e-8b6c-65701c2fed13"",];const examples = await client.listExamples({exampleIds: exampleIds});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,List examples by metadata,"You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.
Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {""foo"": ""bar"", ""baz"": ""qux""}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={""foo"": ""bar""})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: ""bar""}});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,List examples by structured filter,"Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.
Additionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \'{""foo"": ""bar""}\')), exists(metadata, ""tenant_id""))'});"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Update examples,"You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={""input"": ""updated input""},    outputs={""output"": ""updated output""},    metadata={""foo"": ""bar""},    split=""train"")await client.updateExample(example.id, {  inputs: { input: ""updated input"" },  outputs: { output: ""updated output"" },  metadata: { ""foo"": ""bar"" },  split: ""train"",})"
https://docs.smith.langchain.com/how_to_guides/datasets/manage_datasets_programmatically#update-examples,Bulk update examples,"You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{""input"": ""updated input 1""}, {""input"": ""updated input 2""}],    outputs=[        {""output"": ""updated output 1""},        {""output"": ""updated output 2""},    ],    metadata=[{""foo"": ""baz""}, {""foo"": ""qux""}],    splits=[[""training"", ""foo""], ""training""] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: ""updated input 1"" },    outputs: { output: ""updated output 1"" },    metadata: { foo: ""baz"" },    split: [""training"", ""foo""] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: ""updated input 2"" },    outputs: { output: ""updated output 2"" },    metadata: { foo: ""qux"" },    split: ""training""  },])"
https://docs.smith.langchain.com/how_to_guides/evaluation/bind_evaluator_to_dataset,Bind an evaluator to a dataset in the UI,"While you can specify evaluators to grade the results of your experiments programmatically (see this guide for more information), you can also bind evaluators to a dataset in the UI.
This allows you to configure automatic evaluators that grade your experiment results without having to write any code. Currently, only LLM-based evaluators are supported. The process for configuring this is very similar to the process for configuring an online evaluator for traces. Only affects subsequent experiment runsWhen you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured. Navigate to the dataset details page by clicking Datasets and Testing in the sidebar and selecting the dataset you want to configure the evaluator for.Click on the Add Evaluator button to add an evaluator to the dataset. This will open a modal you can use to configure the evaluator. Give your evaluator a name and set an inline prompt or load a prompt from the prompt hub that will be used to evaluate the results of the runs in the experiment. Importantly, evaluator prompts can only contain the following input variables: input (required): the input to the target you are evaluatingoutput (required): the output of the target you are evaluatingreference: the reference output, taken from the dataset noteAutomatic evaluators you configure in the application will only work if the inputs to your evaluation target, outputs from your evaluation target, and examples in your dataset are all single-key dictionaries.
LangSmith will automatically extract the values from the dictionaries and pass them to the evaluator.LangSmith currently doesn't support setting up evaluators in the application that act on multiple keys in the inputs or outputs or examples dictionaries. You can specify the scoring criteria in the ""schema"" field. In this example, we are asking the LLM to grade on ""correctness"" of the output with respect to the reference, with a boolean output of 0 or 1. The name of the field in the schema will be interpreted as the feedback key and the type will be the type of the score. Save the evaluator and navigate back to the dataset details page. Each subsequent experiment run from the dataset will now be evaluated by the evaluator you configured. Note that in the below image, each run in the experiment has a ""correctness"" score."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators,Use LangChain off-the-shelf evaluators (Python only),"Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.
These evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Ankush"", ""Hello Ankush""),    (""Harrison"", ""Hello Harrison""),]dataset_name = ""Hello Set""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": input}, {""expected"": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators,Use question and answer (correctness) evaluators,"Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!
Three QA evaluators you can load are: ""qa"", ""context_qa"", ""cot_qa"". Based on our meta-evals, we recommend using ""cot_qa"", or Chain of Thought QA. Here is a trivial example that uses a ""cot_qa"" evaluator to evaluate a simple pipeline that prefixes the input with ""Hello"": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(""cot_qa"")client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators,Use criteria evaluators,"If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the ""criteria"" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The ""criteria"" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    ""criteria"",    config={        ""criteria"": {            ""says_hello"": ""Does the submission say hello?"",        }    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.
To specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {""creativity"": ""Is this submission creative, imaginative, or novel?""}criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={""criteria"": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent ""direction"" (i.e., higher is not necessarily better).
The direction of the score depends on the criteria being evaluated. For example, a score of 1 for ""helpfulness"" means that the prediction was deemed to be helpful by the model.
However, a score of 1 for ""maliciousness"" means that the prediction contains malicious content, which, of course, is ""bad""."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators,Use labeled criteria evaluators,"If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the ""labeled_criteria"" or ""labeled_score_string"" evaluators. The ""labeled_criteria"" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe ""labeled_score_string"" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    ""labeled_score_string"",    config={        ""criteria"": {            ""accuracy"": ""How accurate is this prediction compared to the reference on a scale of 1-10?""        },        ""normalize_by"": 10,    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators,Use string or embedding distance metrics,"To measure the similarity between a predicted string and a reference, you can use string distance metrics: The ""string_distance"" evaluator computes a normalized string edit distance between the prediction and referenceThe ""embedding_distance"" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    ""string_distance"",    config={""distance"": ""levenshtein"", ""normalize_score"": True})embedding_distance_evaluator = LangChainStringEvaluator(    ""embedding_distance"",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # ""embeddings"": HuggingFaceEmbeddings(model=""distilbert-base-uncased""),      # Can also choose ""euclidean"", ""chebyshev"", ""hamming"", and ""manhattan""        ""distance_metric"": ""cosine"",      })evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators,Use a custom LLM in off-the-shelf evaluators,"You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=""gpt-3.5-turbo"")cot_qa_evaluator = LangChainStringEvaluator(""cot_qa"", config={""llm"": eval_llm})evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators,Handle multiple input or output fields,"LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.
These map the keys ""prediction"", ""reference"", and ""input"" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: ""greeting"" and ""foo"". We want to evaluate the ""greeting"" field against the ""expected"" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    },    prepare_data=lambda run, example: {        ""prediction"": run.outputs[""greeting""],        ""reference"": example.outputs[""expected""],        ""input"": example.inputs[""input""],    })client = Client()evaluate(    lambda input: {""greeting"": ""Hello "" + input[""input""], ""foo"": ""bar""},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts,Use monitoring charts,LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts,Change the time period,"You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts,Slice data by metadata or tag,"By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.
This can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.
After that, you can click the Tag or Metadata tab at the top to group runs accordingly."
https://docs.smith.langchain.com/how_to_guides/monitoring/use_monitoring_charts,Drill down into specific subsets,"Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators,Use LangChain off-the-shelf evaluators (Python only),"Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.
These evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Ankush"", ""Hello Ankush""),    (""Harrison"", ""Hello Harrison""),]dataset_name = ""Hello Set""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": input}, {""expected"": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators,Use question and answer (correctness) evaluators,"Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!
Three QA evaluators you can load are: ""qa"", ""context_qa"", ""cot_qa"". Based on our meta-evals, we recommend using ""cot_qa"", or Chain of Thought QA. Here is a trivial example that uses a ""cot_qa"" evaluator to evaluate a simple pipeline that prefixes the input with ""Hello"": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(""cot_qa"")client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators,Use criteria evaluators,"If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the ""criteria"" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The ""criteria"" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    ""criteria"",    config={        ""criteria"": {            ""says_hello"": ""Does the submission say hello?"",        }    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.
To specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {""creativity"": ""Is this submission creative, imaginative, or novel?""}criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={""criteria"": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent ""direction"" (i.e., higher is not necessarily better).
The direction of the score depends on the criteria being evaluated. For example, a score of 1 for ""helpfulness"" means that the prediction was deemed to be helpful by the model.
However, a score of 1 for ""maliciousness"" means that the prediction contains malicious content, which, of course, is ""bad""."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators,Use labeled criteria evaluators,"If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the ""labeled_criteria"" or ""labeled_score_string"" evaluators. The ""labeled_criteria"" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe ""labeled_score_string"" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    ""labeled_score_string"",    config={        ""criteria"": {            ""accuracy"": ""How accurate is this prediction compared to the reference on a scale of 1-10?""        },        ""normalize_by"": 10,    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators,Use string or embedding distance metrics,"To measure the similarity between a predicted string and a reference, you can use string distance metrics: The ""string_distance"" evaluator computes a normalized string edit distance between the prediction and referenceThe ""embedding_distance"" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    ""string_distance"",    config={""distance"": ""levenshtein"", ""normalize_score"": True})embedding_distance_evaluator = LangChainStringEvaluator(    ""embedding_distance"",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # ""embeddings"": HuggingFaceEmbeddings(model=""distilbert-base-uncased""),      # Can also choose ""euclidean"", ""chebyshev"", ""hamming"", and ""manhattan""        ""distance_metric"": ""cosine"",      })evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators,Use a custom LLM in off-the-shelf evaluators,"You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=""gpt-3.5-turbo"")cot_qa_evaluator = LangChainStringEvaluator(""cot_qa"", config={""llm"": eval_llm})evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-criteria-evaluators,Handle multiple input or output fields,"LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.
These map the keys ""prediction"", ""reference"", and ""input"" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: ""greeting"" and ""foo"". We want to evaluate the ""greeting"" field against the ""expected"" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    },    prepare_data=lambda run, example: {        ""prediction"": run.outputs[""greeting""],        ""reference"": example.outputs[""expected""],        ""input"": example.inputs[""input""],    })client = Client()evaluate(    lambda input: {""greeting"": ""Hello "" + input[""input""], ""foo"": ""bar""},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/fetch_perf_metrics_experiment,Fetch performance metrics for an experiment,"Experiments, Projects, and SessionsTracing projects and experiments use the same underlying data structure in our backend, which is called a ""session.""You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.We are working on unifying the terminology across our documentation and APIs. When you run an experiment using evaluate with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the read_project/readProject methods. The payload for experiment details includes the following values: {  ""start_time"": ""2024-06-06T01:02:51.299960"",  ""end_time"": ""2024-06-06T01:03:04.557530+00:00"",  ""extra"": {    ""metadata"": {      ""git"": {        ""tags"": null,        ""dirty"": true,        ""branch"": ""ankush/agent-eval"",        ""commit"": ""..."",        ""repo_name"": ""..."",        ""remote_url"": ""..."",        ""author_name"": ""Ankush Gola"",        ""commit_time"": ""..."",        ""author_email"": ""...""      },      ""revision_id"": null,      ""dataset_splits"": [""base""],      ""dataset_version"": ""2024-06-05T04:57:01.535578+00:00"",      ""num_repetitions"": 3    }  },  ""name"": ""SQL Database Agent-ae9ad229"",  ""description"": null,  ""default_dataset_id"": null,  ""reference_dataset_id"": ""..."",  ""id"": ""..."",  ""run_count"": 9,  ""latency_p50"": 7.896,  ""latency_p99"": 13.09332,  ""first_token_p50"": null,  ""first_token_p99"": null,  ""total_tokens"": 35573,  ""prompt_tokens"": 32711,  ""completion_tokens"": 2862,  ""total_cost"": 0.206485,  ""prompt_cost"": 0.163555,  ""completion_cost"": 0.04293,  ""tenant_id"": ""..."",  ""last_run_start_time"": ""2024-06-06T01:02:51.366397"",  ""last_run_start_time_live"": null,  ""feedback_stats"": {    ""cot contextual accuracy"": {      ""n"": 9,      ""avg"": 0.6666666666666666,      ""values"": {        ""CORRECT"": 6,        ""INCORRECT"": 3      }    }  },  ""session_feedback_stats"": {},  ""run_facets"": [],  ""error_rate"": 0,  ""streaming_rate"": 0,  ""test_run_number"": 11} From here, you can extract performance metrics such as: latency_p50: The 50th percentile latency in seconds.latency_p99: The 99th percentile latency in seconds.total_tokens: The total number of tokens used.prompt_tokens: The number of prompt tokens used.completion_tokens: The number of completion tokens used.total_cost: The total cost of the experiment.prompt_cost: The cost of the prompt tokens.completion_cost: The cost of the completion tokens.feedback_stats: The feedback statistics for the experiment.error_rate: The error rate for the experiment.first_token_p50: The 50th percentile latency for the time to generate the first token (if using streaming).first_token_p99: The 99th percentile latency for the time to generate the first token (if using streaming). Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs. First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript.
Please view the how-to guide on evaluation for more details. from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Harrison"", ""Hello Harrison""),    (""Ankush"", ""Hello Ankush""),]dataset_name = ""HelloDataset""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": text}, {""expected"": result}) for text, result in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id) Next, we will create an experiment, retrieve the experiment name from the result of evaluate, then fetch the performance metrics for the experiment. PythonTypeScriptfrom langsmith.schemas import Example, Rundataset_name = ""HelloDataset""def foo_label(root_run: Run, example: Example) -> dict:    return {""score"": 1, ""key"": ""foo""}from langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: ""Hello "" + inputs[""input""],    data=dataset_name,    evaluators=[foo_label],    experiment_prefix=""Hello"",)resp = client.read_project(project_name=results.experiment_name, include_stats=True)print(resp.json(indent=2))import { Client } from ""langsmith"";import { evaluate } from ""langsmith/evaluation"";import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction fooLabel(rootRun: Run, example: Example): EvaluationResult {  return {score: 1, key: ""foo""};}const client = new Client();const results = await evaluate((inputs) => {  return { output: ""Hello "" + inputs.input };}, {  data: ""HelloDataset"",  experimentPrefix: ""Hello"",  evaluators: [fooLabel],});const resp = await client.readProject({ projectName: results.experimentName, includeStats: true })console.log(JSON.stringify(resp, null, 2))"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators,Use LangChain off-the-shelf evaluators (Python only),"Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.
These evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Ankush"", ""Hello Ankush""),    (""Harrison"", ""Hello Harrison""),]dataset_name = ""Hello Set""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""input"": input}, {""expected"": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators,Use question and answer (correctness) evaluators,"Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!
Three QA evaluators you can load are: ""qa"", ""context_qa"", ""cot_qa"". Based on our meta-evals, we recommend using ""cot_qa"", or Chain of Thought QA. Here is a trivial example that uses a ""cot_qa"" evaluator to evaluate a simple pipeline that prefixes the input with ""Hello"": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(""cot_qa"")client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators,Use criteria evaluators,"If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the ""criteria"" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The ""criteria"" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    ""criteria"",    config={        ""criteria"": {            ""says_hello"": ""Does the submission say hello?"",        }    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.
To specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {""creativity"": ""Is this submission creative, imaginative, or novel?""}criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={""criteria"": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent ""direction"" (i.e., higher is not necessarily better).
The direction of the score depends on the criteria being evaluated. For example, a score of 1 for ""helpfulness"" means that the prediction was deemed to be helpful by the model.
However, a score of 1 for ""maliciousness"" means that the prediction contains malicious content, which, of course, is ""bad""."
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators,Use labeled criteria evaluators,"If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the ""labeled_criteria"" or ""labeled_score_string"" evaluators. The ""labeled_criteria"" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe ""labeled_score_string"" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    ""labeled_score_string"",    config={        ""criteria"": {            ""accuracy"": ""How accurate is this prediction compared to the reference on a scale of 1-10?""        },        ""normalize_by"": 10,    })client = Client()evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators,Use string or embedding distance metrics,"To measure the similarity between a predicted string and a reference, you can use string distance metrics: The ""string_distance"" evaluator computes a normalized string edit distance between the prediction and referenceThe ""embedding_distance"" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    ""string_distance"",    config={""distance"": ""levenshtein"", ""normalize_score"": True})embedding_distance_evaluator = LangChainStringEvaluator(    ""embedding_distance"",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # ""embeddings"": HuggingFaceEmbeddings(model=""distilbert-base-uncased""),      # Can also choose ""euclidean"", ""chebyshev"", ""hamming"", and ""manhattan""        ""distance_metric"": ""cosine"",      })evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators,Use a custom LLM in off-the-shelf evaluators,"You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=""gpt-3.5-turbo"")cot_qa_evaluator = LangChainStringEvaluator(""cot_qa"", config={""llm"": eval_llm})evaluate(    lambda input: ""Hello "" + input[""input""],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"
https://docs.smith.langchain.com/how_to_guides/evaluation/use_langchain_off_the_shelf_evaluators#use-a-custom-llm-in-off-the-shelf-evaluators,Handle multiple input or output fields,"LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.
These map the keys ""prediction"", ""reference"", and ""input"" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: ""greeting"" and ""foo"". We want to evaluate the ""greeting"" field against the ""expected"" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    ""labeled_criteria"",    config={        ""criteria"": {            ""helpfulness"": (                ""Is this submission helpful to the user,""                "" taking into account the correct reference answer?""            )        }    },    prepare_data=lambda run, example: {        ""prediction"": run.outputs[""greeting""],        ""reference"": example.outputs[""expected""],        ""input"": example.inputs[""input""],    })client = Client()evaluate(    lambda input: {""greeting"": ""Hello "" + input[""input""], ""foo"": ""bar""},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"
https://docs.smith.langchain.com/concepts/admin#users,Admin,"This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."
https://docs.smith.langchain.com/concepts/admin#users,Organizations,"An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.
There are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"
https://docs.smith.langchain.com/concepts/admin#users,Workspaces,"infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:
 The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well
**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.
See the self-hosted user management docs for details."
https://docs.smith.langchain.com/concepts/admin#users,Users,A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:
https://docs.smith.langchain.com/concepts/admin#users,API keys,"Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."
https://docs.smith.langchain.com/concepts/admin#users,Personal Access Tokens (PATs),Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_
https://docs.smith.langchain.com/concepts/admin#users,Service keys,"Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"
https://docs.smith.langchain.com/concepts/admin#users,Organization roles,"Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.
Custom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"
https://docs.smith.langchain.com/concepts/admin#users,Workspace roles (RBAC),"noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev
Other plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#use-the-evaluate_comparative-function,Run pairwise evaluations,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function
with two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#use-the-evaluate_comparative-function,Use theevaluate_comparativefunction,"notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#use-the-evaluate_comparative-function,Configure inputs and outputs for pairwise evaluators,"Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with
runs[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent ""both equally bad"" or both to 1 for ""both equally good"". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#use-the-evaluate_comparative-function,Compare two experiments with LLM-based pairwise evaluators,"The following example uses a prompt
which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.
The prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(""langchain-ai/pairwise-evaluation-2"")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=""gpt-4"")        runnable = prompt | model    response = runnable.invoke({        ""question"": example.inputs[""question""],        ""answer_a"": runs[0].outputs[""output""] if runs[0].outputs is not None else ""N/A"",        ""answer_b"": runs[1].outputs[""output""] if runs[1].outputs is not None else ""N/A"",    })    score = response[""Preference""]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {""key"": ""ranked_preference"", ""scores"": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [""my-experiment-name-1"", ""my-experiment-name-2""],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.
import type { Run, Example } from ""langsmith"";import { evaluateComparative } from ""langsmith/evaluation"";import { wrapOpenAI } from ""langsmith/wrappers"";import OpenAI from ""openai"";const openai = wrapOpenAI(new OpenAI());import { z } from ""zod"";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(""Expected at least two runs"");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? ""N/A"",    answer_b: runB?.outputs?.output ?? ""N/A"",  };    const output = await openai.chat.completions.create({    model: ""gpt-4-turbo"",    messages: [      {        role: ""system"",        content: [          ""Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below."",          ""You should choose the assistant that follows the user's instructions and answers the user's question better."",          ""Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses."",          ""Begin your evaluation by comparing the two responses and provide a short explanation."",          ""Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision."",          ""Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible."",        ].join("" ""),      },      {        role: ""user"",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(""\n\n""),      },    ],    tool_choice: {      type: ""function"",      function: { name: ""Score"" },    },    tools: [      {        type: ""function"",        function: {          name: ""Score"",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output ""1"" if Assistant A answer is better based upon the factors above.`,            `Output ""2"" if Assistant B answer is better based upon the factors above.`,            `Output ""0"" if it is a tie.`,          ].join("" ""),          parameters: {            type: ""object"",            properties: {              Preference: {                type: ""integer"",                description: ""Which assistant answer is preferred?"",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: ""ranked_preference"", scores };}await evaluateComparative([""earnest-name-40"", ""reflecting-pump-91""], {  evaluators: [evaluatePairwise],});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_pairwise#use-the-evaluate_comparative-function,View pairwise experiments,"Navigate to the ""Pairwise Experiments"" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Evaluate an LLM Application,"Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.
This section provides guidance on how to evaluate the performance of your LLM application."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Run an evaluation,"At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as ""Toxic"" or ""Not toxic""."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Step 1: Define your target task,"In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.
We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            ""role"": ""system"",            ""content"": ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        {""role"": ""user"", ""content"": text},    ]    result = openai.chat.completions.create(        messages=messages, model=""gpt-3.5-turbo"", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from ""openai"";import { wrapOpenAI } from ""langsmith/wrappers"";import { traceable } from ""langsmith/traceable"";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: ""system"",          content: ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't."",        },        { role: ""user"", content: text },      ],      model: ""gpt-3.5-turbo"",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: ""labelText"" });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Step 2: Create or select a dataset,"In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.
The values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (""Shut up, idiot"", ""Toxic""),    (""You're a wonderful person"", ""Not toxic""),    (""This is the worst thing ever"", ""Toxic""),    (""I had a great day today"", ""Not toxic""),    (""Nobody likes you"", ""Toxic""),    (""This is unacceptable. I want to speak to the manager."", ""Not toxic""),]dataset_name = ""Toxic Queries""dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({""text"": text}, {""label"": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from ""langsmith"";const langsmith = new Client();// create a datasetconst toxicExamples = [  [""Shut up, idiot"", ""Toxic""],  [""You're a wonderful person"", ""Not toxic""],  [""This is the worst thing ever"", ""Toxic""],  [""I had a great day today"", ""Not toxic""],  [""Nobody likes you"", ""Toxic""],  [""This is unacceptable. I want to speak to the manager."", ""Not toxic""],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = ""Toxic Queries"";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Step 3. Configure evaluators to score the outputs,"In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.
Writing evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };}"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Step 4. Run the evaluation and view the results,"You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = ""Toxic Queries""results = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    description=""Testing the baseline system."",  # optional)import { evaluate } from ""langsmith/evaluation"";const datasetName = ""Toxic Queries"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.
Evaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Use custom evaluators,"At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).
The key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.
The root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(""output"") == example.outputs.get(""label"")    return {""score"": int(score), ""key"": ""correct_label""}import type { EvaluationResult } from ""langsmith/evaluation"";import type { Run, Example } from ""langsmith/schemas"";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: ""correct_label"", score };} default feedback keyIf the ""key"" field is not provided, the default key name will be the name of the evaluator function."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Evaluate on a particular version of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.
Additionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.
Simply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=""latest""),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: ""latest"",  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Evaluate on a subset of a dataset,"Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, metadata={""desired_key"": ""desired_value""}),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {""desired_key"": ""desired_value""},  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Evaluate on a dataset split,"Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=client.list_examples(dataset_name=dataset_name, splits=[""test"", ""training""]),    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [""test"", ""training""],  }),  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Evaluate on a dataset with repetitions,"The optional num_repetitions param to the evaluate function allows you to specify how many times
to run/evaluate each example in your dataset. For instance, if you have 5 examples and set
num_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing
noise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",    num_repetitions=3,)import { evaluate } from ""langsmith/evaluation"";await evaluate((inputs) => labelText(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",  numReptitions=3,});"
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Use a summary evaluator,"Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.
For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.
These are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[""output""] == examples[i].outputs[""label""]:            correct += 1    if correct / len(runs) > 0.5:        return {""key"": ""pass"", ""score"": True}    else:        return {""key"": ""pass"", ""score"": False}import { Run, Example } from ""langsmith/schemas"";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[""output""] === examples[i].outputs[""label""]) {      correct += 1;    }  }    return { key: ""pass"", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[""text""]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=""Toxic Queries"",)await evaluate((inputs) => labelQuery(inputs[""input""]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: ""Toxic Queries"",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Evaluate a LangChain runnable,"You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""),  (""user"", ""{text}"")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from ""@langchain/openai"";import { ChatPromptTemplate } from ""@langchain/core/prompts"";import { StringOutputParser } from ""@langchain/core/output_parsers"";const prompt = ChatPromptTemplate.fromMessages([  [""system"", ""Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.""],  [""user"", ""{text}""]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=""Toxic Queries"",)import { evaluate } from ""langsmith/evaluation"";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: ""Toxic Queries"",}); The runnable is traced appropriately for each output."
https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators,Return multiple scores,"In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    ""results"": [        {""key"":string, ""score"": number},        {""key"":string, ""score"": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      ""results"": [          {""key"": ""precision"", ""score"": 0.8},          {""key"": ""recall"", ""score"": 0.9},          {""key"": ""f1"", ""score"": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher
import type { Run, Example } from ""langsmith/schemas"";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: ""precision"", score: 0.8 },          { key: ""recall"", score: 0.9 },          { key: ""f1"", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."
https://docs.smith.langchain.com/concepts/tracing#tags,Tracing,"This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"
https://docs.smith.langchain.com/concepts/tracing#tags,Runs,"A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#tags,Traces,"A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.
"
https://docs.smith.langchain.com/concepts/tracing#tags,Projects,"A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.
"
https://docs.smith.langchain.com/concepts/tracing#tags,Feedback,"Feedback allows you to score an individual run based on certain criteria.
Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.
Feedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."
https://docs.smith.langchain.com/concepts/tracing#tags,Tags,"Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces
"
https://docs.smith.langchain.com/concepts/tracing#tags,Metadata,"Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.
Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces
"
https://docs.smith.langchain.com/concepts/tracing#tags,Data storage and retention,"For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."
https://docs.smith.langchain.com/concepts/tracing#tags,Deleting traces from LangSmith,"If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the ""Delete"" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."
https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_instructor,Trace withInstructor(Python only),"We provide a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs. In order to use, you first need to set your LangSmith API key. export LANGCHAIN_API_KEY=<your-api-key> Next, you will need to install the LangSmith SDK: pip install -U langsmith Wrap your OpenAI client with langsmith.wrappers.wrap_openai from openai import OpenAIfrom langsmith import wrappersclient = wrappers.wrap_openai(OpenAI()) After this, you can patch the wrapped OpenAI client using instructor: import instructorclient = instructor.patch(client) Now, you can use instructor as you normally would, but now everything is logged to LangSmith! from pydantic import BaseModelclass UserDetail(BaseModel):    name: str    age: intuser = client.chat.completions.create(    model=""gpt-3.5-turbo"",    response_model=UserDetail,    messages=[        {""role"": ""user"", ""content"": ""Extract Jason is 25 years old""},    ]) Oftentimes, you use instructor inside of other functions.
You can get nested traces by using this wrapped client and decorating those functions with @traceable.
Please see this guide for more information on how to annotate your code for tracing with the @traceable decorator. # You can customize the run name with the `name` keyword argument@traceable(name=""Extract User Details"")def my_function(text: str) -> UserDetail:    return client.chat.completions.create(        model=""gpt-3.5-turbo"",        response_model=UserDetail,        messages=[            {""role"": ""user"", ""content"": f""Extract {text}""},        ]    )my_function(""Jason is 25 years old"")"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Set up webhook notifications for rules,"When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Webhook payload,"The payload we send to your webhook endpoint contains ""rule_id"" this is the ID of the automation that sent this payload""start_time"" and ""end_time"" these are the time boundaries where we found matching runs""runs"" this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.""feedback_stats"" this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below. ""feedback_stats"": {    ""about_langchain"": {        ""n"": 1,        ""avg"": 0.0,        ""show_feedback_arrow"": true,        ""values"": {}    },    ""category"": {        ""n"": 0,        ""avg"": null,        ""show_feedback_arrow"": true,        ""values"": {            ""CONCEPTUAL"": 1        }    },    ""user_score"": {        ""n"": 2,        ""avg"": 0.0,        ""show_feedback_arrow"": false,        ""values"": {}    },    ""vagueness"": {        ""n"": 1,        ""avg"": 0.0,        ""show_feedback_arrow"": true,        ""values"": {}    }}, fetching from S3 URLsDepending on how recent your runs are, the inputs_s3_urls and outputs_s3_urls fields may contain S3 URLs to the actual data instead of the data itself.The inputs and outputs can be fetched by the ROOT.presigned_url provided in inputs_s3_urls and outputs_s3_urls respectively. This is an example of the entire payload we send to your webhook endpoint: {  ""rule_id"": ""d75d7417-0c57-4655-88fe-1db3cda3a47a"",  ""start_time"": ""2024-04-05T01:28:54.734491+00:00"",  ""end_time"": ""2024-04-05T01:28:56.492563+00:00"",  ""runs"": [    {      ""status"": ""success"",      ""is_root"": true,      ""trace_id"": ""6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""dotted_order"": ""20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""run_type"": ""tool"",      ""modified_at"": ""2024-04-05T01:28:54.145062"",      ""tenant_id"": ""2ebda79f-2946-4491-a9ad-d642f49e0815"",      ""end_time"": ""2024-04-05T01:28:54.085649"",      ""name"": ""Search"",      ""start_time"": ""2024-04-05T01:28:54.085646"",      ""id"": ""6ab80f10-d79c-4fa2-b441-922ed6feb630"",      ""session_id"": ""6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5"",      ""parent_run_ids"": [],      ""child_run_ids"": null,      ""direct_child_run_ids"": null,      ""total_tokens"": 0,      ""completion_tokens"": 0,      ""prompt_tokens"": 0,      ""total_cost"": null,      ""completion_cost"": null,      ""prompt_cost"": null,      ""first_token_time"": null,      ""app_path"": ""/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809"",      ""in_dataset"": false,      ""last_queued_at"": null,      ""inputs"": null,      ""inputs_s3_urls"": null,      ""outputs"": null,      ""outputs_s3_urls"": null,      ""extra"": null,      ""events"": null,      ""feedback_stats"": null,      ""serialized"": null,      ""share_token"": null    }  ]}"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Webhook Security,"We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications. An example would be https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Webhook custom HTTP headers,"If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the Headers option next to the URL field and add your headers. noteHeaders are stored in encrypted format."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Webhook Delivery,"When delivering events to your webhook endpoint we follow these guidelines If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.Anything your endpoint returns in the body will be ignored"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Setup,"For an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here. First, create a Modal account. Then, locally install the Modal SDK: pip install modal To finish setting up your account, run the command: modal setup and follow the instructions"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Secrets,"Next, you will need to set up some secrets in Modal. First, LangSmith will need to authenticate to Modal by passing in a secret.
The easiest way to do this is to pass in a secret in the query parameters.
To validate this secret, we will need to add a secret in Modal to validate it.
We will do that by creating a Modal secret.
You can see instructions for secrets here.
For this purpose, let's call our secret ls-webhook and have it set an environment variable with the name LS_WEBHOOK. We can also set up a LangSmith secret - luckily there is already an integration template for this!"
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Service,"After that, you can create a Python file that will serve as your endpoint.
An example is below, with comments explaining what is going on: from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub(""auth-example"", image=Image.debian_slim().pip_install(""langsmith""))@stub.function(    secrets=[Secret.from_name(""ls-webhook""), Secret.from_name(""my-langsmith-secret"")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method=""POST"")# We set up a `secret` query parameterdef f(data: dict, secret: str = Query(...)):    # You can import dependencies you don't have locally inside Modal funxtions    from langsmith import Client    # First, we validate the secret key we pass    import os    if secret != os.environ[""LS_WEBHOOK""]:        raise HTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=""Incorrect bearer token"",            headers={""WWW-Authenticate"": ""Bearer""},        )    # This is where we put the logic for what should happen inside this webhook    ls_client = Client()    runs = data[""runs""]    ids = [r[""id""] for r in runs]    feedback = list(ls_client.list_feedback(run_ids=ids))    for r, f in zip(runs, feedback):        try:            ls_client.create_example(                inputs=r[""inputs""],                outputs={""output"": f.correction},                dataset_name=""classifier-github-issues"",            )        except Exception:            raise ValueError(f""{r} and {f}"")    # Function body    return ""success!"" We can now deploy this easily with modal deploy ... (see docs here). You should now get something like:  Created objects.  Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py  Created mount PythonPackage:langsmith  Created f => https://hwchase17--auth-example-f.modal.run App deployed! View Deployment: https://modal.com/apps/hwchase17/auth-example The important thing to remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.
NOTE: this is NOT the final deployment URL, make sure not to accidentally use that."
https://docs.smith.langchain.com/how_to_guides/monitoring/webhooks,Hooking it up,"We can now take the function URL we create above and add it as a webhook.
We have to remember to also pass in the secret key as a query parameter.
Putting it all together, it should look something like: https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET} Replace {SECRET} with the secret key you created to access the Modal service."
https://docs.smith.langchain.com/how_to_guides/playground/custom_tls_certificates,Use custom TLS certificates,"Self-hosted enterprise only (version 0.6 and later)This feature is only available for self-hosted enterprise customers, starting from version 0.6.If you are interested in this plan, please contact sales@langchain.dev for more information. Azure OpenAI onlyThis feature is currently only available for the Azure OpenAI model provider. More model providers will be supported in the future.This will currently only affect model invocations through the LangSmith Playground, not Online Evaluation.
The TLS certificates will apply to all Azure Deployment configurations in the playground. You can use custom TLS certificates to connect to model providers in the LangSmith playground. This is useful if you are using a self-signed certificate, a certificate from a custom certificate authority or mutual TLS authentication. To use custom TLS certificates, you need to set the following environment variables. See the self hosted deployment section for more information on how to set up application configuration. LANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS: A comma-separated list of model providers that require custom TLS certificates. Note that azure_openai is currently the only supported model provider that supports custom TLS certificates, but more providers will be supported in the future.LANGSMITH_PLAYGROUND_TLS_CA: The custom certificate authority (CA) certificate in PEM format. This can be a file path (for a mounted volume) or a base64-encoded string.[Optional] LANGSMITH_PLAYGROUND_TLS_KEY: The private key in PEM format. This can be a file path (for a mounted volume) or a base64-encoded string. This is usually only necessary for mutual TLS authentication.[Optional] LANGSMITH_PLAYGROUND_TLS_CERT: The certificate in PEM format. This can be a file path (for a mounted volume) or a base64-encoded string. This is usually only necessary for mutual TLS authentication. Once you have set these environment variables, enter the playground and select the AzureChatOpenAI model provider. Enter the Deployment Name, Azure Endpoint, and API Version, as well as model invocation parameters. Then, the playground will be able to connect to the model provider using the custom TLS certificate."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Filter traces in the application,"Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.
Being able to accurately filter runs is important for both manual inspection and setting up automations."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Create a filter,"There are two ways to create a filter.
First, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Filter for intermediate runs (spans),"In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.
This relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Advanced: filter for intermediate runs (spans) on properties of the root,"A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Advanced: filter for runs (spans) whose child runs have some attribute,"This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Filter based on inputs and outputs,"You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Filter based on input / output key-value pairs,"In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  ""input"": ""What is the capital of France?""} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  ""documents"": [    {      ""page_content"": ""The capital of France is Paris"",      ""metadata"": {},      ""type"": ""Document""    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Saved filters,You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Save a filter,"In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Use a saved filter,"After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Update a saved filter,"With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Delete a saved filter,"With the filter selected, click on the trash button to delete the saved filter."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Copy the filter,"Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, ""user_score""), eq(feedback_score, 1))) Please see this reference for more information on the query language."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Filtering runs within the trace view,"You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from ""Filtered Only"" to ""Show All"" or ""Most relevant""."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Manually specify a raw query in LangSmith query language,"If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."
https://docs.smith.langchain.com/how_to_guides/monitoring/filter_traces_in_application#advanced-filter-for-runs-spans-whose-child-runs-have-some-attribute,Use an AI Query to auto-generate a query,"Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: ""All runs longer than 10 seconds"" Experimental featureNote that this is an experimental feature and may not work for all queries."
https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators,LangChain off-the-shelf evaluators,"LangChain's evaluation module provides evaluators you can use as-is for common evaluation scenarios.
To learn how to use these evaluators, please refer to the following guide. noteWe currently support off-the-shelf evaluators for Python only, but are adding support for TypeScript soon. noteMost of these evaluators are useful but imperfect! We recommend against blind trust of any single automated metric and to always incorporate them as a part of a holistic testing and evaluation strategy.
Many of the LLM-based evaluators return a binary score for a given datapoint, so measuring differences in prompt or model performance are most reliable in aggregate over a larger dataset. The following table enumerates the off-the-shelf evaluators available in LangSmith, along with their output keys and a simple code sample. Evaluator nameOutput KeySimple Code ExampleQ&AcorrectnessLangChainStringEvaluator(""qa"")Contextual Q&Acontextual accuracyLangChainStringEvaluator(""context_qa"")Chain of Thought Q&Acot contextual accuracyLangChainStringEvaluator(""cot_qa"")CriteriaDepends on criteria keyLangChainStringEvaluator(""criteria"", config={ ""criteria"": <criterion> })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ ""criterion_key"": ""criterion description"" }Labeled CriteriaDepends on criteria keyLangChainStringEvaluator(""labeled_criteria"", config={ ""criteria"": <criterion> })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ ""criterion_key"": ""criterion description"" }ScoreDepends on criteria keyLangChainStringEvaluator(""score_string"", config={ ""criteria"": <criterion>, ""normalize_by"": 10 })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ ""criterion_key"": ""criterion description"" }. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.Labeled ScoreDepends on criteria keyLangChainStringEvaluator(""labeled_score_string"", config={ ""criteria"": <criterion>, ""normalize_by"": 10 })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ ""criterion_key"": ""criterion description"" }. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.Embedding distanceembedding_cosine_distanceLangChainStringEvaluator(""embedding_distance"")String Distancestring_distanceLangChainStringEvaluator(""string_distance"", config={""distance"": ""damerau_levenshtein"" }) distance defines the string difference metric to be applied, such as levenshtein or jaro_winkler.Exact Matchexact_matchLangChainStringEvaluator(""exact_match"")Regex Matchregex_matchLangChainStringEvaluator(""regex_match"")Json Validityjson_validityLangChainStringEvaluator(""json_validity"")Json Equalityjson_equalityLangChainStringEvaluator(""json_equality"")Json Edit Distancejson_edit_distanceLangChainStringEvaluator(""json_edit_distance"")Json Schemajson_schemaLangChainStringEvaluator(""json_schema"")"
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-comparison-view,Audit evaluator scores,"LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-comparison-view,In the comparison view,"In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the ""edit"" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under ""Make correction"".
If you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples
in place of the few_shot_explanation prompt variable."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-comparison-view,In the runs table,"In the runs table, find the ""Feedback"" column and click on the feedback tag to bring up the feedback details. Again, click the ""edit"" icon on the right to bring up the corrections view."
https://docs.smith.langchain.com/how_to_guides/evaluation/audit_evaluator_scores#in-the-comparison-view,In the SDK,"Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      ""score"": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Compare experiment results,"Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Open the comparison view,"To open the comparison view, select two or more experiments from the ""Experiments"" tab from a given dataset page. Then, click on the ""Compare"" button at the bottom of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,View regressions and improvements,"In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved
will be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Filter on regressions or improvements,Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Update baseline experiment,"In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can
change it from the dropdown at the top of the page."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Select feedback key,"You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by
default, but you can adjust as needed."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Open a trace,"If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Expand detailed view,"From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."
https://docs.smith.langchain.com/how_to_guides/evaluation/compare_experiment_results#update-display-settings,Update display settings,"You can adjust the display settings for comparison view by clicking on ""Display"" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."
https://docs.smith.langchain.com/self_hosting/configuration/ttl,TTL and Data Retention,"LangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if you're complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces.
Traces will also have their data retention period automatically extended based on certain actions or run rule applications. For more details on Data Retention, take a look at the section on auto-upgrades in the data retention guide."
https://docs.smith.langchain.com/self_hosting/configuration/ttl,Requirements,"You can configure retention through helm or environment variable settings. There are a few options that are
configurable: Enabled: Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see data retention guide for details).Retention Periods: You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects. HelmDockerconfig:  ttl:    enabled: true    ttl_period_seconds:      # -- TTL seconds - 400 day longlived and 14 day shortlived      longlived: ""34560000""      shortlived: ""1209600""    # In your .env fileFF_TRACE_TIERS_ENABLED=trueTRACE_TIER_TTL_DURATION_SEC_MAP='{""longlived"": 34560000, ""shortlived"": 1209600}'    "
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#versioning,Update a prompt,Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit.
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#versioning,Update metadata,"To update the prompt metadata (description, use cases, etc.) click the ""Edit"" pencil icon. Your prompt metadata will be updated upon save."
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#versioning,Update the prompt content,"To update the prompt content itself, you need to enter the prompt playground. Click ""Edit in playground"".
Now you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click ""Commit"" to save it. 
"
https://docs.smith.langchain.com/how_to_guides/prompts/update_a_prompt#versioning,Version a prompt,"When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the ""Commits"" tab in the prompt view."
