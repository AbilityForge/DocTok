heading,text,question
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in the interaction pattern described?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents through the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",What is a common technique used by agentic systems to overcome the struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method used to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
Complex data extraction with function calling,"Function calling is a core primitive for integrating LLMs within your software stack. We use it throughout the LangGraph docs, since developing with function calling (aka tool usage) tends to be much more stress-free than the traditional way of writing custom string parsers. However, even GPT-4, Opus, and other powerful models still struggle with complex functions, especially if your schema involves any nesting or if you have more advanced data validation rules. There are three basic ways to increase reliability: better prompting, constrained decoding, and validation with re-prompting. We will cover two approaches to the last technique here, since it is generally applicable across any LLM that supports tool calling.",What are the three basic ways to increase reliability when dealing with complex functions in LLMs?
Regular Extraction with Retries,"Both examples here invoke a simple looping graph that takes following approach: 
Prompt the LLM to respond.
If it responds with tool calls, validate those.
If the calls are correct, return. Otherwise, format the validation error as a new ToolMessage and prompt the LLM to fix the errors. Taking us back to step (1).
 The techniques differ only on step (3). In this first step, we will prompt the original LLM to regenerate the function calls to fix the validation errors. In the next section, we will instead prompt the LLM to generate a patch to fix the errors, meaning it doesn't have to re-generate data that is valid.",What approach is taken in the examples for regular extraction with retries?
Try it out,Now we'll ask our model to call a function. We'll add a validator to illustrate how the LLM is able to use the validation error to fix its results.,What will the model do after calling a function and adding a validator?
JSONPatch,"The regular retry method worked well for our simple case, but it still was unable to self-correct when populating a complex schema. LLMs work best on narrow tasks. A tried-and-true principle of LLM interface design is to simplify the task for each LLM run. One way to do this is to patch the state instead of completely regenerating the state. One way to do this is with JSONPatch operations. Let's try it out! Below, create a JSONPatch retry graph. This works as follows: 
First pass: try to generate the full output.
Retries: prompt the LLM to generate JSON patches on top of the first output to heal the erroneous generation.
 The fallback LLM just has to generate a list of paths, ops (add, remove, replace), and optional values. Since the pydantic validation errors include the path in their errors, the LLM should be more reliable.",How can JSONPatch operations be used to simplify the task for each LLM run when populating a complex schema?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Why is my project failing to start?,"There are a few reasons that your project might fail to start, here are some of the most common ones.",Why might a project fail to start?
Docker issues,"LangGraph Studio requires Docker Desktop version 4.24 or higher. Please make sure you have a version of Docker installed that satisfies that requirement and also make sure you have the Docker Desktop app up and running before trying to use LangGraph Studio. In addition, make sure you have docker-compose updated to version 2.22.0 or higher.",What version of Docker Desktop and docker-compose is required for LangGraph Studio?
Configuration or environment issues,"Another reason your project might fail to start is because your configuration file is defined incorrectly, or you are missing required environment variables. ",What are some reasons why a project might fail to start according to the heading and text provided?
How does interrupt work?,When you select the Interrupts dropdown and select a node to interrupt the graph will pause execution before and after (unless the node goes straight to END) that node has run. This means that you will be able to both edit the state before the node is ran and the state after the node has ran. This is intended to allow developers more fine-grained control over the behavior of a node and make it easier to observe how the node is behaving. You will not be able to edit the state after the node has ran if the node is the final node in the graph.,How does the interrupt feature work in the graph execution process?
How do I reload the app?,"If you would like to reload the app, don't use Command+R as you might normally do. Instead, close and reopen the app for a full refresh.",How should I reload the app for a full refresh?
How does automatic rebuilding work?,One of the key features of LangGraph Studio is that it automatically rebuilds your image when you change the source code. This allows for a super fast development and testing cycle which makes it easy to iterate on your graph. There are two different ways that LangGraph rebuilds your image: either by editing the image or completely rebuilding it.,How does LangGraph Studio automatically rebuild your image when you change the source code?
Rebuilds from source code changes,"If you modified the source code only (no configuration or dependency changes!) then the image does not require a full rebuild, and LangGraph Studio will only update the relevant parts. The UI status in the bottom left will switch from Online to Stopping temporarily while the image gets edited. The logs will be shown as this process is happening, and after the image has been edited the status will change back to Online and you will be able to run your graph with the modified code!",What happens to the UI status in LangGraph Studio when the source code is modified?
Rebuilds from configuration or dependency changes,"If you edit your graph configuration file (langgraph.json) or the dependencies (either pyproject.toml or requirements.txt) then the entire image will be rebuilt. This will cause the UI to switch away from the graph view and start showing the logs of the new image building process. This can take a minute or two, and once it is done your updated image will be ready to use!",What happens if you make changes to the graph configuration file or dependencies in the project?
Why is my graph taking so long to startup?,"The LangGraph Studio interacts with a local LangGraph API server. To stay aligned with ongoing updates, the LangGraph API requires regular rebuilding. As a result, you may occasionally experience slight delays when starting up your project.",Why might my graph be taking a long time to startup?
Why are extra edges showing up in my graph?,"If you don't define your conditional edges carefully, you might notice extra edges appearing in your graph. This is because without proper definition, LangGraph Studio assumes the conditional edge could access all other nodes. In order for this to not be the case, you need to be explicit about how you define the nodes the conditional edge routes to. There are two ways you can do this:",How can extra edges be prevented from showing up in a graph in LangGraph Studio?
Solution 1: Include a path map,"The first way to solve this is to add path maps to your conditional edges. A path map is just a dictionary that maps the possible outputs of your router function with the names of the nodes that each output corresponds to. The path map is passed as the third argument to the add_conditional_edges function like so: graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
 In this case, the routing function returns either True or False, which map to node_b and node_c respectively.",How can you solve this by including a path map?
Solution 2: Update the typing of the router,"Instead of passing a path map, you can also be explicit about the typing of your routing function by specifying the nodes it can map to using the Literal python definition. Here is an example of how to define a routing function in that way: def routing_function(state: GraphState) -> Literal[""node_b"",""node_c""]:
    if state['some_condition'] == True:
        return ""node_a""
    else:
        return ""node_b""
",How can you update the typing of the router in Solution 2?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to create a custom checkpointer using MongoDB,"When creating LangGraph agents, you can also set them up so that they persist their state. This allows you to do things like interact with an agent multiple times and have it remember previous interactions. This reference implementation shows how to use MongoDB as the backend for persisting checkpoint state. Make sure that you have MongoDB running on port 27017 for going through this guide. NOTE: this is just an reference implementation. You can implement your own checkpointer using a different database or modify this one as long as it conforms to the BaseCheckpointSaver interface.",What is required to create a custom checkpointer using MongoDB?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with LangGraph's built-in persistence layer?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What information can be obtained by calling graph.get_state_history(config)?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be used in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate within this system?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method used to ensure agents produce reliable results and how can it be applied in coding?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to create map-reduce branches for parallel execution,"Map-reduce operations are essential for efficient task decomposition and parallel processing. This approach involves breaking a task into smaller sub-tasks, processing each sub-task in parallel, and aggregating the results across all of the completed sub-tasks. Consider this example: given a general topic from the user, generate a list of related subjects, generate a joke for each subject, and select the best joke from the resulting list. In this design pattern, a first node may generate a list of objects (e.g., related subjects) and we want to apply some other node (e.g., generate a joke) to all those objects (e.g., subjects). However, two main challenges arise. (1) the number of objects (e.g., subjects) may be unknown ahead of time (meaning the number of edges may not be known) when we lay out the graph and (2) the input State to the downstream Node should be different (one for each generated object). LangGraph addresses these challenges through its Send API. By utilizing conditional edges, Send can distribute different states (e.g., subjects) to multiple instances of a node (e.g., joke generation). Importantly, the sent state can differ from the core graph's state, allowing for flexible and dynamic workflow management.",How does LangGraph address the challenges of unknown number of objects and different input states in map-reduce branches for parallel execution?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to stream state updates of your graph,"LangGraph Cloud supports multiple streaming modes. The main ones are: 
values: This streaming mode streams back values of the graph. This is the full state of the graph after each node is called.
updates: This streaming mode streams back updates to the graph. This is the update to the state of the graph after each node is called.
messages: This streaming mode streams back messages - both complete messages (at the end of a node) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications.
 This guide covers stream_mode=""updates"". First let's set up our client and thread: PythonJavascriptCURL

from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
# create thread
thread = await client.threads.create()
print(thread)

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
// create thread
const thread = await client.threads.create();
console.log(thread)

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

 Output: {'thread_id': '979e3c89-a702-4882-87c2-7a59a250ce16',
 'created_at': '2024-06-21T15:22:07.453100+00:00',
 'updated_at': '2024-06-21T15:22:07.453100+00:00',
 'metadata': {},
 'status': 'idle',
 'config': {}}
 Now we can stream by updates, which outputs updates made to the state by each node after it has executed: PythonJavascriptCURL

input = {
    ""messages"": [
        {
            ""role"": ""human"",
            ""content"": ""what's the weather in la""
        }
    ]
}
async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=input,
    stream_mode=""updates"",
):
    print(f""Receiving new event of type: {chunk.event}..."")
    print(chunk.data)
    print(""\n\n"")

const input = {
  ""messages"": [
    {
      ""role"": ""human"",
      ""content"": ""What's the weather in la"",
    }
  ]
}

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    streamMode: ""updates""
  }
);
for await (const chunk of streamResponse) {
  console.log(f""Receiving new event of type: {chunk.event}..."")
  console.log(chunk.data)
  console.log(""\n\n"")
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""What's the weather in la\""}]},
   \""stream_mode\"": [
     \""updates\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """") {
         print data_content ""\n""
     }
     sub(/^event: /, ""Receiving event of type: "", $0)
     printf ""%s...\n"", $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """") {
         print data_content ""\n""
     }
 }
 '

 Output: Receiving new event of type: metadata...
{'run_id': 'cfc96c16-ed9a-44bd-b5bb-c30e3c0725f0'}

Receiving new event of type: data...
{'agent': {'messages': [{'content': [{'id': 'toolu_0148tMmDK51iLQfG1yaNwRHM', 'input': {'query': 'weather in los angeles'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-1a9d32b0-7007-4a36-abde-8df812a0ed94', 'example': False, 'tool_calls': [{'name': 'tavily_search_results_json', 'args': {'query': 'weather in los angeles'}, 'id': 'toolu_0148tMmDK51iLQfG1yaNwRHM'}], 'invalid_tool_calls': []}]}}

Receiving new event of type: data...
{'action': {'messages': [{'content': '[{""url"": ""https://www.weatherapi.com/"", ""content"": ""{\'location\': {\'name\': \'Los Angeles\', \'region\': \'California\', \'country\': \'United States of America\', \'lat\': 34.05, \'lon\': -118.24, \'tz_id\': \'America/Los_Angeles\', \'localtime_epoch\': 1716062239, \'localtime\': \'2024-05-18 12:57\'}, \'current\': {\'last_updated_epoch\': 1716061500, \'last_updated\': \'2024-05-18 12:45\', \'temp_c\': 18.9, \'temp_f\': 66.0, \'is_day\': 1, \'condition\': {\'text\': \'Overcast\', \'icon\': \'//cdn.weatherapi.com/weather/64x64/day/122.png\', \'code\': 1009}, \'wind_mph\': 2.2, \'wind_kph\': 3.6, \'wind_degree\': 10, \'wind_dir\': \'N\', \'pressure_mb\': 1017.0, \'pressure_in\': 30.02, \'precip_mm\': 0.0, \'precip_in\': 0.0, \'humidity\': 65, \'cloud\': 100, \'feelslike_c\': 18.9, \'feelslike_f\': 66.0, \'vis_km\': 16.0, \'vis_miles\': 9.0, \'uv\': 6.0, \'gust_mph\': 7.5, \'gust_kph\': 12.0}}""}]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'tavily_search_results_json', 'id': 'a36e8cd1-0e96-4417-9c15-f10a945d2b42', 'tool_call_id': 'toolu_0148tMmDK51iLQfG1yaNwRHM'}]}}

Receiving new event of type: data...
{'agent': {'messages': [{'content': 'The weather in Los Angeles is currently overcast with a temperature of around 66F (18.9C). There are light winds from the north at around 2-3 mph. The humidity is 65% and visibility is good at 9 miles. Overall, mild spring weather conditions in LA.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-d5c1c2f0-b12d-41ce-990b-f36570e7483d', 'example': False, 'tool_calls': [], 'invalid_tool_calls': []}]}}

Receiving new event of type: end...
None
",What are the main streaming modes supported by LangGraph Cloud?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State using reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified in the code?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with LangGraph's built-in persistence layer?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using the graph.get_state method?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What method can be used to retrieve the state history of a graph in Python?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly in a graph and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
What does it mean to be agentic?,"Other people may talk about a system being an ""agent"" - we prefer to talk about systems being ""agentic"". But what does this actually mean? When we talk about systems being ""agentic"", we are talking about systems that use an LLM to decide the control flow of an application. There are different levels that an LLM can be used to decide the control flow, and this spectrum of ""agentic"" makes more sense to us than defining an arbitrary cutoff for what is or isn't an agent. Examples of using an LLM to decide the control of an application: 
Using an LLM to route between two potential paths
Using an LLM to decide which of many tools to call
Using an LLM to decide whether the generated answer is sufficient or more work is need
 The more times these types of decisions are made inside an application, the more agentic it is.
If these decisions are being made in a loop, then its even more agentic! There are other concepts often associated with being agentic, but we would argue these are a by-product of the above definition: 
Tool calling: this is often how LLMs make decisions
Action taking: often times, the LLMs' outputs are used as the input to an action
Memory: reliable systems need to have knowledge of things that occurred
Planning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.
",What does it mean to be agentic?
Why LangGraph?,"LangGraph has several core principles that we believe make it the most suitable framework for building agentic applications: 
Controllability
Human-in-the-Loop
Streaming First
 Controllability LangGraph is extremely low level. This gives you a high degree of control over what the system you are building actually does. We believe this is important because it is still hard to get agentic systems to work reliably, and we've seen that the more control you exercise over them, the more likely it is that they will ""work"". Human-in-the-Loop LangGraph comes with a built-in persistence layer as a first-class concept. This enables several different human-in-the-loop interaction patterns. We believe that ""Human-Agent Interaction"" patterns will be the new ""Human-Computer Interaction"", and have built LangGraph with built in persistence to enable this. Streaming First LangGraph comes with first class support for streaming. Agentic applications often take a while to run, and so giving the user some idea of what is happening is important, and streaming is a great way to do that. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.",What are the core principles of LangGraph that make it suitable for building agentic applications?
Deployment,"So you've built your LangGraph object - now what? Now you need to deploy it. 
There are many ways to deploy LangGraph objects, and the right solution depends on your needs and use case.
We'll highlight two ways here: using LangGraph Cloud or rolling your own solution. LangGraph Cloud is an opinionated way to deploy LangGraph objects from the LangChain team. Please see the LangGraph Cloud documentation for all the details about what it involves, to see if it is a good fit for you. If it is not a good fit, you may want to roll your own deployment. In this case, we would recommend using FastAPI to stand up a server. You can then call this graph from inside the FastAPI server as you see fit.",What are two ways to deploy LangGraph objects?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows and manage state using nodes and edges?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state and how can they be stored and updated effectively?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with LangGraph's built-in persistence layer?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Setup,"To start, we can setup our client with whatever URL you are hosting your graph from:",What can we do to setup our client with the URL hosting the graph?
SDK initialization,"First, we need to setup our client so that we can communicate with our hosted graph: PythonJavascriptCURL

from langgraph_sdk import get_client
client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = agent;
const thread = await client.threads.create();

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

","How can we initialize the SDK to communicate with our hosted graph using Python, JavaScript, and CURL?"
Find idle threads,"We can use the following commands to find threads that are idle, which means that all runs executed on the thread have finished running: PythonJavascriptCURL

print(await client.threads.search(status=""idle"",limit=1))

console.log(await client.threads.search({status: ""idle"",limit:1}));

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""status"": ""idle"", ""limit"": 1}'

 Output: [{'thread_id': 'cacf79bb-4248-4d01-aabc-938dbd60ed2c',
'created_at': '2024-08-14T17:36:38.921660+00:00',
'updated_at': '2024-08-14T17:36:38.921660+00:00',
'metadata': {'graph_id': 'agent'},
'status': 'idle',
'config': {'configurable': {}}}]
","How can we find idle threads using Python, Javascript, and CURL commands?"
Find interrupted threads,"We can use the following commands to find threads that have been interrupted in the middle of a run, which could either mean an error occurred before the run finished or a human-in-the-loop breakpoint was reached and the run is waiting to continue:  PythonJavascriptCURL

print(await client.threads.search(status=""interrupted"",limit=1))

console.log(await client.threads.search({status: ""interrupted"",limit:1}));

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""status"": ""interrupted"", ""limit"": 1}'

 Output: [{'thread_id': '0d282b22-bbd5-4d95-9c61-04dcc2e302a5',
'created_at': '2024-08-14T17:41:50.235455+00:00',
'updated_at': '2024-08-14T17:41:50.235455+00:00',
'metadata': {'graph_id': 'agent'},
'status': 'interrupted',
'config': {'configurable': {}}}]
","How can we find interrupted threads using Python, Javascript, and CURL commands?"
Find busy threads,"We can use the following commands to find threads that are busy, meaning they are currently handling the execution of a run: PythonJavascriptCURL

print(await client.threads.search(status=""busy"",limit=1))

console.log(await client.threads.search({status: ""busy"",limit: 1}));

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""status"": ""busy"", ""limit"": 1}'

 Output: [{'thread_id': '0d282b22-bbd5-4d95-9c61-04dcc2e302a5',
'created_at': '2024-08-14T17:41:50.235455+00:00',
'updated_at': '2024-08-14T17:41:50.235455+00:00',
'metadata': {'graph_id': 'agent'},
'status': 'busy',
'config': {'configurable': {}}}]
",How can we find busy threads using the provided commands?
Find specific threads,"You may also want to check the status of specific threads, which you can do in a few ways:",How can you check the status of specific threads?
Find by ID,"You can use the get function to find the status of a specific thread, as long as you have the ID saved PythonJavascriptCURL

print((await client.threads.get(<THREAD_ID>))['status'])

console.log((await client.threads.get(<THREAD_ID>)).status);

curl --request GET \ 
--url <DEPLOYMENT_URL>/threads/<THREAD_ID> \
--header 'Content-Type: application/json' | jq -r '.status'

 Output: 'idle'
","What is the status of a specific thread that can be found using its ID in Python, JavaScript, or CURL?"
Find by metadata,"The search endpoint for threads also allows you to filter on metadata, which can be helpful if you use metadata to tag threads in order to keep them organized: PythonJavascriptCURL

print((await client.threads.search(metadata={""foo"":""bar""},limit=1))[0]['status'])

console.log((await client.threads.search({metadata: {""foo"":""bar""},limit: 1}))[0].status);

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""metadata"": {""foo"":""bar""}, ""limit"": 1}' | jq -r '.[0].status'

 Output: 'idle'
","What is the status of the thread when searching by metadata with the key ""foo"" and value ""bar""?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Self-RAG,"Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. In the paper, a few decisions are made: 
Should I retrieve from retriever, R -
 
Input: x (question) OR x (question), y (generation)
Decides when to retrieve D chunks with R
Output: yes, no, continue
 
Are the retrieved passages D relevant to the question x -
 

Input: (x (question), d (chunk)) for d in D

d provides useful information to solve x
Output: relevant, irrelevant
 
Are the LLM generation from each chunk in D is relevant to the chunk (hallucinations, etc)  -
 
Input: x (question), d (chunk),  y (generation) for d in D
All of the verification-worthy statements in y (generation) are supported by d
Output: {fully supported, partially supported, no support
 
The LLM generation from each chunk in D is a useful response to x (question) -
 
Input: x (question), y (generation) for d in D
y (generation) is a useful response to x (question).
Output: {5, 4, 3, 2, 1}
 We will implement some of these ideas from scratch using LangGraph.",Are the LLM generation from each chunk in D a useful response to the question x?
Tracing,"Optionally, use LangSmith for tracing (shown at bottom)","What tool can be used for tracing, as shown at the bottom?"
Retriever,Let's index 3 blog posts.,"What is the purpose of the ""Retriever"" heading and text?"
Graph,Capture the flow in as a graph.,What can be done to capture the flow as a graph?
Build Graph,The just follows the flow we outlined in the figure above.,What does the build graph process follow according to the figure above?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with LangGraph's built-in persistence layer?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Web Voyager,"WebVoyager by He, et. al., is a vision-enabled web-browsing agent capable of controlling the mouse and keyboard. It works by viewing annotated browser screenshots for each turn, then choosing the next step to take. The agent architecture is a basic reasoning and action (ReAct) loop.
The unique aspects of this agent are: 
It's usage of Set-of-Marks-like image annotations to serve as UI affordances for the agent
It's application in the browser by using tools to control both the mouse and keyboard
 The overall design looks like the following:","What are the unique aspects of the WebVoyager web-browsing agent by He, et. al.?"
Configure environment,"We will first set up LangSmith tracing. Though optional, this lets us inspect and debug agent's trajectory for a given input. You can sign up at smith.langchain.com to get an API key.",What is the first step to configure the environment for LangSmith tracing?
Define Graph State,"The state provides the inputs to each node in the graph. In our case, the agent will track the webpage object (within the browser), annotated images + bounding boxes, the user's initial request, and the messages containing the agent scratchpad, system prompt, and other information.",What inputs does the state provide to each node in the graph?
Define tools,"The agent has 6 simple tools: 
Click (at labeled box)
Type
Scroll
Wait
Go back
Go to search engine (Google)
 We define them below here as functions:",What are the 6 simple tools defined as functions by the agent?
Define Agent,"The agent is driven by a multi-modal model and decides the action to take for each step. It is composed of a few runnable objects: 
A mark_page function to annotate the current page with bounding boxes
A prompt to hold the user question, annotated image, and agent scratchpad
GPT-4V to decide the next steps
Parsing logic to extract the action
 Let's first define the annotation step: Browser Annotations This function annotates all buttons, inputs, text areas, etc. with numbered bounding boxes. GPT-4V then just has to refer to a bounding box
when taking actions, reducing the complexity of the overall task.",What components make up the agent driven by a multi-modal model?
Define graph,We've created most of the important logic. We have one more function to define that will help us update the graph state after a tool is called.,What is the purpose of the function that still needs to be defined in relation to the graph state?
Run agent,"Now that we've created the whole agent executor, we can run it on a few questions! We'll start our browser at ""google.com"" and then let it control the rest. Below is a helper function to help print out the steps to the notebook (and display the intermediate screenshots).",What steps are involved in running the agent executor on a few questions?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes based on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What method can be used to retrieve the state history of a graph in Python?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to add human-in-the-loop processes to the prebuilt ReAct agent,"This tutorial will show how to add human-in-the-loop processes to the prebuilt ReAct agent. Please see this tutorial for how to get started with the prebuilt ReAct agent You can add a a breakpoint before tools are called by passing interrupt_before=[""tools""] to create_react_agent. Note that you need to be using a checkpointer for this to work.",How can human-in-the-loop processes be added to the prebuilt ReAct agent?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Conceptual Guides,"In this guide we will explore the concepts behind build agentic and multi-agent systems with LangGraph. We assume you have already learned the basic covered in the introduction tutorial and want to deepen your understanding of LangGraph's underlying design and inner workings. There are three main parts to this concept guide. First, we'll discuss at a very high level what it means to be agentic. Next, we'll look at lower-level concepts in LangGraph that are core for understanding how to build your own agentic systems. Finally, we'll discuss common agentic patterns and how you can achieve those with LangGraph. These will be mostly conceptual guides - for more technical, hands-on guides see our how-to guides LangGraph for Agentic Applications 
What does it mean to be agentic?
Why LangGraph
Deployment
 Low Level Concepts 
Graphs
StateGraph
MessageGraph
Compiling Your Graph

State
Schema
Reducers
MessageState

Nodes
START node
END node

Edges
Normal Edges
Conditional Edges
Entry Point
Conditional Entry Point

Send
Checkpointer
Threads
Checkpointer states
Get state
Get state history
Update state

Configuration
Visualization
Streaming
 Common Agentic Patterns 
Structured output
Tool calling
Memory
Human in the loop
Approval
Wait for input
Edit agent actions
Time travel

Map-Reduce
Multi-agent
Planning
Reflection
Off-the-shelf ReAct Agent
",What does it mean to be agentic?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output from LLMs inside nodes be used when building agents in LangGraph?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing memory?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent be programmed to wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation using the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",What is a common technique used by agentic systems to overcome the struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method used to check if an agent has completed a task correctly and provide feedback for improvement?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in the interaction pattern described?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What actions can be taken when reviewing tool calls?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in LangGraph to look back at previous checkpoints and resume execution from a desired point?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate within this system?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method used to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
What does it mean to be agentic?,"Other people may talk about a system being an ""agent"" - we prefer to talk about systems being ""agentic"". But what does this actually mean? When we talk about systems being ""agentic"", we are talking about systems that use an LLM to decide the control flow of an application. There are different levels that an LLM can be used to decide the control flow, and this spectrum of ""agentic"" makes more sense to us than defining an arbitrary cutoff for what is or isn't an agent. Examples of using an LLM to decide the control of an application: 
Using an LLM to route between two potential paths
Using an LLM to decide which of many tools to call
Using an LLM to decide whether the generated answer is sufficient or more work is need
 The more times these types of decisions are made inside an application, the more agentic it is.
If these decisions are being made in a loop, then its even more agentic! There are other concepts often associated with being agentic, but we would argue these are a by-product of the above definition: 
Tool calling: this is often how LLMs make decisions
Action taking: often times, the LLMs' outputs are used as the input to an action
Memory: reliable systems need to have knowledge of things that occurred
Planning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.
",What does it mean to be agentic?
Why LangGraph?,"LangGraph has several core principles that we believe make it the most suitable framework for building agentic applications: 
Controllability
Human-in-the-Loop
Streaming First
 Controllability LangGraph is extremely low level. This gives you a high degree of control over what the system you are building actually does. We believe this is important because it is still hard to get agentic systems to work reliably, and we've seen that the more control you exercise over them, the more likely it is that they will ""work"". Human-in-the-Loop LangGraph comes with a built-in persistence layer as a first-class concept. This enables several different human-in-the-loop interaction patterns. We believe that ""Human-Agent Interaction"" patterns will be the new ""Human-Computer Interaction"", and have built LangGraph with built in persistence to enable this. Streaming First LangGraph comes with first class support for streaming. Agentic applications often take a while to run, and so giving the user some idea of what is happening is important, and streaming is a great way to do that. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.",What are the core principles of LangGraph that make it suitable for building agentic applications?
Deployment,"So you've built your LangGraph object - now what? Now you need to deploy it. 
There are many ways to deploy LangGraph objects, and the right solution depends on your needs and use case.
We'll highlight two ways here: using LangGraph Cloud or rolling your own solution. LangGraph Cloud is an opinionated way to deploy LangGraph objects from the LangChain team. Please see the LangGraph Cloud documentation for all the details about what it involves, to see if it is a good fit for you. If it is not a good fit, you may want to roll your own deployment. In this case, we would recommend using FastAPI to stand up a server. You can then call this graph from inside the FastAPI server as you see fit.",What are two ways to deploy LangGraph objects?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Code generation with RAG and self-correction,"AlphaCodium presented an approach for code generation that uses control flow. Main idea: construct an answer to a coding question iteratively.. AlphaCodium iteravely tests and improves an answer on public and AI-generated tests for a particular question. We will implement some of these ideas from scratch using LangGraph: 
We start with a set of documentation specified by a user
We use a long context LLM to ingest it and perform RAG to answer a question based upon it
We will invoke a tool to produce a structured output
We will perform two unit tests (check imports and code execution) prior returning the solution to the user
",What approach does AlphaCodium use for code generation and self-correction?
Docs,Load LangChain Expression Language (LCEL) docs as an example.,What can be loaded as an example in the LangChain Expression Language (LCEL) docs?
Code solution,Try OpenAI and Claude3 with function calling. Create code_gen_chain w/ either OpenAI or Claude and test here.,What tools can be used for function calling in the code_gen_chain and where can it be tested?
State,"Our state is a dict that will contain keys (errors, question, code generation) relevant to code generation.",What keys are contained in the state dict relevant to code generation?
Graph,Our graph lays out the logical flow shown in the figure above.,What does the graph layout in the figure above show?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to visualize your graph,This notebook walks through how to visualize the graphs you create. This works with ANY Graph.,How can you visualize the graphs you create in a notebook?
Set up Graph,"You can visualize any arbitrary Graph, including StateGraph's and MessageGraph's. Let's have some fun by drawing fractals :).",What types of graphs can be visualized using the set up Graph feature?
Ascii,We can easily visualize this graph in ascii,How can we visualize the graph mentioned in the text?
Mermaid,We can also convert a graph class into Mermaid syntax.,What can be converted into Mermaid syntax?
PNG,"If preferred, we could render the Graph into a  .png. Here we could use three options: 
Using Mermaid.ink API (does not require additional packages)
Using Mermaid + Pyppeteer (requires pip install pyppeteer)
Using graphviz (which requires pip install graphviz)
",What are the three options for rendering a Graph into a .png file?
Using Mermaid.Ink,"By default, draw_mermaid_png() uses Mermaid.Ink's API to generate the diagram.",What API does draw_mermaid_png() use by default to generate the diagram?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
LangGraph Studio With Local Deployment,"
Browser Compatibility
Viewing the studio page of a local LangGraph deployment does not work in Safari. Use Chrome instead.
",What browser should be used instead of Safari to view the studio page of a local LangGraph deployment?
Setup,"Make sure you have setup your app correctly, by creating a compiled graph, a .env file with any environment variables, and a langgraph.json config file that points to your environment file and compiled graph. See here for more detailed instructions. After you have your app setup, head into the directory with your langgraph.json file and call langgraph up -c langgraph.json --watch to start the API server in watch mode which means it will restart on code changes, which is ideal for local testing. If the API server start correctly you should see logs that look something like this: Ready!
- API: http://localhost:8123
2024-06-26 19:20:41,056:INFO:uvicorn.access 127.0.0.1:44138 - ""GET /ok HTTP/1.1"" 200
 Read this reference to learn about all the options for starting the API server.",What command should you use to start the API server in watch mode after setting up your app correctly?
Access Studio,"Once you have successfully started the API server, you can access the studio by going to the following URL: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8123 (see warning above if using Safari). If everything is working correctly you should see the studio show up looking something like this (with your graph diagram on the left hand side):",What URL should you visit to access the studio after successfully starting the API server?
Use the Studio for Testing,"To learn about how to use the studio for testing, read the LangGraph Studio how-tos.",What resource should be consulted to learn how to use the studio for testing?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to stream messages from your graph,"LangGraph Cloud supports multiple streaming modes. The main ones are: 
values: This streaming mode streams back values of the graph. This is the full state of the graph after each node is called.
updates: This streaming mode streams back updates to the graph. This is the update to the state of the graph after each node is called.
messages: This streaming mode streams back messages - both complete messages (at the end of a node) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications.
 This guide covers stream_mode=""messages"". In order to use this mode, the state of the graph you are interacting with MUST have a messages key that is a list of messages.
E.g., the state should look something like: Python

from typing import TypedDict, Annotated
from langgraph.graph import add_messages
from langchain_core.messages import AnyMessage

class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

 Alternatively, you can use an instance or subclass of from langgraph.graph import MessagesState (MessagesState is equivalent to the implementation above). 
Note
LangGraph Cloud only supports hosting graphs written in Python at the moment.
 With stream_mode=""messages"" two things will be streamed back: 
It outputs messages produced by any chat model called inside (unless tagged in a special way)
It outputs messages returned from nodes (to allow for nodes to return ToolMessages and the like
 First let's set up our client and thread: PythonJavascriptCURL

from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
# create thread
thread = await client.threads.create()
print(thread)

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
// create thread
const thread = await client.threads.create();
console.log(thread)

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

 Output: {'thread_id': 'e1431c95-e241-4d1d-a252-27eceb1e5c86',
 'created_at': '2024-06-21T15:48:59.808924+00:00',
 'updated_at': '2024-06-21T15:48:59.808924+00:00',
 'metadata': {},
 'status': 'idle',
 'config': {}}
 Let's also define a helper function for better formatting of the tool calls in messages (for CURL we will define a helper script called process_stream.sh) PythonJavascriptCURL

def format_tool_calls(tool_calls):
    if tool_calls:
        formatted_calls = []
        for call in tool_calls:
            formatted_calls.append(
                f""Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}""
            )
        return ""\n"".join(formatted_calls)
    return ""No tool calls""

function formatToolCalls(toolCalls) {
  if (toolCalls && toolCalls.length > 0) {
    const formattedCalls = toolCalls.map(call => {
      return `Tool Call ID: ${call.id}, Function: ${call.name}, Arguments: ${call.args}`;
    });
    return formattedCalls.join(""\n"");
  }
  return ""No tool calls"";
}

# process_stream.sh

format_tool_calls() {
    echo ""$1"" | jq -r 'map(""Tool Call ID: \(.id), Function: \(.name), Arguments: \(.args)"") | join(""\n"")'
}

process_data_item() {
    local data_item=""$1""

    if echo ""$data_item"" | jq -e '.role == ""user""' > /dev/null; then
        echo ""Human: $(echo ""$data_item"" | jq -r '.content')""
    else
        local tool_calls=$(echo ""$data_item"" | jq -r '.tool_calls // []')
        local invalid_tool_calls=$(echo ""$data_item"" | jq -r '.invalid_tool_calls // []')
        local content=$(echo ""$data_item"" | jq -r '.content // """"')
        local response_metadata=$(echo ""$data_item"" | jq -r '.response_metadata // {}')

        if [ -n ""$content"" ] && [ ""$content"" != ""null"" ]; then
            echo ""AI: $content""
        fi

        if [ ""$tool_calls"" != ""[]"" ]; then
            echo ""Tool Calls:""
            format_tool_calls ""$tool_calls""
        fi

        if [ ""$invalid_tool_calls"" != ""[]"" ]; then
            echo ""Invalid Tool Calls:""
            format_tool_calls ""$invalid_tool_calls""
        fi

        if [ ""$response_metadata"" != ""{}"" ]; then
            local finish_reason=$(echo ""$response_metadata"" | jq -r '.finish_reason // ""N/A""')
            echo ""Response Metadata: Finish Reason - $finish_reason""
        fi
    fi
}

while IFS=': ' read -r key value; do
    case ""$key"" in
        event)
            event=""$value""
            ;;
        data)
            if [ ""$event"" = ""metadata"" ]; then
                run_id=$(echo ""$value"" | jq -r '.run_id')
                echo ""Metadata: Run ID - $run_id""
                echo ""------------------------------------------------""
            elif [ ""$event"" = ""messages/partial"" ]; then
                echo ""$value"" | jq -c '.[]' | while read -r data_item; do
                    process_data_item ""$data_item""
                done
                echo ""------------------------------------------------""
            fi
            ;;
    esac
done

 Now we can stream by messages, which will return complete messages (at the end of node execution) as well as tokens for any messages generated inside a node: PythonJavascriptCURL

input = {""messages"": [{""role"": ""user"", ""content"": ""what's the weather in sf""}]}
config = {""configurable"": {""model_name"": ""openai""}}

async for event in client.runs.stream(
    thread[""thread_id""],
    assistant_id=""agent"",
    input=input,
    config=config,
    stream_mode=""messages"",
):
    if event.event == ""metadata"":
        print(f""Metadata: Run ID - {event.data['run_id']}"")
        print(""-"" * 50)
    elif event.event == ""messages/partial"":
        for data_item in event.data:
            if ""role"" in data_item and data_item[""role""] == ""user"":
                print(f""Human: {data_item['content']}"")
            else:
                tool_calls = data_item.get(""tool_calls"", [])
                invalid_tool_calls = data_item.get(""invalid_tool_calls"", [])
                content = data_item.get(""content"", """")
                response_metadata = data_item.get(""response_metadata"", {})

                if content:
                    print(f""AI: {content}"")

                if tool_calls:
                    print(""Tool Calls:"")
                    print(format_tool_calls(tool_calls))

                if invalid_tool_calls:
                    print(""Invalid Tool Calls:"")
                    print(format_tool_calls(invalid_tool_calls))

                if response_metadata:
                    finish_reason = response_metadata.get(""finish_reason"", ""N/A"")
                    print(f""Response Metadata: Finish Reason - {finish_reason}"")
        print(""-"" * 50)

const input = {
  ""messages"": [
    {
      ""role"": ""human"",
      ""content"": ""What's the weather in sf"",
    }
  ]
}
const config = {""configurable"": {""model_name"": ""openai""}}

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    config,
    streamMode: ""messages""
  }
);
for await (const event of streamResponse) {
  if (event.event === ""metadata"") {
    console.log(`Metadata: Run ID - ${event.data.run_id}`);
    console.log(""-"".repeat(50));
  } else if (event.event === ""messages/partial"") {
    event.data.forEach(dataItem => {
      if (dataItem.role && dataItem.role === ""user"") {
        console.log(`Human: ${dataItem.content}`);
      } else {
        const toolCalls = dataItem.tool_calls || [];
        const invalidToolCalls = dataItem.invalid_tool_calls || [];
        const content = dataItem.content || """";
        const responseMetadata = dataItem.response_metadata || {};

        if (content) {
          console.log(`AI: ${content}`);
        }

        if (toolCalls.length > 0) {
          console.log(""Tool Calls:"");
          console.log(formatToolCalls(toolCalls));
        }

        if (invalidToolCalls.length > 0) {
          console.log(""Invalid Tool Calls:"");
          console.log(formatToolCalls(invalidToolCalls));
        }

        if (responseMetadata) {
          const finishReason = responseMetadata.finish_reason || ""N/A"";
          console.log(`Response Metadata: Finish Reason - ${finishReason}`);
        }
      }
    });
    console.log(""-"".repeat(50));
  }
}

curl --request POST \
--url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
--header 'Content-Type: application/json' \
--data ""{
\""assistant_id\"": \""agent\"",
\""config\"":{\""configurable\"":{\""model_name\"":\""openai\""}},
\""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""What's the weather in sf\""}]},
\""stream_mode\"": [
\""messages\""
]
}"" | sed 's/\r$//' | ./process_stream.sh

 Output: Metadata: Run ID - 1ef2fe5c-6a1d-6575-bc09-d7832711c17e
--------------------------------------------------
Invalid Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: 
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': ''}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': 'current'}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': 'current weather'}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': 'current weather in'}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': 'current weather in San'}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': 'current weather in San Francisco'}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': 'current weather in San Francisco'}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_cg14F20jMBqWYrNgEkdWHwB3, Function: tavily_search_results_json, Arguments: {'query': 'current weather in San Francisco'}
Response Metadata: Finish Reason - tool_calls
--------------------------------------------------
--------------------------------------------------
AI: The
--------------------------------------------------
AI: The current
--------------------------------------------------
AI: The current weather
--------------------------------------------------
AI: The current weather in
--------------------------------------------------
AI: The current weather in San
--------------------------------------------------
AI: The current weather in San Francisco
--------------------------------------------------
AI: The current weather in San Francisco is
--------------------------------------------------
AI: The current weather in San Francisco is over
--------------------------------------------------
AI: The current weather in San Francisco is overcast
--------------------------------------------------
AI: The current weather in San Francisco is overcast with
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F).
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-s
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-south
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 k
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph).
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%,
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles).
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The UV
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The UV index
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The UV index is
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The UV index is 
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The UV index is 3
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The UV index is 3.
--------------------------------------------------
AI: The current weather in San Francisco is overcast with a temperature of 13.9C (57.0F). The wind is blowing from the south-southwest at 6.9 mph (11.2 kph). The humidity is at 81%, and the visibility is 16 km (9 miles). The UV index is 3.
Response Metadata: Finish Reason - stop
--------------------------------------------------
",What are the main streaming modes supported by LangGraph Cloud?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
With LangGraph Cloud,Let's now reproduce the same using LangGraph Cloud. Note that instead of using a checkpointer we just create a new thread on the backend and pass the ID to the API,What method is used in LangGraph Cloud instead of using a checkpointer?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to stream arbitrary nested content,"The most common use case for streaming from inside a node is to stream LLM tokens, but you may have other long-running streaming functions you wish to render for the user. While individual nodes in LangGraph cannot return generators (since they are executed to completion for each superstep), we can still stream arbitrary custom functions from within a node using a similar tact and calling astream_events on the graph. We do so using a RunnableGenerator (which your function will automatically behave as if wrapped as a RunnableLambda). Below is a simple toy example.",How can arbitrary nested content be streamed from within a node in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to define input/output schema for your graph,"By default, StateGraph takes in a single schema and all nodes are expected to communicate with that schema. However, it is also possible to define explicit input and output schemas for a graph. This is helpful if you want to draw a distinction between input and output keys. In this notebook we'll walk through an example of this. At a high level, in order to do this you simply have to pass in input=..., output=... when defining the graph. Let's see an example below!",How can you define input/output schema for a graph in StateGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",What is a common technique used by agentic systems to overcome the struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method for agents to improve their reliability and accuracy in completing tasks?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Complex data extraction with function calling,"Function calling is a core primitive for integrating LLMs within your software stack. We use it throughout the LangGraph docs, since developing with function calling (aka tool usage) tends to be much more stress-free than the traditional way of writing custom string parsers. However, even GPT-4, Opus, and other powerful models still struggle with complex functions, especially if your schema involves any nesting or if you have more advanced data validation rules. There are three basic ways to increase reliability: better prompting, constrained decoding, and validation with re-prompting. We will cover two approaches to the last technique here, since it is generally applicable across any LLM that supports tool calling.",What are the three basic ways to increase reliability when dealing with complex data extraction using function calling?
Regular Extraction with Retries,"Both examples here invoke a simple looping graph that takes following approach: 
Prompt the LLM to respond.
If it responds with tool calls, validate those.
If the calls are correct, return. Otherwise, format the validation error as a new ToolMessage and prompt the LLM to fix the errors. Taking us back to step (1).
 The techniques differ only on step (3). In this first step, we will prompt the original LLM to regenerate the function calls to fix the validation errors. In the next section, we will instead prompt the LLM to generate a patch to fix the errors, meaning it doesn't have to re-generate data that is valid.",What approach is taken in the first step of the regular extraction with retries process when validation errors occur?
Try it out,Now we'll ask our model to call a function. We'll add a validator to illustrate how the LLM is able to use the validation error to fix its results.,What will the model do when a function is called and a validator is added to illustrate its ability to use validation errors to fix results?
JSONPatch,"The regular retry method worked well for our simple case, but it still was unable to self-correct when populating a complex schema. LLMs work best on narrow tasks. A tried-and-true principle of LLM interface design is to simplify the task for each LLM run. One way to do this is to patch the state instead of completely regenerating the state. One way to do this is with JSONPatch operations. Let's try it out! Below, create a JSONPatch retry graph. This works as follows: 
First pass: try to generate the full output.
Retries: prompt the LLM to generate JSON patches on top of the first output to heal the erroneous generation.
 The fallback LLM just has to generate a list of paths, ops (add, remove, replace), and optional values. Since the pydantic validation errors include the path in their errors, the LLM should be more reliable.",What is one way to simplify the task for each LLM run when populating a complex schema?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Why is my project failing to start?,"There are a few reasons that your project might fail to start, here are some of the most common ones.",Why might a project fail to start?
Docker issues,"LangGraph Studio requires Docker Desktop version 4.24 or higher. Please make sure you have a version of Docker installed that satisfies that requirement and also make sure you have the Docker Desktop app up and running before trying to use LangGraph Studio. In addition, make sure you have docker-compose updated to version 2.22.0 or higher.",What version of Docker Desktop and docker-compose is required for LangGraph Studio?
Configuration or environment issues,"Another reason your project might fail to start is because your configuration file is defined incorrectly, or you are missing required environment variables. ",What are some reasons why a project might fail to start according to the heading and text provided?
How does interrupt work?,When you select the Interrupts dropdown and select a node to interrupt the graph will pause execution before and after (unless the node goes straight to END) that node has run. This means that you will be able to both edit the state before the node is ran and the state after the node has ran. This is intended to allow developers more fine-grained control over the behavior of a node and make it easier to observe how the node is behaving. You will not be able to edit the state after the node has ran if the node is the final node in the graph.,How does the interrupt feature work in the graph execution process?
How do I reload the app?,"If you would like to reload the app, don't use Command+R as you might normally do. Instead, close and reopen the app for a full refresh.",How should I reload the app for a full refresh?
How does automatic rebuilding work?,One of the key features of LangGraph Studio is that it automatically rebuilds your image when you change the source code. This allows for a super fast development and testing cycle which makes it easy to iterate on your graph. There are two different ways that LangGraph rebuilds your image: either by editing the image or completely rebuilding it.,How does LangGraph Studio automatically rebuild your image when you change the source code?
Rebuilds from source code changes,"If you modified the source code only (no configuration or dependency changes!) then the image does not require a full rebuild, and LangGraph Studio will only update the relevant parts. The UI status in the bottom left will switch from Online to Stopping temporarily while the image gets edited. The logs will be shown as this process is happening, and after the image has been edited the status will change back to Online and you will be able to run your graph with the modified code!",What happens to the UI status in LangGraph Studio when the source code is modified?
Rebuilds from configuration or dependency changes,"If you edit your graph configuration file (langgraph.json) or the dependencies (either pyproject.toml or requirements.txt) then the entire image will be rebuilt. This will cause the UI to switch away from the graph view and start showing the logs of the new image building process. This can take a minute or two, and once it is done your updated image will be ready to use!",What happens if you make changes to the graph configuration file or dependencies in the project?
Why is my graph taking so long to startup?,"The LangGraph Studio interacts with a local LangGraph API server. To stay aligned with ongoing updates, the LangGraph API requires regular rebuilding. As a result, you may occasionally experience slight delays when starting up your project.",Why might my graph be taking longer than usual to startup?
Why are extra edges showing up in my graph?,"If you don't define your conditional edges carefully, you might notice extra edges appearing in your graph. This is because without proper definition, LangGraph Studio assumes the conditional edge could access all other nodes. In order for this to not be the case, you need to be explicit about how you define the nodes the conditional edge routes to. There are two ways you can do this:",Why might extra edges show up in a graph in LangGraph Studio?
Solution 1: Include a path map,"The first way to solve this is to add path maps to your conditional edges. A path map is just a dictionary that maps the possible outputs of your router function with the names of the nodes that each output corresponds to. The path map is passed as the third argument to the add_conditional_edges function like so: graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
 In this case, the routing function returns either True or False, which map to node_b and node_c respectively.",How can you solve this by including a path map?
Solution 2: Update the typing of the router,"Instead of passing a path map, you can also be explicit about the typing of your routing function by specifying the nodes it can map to using the Literal python definition. Here is an example of how to define a routing function in that way: def routing_function(state: GraphState) -> Literal[""node_b"",""node_c""]:
    if state['some_condition'] == True:
        return ""node_a""
    else:
        return ""node_b""
",How can you update the typing of the router in Solution 2?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to create a custom checkpointer using MongoDB,"When creating LangGraph agents, you can also set them up so that they persist their state. This allows you to do things like interact with an agent multiple times and have it remember previous interactions. This reference implementation shows how to use MongoDB as the backend for persisting checkpoint state. Make sure that you have MongoDB running on port 27017 for going through this guide. NOTE: this is just an reference implementation. You can implement your own checkpointer using a different database or modify this one as long as it conforms to the BaseCheckpointSaver interface.",What is required to create a custom checkpointer using MongoDB?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the main graph class used in StateGraph and what is it parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified in the code using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes based on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",How does LangGraph support the design pattern of map-reduce with conditional edges and Send objects?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in the interaction pattern described?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents through the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",What is a common technique used by agentic systems to overcome the struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step taken to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette selection?
How to create map-reduce branches for parallel execution,"Map-reduce operations are essential for efficient task decomposition and parallel processing. This approach involves breaking a task into smaller sub-tasks, processing each sub-task in parallel, and aggregating the results across all of the completed sub-tasks. Consider this example: given a general topic from the user, generate a list of related subjects, generate a joke for each subject, and select the best joke from the resulting list. In this design pattern, a first node may generate a list of objects (e.g., related subjects) and we want to apply some other node (e.g., generate a joke) to all those objects (e.g., subjects). However, two main challenges arise. (1) the number of objects (e.g., subjects) may be unknown ahead of time (meaning the number of edges may not be known) when we lay out the graph and (2) the input State to the downstream Node should be different (one for each generated object). LangGraph addresses these challenges through its Send API. By utilizing conditional edges, Send can distribute different states (e.g., subjects) to multiple instances of a node (e.g., joke generation). Importantly, the sent state can differ from the core graph's state, allowing for flexible and dynamic workflow management.",How does LangGraph address the challenges of unknown number of objects and different input states in map-reduce branches for parallel execution?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to stream state updates of your graph,"LangGraph Cloud supports multiple streaming modes. The main ones are: 
values: This streaming mode streams back values of the graph. This is the full state of the graph after each node is called.
updates: This streaming mode streams back updates to the graph. This is the update to the state of the graph after each node is called.
messages: This streaming mode streams back messages - both complete messages (at the end of a node) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications.
 This guide covers stream_mode=""updates"". First let's set up our client and thread: PythonJavascriptCURL

from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
# create thread
thread = await client.threads.create()
print(thread)

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
// create thread
const thread = await client.threads.create();
console.log(thread)

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

 Output: {'thread_id': '979e3c89-a702-4882-87c2-7a59a250ce16',
 'created_at': '2024-06-21T15:22:07.453100+00:00',
 'updated_at': '2024-06-21T15:22:07.453100+00:00',
 'metadata': {},
 'status': 'idle',
 'config': {}}
 Now we can stream by updates, which outputs updates made to the state by each node after it has executed: PythonJavascriptCURL

input = {
    ""messages"": [
        {
            ""role"": ""human"",
            ""content"": ""what's the weather in la""
        }
    ]
}
async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=input,
    stream_mode=""updates"",
):
    print(f""Receiving new event of type: {chunk.event}..."")
    print(chunk.data)
    print(""\n\n"")

const input = {
  ""messages"": [
    {
      ""role"": ""human"",
      ""content"": ""What's the weather in la"",
    }
  ]
}

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    streamMode: ""updates""
  }
);
for await (const chunk of streamResponse) {
  console.log(f""Receiving new event of type: {chunk.event}..."")
  console.log(chunk.data)
  console.log(""\n\n"")
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""What's the weather in la\""}]},
   \""stream_mode\"": [
     \""updates\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """") {
         print data_content ""\n""
     }
     sub(/^event: /, ""Receiving event of type: "", $0)
     printf ""%s...\n"", $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """") {
         print data_content ""\n""
     }
 }
 '

 Output: Receiving new event of type: metadata...
{'run_id': 'cfc96c16-ed9a-44bd-b5bb-c30e3c0725f0'}

Receiving new event of type: data...
{'agent': {'messages': [{'content': [{'id': 'toolu_0148tMmDK51iLQfG1yaNwRHM', 'input': {'query': 'weather in los angeles'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-1a9d32b0-7007-4a36-abde-8df812a0ed94', 'example': False, 'tool_calls': [{'name': 'tavily_search_results_json', 'args': {'query': 'weather in los angeles'}, 'id': 'toolu_0148tMmDK51iLQfG1yaNwRHM'}], 'invalid_tool_calls': []}]}}

Receiving new event of type: data...
{'action': {'messages': [{'content': '[{""url"": ""https://www.weatherapi.com/"", ""content"": ""{\'location\': {\'name\': \'Los Angeles\', \'region\': \'California\', \'country\': \'United States of America\', \'lat\': 34.05, \'lon\': -118.24, \'tz_id\': \'America/Los_Angeles\', \'localtime_epoch\': 1716062239, \'localtime\': \'2024-05-18 12:57\'}, \'current\': {\'last_updated_epoch\': 1716061500, \'last_updated\': \'2024-05-18 12:45\', \'temp_c\': 18.9, \'temp_f\': 66.0, \'is_day\': 1, \'condition\': {\'text\': \'Overcast\', \'icon\': \'//cdn.weatherapi.com/weather/64x64/day/122.png\', \'code\': 1009}, \'wind_mph\': 2.2, \'wind_kph\': 3.6, \'wind_degree\': 10, \'wind_dir\': \'N\', \'pressure_mb\': 1017.0, \'pressure_in\': 30.02, \'precip_mm\': 0.0, \'precip_in\': 0.0, \'humidity\': 65, \'cloud\': 100, \'feelslike_c\': 18.9, \'feelslike_f\': 66.0, \'vis_km\': 16.0, \'vis_miles\': 9.0, \'uv\': 6.0, \'gust_mph\': 7.5, \'gust_kph\': 12.0}}""}]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'tavily_search_results_json', 'id': 'a36e8cd1-0e96-4417-9c15-f10a945d2b42', 'tool_call_id': 'toolu_0148tMmDK51iLQfG1yaNwRHM'}]}}

Receiving new event of type: data...
{'agent': {'messages': [{'content': 'The weather in Los Angeles is currently overcast with a temperature of around 66F (18.9C). There are light winds from the north at around 2-3 mph. The humidity is 65% and visibility is good at 9 miles. Overall, mild spring weather conditions in LA.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-d5c1c2f0-b12d-41ce-990b-f36570e7483d', 'example': False, 'tool_calls': [], 'invalid_tool_calls': []}]}}

Receiving new event of type: end...
None
",What are the main streaming modes supported by LangGraph Cloud for state updates of a graph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in the context of reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state and how can they be stored and updated effectively?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes in a graph depending on custom logic using a conditional entry point?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
What does it mean to be agentic?,"Other people may talk about a system being an ""agent"" - we prefer to talk about systems being ""agentic"". But what does this actually mean? When we talk about systems being ""agentic"", we are talking about systems that use an LLM to decide the control flow of an application. There are different levels that an LLM can be used to decide the control flow, and this spectrum of ""agentic"" makes more sense to us than defining an arbitrary cutoff for what is or isn't an agent. Examples of using an LLM to decide the control of an application: 
Using an LLM to route between two potential paths
Using an LLM to decide which of many tools to call
Using an LLM to decide whether the generated answer is sufficient or more work is need
 The more times these types of decisions are made inside an application, the more agentic it is.
If these decisions are being made in a loop, then its even more agentic! There are other concepts often associated with being agentic, but we would argue these are a by-product of the above definition: 
Tool calling: this is often how LLMs make decisions
Action taking: often times, the LLMs' outputs are used as the input to an action
Memory: reliable systems need to have knowledge of things that occurred
Planning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.
",What does it mean to be agentic?
Why LangGraph?,"LangGraph has several core principles that we believe make it the most suitable framework for building agentic applications: 
Controllability
Human-in-the-Loop
Streaming First
 Controllability LangGraph is extremely low level. This gives you a high degree of control over what the system you are building actually does. We believe this is important because it is still hard to get agentic systems to work reliably, and we've seen that the more control you exercise over them, the more likely it is that they will ""work"". Human-in-the-Loop LangGraph comes with a built-in persistence layer as a first-class concept. This enables several different human-in-the-loop interaction patterns. We believe that ""Human-Agent Interaction"" patterns will be the new ""Human-Computer Interaction"", and have built LangGraph with built in persistence to enable this. Streaming First LangGraph comes with first class support for streaming. Agentic applications often take a while to run, and so giving the user some idea of what is happening is important, and streaming is a great way to do that. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.",What are the core principles of LangGraph that make it suitable for building agentic applications?
Deployment,"So you've built your LangGraph object - now what? Now you need to deploy it. 
There are many ways to deploy LangGraph objects, and the right solution depends on your needs and use case.
We'll highlight two ways here: using LangGraph Cloud or rolling your own solution. LangGraph Cloud is an opinionated way to deploy LangGraph objects from the LangChain team. Please see the LangGraph Cloud documentation for all the details about what it involves, to see if it is a good fit for you. If it is not a good fit, you may want to roll your own deployment. In this case, we would recommend using FastAPI to stand up a server. You can then call this graph from inside the FastAPI server as you see fit.",What are two ways to deploy LangGraph objects?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes in a graph based on custom logic using a conditional entry point?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Setup,"To start, we can setup our client with whatever URL you are hosting your graph from:",What can we do to setup our client with the URL hosting the graph?
SDK initialization,"First, we need to setup our client so that we can communicate with our hosted graph: PythonJavascriptCURL

from langgraph_sdk import get_client
client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = agent;
const thread = await client.threads.create();

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

","How can we initialize the SDK to communicate with our hosted graph using Python, JavaScript, and CURL?"
Find idle threads,"We can use the following commands to find threads that are idle, which means that all runs executed on the thread have finished running: PythonJavascriptCURL

print(await client.threads.search(status=""idle"",limit=1))

console.log(await client.threads.search({status: ""idle"",limit:1}));

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""status"": ""idle"", ""limit"": 1}'

 Output: [{'thread_id': 'cacf79bb-4248-4d01-aabc-938dbd60ed2c',
'created_at': '2024-08-14T17:36:38.921660+00:00',
'updated_at': '2024-08-14T17:36:38.921660+00:00',
'metadata': {'graph_id': 'agent'},
'status': 'idle',
'config': {'configurable': {}}}]
","How can idle threads be found using Python, Javascript, and CURL commands?"
Find interrupted threads,"We can use the following commands to find threads that have been interrupted in the middle of a run, which could either mean an error occurred before the run finished or a human-in-the-loop breakpoint was reached and the run is waiting to continue:  PythonJavascriptCURL

print(await client.threads.search(status=""interrupted"",limit=1))

console.log(await client.threads.search({status: ""interrupted"",limit:1}));

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""status"": ""interrupted"", ""limit"": 1}'

 Output: [{'thread_id': '0d282b22-bbd5-4d95-9c61-04dcc2e302a5',
'created_at': '2024-08-14T17:41:50.235455+00:00',
'updated_at': '2024-08-14T17:41:50.235455+00:00',
'metadata': {'graph_id': 'agent'},
'status': 'interrupted',
'config': {'configurable': {}}}]
","How can we find interrupted threads using Python, Javascript, and CURL commands?"
Find busy threads,"We can use the following commands to find threads that are busy, meaning they are currently handling the execution of a run: PythonJavascriptCURL

print(await client.threads.search(status=""busy"",limit=1))

console.log(await client.threads.search({status: ""busy"",limit: 1}));

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""status"": ""busy"", ""limit"": 1}'

 Output: [{'thread_id': '0d282b22-bbd5-4d95-9c61-04dcc2e302a5',
'created_at': '2024-08-14T17:41:50.235455+00:00',
'updated_at': '2024-08-14T17:41:50.235455+00:00',
'metadata': {'graph_id': 'agent'},
'status': 'busy',
'config': {'configurable': {}}}]
",How can we find busy threads using the provided commands?
Find specific threads,"You may also want to check the status of specific threads, which you can do in a few ways:",How can you check the status of specific threads?
Find by ID,"You can use the get function to find the status of a specific thread, as long as you have the ID saved PythonJavascriptCURL

print((await client.threads.get(<THREAD_ID>))['status'])

console.log((await client.threads.get(<THREAD_ID>)).status);

curl --request GET \ 
--url <DEPLOYMENT_URL>/threads/<THREAD_ID> \
--header 'Content-Type: application/json' | jq -r '.status'

 Output: 'idle'
",What is the status of the thread with the ID <THREAD_ID>?
Find by metadata,"The search endpoint for threads also allows you to filter on metadata, which can be helpful if you use metadata to tag threads in order to keep them organized: PythonJavascriptCURL

print((await client.threads.search(metadata={""foo"":""bar""},limit=1))[0]['status'])

console.log((await client.threads.search({metadata: {""foo"":""bar""},limit: 1}))[0].status);

curl --request POST \  
--url <DEPLOYMENT_URL>/threads/search \
--header 'Content-Type: application/json' \
--data '{""metadata"": {""foo"":""bar""}, ""limit"": 1}' | jq -r '.[0].status'

 Output: 'idle'
","What is the status of the thread when searching by metadata with the key ""foo"" and value ""bar""?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Self-RAG,"Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. In the paper, a few decisions are made: 
Should I retrieve from retriever, R -
 
Input: x (question) OR x (question), y (generation)
Decides when to retrieve D chunks with R
Output: yes, no, continue
 
Are the retrieved passages D relevant to the question x -
 

Input: (x (question), d (chunk)) for d in D

d provides useful information to solve x
Output: relevant, irrelevant
 
Are the LLM generation from each chunk in D is relevant to the chunk (hallucinations, etc)  -
 
Input: x (question), d (chunk),  y (generation) for d in D
All of the verification-worthy statements in y (generation) are supported by d
Output: {fully supported, partially supported, no support
 
The LLM generation from each chunk in D is a useful response to x (question) -
 
Input: x (question), y (generation) for d in D
y (generation) is a useful response to x (question).
Output: {5, 4, 3, 2, 1}
 We will implement some of these ideas from scratch using LangGraph.","Are the LLM generation from each chunk in D relevant to the chunk (hallucinations, etc)?"
Tracing,"Optionally, use LangSmith for tracing (shown at bottom)","What tool can be used for tracing, as shown at the bottom?"
Retriever,Let's index 3 blog posts.,What are we indexing 3 of in this text?
Graph,Capture the flow in as a graph.,What can be used to capture the flow as a graph?
Build Graph,The just follows the flow we outlined in the figure above.,What does the build graph process follow?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state and how can they be stored and updated effectively?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes in a graph depending on custom logic using a conditional entry point?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
","What changes can be made to the topology of a graph in LangGraph, and how does it handle migrations of graph definitions?"
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Web Voyager,"WebVoyager by He, et. al., is a vision-enabled web-browsing agent capable of controlling the mouse and keyboard. It works by viewing annotated browser screenshots for each turn, then choosing the next step to take. The agent architecture is a basic reasoning and action (ReAct) loop.
The unique aspects of this agent are: 
It's usage of Set-of-Marks-like image annotations to serve as UI affordances for the agent
It's application in the browser by using tools to control both the mouse and keyboard
 The overall design looks like the following:","What are the unique aspects of the WebVoyager web-browsing agent by He, et. al.?"
Configure environment,"We will first set up LangSmith tracing. Though optional, this lets us inspect and debug agent's trajectory for a given input. You can sign up at smith.langchain.com to get an API key.",What is the first step in configuring the environment for LangSmith tracing?
Define Graph State,"The state provides the inputs to each node in the graph. In our case, the agent will track the webpage object (within the browser), annotated images + bounding boxes, the user's initial request, and the messages containing the agent scratchpad, system prompt, and other information.",What inputs does the state provide to each node in the graph?
Define tools,"The agent has 6 simple tools: 
Click (at labeled box)
Type
Scroll
Wait
Go back
Go to search engine (Google)
 We define them below here as functions:",What are the 6 simple tools defined as functions by the agent?
Define Agent,"The agent is driven by a multi-modal model and decides the action to take for each step. It is composed of a few runnable objects: 
A mark_page function to annotate the current page with bounding boxes
A prompt to hold the user question, annotated image, and agent scratchpad
GPT-4V to decide the next steps
Parsing logic to extract the action
 Let's first define the annotation step: Browser Annotations This function annotates all buttons, inputs, text areas, etc. with numbered bounding boxes. GPT-4V then just has to refer to a bounding box
when taking actions, reducing the complexity of the overall task.",What components make up the agent driven by a multi-modal model?
Define graph,We've created most of the important logic. We have one more function to define that will help us update the graph state after a tool is called.,What is the purpose of the function that still needs to be defined in relation to the graph state?
Run agent,"Now that we've created the whole agent executor, we can run it on a few questions! We'll start our browser at ""google.com"" and then let it control the rest. Below is a helper function to help print out the steps to the notebook (and display the intermediate screenshots).",What steps are involved in running the agent executor on a few questions?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in the context of reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges or terminate using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
","What changes can be made to the topology of a graph in LangGraph, and how does it handle migrations of graph definitions?"
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What are some built-in ways to visualize graphs in LangGraph?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to add human-in-the-loop processes to the prebuilt ReAct agent,"This tutorial will show how to add human-in-the-loop processes to the prebuilt ReAct agent. Please see this tutorial for how to get started with the prebuilt ReAct agent You can add a a breakpoint before tools are called by passing interrupt_before=[""tools""] to create_react_agent. Note that you need to be using a checkpointer for this to work.",How can human-in-the-loop processes be added to the prebuilt ReAct agent?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Conceptual Guides,"In this guide we will explore the concepts behind build agentic and multi-agent systems with LangGraph. We assume you have already learned the basic covered in the introduction tutorial and want to deepen your understanding of LangGraph's underlying design and inner workings. There are three main parts to this concept guide. First, we'll discuss at a very high level what it means to be agentic. Next, we'll look at lower-level concepts in LangGraph that are core for understanding how to build your own agentic systems. Finally, we'll discuss common agentic patterns and how you can achieve those with LangGraph. These will be mostly conceptual guides - for more technical, hands-on guides see our how-to guides LangGraph for Agentic Applications 
What does it mean to be agentic?
Why LangGraph
Deployment
 Low Level Concepts 
Graphs
StateGraph
MessageGraph
Compiling Your Graph

State
Schema
Reducers
MessageState

Nodes
START node
END node

Edges
Normal Edges
Conditional Edges
Entry Point
Conditional Entry Point

Send
Checkpointer
Threads
Checkpointer states
Get state
Get state history
Update state

Configuration
Visualization
Streaming
 Common Agentic Patterns 
Structured output
Tool calling
Memory
Human in the loop
Approval
Wait for input
Edit agent actions
Time travel

Map-Reduce
Multi-agent
Planning
Reflection
Off-the-shelf ReAct Agent
",What does it mean to be agentic?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can LangGraph nodes be used to return structured output when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in LangGraph to look back at previous checkpoints and resume execution from a desired point?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",What is a common technique used by agentic systems to overcome the struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method used to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
TNT-LLM: Text Mining at Scale,"TNT-LLM by Wan, et. al describes a taxonomy generation and classification system developed by Microsoft for their Bing Copilot application. It generates a rich, interpretable taxonomy of user intents (or other categories) from raw conversation logs. This taxonomy can then be used downstream by LLMs to label logs, which in turn can be used as training data to adapt a cheap classifier (such as logistic regression classifier on embeddings) that can be deployed in your app. TNT-LLM has three main phases: 
Generate Taxonomy
Label Training Data
Finetune classifier + deploy
 When applying LangGraph in this notebook, we will focus on the first phase: taxonomy generation (blue in the diagram below). We then show how to label and fit the classifier in subsequent steps below. To generate the taxonomy, TNT-LLM proposes 5 steps: 
Summarize chat logs using a lower-cost LLM (batched over all logs in the sample)
Batch the logs into random minibatches
Generate an initial taxonomy from the first minibatch
Update the taxonomy on each subsequent minibatch via a ritique and revise prompt
Review the final taxonomy, scoring its quality and generating a final value using a final sample.
",What are the five steps proposed by TNT-LLM for generating a taxonomy from raw conversation logs?
Define the Graph,"With all the functionality defined, we can define the graph!",What can be defined with all the functionality defined?
Usage,"The docs can contain any content, but we've found it works really well on chat bot logs, such as those captured by LangSmith. We will use that as an example below. Update the project_name to your own LangSmith project. You will likely have to customize the run_to_doc function below, since your expected keys may differ from those of this notebook's author.","What type of content works well with the docs, according to the text?"
Final Result,"Below, render the final result as markdown:",What is the final result rendered as markdown?
Final Taxonomy,"

ID
Name
Description

1
Troubleshooting Network Connectivity Issues
Resolving problems with DNS, network connections, and GitHub extension activation.

2
Extracting and Analyzing Data
Retrieving and processing data from various sources like text files, databases, and APIs.

3
Providing Healthcare Insights
Generating medical diagnosis, symptom checking, drug information, and skin condition analysis.

4
Configuring and Optimizing Models
Adjusting model parameters and hyperparameters to improve performance for a given task.

5
Generating Creative Poetry
Creating poems using language models and AI-powered tools.

6
Interacting with Databases
Querying databases, extracting data, and managing errors during data processing.

7
Querying Vector Databases
Interacting with vector databases like Milvus to store and retrieve high-dimensional data.

8
Generating Synthetic Data
Creating synthetic data using language models and machine learning techniques.

9
Integrating Tools and Workflows
Incorporating various tools and libraries into a cohesive workflow for different tasks.

10
Improving Information Retrieval
Storing and querying multiple vectors per document for better semantic understanding.

11
Processing Documents and Extracting Text
Parsing and extracting text from various document formats like PDF, DOCX, and HTML.

12
Building Local Knowledge Bases
Creating knowledge bases from text files, handling text splitting, embeddings, and storage.

13
Optimizing Conversational Retrieval
Troubleshooting and improving the performance of the ConversationalRetrievalChain in LangChain.

14
Connecting Databases and Using Agents
Connecting to databases, using agents, and understanding the differences between agent types.

15
Introspecting LangChain Tools
Accessing and retrieving details about the functions and source code of LangChain tools.

16
Generating Styled Answers with Retrieval Augmentation
Creating a QA system that generates well-cited answers in a specific style.

17
Using ZERO_SHOT_REACT_DESCRIPTION Agents
Applying the ZERO_SHOT_REACT_DESCRIPTION agent type in LangChain for chat models.

18
Automating Microlearning Course Creation
Generating microlearning courses based on input parameters like topic, volume, and learning style.

19
Integrating with Chroma Vector Store
Storing and retrieving data in the Chroma vector database, including handling document embeddings.

20
Managing LangChain Callback Tokens
Understanding and utilizing the callback token feature in the LCEL chain.

21
Troubleshooting FastAPI Deployments
Resolving issues with deploying a React app with a FastAPI backend.

22
Analyzing Data with LangChain Agents
Using LangChain agents to interact with Pandas and Spark DataFrames for data exploration.

23
Implementing the OpenAI Chat API
Implementing the OpenAI chat completion API and understanding the required inputs and outputs.

24
Comparing LangChain and LLMIndex
Evaluating the differences between LangChain and LLMIndex, including their UI support for Markdown.

25
Suppressing Tools in AgentExecutor
Temporarily disabling tools in an AgentExecutor for a fixed number of invocations.

",What are some examples of tasks that can be performed using the Final Taxonomy?
Phase 2: Labeling,"Now that we have our taxonomy, it's time to label a subset of our data to train a classifier. Input classification can be useful for anything from in-line prompt optimization (tailor the prompt for each classified intent), to system improvements (identifying categories for which the system doesn't produce good responses) to product analytics (understand which intent categories could be improved to drive profits). The problem is that LLM-based tagging can be expensive. Embeddings can be ~100x cheaper to compute, and a simple logistic regression classifier on top of that would add negligible cost. Let's tag and train a classifier! Label Training Data Use an LLM to label the data in a fully-automated fashion. For better accuracy, you can sample a portion of the results to label by hand as well to verify the quality.",What are some benefits of using embeddings and logistic regression classifier for labeling data in Phase 2 of the process?
Phase 3: Deploy,"Now that you have your classifier, you can easily deploy it and apply to future runs! All you need is to embed the input and apply your LogisticRegression classifier. Let's try it. We will use python's joblib library to serialize our sklearn classifier. Below is an example:",What library can be used to serialize the sklearn classifier for deployment in Phase 3?
Conclusion,"Congrats on implementing TNT-LLM! While most folks use clustering-based approaches like LDA, k-means, etc. it can often be hard to really interpret what each cluster represents. TNT-LLM generates human-interpretable labels you can use downstream to monitor and improve your application. The technique also lends itself to hierarchical sub-categorizing: once you have the above taxonomy, use it to label your data, then on each sub-category, generate a new taxonomy using a similar technique to the one described above!",What are the benefits of using TNT-LLM compared to clustering-based approaches like LDA and k-means for data interpretation and labeling?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the main graph class used in StateGraph and what is it parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State using reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state and how can they be stored and updated effectively?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified in the code?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",How does LangGraph support the design pattern of map-reduce by allowing the return of Send objects from conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using the graph.get_state method?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What information can be obtained by calling graph.get_state_history(config)?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
API Reference,The LangGraph Cloud API reference is available with each deployment at the /docs URL path (e.g. http://localhost:8124/docs). Click here to view the API reference.,What is the URL path to access the LangGraph Cloud API reference?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to run graph asynchronously,"In this example we will build a ReAct agent with native async implementations of the core logic. When chat models have async clients, this can give us some nice performance improvements if you
are running concurrent branches in your graph or if your graph is running within a larger web server process. In general, you don't need to change anything about your graph to add async support. That's one of the beauties of Runnables. 
Note:

        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the create_react_agent(model, tools=tool) (API doc) constructor. This may be more appropriate if you are used to LangChains AgentExecutor class.
    
",How can async support be added to a graph without changing anything about the graph itself?
Setup,First we need to install the packages required,What is the first step in the setup process?
Set up the State,"The main type of graph in langgraph is the StateGraph.
This graph is parameterized by a State object that it passes around to each node.
Each node then returns operations the graph uses to update that state.
These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute.
Whether to set or add is denoted by annotating the State object you use to construct the graph. For this example, the state we will track will just be a list of messages.
We want each node to just add messages to that list.
Therefore, we will use a TypedDict with one key (messages) and annotate it so that the messages attribute is ""append-only"".",What type of graph is used in langgraph and how is the State object parameterized within it?
Set up the tools,"We will first define the tools we want to use.
For this simple example, we will use create a placeholder search engine.
It is really easy to create your own tools - see documentation here on how to do that.",What will be the first step in the process of setting up the tools?
Set up the model,"Now we need to load the chat model we want to use.
This should satisfy two criteria: 
It should work with messages, since our state is primarily a list of messages (chat history).
It should work with tool calling, since we are using a prebuilt ToolNode
 Note: these model requirements are not requirements for using LangGraph - they are just requirements for this particular example.",What criteria should the chat model satisfy in order to work with messages and tool calling in this example?
Define the nodes,"We now need to define a few different nodes in our graph.
In langgraph, a node can be either a function or a runnable.
There are two main nodes we need for this: 
The agent: responsible for deciding what (if any) actions to take.
A function to invoke tools: if the agent decides to take an action, this node will then execute that action.
 We will also need to define some edges.
Some of these edges may be conditional.
The reason they are conditional is that based on the output of a node, one of several paths may be taken.
The path that is taken is not known until that node is run (the LLM decides). 
Conditional Edge: after the agent is called, we should either:
a. If the agent said to take an action, then the function to invoke tools should be called
b. If the agent said that it was finished, then it should finish
Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next
 Let's define the nodes, as well as a function to decide how what conditional edge to take. MODIFICATION We define each node as an async function.",What are the two main nodes that need to be defined in the graph?
Define the graph,We can now put it all together and define the graph!,What can we define by putting it all together?
Use it!,"We can now use it!
This now exposes the same interface as all other LangChain runnables.",What interface does it now expose?
Streaming,LangGraph has support for several different types of streaming.,What types of streaming does LangGraph support?
Streaming Node Output,One of the benefits of using LangGraph is that it is easy to stream output as it's produced by each node.,What is one benefit of using LangGraph in terms of streaming node output?
Streaming LLM Tokens,"You can also access the LLM tokens as they are produced by each node.
In this case only the ""agent"" node produces LLM tokens.
In order for this to work properly, you must be using an LLM that supports streaming as well as have set it when constructing the LLM (e.g. ChatOpenAI(model=""gpt-3.5-turbo-1106"", streaming=True))",What must be done in order to access the LLM tokens as they are produced by each node?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Quick Start,"This quick start guide will cover how to build a simple agent that can look up things on the internet. We will then deploy it to LangGraph Cloud, use the LangGraph Studio to visualize and test it out, and use the LangGraph SDK to interact with it.",What steps are involved in building a simple agent that can look up things on the internet and deploying it to LangGraph Cloud?
Set up requirements,"This tutorial will use: 
Anthropic for the LLM - sign up and get an API key here
Tavily for the search engine - sign up and get an API key here
LangSmith for hosting - sign up and get an API key here
",What tools are required for setting up the tutorial mentioned?
Set up local files,"

Create a new application with the following directory and files:
<my-app>/
|-- agent.py            # code for your LangGraph agent
|-- requirements.txt    # Python packages required for your graph
|-- langgraph.json      # configuration file for LangGraph
|-- .env                # environment files with API keys

The agent.py file should contain Python code for defining your graph. The following code is a simple example, the important thing is that at some point in your file you compile your graph and assign the compiled graph to a variable (in this case the graph variable). This example code uses create_react_agent, a prebuilt agent, read more about it here.
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.prebuilt import create_react_agent

model = ChatAnthropic(model=""claude-3-5-sonnet-20240620"")

tools = [TavilySearchResults(max_results=2)]

graph = create_react_agent(model, tools)

The requirements.txt file should contain any dependencies for your graph(s). In this case we only require four packages for our graph to run:
langgraph
langchain_anthropic
tavily-python
langchain_community

The langgraph.json file is a configuration file that describes what graph(s) you are going to host. In this case we only have one graph to host: the compiled graph object from agent.py.
{
  ""dependencies"": ["".""],
  ""graphs"": {
    ""agent"": ""./agent.py:graph""
  },
  ""env"": "".env""
}

Learn more about the LangGraph CLI configuration file here.

The .env file should have any environment variables needed to run your graph. This will only be used for local testing, so if you are not testing locally you can skip this step. NOTE: if you do add this, you should NOT check this into git. For this graph, we need two environment variables:
ANTHROPIC_API_KEY=...
TAVILY_API_KEY=...

 Now that we have set everything up on our local file system, we are ready to host our graph.",What files and directories are needed to set up a local LangGraph application?
Test the graph build locally,"Before deploying to the cloud, we probably want to test the building of our graph locally. This is useful to make sure we have configured our CLI configuration file correctly and our graph runs. In order to do this we can first install the LangGraph CLI pip install langgraph-cli
 We can then test our API server locally. This requires access to LangGraph closed beta. In order to run the server locally, you will need to add your LANGSMITH_API_KEY to the .env file so we can validate you have access to LangGraph closed beta. langgraph up
 This will start up the LangGraph API server locally. If this runs successfully, you should see something like: Ready!
- API: http://localhost:8123
2024-06-26 19:20:41,056:INFO:uvicorn.access 127.0.0.1:44138 - ""GET /ok HTTP/1.1"" 200
 You can now test this out! Note: this local server is intended SOLELY for local testing purposes and is not performant enough for production applications, so please do not use it as such. To test it out, you can go to another terminal window and run: curl --request POST \
    --url http://localhost:8123/runs/stream \
    --header 'Content-Type: application/json' \
    --data '{
    ""assistant_id"": ""agent"",
    ""input"": {
        ""messages"": [
            {
                ""role"": ""user"",
                ""content"": ""How are you?""
            }
        ]
    },
    ""metadata"": {},
    ""config"": {
        ""configurable"": {}
    },
    ""multitask_strategy"": ""reject"",
    ""stream_mode"": [
        ""values""
    ]
}'
 If you get back a valid response, then all is functioning properly!",What steps are necessary to test the building of the graph locally before deploying to the cloud?
Push your code to GitHub,"Turn the <my-app> directory into a GitHub repo. You can use the GitHub CLI if you like, or just create a repo manually (if unfamiliar, instructions here).",What are the steps to turn the <my-app> directory into a GitHub repo?
Deploy from GitHub with LangGraph Cloud,"Once you have created your github repository with a Python file containing your compiled graph as well as a langgraph.json file containing the configuration for hosting your graph, you can head over to LangSmith and click on the icon on the left navbar to create a new deployment. Then click the + New Deployment button. If you have not deployed to LangGraph Cloud before: there will be a button that shows up saying Import from GitHub. Youll need to follow that flow to connect LangGraph Cloud to GitHub. Once you have set up your GitHub connection: the new deployment page will look as follows: To deploy your application, you should do the following: 
Select your GitHub username or organization from the selector
Search for your repo to deploy in the search bar and select it
Choose any name
In the LangGraph API config file field, enter the path to your langgraph.json file (which in this case is just langgraph.json)
For Git Reference, you can select either the git branch for the code you want to deploy, or the exact commit SHA.
If your chain relies on environment variables, add those in. They will be propagated to the underlying server so your code can access them. In this case, we need ANTHROPIC_API_KEY and TAVILY_API_KEY.
 Putting this all together, you should have something as follows for your deployment details: Hit Submit and your application will start deploying!",How can you deploy your application from GitHub using LangGraph Cloud?
Deployments View,"After your deployment is complete, your deployments page should look as follows: You can see that by default, you get access to the Trace Count monitoring chart and Recent Traces run view. These are powered by LangSmith. You can click on All Charts to view all monitoring info for your server, or click on See tracing project to get more information on an individual trace.",What monitoring charts and views are available on the deployments page after a deployment is complete?
Access the Docs,"You can access the docs by clicking on the API docs link, which should send you to a page that looks like this: You wont actually be able to test any of the API endpoints without authorizing first. To do so, grab your Langsmith API key and add it at the top where it says API KEY (X-API-KEY). You should now be able to select any of the API endpoints, click Test Request, enter the parameters you would like to pass, and then click Send to view the results of the API call.",How can you authorize and test API endpoints in the Langsmith documentation?
Interact with your deployment via LangGraph Studio,If you click on your deployment you should see a blue button in the top right that says LangGraph Studio. Clicking on this button will take you to a page that looks like this: On this page you can test out your graph by passing in starting states and clicking Start Run (this should behave identically to calling .invoke). You will then be able to look into the execution thread for each run and explore the steps your graph is taking to produce its output.,How can you test out your graph and explore its execution thread using LangGraph Studio?
Use with the SDK,"Once you have tested that your hosted graph works as expected using LangGraph Studio, you can start using your hosted graph all over your organization by using the LangGraph SDK. Let's see how we can access our hosted graph and execute our run from a python file. First, make sure you have the SDK installed by calling pip install langgraph_sdk. Before using, you need to get the URL of your LangGraph deployment. You can find this in the Deployment view. Click the URL to copy it to the clipboard. You also need to make sure you have set up your API key properly so you can authenticate with LangGraph Cloud. export LANGSMITH_API_KEY=...
 The first thing to do when using the SDK is to setup our client, access our assistant, and create a thread to execute a run on: from langgraph_sdk import get_client

# Replace this with the URL of your own deployed graph
URL = ""https://chatbot-23a570f3210f52a7b167f09f6158e3b3-ffoprvkqsa-uc.a.run.app""
client = get_client(url=URL)

# Search all hosted graphs
assistants = await client.assistants.search()
# In this example we select the first assistant since we are only hosting a single graph
assistant = assistants[0]

# We create a thread for tracking the state of our run
thread = await client.threads.create()
 We can then execute a run on the thread: input = {""messages"":[{""role"": ""user"", ""content"": ""Hello! My name is Bagatur and I am 26 years old.""}]}

async for chunk in client.runs.stream(
        thread['thread_id'],
        assistant[""assistant_id""],
        input=input,
        stream_mode=""updates"",
    ):
    if chunk.data and chunk.event != ""metadata"":
        print(chunk.data)
 {'agent': {'messages': [{'content': ""Hi Bagatur! It's nice to meet you. How can I assist you today?"", 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_9cb5d38cf7'}, 'type': 'ai', 'name': None, 'id': 'run-c89118b7-1b1e-42b9-a85d-c43fe99881cd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
",How can you access and execute a run on your hosted graph using the LangGraph SDK?
What's Next,Congratulations! If you've worked your way through this tutorial you are well on your way to becoming a LangGraph Cloud expert. Here are some other resources to check out to help you out on the path to expertise:,What are some additional resources to help on the path to becoming a LangGraph Cloud expert?
LangGraph Cloud How-tos,"If you want to learn more about streaming from hosted graphs, check out the Streaming how-to guides. To learn more about double-texting and all the ways you can handle it in your application, read up on these how-to guides. To learn about how to include different human-in-the-loop behavior in your graph, take a look at these how-tos.",What topics are covered in the LangGraph Cloud How-tos?
LangGraph Tutorials,"Before hosting, you have to write a graph to host. Here are some tutorials to get you more comfortable with writing LangGraph graphs and give you inspiration for the types of graphs you want to host. This tutorial walks you through how to write a customer support bot using LangGraph. If you are interested in writing a SQL agent, check out this tutorial. Check out the LangGraph tutorials page to read about more exciting use cases.",What types of tutorials are available on the LangGraph tutorials page?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to Wait for User Input,"One of the main human-in-the-loop interaction patterns is waiting for human input. A key use case involves asking the user clarifying questions. One way to accomplish this is simply go to the END node and exit the graph. Then, any user response comes back in as fresh invocation of the graph. This is basically just creating a chatbot architecture. The issue with this is it is tough to resume back in a particular point in the graph. Often times the agent is halfway through some process, and just needs a bit of a user input. Although it is possible to design your graph in such a way where you have a conditional_entry_point to route user messages back to the right place, that is not super scalable (as it essentially involves having a routing function that can end up almost anywhere). A separate way to do this is to have a node explicitly for getting user input. This is easy to implement in a notebook setting - you just put an input() call in the node. But that isn't exactly production ready. Luckily, LangGraph makes it possible to do similar things in a production way. The basic idea is: 
Set up a node that represents human input. This can have specific incoming/outgoing edges (as you desire). There shouldn't actually be any logic inside this node.
Add a breakpoint before the node. This will stop the graph before this node executes (which is good, because there's no real logic in it anyways)
Use .update_state to update the state of the graph. Pass in whatever human response you get. The key here is to use the as_node parameter to apply this update as if you were that node. This will have the effect of making it so that when you resume execution next it resumes as if that node just acted, and not from the beginning.
",How can you wait for user input in a production-ready way using LangGraph?
Setup,"We are not going to show the full code for the graph we are hosting, but you can see it here if you want to. Once this graph is hosted, we are ready to invoke it and wait for user input. ",What is the next step after hosting the graph?
SDK initialization,"First, we need to setup our client so that we can communicate with our hosted graph: PythonJavascriptCURL

from langgraph_sdk import get_client
client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent"";
const thread = await client.threads.create();

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

","How can we initialize the SDK to communicate with our hosted graph using Python, JavaScript, and CURL?"
Initial invocation,"Now, let's invoke our graph by interrupting before ask_human node: PythonJavascriptCURL

input = { 'messages':[{ ""role"":""user"", ""content"":""Use the search tool to ask the user where they are, then look up the weather there"" }] }

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id,
    input=input,
    stream_mode=""updates"",
    interrupt_before=[""ask_human""],
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const input = { ""messages"":[{ ""role"":""human"", ""content"": ""Use the search tool to ask the user where they are, then look up the weather there""}] }

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""updates"",
    interruptBefore: [""ask_human""],
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""Use the search tool to ask the user where they are, then look up the weather there\""}]},
   \""interrupt_before\"": [\""ask_human\""],
   \""stream_mode\"": [
     \""updates\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
     sub(/^event: /, """", $0)
     event_type = $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
 }
 '

 Output: {'agent': {'messages': [{'content': [{'text': ""Certainly! I'll use the AskHuman function to ask the user about their location, and then I'll use the search function to look up the weather for that location. Let's start by asking the user where they are."", 'type': 'text'}, {'id': 'toolu_01RFahzYPvnPWTb2USk2RdKR', 'input': {'question': 'Where are you currently located?'}, 'name': 'AskHuman', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-a8422215-71d3-4093-afb4-9db141c94ddb', 'example': False, 'tool_calls': [{'name': 'AskHuman', 'args': {'question': 'Where are you currently located?'}, 'id': 'toolu_01RFahzYPvnPWTb2USk2RdKR'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
",What function is used to ask the user about their location before looking up the weather?
Adding user input to state,"We now want to update this thread with a response from the user. We then can kick off another run. Because we are treating this as a tool call, we will need to update the state as if it is a response from a tool call. In order to do this, we will need to check the state to get the ID of the tool call. PythonJavascriptCURL

state = await client.threads.get_state(thread['thread_id'])
tool_call_id = state['values']['messages'][-1]['tool_calls'][0]['id']

# We now create the tool call with the id and the response we want
tool_message = [{""tool_call_id"": tool_call_id, ""type"": ""tool"", ""content"": ""san francisco""}]

await client.threads.update_state(thread['thread_id'], {""messages"": tool_message}, as_node=""ask_human"")

const state = await client.threads.getState(thread['thread_id']);
const toolCallId = state['values']['messages'][-1]['tool_calls'][0]['id'];

# We now create the tool call with the id and the response we want
const toolMessage = [{""tool_call_id"": toolCallId, ""type"": ""tool"", ""content"": ""san francisco""}];

await client.threads.updateState(thread['thread_id'], {values: {""messages"": toolMessage}, asNode:""ask_human""})

curl --request GET \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \
 | jq -r '.values.messages[-1].tool_calls[0].id' \
 | sh -c '
     TOOL_CALL_ID=""$1""

     # Construct the JSON payload
     JSON_PAYLOAD=$(printf ""{\""messages\"": [{\""tool_call_id\"": \""%s\"", \""type\"": \""tool\"", \""content\"": \""san francisco\""}], \""as_node\"": \""ask_human\""}"" ""$TOOL_CALL_ID"")

     # Send the updated state
     curl --request POST \
          --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \
          --header ""Content-Type: application/json"" \
          --data ""${JSON_PAYLOAD}""
 ' _ 

 Output: {'configurable': {'thread_id': 'a9f322ae-4ed1-41ec-942b-38cb3d342c3a',
'checkpoint_ns': '',
'checkpoint_id': '1ef58e97-a623-63dd-8002-39a9a9b20be3'}}
","What is the process for updating the state with a response from the user in Python, JavaScript, and CURL?"
Invoking after receiving human input,"We can now tell the agent to continue. We can just pass in None as the input to the graph, since no additional input is needed: PythonJavascriptCURL

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id,
    input=None,
    stream_mode=""updates"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: null,
    streamMode: ""updates"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl --request POST \                                                                             
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""stream_mode\"": [
     \""updates\""
   ]
 }""| \ 
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
     sub(/^event: /, """", $0)
     event_type = $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
 }
 '

 Output: {'agent': {'messages': [{'content': [{'text': ""Thank you for letting me know that you're in San Francisco. Now, I'll use the search function to look up the weather in San Francisco."", 'type': 'text'}, {'id': 'toolu_01K57ofmgG2wyJ8tYJjbq5k7', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-241baed7-db5e-44ce-ac3c-56431705c22b', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_01K57ofmgG2wyJ8tYJjbq5k7'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
{'action': {'messages': [{'content': '[""I looked up: current weather in San Francisco. Result: It\'s sunny in San Francisco, but you better look out if you\'re a Gemini .""]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '8b699b95-8546-4557-8e66-14ea71a15ed8', 'tool_call_id': 'toolu_01K57ofmgG2wyJ8tYJjbq5k7'}]}}
{'agent': {'messages': [{'content': ""Based on the search results, I can provide you with information about the current weather in San Francisco:\n\nThe weather in San Francisco is currently sunny. It's a beautiful day in the city! \n\nHowever, I should note that the search result included an unusual comment about Gemini zodiac signs. This appears to be either a joke or potentially irrelevant information added by the search engine. For accurate and detailed weather information, you might want to check a reliable weather service or app for San Francisco.\n\nIs there anything else you'd like to know about the weather or San Francisco?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-b4d7309f-f849-46aa-b6ef-475bcabd2be9', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
","What is the process for invoking after receiving human input in Python, Javascript, and CURL?"
Adding user input to state,"We now want to update this thread with a response from the user. We then can kick off another run. Because we are treating this as a tool call, we will need to update the state as if it is a response from a tool call. In order to do this, we will need to check the state to get the ID of the tool call. PythonJavascriptCURL

state = await client.threads.get_state(thread['thread_id'])
tool_call_id = state['values']['messages'][-1]['tool_calls'][0]['id']

# We now create the tool call with the id and the response we want
tool_message = [{""tool_call_id"": tool_call_id, ""type"": ""tool"", ""content"": ""san francisco""}]

await client.threads.update_state(thread['thread_id'], {""messages"": tool_message}, as_node=""ask_human"")

const state = await client.threads.getState(thread['thread_id']);
const toolCallId = state['values']['messages'][-1]['tool_calls'][0]['id'];

# We now create the tool call with the id and the response we want
const toolMessage = [{""tool_call_id"": toolCallId, ""type"": ""tool"", ""content"": ""san francisco""}];

await client.threads.updateState(thread['thread_id'], {values: {""messages"": toolMessage}, asNode:""ask_human""})

curl --request GET \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \
 | jq -r '.values.messages[-1].tool_calls[0].id' \
 | sh -c '
     TOOL_CALL_ID=""$1""

     # Construct the JSON payload
     JSON_PAYLOAD=$(printf ""{\""messages\"": [{\""tool_call_id\"": \""%s\"", \""type\"": \""tool\"", \""content\"": \""san francisco\""}], \""as_node\"": \""ask_human\""}"" ""$TOOL_CALL_ID"")

     # Send the updated state
     curl --request POST \
          --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \
          --header ""Content-Type: application/json"" \
          --data ""${JSON_PAYLOAD}""
 ' _ 

 Output: {'configurable': {'thread_id': 'a9f322ae-4ed1-41ec-942b-38cb3d342c3a',
'checkpoint_ns': '',
'checkpoint_id': '1ef58e97-a623-63dd-8002-39a9a9b20be3'}}
","What is the process for updating the state with a response from the user in Python, JavaScript, and CURL?"
Invoking after receiving human input,"We can now tell the agent to continue. We can just pass in None as the input to the graph, since no additional input is needed: PythonJavascriptCURL

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id,
    input=None,
    stream_mode=""updates"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: null,
    streamMode: ""updates"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl --request POST \                                                                             
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""stream_mode\"": [
     \""updates\""
   ]
 }""| \ 
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
     sub(/^event: /, """", $0)
     event_type = $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
 }
 '

 Output: {'agent': {'messages': [{'content': [{'text': ""Thank you for letting me know that you're in San Francisco. Now, I'll use the search function to look up the weather in San Francisco."", 'type': 'text'}, {'id': 'toolu_01K57ofmgG2wyJ8tYJjbq5k7', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-241baed7-db5e-44ce-ac3c-56431705c22b', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_01K57ofmgG2wyJ8tYJjbq5k7'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
{'action': {'messages': [{'content': '[""I looked up: current weather in San Francisco. Result: It\'s sunny in San Francisco, but you better look out if you\'re a Gemini .""]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '8b699b95-8546-4557-8e66-14ea71a15ed8', 'tool_call_id': 'toolu_01K57ofmgG2wyJ8tYJjbq5k7'}]}}
{'agent': {'messages': [{'content': ""Based on the search results, I can provide you with information about the current weather in San Francisco:\n\nThe weather in San Francisco is currently sunny. It's a beautiful day in the city! \n\nHowever, I should note that the search result included an unusual comment about Gemini zodiac signs. This appears to be either a joke or potentially irrelevant information added by the search engine. For accurate and detailed weather information, you might want to check a reliable weather service or app for San Francisco.\n\nIs there anything else you'd like to know about the weather or San Francisco?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-b4d7309f-f849-46aa-b6ef-475bcabd2be9', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
","What is the process for invoking after receiving human input in Python, Javascript, and CURL?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to create agents with configuration,"One of the benefits of LangGraph API is that it lets you create agents with different configurations.
This is useful when you want to: 
Define a cognitive architecture once as a LangGraph
Let that LangGraph be configurable across some attributes (for example, system message or LLM to use)
Let users create agents with arbitrary configurations, save them, and then use them in the future
 In this guide we will show how to do that for the default agent we have built in. If you look at the agent we defined, you can see that inside the call_model node we have created the model based on some configuration. That node looks like: def call_model(state, config):
    messages = state[""messages""]
    model_name = config.get('configurable', {}).get(""model_name"", ""anthropic"")
    model = _get_model(model_name)
    response = model.invoke(messages)
    # We return a list, because this will get added to the existing list
    return {""messages"": [response]}
 We are looking inside the config for a model_name parameter (which defaults to anthropic if none is found).
That means that by default we are using Anthropic as our model provider.
In this example we will see an example of how to create an example agent that is configured to use OpenAI. We've also communicated to the graph that it should expect configuration with this key.
We've done this by passing config_schema when constructing the graph, eg: class GraphConfig(TypedDict):
    model_name: Literal[""anthropic"", ""openai""]

# Define a new graph
workflow = StateGraph(AgentState, config_schema=GraphConfig)
",How can agents be created with different configurations using LangGraph API?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Reject,"This guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide. The guide covers the reject option for double texting, which rejects the new run of the graph by throwing an error and continues with the original run until completion. Below is a quick example of using the reject option. First, we will define a quick helper function for printing out JS model outputs (you can skip this if using Python): function prettyPrint(m) {
  const padded = "" "" + m['type'] + "" "";
  const sepLen = Math.floor((80 - padded.length) / 2);
  const sep = ""="".repeat(sepLen);
  const secondSep = sep + (padded.length % 2 ? ""="" : """");

  console.log(`${sep}${padded}${secondSep}`);
  console.log(""\n\n"");
  console.log(m.content);
}
 Now, let's import our required packages and instantiate our client, assistant, and thread. PythonJavascript

import httpx
from langchain_core.messages import convert_to_messages
from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent"";
const thread = await client.threads.create();

 Now we can run a thread and try to run a second one with the ""reject"" option, which should fail since we have already started a run: PythonJavascript

run = await client.runs.create(
    thread[""thread_id""],
    assistant_id,
    input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in sf?""}]},
)
try:
    await client.runs.create(
        thread[""thread_id""],
        assistant_id,
        input={
            ""messages"": [{""role"": ""human"", ""content"": ""what's the weather in nyc?""}]
        },
        multitask_strategy=""reject"",
    )
except httpx.HTTPStatusError as e:
    print(""Failed to start concurrent run"", e)

const run = await client.runs.create(
  thread[""thread_id""],
  assistantId,
  input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in sf?""}]},
);

try {
  await client.runs.create(
    thread[""thread_id""],
    assistantId,
    { 
      input: {""messages"": [{""role"": ""human"", ""content"": ""what's the weather in nyc?""}]},
      multitask_strategy:""reject""
    },
  );
} catch (e) {
  console.error(""Failed to start concurrent run"", e);
}

Failed to start concurrent run Client error '409 Conflict' for url 'http://localhost:8123/threads/f9e7088b-8028-4e5c-88d2-9cc9a2870e50/runs'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409

 We can verify that the original thread finished executing: PythonJavascript

# wait until the original run completes
await client.runs.join(thread[""thread_id""], run[""run_id""])

state = await client.threads.get_state(thread[""thread_id""])

for m in convert_to_messages(state[""values""][""messages""]):
    m.pretty_print()

await client.runs.join(thread[""thread_id""], run[""run_id""]);

const state = await client.threads.getState(thread[""thread_id""]);

for (const m of state[""values""][""messages""]) {
  prettyPrint(m);
}

 Output: ================================[1m Human Message [0m=================================

what's the weather in sf?
==================================[1m Ai Message [0m==================================

[{'id': 'toolu_01CyewEifV2Kmi7EFKHbMDr1', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01CyewEifV2Kmi7EFKHbMDr1)
 Call ID: toolu_01CyewEifV2Kmi7EFKHbMDr1
  Args:
    query: weather in san francisco
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{""url"": ""https://www.accuweather.com/en/us/san-francisco/94103/june-weather/347629"", ""content"": ""Get the monthly weather forecast for San Francisco, CA, including daily high/low, historical averages, to help you plan ahead.""}]
==================================[1m Ai Message [0m==================================

According to the search results from Tavily, the current weather in San Francisco is:

The average high temperature in San Francisco in June is around 65F (18C), with average lows around 54F (12C). June tends to be one of the cooler and foggier months in San Francisco due to the marine layer of fog that often blankets the city during the summer months.

Some key points about the typical June weather in San Francisco:

- Mild temperatures with highs in the 60s F and lows in the 50s F
- Foggy mornings that often burn off to sunny afternoons
- Little to no rainfall, as June falls in the dry season
- Breezy conditions, with winds off the Pacific Ocean
- Layers are recommended for changing weather conditions

So in summary, you can expect mild, foggy mornings giving way to sunny but cool afternoons in San Francisco this time of year. The marine layer keeps temperatures moderate compared to other parts of California in June.
","What happens when trying to run a second thread with the ""reject"" option after already starting a run in the original thread?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
View Thread,"
In the top of the right-hand pane, select the New Thread dropdown menu to view existing threads.
View the state of the thread (i.e. the output) in the right-hand pane.
To create a new thread, select + New Thread.
 The following video shows these exact steps being carried out: 

",What steps can be taken to view existing threads and create a new thread in the right-hand pane?
Edit Thread State,"The LangGraph Studio UI contains features for editing thread state. Explore these features in the right-hand pane. Select the Edit icon, modify the desired state, and then select Fork to invoke the assistant with the updated state. The following video shows how to edit a thread in the studio: 

",How can thread state be edited in LangGraph Studio UI?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to use Postgres checkpointer for persistence,"When creating LangGraph agents, you can also set them up so that they persist their state. This allows you to do things like interact with an agent multiple times and have it remember previous interactions. This example shows how to use Postgres as the backend for persisting checkpoint state using langgraph-checkpoint-postgres library. To start a Postgres database to work with you can do the following: $ cd libs/langgraph
$ make start-postgres
",How can Postgres checkpointer be used for persistence in LangGraph agents?
Use sync connection,"This sets up a synchronous connection to the database. Synchronous connections execute operations in a blocking manner, meaning each operation waits for completion before moving to the next one. The DB_URI is the database connection URI, with the protocol used for connecting to a PostgreSQL database, authentication, and host where database is running. The connection_kwargs dictionary defines additional parameters for the database connection.",What does setting up a synchronous connection to the database entail?
With a connection pool,"This manages a pool of reusable database connections: 
Advantages: Efficient resource utilization, improved performance for frequent connections
Best for: Applications with many short-lived database operations
",What are the advantages of using a connection pool in applications with many short-lived database operations?
With a connection,"This creates a single, dedicated connection to the database: 
Advantages: Simple to use, suitable for longer transactions
Best for: Applications with fewer, longer-lived database operations
","What are the advantages and best use cases of creating a single, dedicated connection to the database?"
With a connection string,"This creates a connection based on a connection string: 
Advantages: Simplicity, encapsulates connection details
Best for: Quick setup or when connection details are provided as a string
",What are the advantages of creating a connection based on a connection string?
Use async connection,This sets up an asynchronous connection to the database. Async connections allow non-blocking database operations. This means other parts of your application can continue running while waiting for database operations to complete. It's particularly useful in high-concurrency scenarios or when dealing with I/O-bound operations.,What are the benefits of using async connection to the database?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Test Cloud Deployment,"The LangGraph Studio UI connects directly to LangGraph Cloud deployments. Starting from the LangSmith UI... 
In the left-hand navigation panel, select Deployments. The Deployments view contains a list of existing LangGraph Cloud deployments.
Select an existing deployment to test with LangGraph Studio.
In the top-right corner, select Open LangGraph Studio.
Invoke an assistant or view an existing thread.
 The following video shows these exact steps being carried out: 

",How can you test a LangGraph Cloud deployment using LangGraph Studio?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to add node retry policies,"There are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. In order to configure the retry policy, you have to pass the retry parameter to the add_node function. The retry parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters:",How can you add node retry policies in your node configuration?
Passing a retry policy to a node,"Lastly, we can pass RetryPolicy objects when we call the add_node function. In the example below we pass two different retry policies to each of our nodes:",What can be passed when calling the add_node function in order to implement a retry policy for a node?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Reasoning without Observation,"In ReWOO, Xu, et. al, propose an agent that combines a multi-step planner and variable substitution for effective tool use. It was designed to improve on the ReACT-style agent architecture in the following ways: 
Reduce token consumption and execution time by generating the full chain of tools used in a single pass. (ReACT-style agent architecture requires many LLM calls with redundant prefixes (since the system prompt and previous steps are provided to the LLM for each reasoning step)
Simplify the fine-tuning process. Since the planning data doesn't depend on the outputs of the tool, models can be fine-tuned without actually invoking the tools (in theory).
 The following diagram outlines ReWOO's overall computation graph: ReWOO is made of 3 modules: 
Planner: Generate the plan in the following format:
 Plan: <reasoning>
#E1 = Tool[argument for tool]
Plan: <reasoning>
#E2 = Tool[argument for tool with #E1 variable substitution]
...
 
Worker: executes the tool with the provided arguments.
Solver: generates the answer for the initial task based on the tool observations.
 The modules with a  emoji depend on an LLM call. Notice that we avoid redundant calls to the planner LLM by using variable substitution. In this example, each module is represented by a LangGraph node. The end result will leave a trace that looks like this one. Let's get started!",How does ReWOO improve on the ReACT-style agent architecture in terms of reducing token consumption and execution time?
0. Prerequisites,"For this example, we will provide the agent with a Tavily search engine tool. You can get an API key here or replace with a free tool option (e.g., duck duck go search). To see the full langsmith trace, you can s",What tool will be provided to the agent in this example?
1. Planner,"The planner prompts an LLM to generate a plan in the form of a task list. The arguments to each task are strings that may contain special variables (#E{0-9}+) that are used for variable substitution from other task results. Our example agent will have two tools: 
Google - a search engine (in this case Tavily)
LLM - an LLM call to reason about previous outputs.
 The LLM tool receives less of the prompt context and so can be more token-efficient than the ReACT paradigm.",What are the two tools available to the example agent in generating a plan using the planner?
2. Executor,"The executor receives the plan and executes the tools in sequence. Below, instantiate the search engine and define the tool execution node.",What is the role of the executor in the given context?
3. Solver,The solver receives the full plan and generates the final response based on the responses of the tool calls from the worker.,What role does the solver play in the process described in the text?
4. Define Graph,"Our graph defines the workflow. Each of the planner, tool executor, and solver modules are added as nodes.","What does the graph define and how are the planner, tool executor, and solver modules represented within it?"
Conclusion,"Congratulations on implementing ReWOO! Before you leave, I'll leave you with a couple limitations of the current implementation from the paper: 
If little context of the environment is available, the planner will be ineffective in its tool use. This can typically be ameliorated through few-shot prompting and/or fine-tuning.
The tasks are still executed in sequence, meaning the total execution time is impacted by every tool call, not just the longest-running in a given step.
",What are some limitations of the current implementation of ReWOO mentioned in the conclusion?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to pass graph state to tools,"Sometimes we need to pass in agent state to our tools. This type of stateful tools is useful when a tool's output is affected by past agent steps (e.g. if you're using a sub-agent as a tool, and want to pass the message history in to the sub-agent), or when a tool's input needs to be validated given context from past agent steps. In this guide we'll demonstrate how to create tools that take agent state as input. This is a special case of passing runtime arguments to tools, which you can learn about in the LangChain docs.",How can agent state be passed to tools?
Setup,First we need to install the packages required,What is the first step in the setup process?
Defining the tools,"We'll want our tool to take graph state as an input, but we don't want the model to try to generate this input when calling the tool. We can use the InjectedState annotation to mark arguments as required graph state (or some field of graph state. These arguments will not be generated by the model. When using ToolNode, graph state will automatically be passed in to the relevant tools and arguments. In this example we'll create a tool that returns Documents and then another tool that actually cites the Documents that justify a claim.",How can we ensure that the tool does not generate the input graph state when called?
Define the agent state,"The main type of graph in langgraph is the StateGraph.
This graph is parameterized by a state object that it passes around to each node.
Each node then returns operations to update that state.
These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute.
Whether to set or add is denoted by annotating the state object you construct the graph with. For this example, the state we will track will just be a list of messages.
We want each node to just add messages to that list.
Therefore, we will use a TypedDict with one key (messages) and annotate it so that the messages attribute is always added to.",What type of graph is used in langgraph and how is the state object parameterized in it?
Define the nodes,"We now need to define a few different nodes in our graph.
In langgraph, a node can be either a function or a runnable.
There are two main nodes we need for this: 
The agent: responsible for deciding what (if any) actions to take.
A function to invoke tools: if the agent decides to take an action, this node will then execute that action.
 We will also need to define some edges.
Some of these edges may be conditional.
The reason they are conditional is that based on the output of a node, one of several paths may be taken.
The path that is taken is not known until that node is run (the LLM decides). 
Conditional Edge: after the agent is called, we should either:
a. If the agent said to take an action, then the function to invoke tools should be called
b. If the agent said that it was finished, then it should finish
Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next
 Let's define the nodes, as well as a function to decide how what conditional edge to take.",What are the two main nodes that need to be defined in the graph?
Define the graph,We can now put it all together and define the graph!,What can we define by putting it all together?
Use it!,"We can now use it!
This now exposes the same interface as all other LangChain runnables.",What interface does it now expose?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to add breakpoints,"Human-in-the-loop (HIL) interactions are crucial for agentic systems. Breakpoints are a common HIL interaction pattern, allowing the graph to stop at specific steps and seek human approval before proceeding (e.g., for sensitive actions). Breakpoints are built on top of LangGraph checkpoints, which save the graph's state after each node execution. Checkpoints are saved in threads that preserve graph state and can be accessed after a graph has finished execution. This allows for graph execution to pause at specific points, await human approval, and then resume execution from the last checkpoint.",What is the purpose of breakpoints in agentic systems and how are they implemented using LangGraph checkpoints?
Setup,First we need to install the packages required,What is the first step in the setup process?
Simple Usage,"Let's look at very basic usage of this. Below, we do two things: 
We specify the breakpoint using interrupt_before the specified step.

We set up a checkpointer to save the state of the graph.

",What are the two things done in the very basic usage of this tool?
Agent,"In the context of agents, breakpoints are useful to manually approve certain agent actions. To show this, we will build a relatively simple ReAct-style agent that does tool calling. We'll add a breakpoint before the action node is called.",What is the purpose of breakpoints in the context of agents?
Interacting with the Agent,"We can now interact with the agent. We see that it stops before calling a tool, because interrupt_before is set before the action node.",What happens when the interrupt_before is set before the action node while interacting with the agent?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to delete messages,"One of the common states for a graph is a list of messages. Usually you only add messages to that state. However, sometimes you may want to remove messages (either by directly modifying the state or as part of the graph). To do that, you can use the RemoveMessage modifier. In this guide, we will cover how to do that. The key idea is that each state key has a reducer key. This key specifies how to combine updates to the state. The default MessagesState has a messages key, and the reducer for that key accepts these RemoveMessage modifiers. That reducer then uses these RemoveMessage to delete messages from the key. So note that just because your graph state has a key that is a list of messages, it doesn't mean that that this RemoveMessage modifier will work. You also have to have a reducer defined that knows how to work with this. NOTE: Many models expect certain rules around lists of messages. For example, some expect them to start with a user message, others expect all messages with tool calls to be followed by a tool message. When deleting messages, you will want to make sure you don't violate these rules.",How can messages be deleted in a graph state?
Setup,"First, let's build a simple graph that uses messages. Note that it's using the MessagesState which has the required reducer.",What state is being used in the simple graph setup that involves messages?
Build the agent,Let's now build a simple ReAct style agent.,What style of agent are we building?
Manually deleting messages,"First, we will cover how to manually delete messages. Let's take a look at the current state of the thread:",What will be covered first in the instructions?
Programmatically deleting messages,We can also delete messages programmatically from inside the graph. Here we'll modify the graph to delete any old messages (longer than 3 messages ago) at the end of a graph run.,How can messages be deleted programmatically from inside the graph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to call tools using ToolNode,"This guide covers how to use LangGraph's prebuilt ToolNode for tool calling. ToolNode is a LangChain Runnable that takes graph state (with a list of messages) as input and outputs state update with the result of tool calls. It is designed to work well out-of-box with LangGraph's prebuilt ReAct agent, but can also work with any StateGraph as long as its state has a messages key with an appropriate reducer (see MessagesState).",What is ToolNode and how can it be used for tool calling in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to view and update past graph state,"Once you start checkpointing your graphs, you can easily get or update the state of the agent at any point in time. This permits a few things: 
You can surface a state during an interrupt to a user to let them accept an action.
You can rewind the graph to reproduce or avoid issues.
You can modify the state to embed your agent into a larger system, or to let the user better control its actions.
 The key methods used for this functionality are: 
get_state: fetch the values from the target config
update_state: apply the given values to the target state
 Note: this requires passing in a checkpointer. Below is a quick example.",How can you view and update the past graph state using checkpointing in your graphs?
Setup,First we need to install the packages required,What is the first step in the setup process?
Build the agent,We can now build the agent. We will build a relatively simple ReAct-style agent that does tool calling. We will use Anthropic's models and a fake tool (just for demo purposes).,What type of agent will be built using Anthropic's models and a fake tool for demo purposes?
Interacting with the Agent,We can now interact with the agent. Let's ask it for the weather in SF.,What can we ask the agent to provide information about?
Checking history,"Let's browse the history of this thread, from start to finish.",What action are we taking in regards to the thread's history?
Replay a state,We can go back to any of these states and restart the agent from there! Let's go back to right before the tool call gets executed.,What can we do with the states in the system?
Branch off a past state,"Using LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user ""version control"" changes in a workflow. Let's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool",How can LangGraph's checkpointing feature be used to branch off a past state and explore alternate trajectories or version control changes in a workflow?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to stream debug events,"This guide covers how to stream debug events from your graph (stream_mode=""debug""). First let's set up our client and thread: PythonJavascript

from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
# create thread
thread = await client.threads.create()
print(thread)

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
// create thread
const thread = await client.threads.create();
console.log(thread)

 Output: {'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3',
 'created_at': '2024-06-21T22:10:27.696862+00:00',
 'updated_at': '2024-06-21T22:10:27.696862+00:00',
 'metadata': {}}
 Streaming debug events produces responses containing type and timestamp keys. Debug events correspond to different steps in the graph's execution (e.g. task, task_result, checkpoint). PythonJavascript

# create input
input = {
    ""messages"": [
        {
            ""role"": ""human"",
            ""content"": ""What's the weather in SF?"",
        }
    ]
}

# stream debug
async for chunk in client.runs.stream(
    thread_id=thread[""thread_id""],
    assistant_id=""agent"",
    input=input,
    stream_mode=""debug"",
):
    print(f""Receiving new event of type: {chunk.event}..."")
    print(chunk.data)
    print(""\n\n"")

// create input
const input = {
    ""messages"": [
        {
            ""role"": ""human"",
            ""content"": ""What's the weather in SF?"",
        }
    ]
}

// stream debug
const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    streamMode: ""debug""
  }
);
for await (const chunk of streamResponse) {
  console.log(f""Receiving new event of type: {chunk.event}..."")
  console.log(chunk.data)
  console.log(""\n\n"")
}

 Output: Receiving new event of type: metadata...
{'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a'}

Receiving new event of type: debug...
{'type': 'checkpoint', 'timestamp': '2024-06-21T22:11:09.256850+00:00', 'step': -1, 'payload': {'config': {'tags': [], 'metadata': {'created_by': 'system', 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'callbacks': [None], 'recursion_limit': 25, 'configurable': {'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'thread_ts': '1ef301b2-9a2e-6bb6-bfff-8423bcf47561', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a'}, 'values': {'messages': []}, 'metadata': {'source': 'input', 'step': -1, 'writes': {'messages': [{'role': 'human', 'content': ""What's the weather in SF?""}]}}}}

Receiving new event of type: debug...
{'type': 'checkpoint', 'timestamp': '2024-06-21T22:11:09.259723+00:00', 'step': 0, 'payload': {'config': {'tags': [], 'metadata': {'created_by': 'system', 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'callbacks': [None], 'recursion_limit': 25, 'configurable': {'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'thread_ts': '1ef301b2-9a35-6c86-8000-f4a85315dbeb', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a'}, 'values': {'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '906529f7-fbf2-41c9-a28c-b1fe8f891e4e', 'example': False}]}, 'metadata': {'source': 'loop', 'step': 0, 'writes': None}}}

Receiving new event of type: debug...
{'type': 'task', 'timestamp': '2024-06-21T22:11:09.260021+00:00', 'step': 1, 'payload': {'id': '12ab1026-a551-5f96-9ad3-43424f094774', 'name': 'agent', 'input': {'some_bytes': None, 'some_byte_array': None, 'dict_with_bytes': None, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '906529f7-fbf2-41c9-a28c-b1fe8f891e4e', 'example': False}], 'sleep': None}, 'triggers': ['start:agent']}}

Receiving new event of type: debug...
{'type': 'task_result', 'timestamp': '2024-06-21T22:11:09.267632+00:00', 'step': 1, 'payload': {'id': '12ab1026-a551-5f96-9ad3-43424f094774', 'name': 'agent', 'result': [['some_bytes', 'c29tZV9ieXRlcw=='], ['some_byte_array', 'c29tZV9ieXRlX2FycmF5'], ['dict_with_bytes', {'more_bytes': 'bW9yZV9ieXRlcw=='}], ['messages', [{'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-54bd965b-734a-4a0a-8d4d-840865054810', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]]]}}

Receiving new event of type: debug...
{'type': 'checkpoint', 'timestamp': '2024-06-21T22:11:09.268469+00:00', 'step': 1, 'payload': {'config': {'tags': [], 'metadata': {'created_by': 'system', 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'callbacks': [None], 'recursion_limit': 25, 'configurable': {'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'thread_ts': '1ef301b2-9a4b-60ae-8001-dd378f965bf7', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a'}, 'values': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '906529f7-fbf2-41c9-a28c-b1fe8f891e4e', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-54bd965b-734a-4a0a-8d4d-840865054810', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}, 'metadata': {'source': 'loop', 'step': 1, 'writes': {'agent': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-54bd965b-734a-4a0a-8d4d-840865054810', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}}}}

Receiving new event of type: debug...
{'type': 'task', 'timestamp': '2024-06-21T22:11:09.268659+00:00', 'step': 2, 'payload': {'id': '494ad427-fe8d-5654-91e6-50495a2699f5', 'name': 'tool', 'input': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '906529f7-fbf2-41c9-a28c-b1fe8f891e4e', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-54bd965b-734a-4a0a-8d4d-840865054810', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'sleep': None}, 'triggers': ['branch:agent:should_continue:tool']}}

Receiving new event of type: debug...
{'type': 'task_result', 'timestamp': '2024-06-21T22:11:09.272916+00:00', 'step': 2, 'payload': {'id': '494ad427-fe8d-5654-91e6-50495a2699f5', 'name': 'tool', 'result': [['messages', [{'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '222ed3b8-450f-41cb-ac40-905def3c700a', 'tool_call_id': 'tool_call_id'}]]]}}

Receiving new event of type: debug...
{'type': 'checkpoint', 'timestamp': '2024-06-21T22:11:09.273113+00:00', 'step': 2, 'payload': {'config': {'tags': [], 'metadata': {'created_by': 'system', 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'callbacks': [None], 'recursion_limit': 25, 'configurable': {'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'thread_ts': '1ef301b2-9a56-6832-8002-8ab17e662980', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a'}, 'values': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '906529f7-fbf2-41c9-a28c-b1fe8f891e4e', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-54bd965b-734a-4a0a-8d4d-840865054810', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '222ed3b8-450f-41cb-ac40-905def3c700a', 'tool_call_id': 'tool_call_id'}]}, 'metadata': {'source': 'loop', 'step': 2, 'writes': {'tool': {'messages': [{'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '222ed3b8-450f-41cb-ac40-905def3c700a', 'tool_call_id': 'tool_call_id'}]}}}}}

Receiving new event of type: debug...
{'type': 'task', 'timestamp': '2024-06-21T22:11:09.273192+00:00', 'step': 3, 'payload': {'id': '677de327-99b7-5d97-9bbd-0092abb62d46', 'name': 'agent', 'input': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '906529f7-fbf2-41c9-a28c-b1fe8f891e4e', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-54bd965b-734a-4a0a-8d4d-840865054810', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '222ed3b8-450f-41cb-ac40-905def3c700a', 'tool_call_id': 'tool_call_id'}], 'sleep': None}, 'triggers': ['tool']}}

Receiving new event of type: debug...
{'type': 'task_result', 'timestamp': '2024-06-21T22:11:09.277262+00:00', 'step': 3, 'payload': {'id': '677de327-99b7-5d97-9bbd-0092abb62d46', 'name': 'agent', 'result': [['some_bytes', 'c29tZV9ieXRlcw=='], ['some_byte_array', 'c29tZV9ieXRlX2FycmF5'], ['dict_with_bytes', {'more_bytes': 'bW9yZV9ieXRlcw=='}], ['messages', [{'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-006e1758-b1ca-4c90-9ff3-d2e75b9ca9a7', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]]]}}

Receiving new event of type: debug...
{'type': 'checkpoint', 'timestamp': '2024-06-21T22:11:09.277519+00:00', 'step': 3, 'payload': {'config': {'tags': [], 'metadata': {'created_by': 'system', 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'callbacks': [None], 'recursion_limit': 25, 'configurable': {'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'd0cbe9ad-f11c-443a-9f6f-dca0ae5a0dd3', 'thread_ts': '1ef301b2-9a61-6462-8003-1316d9875b7f', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'run_id': '1ef301b2-9a0c-68d6-bbb1-0763efc8489a'}, 'values': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '906529f7-fbf2-41c9-a28c-b1fe8f891e4e', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-54bd965b-734a-4a0a-8d4d-840865054810', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '222ed3b8-450f-41cb-ac40-905def3c700a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-006e1758-b1ca-4c90-9ff3-d2e75b9ca9a7', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}, 'metadata': {'source': 'loop', 'step': 3, 'writes': {'agent': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-006e1758-b1ca-4c90-9ff3-d2e75b9ca9a7', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}}}}

Receiving new event of type: end...
None
",What are the different types of debug events that can be streamed from the graph's execution?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
How to Replay and Branch from Prior States,"With LangGraph Cloud you have the ability to return to any of your prior states and either re-run the graph to reproduce issues noticed during testing, or branch out in a different way from what was originally done in the prior states. In this guide we will show a quick example of how to rerun past states and how to branch off from previous states as well.",How can LangGraph Cloud users replay and branch from prior states?
Setup,"We are not going to show the full code for the graph we are hosting, but you can see it here if you want to. Once this graph is hosted, we are ready to invoke it and wait for user input. ",What is the next step after hosting the graph?
SDK initialization,"First, we need to setup our client so that we can communicate with our hosted graph: PythonJavascriptCURL

from langgraph_sdk import get_client
client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = agent;
const thread = await client.threads.create();

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

","How can we initialize the SDK to communicate with our hosted graph using Python, JavaScript, and CURL?"
Initial invocation,"Before replaying a state - we need to create states to replay from! In order to do this, let's invoke our graph with a simple message: PythonJavascriptCURL

input = { 'messages':[{ ""role"":""user"", ""content"":""Please search the weather in SF"" }] }

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id, # graph_id
    input=input,
    stream_mode=""updates"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const input = {""messages"": [{ ""role"": ""human"", ""content"": ""Please search the weather in SF""}] }

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""updates"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""Please search the weather in SF\""}]},
   \""stream_mode\"": [
     \""updates\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
     sub(/^event: /, """", $0)
     event_type = $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
 }
 '

 Output: {'agent': {'messages': [{'content': [{'text': ""Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now."", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
{'action': {'messages': [{'content': '[""I looked up: current weather in San Francisco. Result: It\'s sunny in San Francisco, but you better look out if you\'re a Gemini .""]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}
{'agent': {'messages': [{'content': ""Based on the search results, I can provide you with information about the current weather in San Francisco:\n\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\n\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\n\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you."", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
 Now let's get our list of states, and invoke from the third state (right before the tool get called): PythonJavascriptCURL

states = await client.threads.get_history(thread['thread_id'])

# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node
state_to_replay = states[2]
print(state_to_replay['next'])

const states = await client.threads.getHistory(thread['thread_id']);

// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node
const stateToReplay = states[2];
console.log(stateToReplay['next']);

curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'

 Output: ['action']
 To rerun from a state, we need to pass in the checkpoint_id into the config of the run like follows: PythonJavascriptCURL

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id, # graph_id
    input=None,
    stream_mode=""updates"",
    config={""configurable"": {""checkpoint_id"": state_to_replay['checkpoint_id']}}
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: null,
    streamMode: ""updates"",
    config: {""configurable"": {""checkpoint_id"": stateToReplay['checkpoint_id']}},
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].checkpoint_id' | {  
read checkpoint_id   
curl --request POST \                                                            
     --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
     --header 'Content-Type: application/json' \
     --data ""{                     
       \""assistant_id\"": \""agent\"",
       \""config\"": {\""configurable\"": {\""checkpoint_id\"": \""$checkpoint_id\""}},
       \""stream_mode\"": [
         \""updates\""
       ]   
     }"" | \           
     sed 's/\r$//' | \
     awk '      
     /^event:/ {                                              
         if (data_content != """" && event_type != ""metadata"") {
             print data_content ""\n""
         }                      
         sub(/^event: /, """", $0)
         event_type = $0  
         data_content = """"
     }         
     /^data:/ {                
         sub(/^data: /, """", $0)
         data_content = $0
     }    
     END {                                                    
         if (data_content != """" && event_type != ""metadata"") {
             print data_content ""\n""
         }
     }
     ' 

 Output: {'action': {'messages': [{'content': '[""I looked up: current weather in San Francisco. Result: It\'s sunny in San Francisco, but you better look out if you\'re a Gemini .""]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}
{'agent': {'messages': [{'content': ""Based on the search results, I can provide you with information about the current weather in San Francisco:\n\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\n\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\n\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
 As we can see, the graph restarted from the tool node with the same input as our original graph run.",What is the next step to rerun a state in the graph with the same input as the original run?
Branch off from previous state,"Using LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user ""version control"" changes in a workflow. Let's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool PythonJavascriptCURL

# Let's now get the last message in the state
# This is the one with the tool calls that we want to update
last_message = state_to_replay['values']['messages'][-1]

# Let's now update the args for that tool call
last_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}

new_state = await client.threads.update_state(thread['thread_id'],{""messages"":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])

// Let's now get the last message in the state
// This is the one with the tool calls that we want to update
let lastMessage = stateToReplay['values']['messages'][-1];

// Let's now update the args for that tool call
lastMessage['tool_calls'][0]['args'] = {'query': 'current weather in SF'};

const newState = await client.threads.updateState(thread['thread_id'],{values:{""messages"":[lastMessage]},checkpointId:stateToReplay['checkpoint_id']});

curl -s --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | \
jq -c '
    .[2] as $state_to_replay |
    .[2].values.messages[-1].tool_calls[0].args.query = ""current weather in SF"" |
    {
        values: { messages: .[2].values.messages[-1] },
        checkpoint_id: $state_to_replay.checkpoint_id
    }' | \
curl --request POST \
    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \
    --header 'Content-Type: application/json' \
    --data @-

 Now we can rerun our graph with this new config, starting from the new_state, which is a branch of our state_to_replay: PythonJavascriptCURL

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant[""assistant_id""], # graph_id
    input=None,
    stream_mode=""updates"",
    config={""configurable"": {""checkpoint_id"": new_state['configurable']['checkpoint_id']}}
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistant[""assistant_id""],
  {
    input: null,
    streamMode: ""updates"",
    config: {""configurable"": {""checkpoint_id"": newState['configurable']['checkpoint_id']}},
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl -s --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state | \
jq -r '.config.configurable.checkpoint_id' | \
sh -c '
    CHECKPOINT_ID=""$1""
    curl --request POST \
        --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
        --header ""Content-Type: application/json"" \
        --data ""{\""assistant_id\"": \""agent\"", \""config\"": {\""configurable\"": {\""checkpoint_id\"": \""$CHECKPOINT_ID\""}}, \""stream_mode\"": [\""updates\""]}"" | \
    sed ""s/\r$//"" | \
    awk ""
    /^event:/ {
        if (data_content != \""\"" && event_type != \""metadata\"") {
            print data_content \""\n\""
        }
        sub(/^event: /, \""\"", \$0)
        event_type = \$0
        data_content = \""\""
    }
    /^data:/ {
        sub(/^data: /, \""\"", \$0)
        data_content = \$0
    }
    END {
        if (data_content != \""\"" && event_type != \""metadata\"") {
            print data_content \""\n\""
        }
    }""
' _ 

 Output: {'action': {'messages': [{'content': '[""I looked up: current weather in SF. Result: It\'s sunny in San Francisco, but you better look out if you\'re a Gemini .""]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '2baf9941-4fda-4081-9f87-d76795d289f1', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}
{'agent': {'messages': [{'content': ""Based on the search results, I can provide you with information about the current weather in San Francisco (SF):\n\nThe weather in San Francisco is currently sunny. This means it's a clear day with plenty of sunshine. \n\nIt's worth noting that the specific temperature wasn't provided in the search result, but sunny weather in San Francisco typically means comfortable temperatures. San Francisco is known for its mild climate, so even on sunny days, it's often not too hot.\n\nThe search result also included a playful reference to astrological signs, mentioning Gemini. However, this is likely just a joke or part of the search engine's presentation and not related to the actual weather conditions.\n\nIs there any specific information about the weather in San Francisco you'd like to know more about? I'd be happy to perform another search if you need details on temperature, wind conditions, or the forecast for the coming days."", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-a83de52d-ed18-4402-9384-75c462485743', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
 As we can see, the search query changed from San Francisco to SF, just as we had hoped!",What changes were made to the search query in the output based on the provided information?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Corrective RAG (CRAG),"Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents. In the paper here, a few steps are taken: 
If at least one document exceeds the threshold for relevance, then it proceeds to generation
Before generation, it performs knowledge refinement
This partitions the document into ""knowledge strips""
It grades each strip, and filters our irrelevant ones
If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource
It will use web search to supplement retrieval
 We will implement some of these ideas from scratch using LangGraph: 
Let's skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired.
If any documents are irrelevant, let's opt to supplement retrieval with web search.
We'll use Tavily Search for web search.
Let's use query re-writing to optimize the query for web search.
",What steps are taken in the Corrective RAG (CRAG) strategy for RAG?
Search,We'll use Tavily Search for web search.,What search engine will be used for web search?
Tracing,"Optionally, use LangSmith for tracing (shown at bottom) by setting","What tool can be used for tracing, as shown at the bottom?"
Index,Let's index 3 blog posts.,What is the purpose of the index in this context?
Graph,Capture the flow in as a graph.,What can be used to capture the flow as a graph?
Build Graph,The just follows the flow we outlined in the figure above.,What does the build graph process follow according to the figure above?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling a graph and why is it necessary?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes based on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Reflexion,"Reflexion by Shinn, et. al., is an architecture designed to learn through verbal feedback and self-reflection. The agent explicitly critiques its responses for tasks to generate a higher quality final response, at the expense of longer execution time. The paper outlines 3 main components: 
Actor (agent) with self-reflection
External evaluator (task-specific, e.g. code compilation steps)
Episodic memory that stores the reflections from (1).
 In their code, the last two components are very task-specific, so in this notebook, you will build the actor in LangGraph. To skip to the graph definition, see the Construct Graph section below.","What are the three main components outlined in the architecture of Reflexion by Shinn, et. al.?"
0. Prerequisites,"Install langgraph (for the framework), langchain_openai (for the LLM), and langchain + tavily-python (for the search engine). We will use tavily search as a tool. You can get an API key here or replace with a different tool of your choosing.",What tools are required as prerequisites for this project?
1. Actor (with reflection),"The main component of Reflexion is the ""actor"", which is an agent that reflects on its response and re-executes to improve based on self-critique. It's main sub-components include: 
Tools/tool execution
Initial responder: generate an initial response (and self-reflection)
Revisor: re-respond (and reflec) based on previous reflections
 We'll first define the tool execution context. Construct tools",What is the main component of Reflexion and how does it improve based on self-critique?
Create Tool Node,"Next, create a node to execute the tool calls. While we give the LLMs different schema names (and use those for validation), we want them both to route to the same tool.",What is the purpose of creating a node to execute the tool calls in the given text?
Construct Graph,Now we can wire all our components together.,What can we do now that we have wired all our components together?
Conclusion,"Congrats on building a Reflexion actor! I'll leave you with a few observations to save you some time when choosing which parts of this agent to adapt to your workflow: 
This agent trades off execution time for quality. It explicitly forces the agent to critique and revise the output over several steps, which usually (not always) increases the response quality but takes much longer to return a final answer
The 'reflections' can be paired with additional external feedback (such as validators), to further guide the actor.
In the paper, 1 environment (AlfWorld) uses external memory. It does this by storing summaries of the reflections to an external store and using them in subsequent trials/invocations.
",How does the Reflexion actor trade off execution time for quality in its output?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to Edit State of a Deployed Graph,"When creating LangGraph agents, it is often nice to add a human-in-the-loop component. This can be helpful when giving them access to tools. Often in these situations you may want to edit the graph state before continuing (for example, to edit what tool is being called, or how it is being called). This can be in several ways, but the primary supported way is to add an ""interrupt"" before a node is executed. This interrupts execution at that node. You can then use update_state to update the state, and then resume from that spot to continue.",How can you edit the state of a deployed graph when creating LangGraph agents with a human-in-the-loop component?
Setup,"We are not going to show the full code for the graph we are hosting, but you can see it here if you want to. Once this graph is hosted, we are ready to invoke it and wait for user input. ",What is the next step after hosting the graph?
SDK initialization,"First, we need to setup our client so that we can communicate with our hosted graph: PythonJavascriptCURL

from langgraph_sdk import get_client
client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent"";
const thread = await client.threads.create();

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

","How can we initialize the SDK to communicate with our hosted graph using Python, JavaScript, and CURL?"
Initial invocation,"Now let's invoke our graph, making sure to interrupt before the action node. PythonJavascriptCURL

input = { 'messages':[{ ""role"":""user"", ""content"":""search for weather in SF"" }] }

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id,
    input=input,
    stream_mode=""updates"",
    interrupt_before=[""action""],
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const input = {""messages"": [{ ""role"": ""human"", ""content"": ""search for weather in SF""}] }

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""updates"",
    interruptBefore: [""action""],
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""search for weather in SF\""}]},
   \""interrupt_before\"": [\""action\""],
   \""stream_mode\"": [
     \""updates\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
     sub(/^event: /, """", $0)
     event_type = $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
 }
 '

 Output: {'agent': {'messages': [{'content': [{'text': ""Certainly! I'll search for the current weather in San Francisco for you using the search function. Here's how I'll do that:"", 'type': 'text'}, {'id': 'toolu_01KEJMBFozSiZoS4mAcPZeqQ', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-6dbb0167-f8f6-4e2a-ab68-229b2d1fbb64', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_01KEJMBFozSiZoS4mAcPZeqQ'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
","What is the process for invoking the graph and ensuring interruption before reaching the action node in Python, JavaScript, and CURL?"
Edit the state,"Now, let's assume we actually meant to search for the weather in Sidi Frej (another city with the initials SF). We can edit the state to properly reflect that: PythonJavascriptCURL

# First, lets get the current state
current_state = await client.threads.get_state(thread['thread_id'])

# Let's now get the last message in the state
# This is the one with the tool calls that we want to update
last_message = current_state['values']['messages'][-1]

# Let's now update the args for that tool call
last_message['tool_calls'][0]['args'] = {'query': 'current weather in Sidi Frej'}

# Let's now call `update_state` to pass in this message in the `messages` key
# This will get treated as any other update to the state
# It will get passed to the reducer function for the `messages` key
# That reducer function will use the ID of the message to update it
# It's important that it has the right ID! Otherwise it would get appended
# as a new message
await client.threads.update_state(thread['thread_id'], {""messages"": last_message})

// First, lets get the current state
const currentState = await client.threads.getState(thread['thread_id']);

// Let's now get the last message in the state
// This is the one with the tool calls that we want to update
let lastMessage = currentState['values']['messages'][-1];

// Let's now update the args for that tool call
lastMessage['tool_calls'][0]['args'] = {'query': 'current weather in Sidi Frej'};

// Let's now call `update_state` to pass in this message in the `messages` key
// This will get treated as any other update to the state
// It will get passed to the reducer function for the `messages` key
// That reducer function will use the ID of the message to update it
// It's important that it has the right ID! Otherwise it would get appended
// as a new message
await client.threads.updateState(thread['thread_id'], {values:{""messages"": lastMessage}});

curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state | \                                                                                      
jq '.values.messages[-1] | (.tool_calls[0].args = {""query"": ""current weather in Sidi Frej""})' | \
curl --request POST \
  --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \
  --header 'Content-Type: application/json' \
  --data @-

 Output: {'configurable': {'thread_id': '9c8f1a43-9dd8-4017-9271-2c53e57cf66a',
  'checkpoint_ns': '',
  'checkpoint_id': '1ef58e7e-3641-649f-8002-8b4305a64858'}}
",How can the state be edited to search for the weather in Sidi Frej instead of the current location?
Resume invocation,"Now we can resume our graph run but with the updated state: PythonJavascriptCURL

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id,
    input=None,
    stream_mode=""updates"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: null,
    streamMode: ""updates"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

curl --request POST \                                                                             
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""stream_mode\"": [
     \""updates\""
   ]
 }""| \ 
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
     sub(/^event: /, """", $0)
     event_type = $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """" && event_type != ""metadata"") {
         print data_content ""\n""
     }
 }
 '

 Output: {'action': {'messages': [{'content': '[""I looked up: current weather in Sidi Frej. Result: It\'s sunny in San Francisco, but you better look out if you\'re a Gemini .""]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '1161b8d1-bee4-4188-9be8-698aecb69f10', 'tool_call_id': 'toolu_01KEJMBFozSiZoS4mAcPZeqQ'}]}}
{'agent': {'messages': [{'content': [{'text': 'I apologize for the confusion in my search query. It seems the search function interpreted ""SF"" as ""Sidi Frej"" instead of ""San Francisco"" as we intended. Let me search again with the full city name to get the correct information:', 'type': 'text'}, {'id': 'toolu_0111rrwgfAcmurHZn55qjqTR', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-b8c25779-cfb4-46fc-a421-48553551242f', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_0111rrwgfAcmurHZn55qjqTR'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
{'action': {'messages': [{'content': '[""I looked up: current weather in San Francisco. Result: It\'s sunny in San Francisco, but you better look out if you\'re a Gemini .""]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '6bc632ae-5ee6-4d01-9532-79c524a2d443', 'tool_call_id': 'toolu_0111rrwgfAcmurHZn55qjqTR'}]}}
{'agent': {'messages': [{'content': ""Now, based on the search results, I can provide you with information about the current weather in San Francisco:\n\nThe weather in San Francisco is currently sunny. \n\nIt's worth noting that the search result included an unusual comment about Gemini, which doesn't seem directly related to the weather. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of weather information, we can focus on the fact that it's sunny in San Francisco right now.\n\nIs there anything else you'd like to know about the weather in San Francisco or any other location?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-227a042b-dd97-476e-af32-76a3703af5d8', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}
 As you can see it now looks up the current weather in Sidi Frej (although our dummy search node still returns results for SF because we don't actually do a search in this example, we just return the same ""It's sunny in San Francisco ..."" result every time).","What is the updated state for resuming the graph run in Python, JavaScript, and CURL?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graph Definitions,"Graphs are the core abstraction of LangGraph. Each StateGraph implementation is used to create graph workflows. Once compiled, you can run the CompiledGraph to run the application.",What are the core abstractions of LangGraph and how are they used to create graph workflows?
StateGraph,"from langgraph.graph import StateGraph
from typing_extensions import TypedDict
class MyState(TypedDict)
    ...
graph = StateGraph(MyState)
 

              Bases: Graph
A graph whose nodes communicate by reading and writing to a shared state.
The signature of each node is State -> Partial.
Each state key can optionally be annotated with a reducer function that
will be used to aggregate the values of that key received from multiple nodes.
The signature of a reducer function is (Value, Value) -> Value.
Parameters:

state_schema
              (Type[Any], default:
                  None
)
          
          
The schema class that defines the state.

config_schema
              (Optional[Type[Any]], default:
                  None
)
          
          
The schema class that defines the configuration.
Use this to expose configurable parameters in your API.

Examples:
>>> from langchain_core.runnables import RunnableConfig
>>> from typing_extensions import Annotated, TypedDict
>>> from langgraph.checkpoint.memory import MemorySaver
>>> from langgraph.graph import StateGraph
>>>
>>> def reducer(a: list, b: int | None) -> int:
...     if b is not None:
...         return a + [b]
...     return a
>>>
>>> class State(TypedDict):
...     x: Annotated[list, reducer]
>>>
>>> class ConfigSchema(TypedDict):
...     r: float
>>>
>>> graph = StateGraph(State, config_schema=ConfigSchema)
>>>
>>> def node(state: State, config: RunnableConfig) -> dict:
...     r = config[""configurable""].get(""r"", 1.0)
...     x = state[""x""][-1]
...     next_value = x * r * (1 - x)
...     return {""x"": next_value}
>>>
>>> graph.add_node(""A"", node)
>>> graph.set_entry_point(""A"")
>>> graph.set_finish_point(""A"")
>>> compiled = graph.compile()
>>>
>>> print(compiled.config_specs)
[ConfigurableFieldSpec(id='r', annotation=<class 'float'>, name=None, description=None, default=None, is_shared=False, dependencies=None)]
>>>
>>> step1 = compiled.invoke({""x"": 0.5}, {""configurable"": {""r"": 3.0}})
>>> print(step1)
{'x': [0.5, 0.75]}

Source code in libs/langgraph/langgraph/graph/state.py
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 83
 84
 85
 86
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461class StateGraph(Graph):
    """"""A graph whose nodes communicate by reading and writing to a shared state.
    The signature of each node is State -> Partial<State>.

    Each state key can optionally be annotated with a reducer function that
    will be used to aggregate the values of that key received from multiple nodes.
    The signature of a reducer function is (Value, Value) -> Value.

    Args:
        state_schema (Type[Any]): The schema class that defines the state.
        config_schema (Optional[Type[Any]]): The schema class that defines the configuration.
            Use this to expose configurable parameters in your API.

    Examples:
        >>> from langchain_core.runnables import RunnableConfig
        >>> from typing_extensions import Annotated, TypedDict
        >>> from langgraph.checkpoint.memory import MemorySaver
        >>> from langgraph.graph import StateGraph
        >>>
        >>> def reducer(a: list, b: int | None) -> int:
        ...     if b is not None:
        ...         return a + [b]
        ...     return a
        >>>
        >>> class State(TypedDict):
        ...     x: Annotated[list, reducer]
        >>>
        >>> class ConfigSchema(TypedDict):
        ...     r: float
        >>>
        >>> graph = StateGraph(State, config_schema=ConfigSchema)
        >>>
        >>> def node(state: State, config: RunnableConfig) -> dict:
        ...     r = config[""configurable""].get(""r"", 1.0)
        ...     x = state[""x""][-1]
        ...     next_value = x * r * (1 - x)
        ...     return {""x"": next_value}
        >>>
        >>> graph.add_node(""A"", node)
        >>> graph.set_entry_point(""A"")
        >>> graph.set_finish_point(""A"")
        >>> compiled = graph.compile()
        >>>
        >>> print(compiled.config_specs)
        [ConfigurableFieldSpec(id='r', annotation=<class 'float'>, name=None, description=None, default=None, is_shared=False, dependencies=None)]
        >>>
        >>> step1 = compiled.invoke({""x"": 0.5}, {""configurable"": {""r"": 3.0}})
        >>> print(step1)
        {'x': [0.5, 0.75]}""""""

    nodes: dict[str, StateNodeSpec]
    channels: dict[str, BaseChannel]
    managed: dict[str, Type[ManagedValue]]
    schemas: dict[Type[Any], dict[str, Union[BaseChannel, Type[ManagedValue]]]]

    def __init__(
        self,
        state_schema: Optional[Type[Any]] = None,
        config_schema: Optional[Type[Any]] = None,
        *,
        input: Optional[Type[Any]] = None,
        output: Optional[Type[Any]] = None,
    ) -> None:
        super().__init__()
        if state_schema is None:
            if input is None or output is None:
                raise ValueError(""Must provide state_schema or input and output"")
            state_schema = input
        else:
            if input is None:
                input = state_schema
            if output is None:
                output = state_schema
        self.schemas = {}
        self.channels = {}
        self.managed = {}
        self.schema = state_schema
        self.input = input
        self.output = output
        self._add_schema(state_schema)
        self._add_schema(input)
        self._add_schema(output)
        self.config_schema = config_schema
        self.waiting_edges: set[tuple[tuple[str, ...], str]] = set()

    @property
    def _all_edges(self) -> set[tuple[str, str]]:
        return self.edges | {
            (start, end) for starts, end in self.waiting_edges for start in starts
        }

    def _add_schema(self, schema: Type[Any]) -> None:
        if schema not in self.schemas:
            _warn_invalid_state_schema(schema)
            channels, managed = _get_channels(schema)
            self.schemas[schema] = {**channels, **managed}
            for key, channel in channels.items():
                if key in self.channels:
                    if self.channels[key] != channel:
                        if isinstance(channel, LastValue):
                            pass
                        else:
                            raise ValueError(
                                f""Channel '{key}' already exists with a different type""
                            )
                else:
                    self.channels[key] = channel
            for key, managed in managed.items():
                if key in self.managed:
                    if self.managed[key] != managed:
                        raise ValueError(
                            f""Managed value '{key}' already exists with a different type""
                        )
                else:
                    self.managed[key] = managed
            if any(
                isinstance(c, BinaryOperatorAggregate) for c in self.channels.values()
            ):
                self.support_multiple_edges = True

    @overload
    def add_node(
        self,
        node: RunnableLike,
        *,
        metadata: Optional[dict[str, Any]] = None,
        input: Optional[Type[Any]] = None,
        retry: Optional[RetryPolicy] = None,
    ) -> None:
        """"""Adds a new node to the state graph.
        Will take the name of the function/runnable as the node name.

        Args:
            node (RunnableLike): The function or runnable this node will run.

        Raises:
            ValueError: If the key is already being used as a state key.

        Returns:
            None
        """"""
        ...

    @overload
    def add_node(
        self,
        node: str,
        action: RunnableLike,
        *,
        metadata: Optional[dict[str, Any]] = None,
        input: Optional[Type[Any]] = None,
        retry: Optional[RetryPolicy] = None,
    ) -> None:
        """"""Adds a new node to the state graph.

        Args:
            node (str): The key of the node.
            action (RunnableLike): The action associated with the node.

        Raises:
            ValueError: If the key is already being used as a state key.

        Returns:
            None
        """"""
        ...

    def add_node(
        self,
        node: Union[str, RunnableLike],
        action: Optional[RunnableLike] = None,
        *,
        metadata: Optional[dict[str, Any]] = None,
        input: Optional[Type[Any]] = None,
        retry: Optional[RetryPolicy] = None,
    ) -> None:
        """"""Adds a new node to the state graph.

        Will take the name of the function/runnable as the node name.

        Args:
            node (Union[str, RunnableLike)]: The function or runnable this node will run.
            action (Optional[RunnableLike]): The action associated with the node. (default: None)
            metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)
            input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)
            retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)
        Raises:
            ValueError: If the key is already being used as a state key.

        Examples:
            ```pycon
            >>> from langgraph.graph import START, StateGraph
            ...
            >>> def my_node(state, config):
            ...    return {""x"": state[""x""] + 1}
            ...
            >>> builder = StateGraph(dict)
            >>> builder.add_node(my_node)  # node name will be 'my_node'
            >>> builder.add_edge(START, ""my_node"")
            >>> graph = builder.compile()
            >>> graph.invoke({""x"": 1})
            {'x': 2}
            ```
            Customize the name:

            ```pycon
            >>> builder = StateGraph(dict)
            >>> builder.add_node(""my_fair_node"", my_node)
            >>> builder.add_edge(START, ""my_fair_node"")
            >>> graph = builder.compile()
            >>> graph.invoke({""x"": 1})
            {'x': 2}
            ```

        Returns:
            None
        """"""
        if not isinstance(node, str):
            action = node
            if isinstance(action, Runnable):
                node = action.name
            else:
                node = getattr(action, ""__name__"", action.__class__.__name__)
            if node is None:
                raise ValueError(
                    ""Node name must be provided if action is not a function""
                )
        if node in self.channels:
            raise ValueError(f""'{node}' is already being used as a state key"")
        if self.compiled:
            logger.warning(
                ""Adding a node to a graph that has already been compiled. This will ""
                ""not be reflected in the compiled graph.""
            )
        if not isinstance(node, str):
            action = node
            node = getattr(action, ""name"", action.__name__)
        if node in self.nodes:
            raise ValueError(f""Node `{node}` already present."")
        if node == END or node == START:
            raise ValueError(f""Node `{node}` is reserved."")

        if CHECKPOINT_NAMESPACE_SEPARATOR in node:
            raise ValueError(
                f""'{CHECKPOINT_NAMESPACE_SEPARATOR}' is a reserved character and is not allowed in the node names.""
            )

        try:
            if isfunction(action) and (
                hints := get_type_hints(action.__call__) or get_type_hints(action)
            ):
                if input is None:
                    input_hint = hints[list(hints.keys())[0]]
                    if isinstance(input_hint, type) and get_type_hints(input_hint):
                        input = input_hint
        except TypeError:
            pass
        if input is not None:
            self._add_schema(input)
        self.nodes[node] = StateNodeSpec(
            coerce_to_runnable(action, name=node, trace=False),
            metadata,
            input=input or self.schema,
            retry_policy=retry,
        )

    def add_edge(self, start_key: Union[str, list[str]], end_key: str) -> None:
        """"""Adds a directed edge from the start node to the end node.

        If the graph transitions to the start_key node, it will always transition to the end_key node next.

        Args:
            start_key (Union[str, list[str]]): The key(s) of the start node(s) of the edge.
            end_key (str): The key of the end node of the edge.

        Raises:
            ValueError: If the start key is 'END' or if the start key or end key is not present in the graph.

        Returns:
            None
        """"""
        if isinstance(start_key, str):
            return super().add_edge(start_key, end_key)

        if self.compiled:
            logger.warning(
                ""Adding an edge to a graph that has already been compiled. This will ""
                ""not be reflected in the compiled graph.""
            )
        for start in start_key:
            if start == END:
                raise ValueError(""END cannot be a start node"")
            if start not in self.nodes:
                raise ValueError(f""Need to add_node `{start}` first"")
        if end_key == START:
            raise ValueError(""START cannot be an end node"")
        if end_key not in self.nodes:
            raise ValueError(f""Need to add_node `{end_key}` first"")

        self.waiting_edges.add((tuple(start_key), end_key))

    def compile(
        self,
        checkpointer: Optional[BaseCheckpointSaver] = None,
        interrupt_before: Optional[Union[All, Sequence[str]]] = None,
        interrupt_after: Optional[Union[All, Sequence[str]]] = None,
        debug: bool = False,
    ) -> ""CompiledStateGraph"":
        """"""Compiles the state graph into a `CompiledGraph` object.

        The compiled graph implements the `Runnable` interface and can be invoked,
        streamed, batched, and run asynchronously.

        Args:
            checkpointer (Optional[BaseCheckpointSaver]): An optional checkpoint saver object.
                This serves as a fully versioned ""memory"" for the graph, allowing
                the graph to be paused and resumed, and replayed from any point.
            interrupt_before (Optional[Sequence[str]]): An optional list of node names to interrupt before.
            interrupt_after (Optional[Sequence[str]]): An optional list of node names to interrupt after.
            debug (bool): A flag indicating whether to enable debug mode.

        Returns:
            CompiledStateGraph: The compiled state graph.
        """"""
        # assign default values
        interrupt_before = interrupt_before or []
        interrupt_after = interrupt_after or []

        # validate the graph
        self.validate(
            interrupt=(
                (interrupt_before if interrupt_before != ""*"" else []) + interrupt_after
                if interrupt_after != ""*""
                else []
            )
        )

        # prepare output channels
        output_channels = (
            ""__root__""
            if len(self.schemas[self.output]) == 1
            and ""__root__"" in self.schemas[self.output]
            else [
                key
                for key, val in self.schemas[self.output].items()
                if not isinstance(val, Context) and not is_managed_value(val)
            ]
        )
        stream_channels = (
            ""__root__""
            if len(self.channels) == 1 and ""__root__"" in self.channels
            else [
                key
                for key, val in self.channels.items()
                if not isinstance(val, Context) and not is_managed_value(val)
            ]
        )

        compiled = CompiledStateGraph(
            builder=self,
            config_type=self.config_schema,
            nodes={},
            channels={**self.channels, START: EphemeralValue(self.input)},
            input_channels=START,
            stream_mode=""updates"",
            output_channels=output_channels,
            stream_channels=stream_channels,
            checkpointer=checkpointer,
            interrupt_before_nodes=interrupt_before,
            interrupt_after_nodes=interrupt_after,
            auto_validate=False,
            debug=debug,
        )

        compiled.attach_node(START, None)
        for key, node in self.nodes.items():
            compiled.attach_node(key, node)

        for start, end in self.edges:
            compiled.attach_edge(start, end)

        for starts, end in self.waiting_edges:
            compiled.attach_edge(starts, end)

        for start, branches in self.branches.items():
            for name, branch in branches.items():
                compiled.attach_branch(start, name, branch)

        return compiled.validate()

add_conditional_edges(source, path, path_map=None, then=None)

Add a conditional edge from the starting node to any number of destination nodes.
Parameters:

source
              (str)
          
          
The starting node. This conditional edge will run when
exiting this node.

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[Hashable, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Without typehints on the path function's return value (e.g., -> Literal[""foo"", ""__end__""]:)
or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

Source code in libs/langgraph/langgraph/graph/graph.py
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263def add_conditional_edges(
    self,
    source: str,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Add a conditional edge from the starting node to any number of destination nodes.

    Args:
        source (str): The starting node. This conditional edge will run when
            exiting this node.
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[Hashable, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None

    Note: Without typehints on the `path` function's return value (e.g., `-> Literal[""foo"", ""__end__""]:`)
        or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

    """"""  # noqa: E501
    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    # coerce path_map to a dictionary
    try:
        if isinstance(path_map, dict):
            path_map = path_map.copy()
        elif isinstance(path_map, list):
            path_map = {name: name for name in path_map}
        elif rtn_type := get_type_hints(path.__call__).get(
            ""return""
        ) or get_type_hints(path).get(""return""):
            if get_origin(rtn_type) is Literal:
                path_map = {name: name for name in get_args(rtn_type)}
    except Exception:
        pass
    # find a name for the condition
    path = coerce_to_runnable(path, name=None, trace=True)
    name = path.name or ""condition""
    # validate the condition
    if name in self.branches[source]:
        raise ValueError(
            f""Branch with name `{path.name}` already exists for node "" f""`{source}`""
        )
    # save it
    self.branches[source][name] = Branch(path, path_map, then)

set_entry_point(key)

Specifies the first node to be called in the graph.
Equivalent to calling add_edge(START, key).
Parameters:

key
              (str)
          
          
The key of the node to set as the entry point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
265
266
267
268
269
270
271
272
273
274
275
276def set_entry_point(self, key: str) -> None:
    """"""Specifies the first node to be called in the graph.

    Equivalent to calling `add_edge(START, key)`.

    Parameters:
        key (str): The key of the node to set as the entry point.

    Returns:
        None
    """"""
    return self.add_edge(START, key)

set_conditional_entry_point(path, path_map=None, then=None)

Sets a conditional entry point in the graph.
Parameters:

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[str, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302def set_conditional_entry_point(
    self,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Sets a conditional entry point in the graph.

    Args:
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[str, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None
    """"""
    return self.add_conditional_edges(START, path, path_map, then)

set_finish_point(key)

Marks a node as a finish point of the graph.
If the graph reaches this node, it will cease execution.
Parameters:

key
              (str)
          
          
The key of the node to set as the finish point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
304
305
306
307
308
309
310
311
312
313
314
315def set_finish_point(self, key: str) -> None:
    """"""Marks a node as a finish point of the graph.

    If the graph reaches this node, it will cease execution.

    Parameters:
        key (str): The key of the node to set as the finish point.

    Returns:
        None
    """"""
    return self.add_edge(key, END)

add_node(node, action=None, *, metadata=None, input=None, retry=None)

Adds a new node to the state graph.
Will take the name of the function/runnable as the node name.
Parameters:

node
              (Union[str, RunnableLike)])
          
          
The function or runnable this node will run.

action
              (Optional[RunnableLike], default:
                  None
)
          
          
The action associated with the node. (default: None)

metadata
              (Optional[dict[str, Any]], default:
                  None
)
          
          
The metadata associated with the node. (default: None)

input
              (Optional[Type[Any]], default:
                  None
)
          
          
The input schema for the node. (default: the graph's input schema)

retry
              (Optional[RetryPolicy], default:
                  None
)
          
          
The policy for retrying the node. (default: None)

Raises:
    ValueError: If the key is already being used as a state key.
Examples:
>>> from langgraph.graph import START, StateGraph
...
>>> def my_node(state, config):
...    return {""x"": state[""x""] + 1}
...
>>> builder = StateGraph(dict)
>>> builder.add_node(my_node)  # node name will be 'my_node'
>>> builder.add_edge(START, ""my_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Customize the name:
>>> builder = StateGraph(dict)
>>> builder.add_node(""my_fair_node"", my_node)
>>> builder.add_edge(START, ""my_fair_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337def add_node(
    self,
    node: Union[str, RunnableLike],
    action: Optional[RunnableLike] = None,
    *,
    metadata: Optional[dict[str, Any]] = None,
    input: Optional[Type[Any]] = None,
    retry: Optional[RetryPolicy] = None,
) -> None:
    """"""Adds a new node to the state graph.

    Will take the name of the function/runnable as the node name.

    Args:
        node (Union[str, RunnableLike)]: The function or runnable this node will run.
        action (Optional[RunnableLike]): The action associated with the node. (default: None)
        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)
        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)
        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)
    Raises:
        ValueError: If the key is already being used as a state key.

    Examples:
        ```pycon
        >>> from langgraph.graph import START, StateGraph
        ...
        >>> def my_node(state, config):
        ...    return {""x"": state[""x""] + 1}
        ...
        >>> builder = StateGraph(dict)
        >>> builder.add_node(my_node)  # node name will be 'my_node'
        >>> builder.add_edge(START, ""my_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```
        Customize the name:

        ```pycon
        >>> builder = StateGraph(dict)
        >>> builder.add_node(""my_fair_node"", my_node)
        >>> builder.add_edge(START, ""my_fair_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```

    Returns:
        None
    """"""
    if not isinstance(node, str):
        action = node
        if isinstance(action, Runnable):
            node = action.name
        else:
            node = getattr(action, ""__name__"", action.__class__.__name__)
        if node is None:
            raise ValueError(
                ""Node name must be provided if action is not a function""
            )
    if node in self.channels:
        raise ValueError(f""'{node}' is already being used as a state key"")
    if self.compiled:
        logger.warning(
            ""Adding a node to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    if not isinstance(node, str):
        action = node
        node = getattr(action, ""name"", action.__name__)
    if node in self.nodes:
        raise ValueError(f""Node `{node}` already present."")
    if node == END or node == START:
        raise ValueError(f""Node `{node}` is reserved."")

    if CHECKPOINT_NAMESPACE_SEPARATOR in node:
        raise ValueError(
            f""'{CHECKPOINT_NAMESPACE_SEPARATOR}' is a reserved character and is not allowed in the node names.""
        )

    try:
        if isfunction(action) and (
            hints := get_type_hints(action.__call__) or get_type_hints(action)
        ):
            if input is None:
                input_hint = hints[list(hints.keys())[0]]
                if isinstance(input_hint, type) and get_type_hints(input_hint):
                    input = input_hint
    except TypeError:
        pass
    if input is not None:
        self._add_schema(input)
    self.nodes[node] = StateNodeSpec(
        coerce_to_runnable(action, name=node, trace=False),
        metadata,
        input=input or self.schema,
        retry_policy=retry,
    )

add_edge(start_key, end_key)

Adds a directed edge from the start node to the end node.
If the graph transitions to the start_key node, it will always transition to the end_key node next.
Parameters:

start_key
              (Union[str, list[str]])
          
          
The key(s) of the start node(s) of the edge.

end_key
              (str)
          
          
The key of the end node of the edge.

Raises:

ValueError
            
          
If the start key is 'END' or if the start key or end key is not present in the graph.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372def add_edge(self, start_key: Union[str, list[str]], end_key: str) -> None:
    """"""Adds a directed edge from the start node to the end node.

    If the graph transitions to the start_key node, it will always transition to the end_key node next.

    Args:
        start_key (Union[str, list[str]]): The key(s) of the start node(s) of the edge.
        end_key (str): The key of the end node of the edge.

    Raises:
        ValueError: If the start key is 'END' or if the start key or end key is not present in the graph.

    Returns:
        None
    """"""
    if isinstance(start_key, str):
        return super().add_edge(start_key, end_key)

    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    for start in start_key:
        if start == END:
            raise ValueError(""END cannot be a start node"")
        if start not in self.nodes:
            raise ValueError(f""Need to add_node `{start}` first"")
    if end_key == START:
        raise ValueError(""START cannot be an end node"")
    if end_key not in self.nodes:
        raise ValueError(f""Need to add_node `{end_key}` first"")

    self.waiting_edges.add((tuple(start_key), end_key))

compile(checkpointer=None, interrupt_before=None, interrupt_after=None, debug=False)

Compiles the state graph into a CompiledGraph object.
The compiled graph implements the Runnable interface and can be invoked,
streamed, batched, and run asynchronously.
Parameters:

checkpointer
              (Optional[BaseCheckpointSaver], default:
                  None
)
          
          
An optional checkpoint saver object.
This serves as a fully versioned ""memory"" for the graph, allowing
the graph to be paused and resumed, and replayed from any point.

interrupt_before
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt before.

interrupt_after
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt after.

debug
              (bool, default:
                  False
)
          
          
A flag indicating whether to enable debug mode.

Returns:

CompiledStateGraph (              CompiledStateGraph
)          
          
The compiled state graph.

Source code in libs/langgraph/langgraph/graph/state.py
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461def compile(
    self,
    checkpointer: Optional[BaseCheckpointSaver] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: bool = False,
) -> ""CompiledStateGraph"":
    """"""Compiles the state graph into a `CompiledGraph` object.

    The compiled graph implements the `Runnable` interface and can be invoked,
    streamed, batched, and run asynchronously.

    Args:
        checkpointer (Optional[BaseCheckpointSaver]): An optional checkpoint saver object.
            This serves as a fully versioned ""memory"" for the graph, allowing
            the graph to be paused and resumed, and replayed from any point.
        interrupt_before (Optional[Sequence[str]]): An optional list of node names to interrupt before.
        interrupt_after (Optional[Sequence[str]]): An optional list of node names to interrupt after.
        debug (bool): A flag indicating whether to enable debug mode.

    Returns:
        CompiledStateGraph: The compiled state graph.
    """"""
    # assign default values
    interrupt_before = interrupt_before or []
    interrupt_after = interrupt_after or []

    # validate the graph
    self.validate(
        interrupt=(
            (interrupt_before if interrupt_before != ""*"" else []) + interrupt_after
            if interrupt_after != ""*""
            else []
        )
    )

    # prepare output channels
    output_channels = (
        ""__root__""
        if len(self.schemas[self.output]) == 1
        and ""__root__"" in self.schemas[self.output]
        else [
            key
            for key, val in self.schemas[self.output].items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )
    stream_channels = (
        ""__root__""
        if len(self.channels) == 1 and ""__root__"" in self.channels
        else [
            key
            for key, val in self.channels.items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )

    compiled = CompiledStateGraph(
        builder=self,
        config_type=self.config_schema,
        nodes={},
        channels={**self.channels, START: EphemeralValue(self.input)},
        input_channels=START,
        stream_mode=""updates"",
        output_channels=output_channels,
        stream_channels=stream_channels,
        checkpointer=checkpointer,
        interrupt_before_nodes=interrupt_before,
        interrupt_after_nodes=interrupt_after,
        auto_validate=False,
        debug=debug,
    )

    compiled.attach_node(START, None)
    for key, node in self.nodes.items():
        compiled.attach_node(key, node)

    for start, end in self.edges:
        compiled.attach_edge(start, end)

    for starts, end in self.waiting_edges:
        compiled.attach_edge(starts, end)

    for start, branches in self.branches.items():
        for name, branch in branches.items():
            compiled.attach_branch(start, name, branch)

    return compiled.validate()

 handler: python",What method is used to specify the first node to be called in the StateGraph?
"add_conditional_edges(source,path,path_map=None,then=None)","
Add a conditional edge from the starting node to any number of destination nodes.
Parameters:

source
              (str)
          
          
The starting node. This conditional edge will run when
exiting this node.

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[Hashable, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Without typehints on the path function's return value (e.g., -> Literal[""foo"", ""__end__""]:)
or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

Source code in libs/langgraph/langgraph/graph/graph.py
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263def add_conditional_edges(
    self,
    source: str,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Add a conditional edge from the starting node to any number of destination nodes.

    Args:
        source (str): The starting node. This conditional edge will run when
            exiting this node.
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[Hashable, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None

    Note: Without typehints on the `path` function's return value (e.g., `-> Literal[""foo"", ""__end__""]:`)
        or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

    """"""  # noqa: E501
    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    # coerce path_map to a dictionary
    try:
        if isinstance(path_map, dict):
            path_map = path_map.copy()
        elif isinstance(path_map, list):
            path_map = {name: name for name in path_map}
        elif rtn_type := get_type_hints(path.__call__).get(
            ""return""
        ) or get_type_hints(path).get(""return""):
            if get_origin(rtn_type) is Literal:
                path_map = {name: name for name in get_args(rtn_type)}
    except Exception:
        pass
    # find a name for the condition
    path = coerce_to_runnable(path, name=None, trace=True)
    name = path.name or ""condition""
    # validate the condition
    if name in self.branches[source]:
        raise ValueError(
            f""Branch with name `{path.name}` already exists for node "" f""`{source}`""
        )
    # save it
    self.branches[source][name] = Branch(path, path_map, then)

",What parameters are required for adding a conditional edge from the starting node to destination nodes in the given function?
set_entry_point(key),"
Specifies the first node to be called in the graph.
Equivalent to calling add_edge(START, key).
Parameters:

key
              (str)
          
          
The key of the node to set as the entry point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
265
266
267
268
269
270
271
272
273
274
275
276def set_entry_point(self, key: str) -> None:
    """"""Specifies the first node to be called in the graph.

    Equivalent to calling `add_edge(START, key)`.

    Parameters:
        key (str): The key of the node to set as the entry point.

    Returns:
        None
    """"""
    return self.add_edge(START, key)

",What does the set_entry_point(key) function in the graph.py file do?
"set_conditional_entry_point(path,path_map=None,then=None)","
Sets a conditional entry point in the graph.
Parameters:

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[str, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302def set_conditional_entry_point(
    self,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Sets a conditional entry point in the graph.

    Args:
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[str, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None
    """"""
    return self.add_conditional_edges(START, path, path_map, then)

",What parameters are accepted by the set_conditional_entry_point function in the graph.py file?
set_finish_point(key),"
Marks a node as a finish point of the graph.
If the graph reaches this node, it will cease execution.
Parameters:

key
              (str)
          
          
The key of the node to set as the finish point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
304
305
306
307
308
309
310
311
312
313
314
315def set_finish_point(self, key: str) -> None:
    """"""Marks a node as a finish point of the graph.

    If the graph reaches this node, it will cease execution.

    Parameters:
        key (str): The key of the node to set as the finish point.

    Returns:
        None
    """"""
    return self.add_edge(key, END)

",What does the set_finish_point(key) function do in the graph?
"add_node(node,action=None,*,metadata=None,input=None,retry=None)","
Adds a new node to the state graph.
Will take the name of the function/runnable as the node name.
Parameters:

node
              (Union[str, RunnableLike)])
          
          
The function or runnable this node will run.

action
              (Optional[RunnableLike], default:
                  None
)
          
          
The action associated with the node. (default: None)

metadata
              (Optional[dict[str, Any]], default:
                  None
)
          
          
The metadata associated with the node. (default: None)

input
              (Optional[Type[Any]], default:
                  None
)
          
          
The input schema for the node. (default: the graph's input schema)

retry
              (Optional[RetryPolicy], default:
                  None
)
          
          
The policy for retrying the node. (default: None)

Raises:
    ValueError: If the key is already being used as a state key.
Examples:
>>> from langgraph.graph import START, StateGraph
...
>>> def my_node(state, config):
...    return {""x"": state[""x""] + 1}
...
>>> builder = StateGraph(dict)
>>> builder.add_node(my_node)  # node name will be 'my_node'
>>> builder.add_edge(START, ""my_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Customize the name:
>>> builder = StateGraph(dict)
>>> builder.add_node(""my_fair_node"", my_node)
>>> builder.add_edge(START, ""my_fair_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337def add_node(
    self,
    node: Union[str, RunnableLike],
    action: Optional[RunnableLike] = None,
    *,
    metadata: Optional[dict[str, Any]] = None,
    input: Optional[Type[Any]] = None,
    retry: Optional[RetryPolicy] = None,
) -> None:
    """"""Adds a new node to the state graph.

    Will take the name of the function/runnable as the node name.

    Args:
        node (Union[str, RunnableLike)]: The function or runnable this node will run.
        action (Optional[RunnableLike]): The action associated with the node. (default: None)
        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)
        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)
        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)
    Raises:
        ValueError: If the key is already being used as a state key.

    Examples:
        ```pycon
        >>> from langgraph.graph import START, StateGraph
        ...
        >>> def my_node(state, config):
        ...    return {""x"": state[""x""] + 1}
        ...
        >>> builder = StateGraph(dict)
        >>> builder.add_node(my_node)  # node name will be 'my_node'
        >>> builder.add_edge(START, ""my_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```
        Customize the name:

        ```pycon
        >>> builder = StateGraph(dict)
        >>> builder.add_node(""my_fair_node"", my_node)
        >>> builder.add_edge(START, ""my_fair_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```

    Returns:
        None
    """"""
    if not isinstance(node, str):
        action = node
        if isinstance(action, Runnable):
            node = action.name
        else:
            node = getattr(action, ""__name__"", action.__class__.__name__)
        if node is None:
            raise ValueError(
                ""Node name must be provided if action is not a function""
            )
    if node in self.channels:
        raise ValueError(f""'{node}' is already being used as a state key"")
    if self.compiled:
        logger.warning(
            ""Adding a node to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    if not isinstance(node, str):
        action = node
        node = getattr(action, ""name"", action.__name__)
    if node in self.nodes:
        raise ValueError(f""Node `{node}` already present."")
    if node == END or node == START:
        raise ValueError(f""Node `{node}` is reserved."")

    if CHECKPOINT_NAMESPACE_SEPARATOR in node:
        raise ValueError(
            f""'{CHECKPOINT_NAMESPACE_SEPARATOR}' is a reserved character and is not allowed in the node names.""
        )

    try:
        if isfunction(action) and (
            hints := get_type_hints(action.__call__) or get_type_hints(action)
        ):
            if input is None:
                input_hint = hints[list(hints.keys())[0]]
                if isinstance(input_hint, type) and get_type_hints(input_hint):
                    input = input_hint
    except TypeError:
        pass
    if input is not None:
        self._add_schema(input)
    self.nodes[node] = StateNodeSpec(
        coerce_to_runnable(action, name=node, trace=False),
        metadata,
        input=input or self.schema,
        retry_policy=retry,
    )

",What parameters are accepted by the add_node function in the StateGraph class?
"add_edge(start_key,end_key)","
Adds a directed edge from the start node to the end node.
If the graph transitions to the start_key node, it will always transition to the end_key node next.
Parameters:

start_key
              (Union[str, list[str]])
          
          
The key(s) of the start node(s) of the edge.

end_key
              (str)
          
          
The key of the end node of the edge.

Raises:

ValueError
            
          
If the start key is 'END' or if the start key or end key is not present in the graph.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372def add_edge(self, start_key: Union[str, list[str]], end_key: str) -> None:
    """"""Adds a directed edge from the start node to the end node.

    If the graph transitions to the start_key node, it will always transition to the end_key node next.

    Args:
        start_key (Union[str, list[str]]): The key(s) of the start node(s) of the edge.
        end_key (str): The key of the end node of the edge.

    Raises:
        ValueError: If the start key is 'END' or if the start key or end key is not present in the graph.

    Returns:
        None
    """"""
    if isinstance(start_key, str):
        return super().add_edge(start_key, end_key)

    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    for start in start_key:
        if start == END:
            raise ValueError(""END cannot be a start node"")
        if start not in self.nodes:
            raise ValueError(f""Need to add_node `{start}` first"")
    if end_key == START:
        raise ValueError(""START cannot be an end node"")
    if end_key not in self.nodes:
        raise ValueError(f""Need to add_node `{end_key}` first"")

    self.waiting_edges.add((tuple(start_key), end_key))

",What happens if the start key is 'END' or if the start key or end key is not present in the graph when using the add_edge function?
"compile(checkpointer=None,interrupt_before=None,interrupt_after=None,debug=False)","
Compiles the state graph into a CompiledGraph object.
The compiled graph implements the Runnable interface and can be invoked,
streamed, batched, and run asynchronously.
Parameters:

checkpointer
              (Optional[BaseCheckpointSaver], default:
                  None
)
          
          
An optional checkpoint saver object.
This serves as a fully versioned ""memory"" for the graph, allowing
the graph to be paused and resumed, and replayed from any point.

interrupt_before
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt before.

interrupt_after
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt after.

debug
              (bool, default:
                  False
)
          
          
A flag indicating whether to enable debug mode.

Returns:

CompiledStateGraph (              CompiledStateGraph
)          
          
The compiled state graph.

Source code in libs/langgraph/langgraph/graph/state.py
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461def compile(
    self,
    checkpointer: Optional[BaseCheckpointSaver] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: bool = False,
) -> ""CompiledStateGraph"":
    """"""Compiles the state graph into a `CompiledGraph` object.

    The compiled graph implements the `Runnable` interface and can be invoked,
    streamed, batched, and run asynchronously.

    Args:
        checkpointer (Optional[BaseCheckpointSaver]): An optional checkpoint saver object.
            This serves as a fully versioned ""memory"" for the graph, allowing
            the graph to be paused and resumed, and replayed from any point.
        interrupt_before (Optional[Sequence[str]]): An optional list of node names to interrupt before.
        interrupt_after (Optional[Sequence[str]]): An optional list of node names to interrupt after.
        debug (bool): A flag indicating whether to enable debug mode.

    Returns:
        CompiledStateGraph: The compiled state graph.
    """"""
    # assign default values
    interrupt_before = interrupt_before or []
    interrupt_after = interrupt_after or []

    # validate the graph
    self.validate(
        interrupt=(
            (interrupt_before if interrupt_before != ""*"" else []) + interrupt_after
            if interrupt_after != ""*""
            else []
        )
    )

    # prepare output channels
    output_channels = (
        ""__root__""
        if len(self.schemas[self.output]) == 1
        and ""__root__"" in self.schemas[self.output]
        else [
            key
            for key, val in self.schemas[self.output].items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )
    stream_channels = (
        ""__root__""
        if len(self.channels) == 1 and ""__root__"" in self.channels
        else [
            key
            for key, val in self.channels.items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )

    compiled = CompiledStateGraph(
        builder=self,
        config_type=self.config_schema,
        nodes={},
        channels={**self.channels, START: EphemeralValue(self.input)},
        input_channels=START,
        stream_mode=""updates"",
        output_channels=output_channels,
        stream_channels=stream_channels,
        checkpointer=checkpointer,
        interrupt_before_nodes=interrupt_before,
        interrupt_after_nodes=interrupt_after,
        auto_validate=False,
        debug=debug,
    )

    compiled.attach_node(START, None)
    for key, node in self.nodes.items():
        compiled.attach_node(key, node)

    for start, end in self.edges:
        compiled.attach_edge(start, end)

    for starts, end in self.waiting_edges:
        compiled.attach_edge(starts, end)

    for start, branches in self.branches.items():
        for name, branch in branches.items():
            compiled.attach_branch(start, name, branch)

    return compiled.validate()

",What does the compile function in the state graph do?
MessageGraph,"

              Bases: StateGraph
A StateGraph where every node receives a list of messages as input and returns one or more messages as output.
MessageGraph is a subclass of StateGraph whose entire state is a single, append-only* list of messages.
Each node in a MessageGraph takes a list of messages as input and returns zero or more
messages as output. The add_messages function is used to merge the output messages from each node
into the existing list of messages in the graph's state.
Examples:
>>> from langgraph.graph.message import MessageGraph
...
>>> builder = MessageGraph()
>>> builder.add_node(""chatbot"", lambda state: [(""assistant"", ""Hello!"")])
>>> builder.set_entry_point(""chatbot"")
>>> builder.set_finish_point(""chatbot"")
>>> builder.compile().invoke([(""user"", ""Hi there."")])
[HumanMessage(content=""Hi there."", id='...'), AIMessage(content=""Hello!"", id='...')]

>>> from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
>>> from langgraph.graph.message import MessageGraph
...
>>> builder = MessageGraph()
>>> builder.add_node(
...     ""chatbot"",
...     lambda state: [
...         AIMessage(
...             content=""Hello!"",
...             tool_calls=[{""name"": ""search"", ""id"": ""123"", ""args"": {""query"": ""X""}}],
...         )
...     ],
... )
>>> builder.add_node(
...     ""search"", lambda state: [ToolMessage(content=""Searching..."", tool_call_id=""123"")]
... )
>>> builder.set_entry_point(""chatbot"")
>>> builder.add_edge(""chatbot"", ""search"")
>>> builder.set_finish_point(""search"")
>>> builder.compile().invoke([HumanMessage(content=""Hi there. Can you search for X?"")])
{'messages': [HumanMessage(content=""Hi there. Can you search for X?"", id='b8b7d8f4-7f4d-4f4d-9c1d-f8b8d8f4d9c1'),
             AIMessage(content=""Hello!"", id='f4d9c1d8-8d8f-4d9c-b8b7-d8f4f4d9c1d8'),
             ToolMessage(content=""Searching..."", id='d8f4f4d9-c1d8-4f4d-b8b7-d8f4f4d9c1d8', tool_call_id=""123"")]}

Source code in libs/langgraph/langgraph/graph/message.py
 99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147class MessageGraph(StateGraph):
    """"""A StateGraph where every node receives a list of messages as input and returns one or more messages as output.

    MessageGraph is a subclass of StateGraph whose entire state is a single, append-only* list of messages.
    Each node in a MessageGraph takes a list of messages as input and returns zero or more
    messages as output. The `add_messages` function is used to merge the output messages from each node
    into the existing list of messages in the graph's state.

    Examples:
        ```pycon
        >>> from langgraph.graph.message import MessageGraph
        ...
        >>> builder = MessageGraph()
        >>> builder.add_node(""chatbot"", lambda state: [(""assistant"", ""Hello!"")])
        >>> builder.set_entry_point(""chatbot"")
        >>> builder.set_finish_point(""chatbot"")
        >>> builder.compile().invoke([(""user"", ""Hi there."")])
        [HumanMessage(content=""Hi there."", id='...'), AIMessage(content=""Hello!"", id='...')]
        ```

        ```pycon
        >>> from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
        >>> from langgraph.graph.message import MessageGraph
        ...
        >>> builder = MessageGraph()
        >>> builder.add_node(
        ...     ""chatbot"",
        ...     lambda state: [
        ...         AIMessage(
        ...             content=""Hello!"",
        ...             tool_calls=[{""name"": ""search"", ""id"": ""123"", ""args"": {""query"": ""X""}}],
        ...         )
        ...     ],
        ... )
        >>> builder.add_node(
        ...     ""search"", lambda state: [ToolMessage(content=""Searching..."", tool_call_id=""123"")]
        ... )
        >>> builder.set_entry_point(""chatbot"")
        >>> builder.add_edge(""chatbot"", ""search"")
        >>> builder.set_finish_point(""search"")
        >>> builder.compile().invoke([HumanMessage(content=""Hi there. Can you search for X?"")])
        {'messages': [HumanMessage(content=""Hi there. Can you search for X?"", id='b8b7d8f4-7f4d-4f4d-9c1d-f8b8d8f4d9c1'),
                     AIMessage(content=""Hello!"", id='f4d9c1d8-8d8f-4d9c-b8b7-d8f4f4d9c1d8'),
                     ToolMessage(content=""Searching..."", id='d8f4f4d9-c1d8-4f4d-b8b7-d8f4f4d9c1d8', tool_call_id=""123"")]}
        ```
    """"""

    def __init__(self) -> None:
        super().__init__(Annotated[list[AnyMessage], add_messages])

add_node(node, action=None, *, metadata=None, input=None, retry=None)

Adds a new node to the state graph.
Will take the name of the function/runnable as the node name.
Parameters:

node
              (Union[str, RunnableLike)])
          
          
The function or runnable this node will run.

action
              (Optional[RunnableLike], default:
                  None
)
          
          
The action associated with the node. (default: None)

metadata
              (Optional[dict[str, Any]], default:
                  None
)
          
          
The metadata associated with the node. (default: None)

input
              (Optional[Type[Any]], default:
                  None
)
          
          
The input schema for the node. (default: the graph's input schema)

retry
              (Optional[RetryPolicy], default:
                  None
)
          
          
The policy for retrying the node. (default: None)

Raises:
    ValueError: If the key is already being used as a state key.
Examples:
>>> from langgraph.graph import START, StateGraph
...
>>> def my_node(state, config):
...    return {""x"": state[""x""] + 1}
...
>>> builder = StateGraph(dict)
>>> builder.add_node(my_node)  # node name will be 'my_node'
>>> builder.add_edge(START, ""my_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Customize the name:
>>> builder = StateGraph(dict)
>>> builder.add_node(""my_fair_node"", my_node)
>>> builder.add_edge(START, ""my_fair_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337def add_node(
    self,
    node: Union[str, RunnableLike],
    action: Optional[RunnableLike] = None,
    *,
    metadata: Optional[dict[str, Any]] = None,
    input: Optional[Type[Any]] = None,
    retry: Optional[RetryPolicy] = None,
) -> None:
    """"""Adds a new node to the state graph.

    Will take the name of the function/runnable as the node name.

    Args:
        node (Union[str, RunnableLike)]: The function or runnable this node will run.
        action (Optional[RunnableLike]): The action associated with the node. (default: None)
        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)
        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)
        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)
    Raises:
        ValueError: If the key is already being used as a state key.

    Examples:
        ```pycon
        >>> from langgraph.graph import START, StateGraph
        ...
        >>> def my_node(state, config):
        ...    return {""x"": state[""x""] + 1}
        ...
        >>> builder = StateGraph(dict)
        >>> builder.add_node(my_node)  # node name will be 'my_node'
        >>> builder.add_edge(START, ""my_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```
        Customize the name:

        ```pycon
        >>> builder = StateGraph(dict)
        >>> builder.add_node(""my_fair_node"", my_node)
        >>> builder.add_edge(START, ""my_fair_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```

    Returns:
        None
    """"""
    if not isinstance(node, str):
        action = node
        if isinstance(action, Runnable):
            node = action.name
        else:
            node = getattr(action, ""__name__"", action.__class__.__name__)
        if node is None:
            raise ValueError(
                ""Node name must be provided if action is not a function""
            )
    if node in self.channels:
        raise ValueError(f""'{node}' is already being used as a state key"")
    if self.compiled:
        logger.warning(
            ""Adding a node to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    if not isinstance(node, str):
        action = node
        node = getattr(action, ""name"", action.__name__)
    if node in self.nodes:
        raise ValueError(f""Node `{node}` already present."")
    if node == END or node == START:
        raise ValueError(f""Node `{node}` is reserved."")

    if CHECKPOINT_NAMESPACE_SEPARATOR in node:
        raise ValueError(
            f""'{CHECKPOINT_NAMESPACE_SEPARATOR}' is a reserved character and is not allowed in the node names.""
        )

    try:
        if isfunction(action) and (
            hints := get_type_hints(action.__call__) or get_type_hints(action)
        ):
            if input is None:
                input_hint = hints[list(hints.keys())[0]]
                if isinstance(input_hint, type) and get_type_hints(input_hint):
                    input = input_hint
    except TypeError:
        pass
    if input is not None:
        self._add_schema(input)
    self.nodes[node] = StateNodeSpec(
        coerce_to_runnable(action, name=node, trace=False),
        metadata,
        input=input or self.schema,
        retry_policy=retry,
    )

add_edge(start_key, end_key)

Adds a directed edge from the start node to the end node.
If the graph transitions to the start_key node, it will always transition to the end_key node next.
Parameters:

start_key
              (Union[str, list[str]])
          
          
The key(s) of the start node(s) of the edge.

end_key
              (str)
          
          
The key of the end node of the edge.

Raises:

ValueError
            
          
If the start key is 'END' or if the start key or end key is not present in the graph.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372def add_edge(self, start_key: Union[str, list[str]], end_key: str) -> None:
    """"""Adds a directed edge from the start node to the end node.

    If the graph transitions to the start_key node, it will always transition to the end_key node next.

    Args:
        start_key (Union[str, list[str]]): The key(s) of the start node(s) of the edge.
        end_key (str): The key of the end node of the edge.

    Raises:
        ValueError: If the start key is 'END' or if the start key or end key is not present in the graph.

    Returns:
        None
    """"""
    if isinstance(start_key, str):
        return super().add_edge(start_key, end_key)

    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    for start in start_key:
        if start == END:
            raise ValueError(""END cannot be a start node"")
        if start not in self.nodes:
            raise ValueError(f""Need to add_node `{start}` first"")
    if end_key == START:
        raise ValueError(""START cannot be an end node"")
    if end_key not in self.nodes:
        raise ValueError(f""Need to add_node `{end_key}` first"")

    self.waiting_edges.add((tuple(start_key), end_key))

add_conditional_edges(source, path, path_map=None, then=None)

Add a conditional edge from the starting node to any number of destination nodes.
Parameters:

source
              (str)
          
          
The starting node. This conditional edge will run when
exiting this node.

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[Hashable, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Without typehints on the path function's return value (e.g., -> Literal[""foo"", ""__end__""]:)
or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

Source code in libs/langgraph/langgraph/graph/graph.py
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263def add_conditional_edges(
    self,
    source: str,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Add a conditional edge from the starting node to any number of destination nodes.

    Args:
        source (str): The starting node. This conditional edge will run when
            exiting this node.
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[Hashable, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None

    Note: Without typehints on the `path` function's return value (e.g., `-> Literal[""foo"", ""__end__""]:`)
        or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

    """"""  # noqa: E501
    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    # coerce path_map to a dictionary
    try:
        if isinstance(path_map, dict):
            path_map = path_map.copy()
        elif isinstance(path_map, list):
            path_map = {name: name for name in path_map}
        elif rtn_type := get_type_hints(path.__call__).get(
            ""return""
        ) or get_type_hints(path).get(""return""):
            if get_origin(rtn_type) is Literal:
                path_map = {name: name for name in get_args(rtn_type)}
    except Exception:
        pass
    # find a name for the condition
    path = coerce_to_runnable(path, name=None, trace=True)
    name = path.name or ""condition""
    # validate the condition
    if name in self.branches[source]:
        raise ValueError(
            f""Branch with name `{path.name}` already exists for node "" f""`{source}`""
        )
    # save it
    self.branches[source][name] = Branch(path, path_map, then)

set_entry_point(key)

Specifies the first node to be called in the graph.
Equivalent to calling add_edge(START, key).
Parameters:

key
              (str)
          
          
The key of the node to set as the entry point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
265
266
267
268
269
270
271
272
273
274
275
276def set_entry_point(self, key: str) -> None:
    """"""Specifies the first node to be called in the graph.

    Equivalent to calling `add_edge(START, key)`.

    Parameters:
        key (str): The key of the node to set as the entry point.

    Returns:
        None
    """"""
    return self.add_edge(START, key)

set_conditional_entry_point(path, path_map=None, then=None)

Sets a conditional entry point in the graph.
Parameters:

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[str, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302def set_conditional_entry_point(
    self,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Sets a conditional entry point in the graph.

    Args:
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[str, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None
    """"""
    return self.add_conditional_edges(START, path, path_map, then)

set_finish_point(key)

Marks a node as a finish point of the graph.
If the graph reaches this node, it will cease execution.
Parameters:

key
              (str)
          
          
The key of the node to set as the finish point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
304
305
306
307
308
309
310
311
312
313
314
315def set_finish_point(self, key: str) -> None:
    """"""Marks a node as a finish point of the graph.

    If the graph reaches this node, it will cease execution.

    Parameters:
        key (str): The key of the node to set as the finish point.

    Returns:
        None
    """"""
    return self.add_edge(key, END)

compile(checkpointer=None, interrupt_before=None, interrupt_after=None, debug=False)

Compiles the state graph into a CompiledGraph object.
The compiled graph implements the Runnable interface and can be invoked,
streamed, batched, and run asynchronously.
Parameters:

checkpointer
              (Optional[BaseCheckpointSaver], default:
                  None
)
          
          
An optional checkpoint saver object.
This serves as a fully versioned ""memory"" for the graph, allowing
the graph to be paused and resumed, and replayed from any point.

interrupt_before
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt before.

interrupt_after
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt after.

debug
              (bool, default:
                  False
)
          
          
A flag indicating whether to enable debug mode.

Returns:

CompiledStateGraph (              CompiledStateGraph
)          
          
The compiled state graph.

Source code in libs/langgraph/langgraph/graph/state.py
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461def compile(
    self,
    checkpointer: Optional[BaseCheckpointSaver] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: bool = False,
) -> ""CompiledStateGraph"":
    """"""Compiles the state graph into a `CompiledGraph` object.

    The compiled graph implements the `Runnable` interface and can be invoked,
    streamed, batched, and run asynchronously.

    Args:
        checkpointer (Optional[BaseCheckpointSaver]): An optional checkpoint saver object.
            This serves as a fully versioned ""memory"" for the graph, allowing
            the graph to be paused and resumed, and replayed from any point.
        interrupt_before (Optional[Sequence[str]]): An optional list of node names to interrupt before.
        interrupt_after (Optional[Sequence[str]]): An optional list of node names to interrupt after.
        debug (bool): A flag indicating whether to enable debug mode.

    Returns:
        CompiledStateGraph: The compiled state graph.
    """"""
    # assign default values
    interrupt_before = interrupt_before or []
    interrupt_after = interrupt_after or []

    # validate the graph
    self.validate(
        interrupt=(
            (interrupt_before if interrupt_before != ""*"" else []) + interrupt_after
            if interrupt_after != ""*""
            else []
        )
    )

    # prepare output channels
    output_channels = (
        ""__root__""
        if len(self.schemas[self.output]) == 1
        and ""__root__"" in self.schemas[self.output]
        else [
            key
            for key, val in self.schemas[self.output].items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )
    stream_channels = (
        ""__root__""
        if len(self.channels) == 1 and ""__root__"" in self.channels
        else [
            key
            for key, val in self.channels.items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )

    compiled = CompiledStateGraph(
        builder=self,
        config_type=self.config_schema,
        nodes={},
        channels={**self.channels, START: EphemeralValue(self.input)},
        input_channels=START,
        stream_mode=""updates"",
        output_channels=output_channels,
        stream_channels=stream_channels,
        checkpointer=checkpointer,
        interrupt_before_nodes=interrupt_before,
        interrupt_after_nodes=interrupt_after,
        auto_validate=False,
        debug=debug,
    )

    compiled.attach_node(START, None)
    for key, node in self.nodes.items():
        compiled.attach_node(key, node)

    for start, end in self.edges:
        compiled.attach_edge(start, end)

    for starts, end in self.waiting_edges:
        compiled.attach_edge(starts, end)

    for start, branches in self.branches.items():
        for name, branch in branches.items():
            compiled.attach_branch(start, name, branch)

    return compiled.validate()

",What is the purpose of the `MessageGraph` class and how does it differ from the `StateGraph` class?
"add_node(node,action=None,*,metadata=None,input=None,retry=None)","
Adds a new node to the state graph.
Will take the name of the function/runnable as the node name.
Parameters:

node
              (Union[str, RunnableLike)])
          
          
The function or runnable this node will run.

action
              (Optional[RunnableLike], default:
                  None
)
          
          
The action associated with the node. (default: None)

metadata
              (Optional[dict[str, Any]], default:
                  None
)
          
          
The metadata associated with the node. (default: None)

input
              (Optional[Type[Any]], default:
                  None
)
          
          
The input schema for the node. (default: the graph's input schema)

retry
              (Optional[RetryPolicy], default:
                  None
)
          
          
The policy for retrying the node. (default: None)

Raises:
    ValueError: If the key is already being used as a state key.
Examples:
>>> from langgraph.graph import START, StateGraph
...
>>> def my_node(state, config):
...    return {""x"": state[""x""] + 1}
...
>>> builder = StateGraph(dict)
>>> builder.add_node(my_node)  # node name will be 'my_node'
>>> builder.add_edge(START, ""my_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Customize the name:
>>> builder = StateGraph(dict)
>>> builder.add_node(""my_fair_node"", my_node)
>>> builder.add_edge(START, ""my_fair_node"")
>>> graph = builder.compile()
>>> graph.invoke({""x"": 1})
{'x': 2}

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337def add_node(
    self,
    node: Union[str, RunnableLike],
    action: Optional[RunnableLike] = None,
    *,
    metadata: Optional[dict[str, Any]] = None,
    input: Optional[Type[Any]] = None,
    retry: Optional[RetryPolicy] = None,
) -> None:
    """"""Adds a new node to the state graph.

    Will take the name of the function/runnable as the node name.

    Args:
        node (Union[str, RunnableLike)]: The function or runnable this node will run.
        action (Optional[RunnableLike]): The action associated with the node. (default: None)
        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)
        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)
        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)
    Raises:
        ValueError: If the key is already being used as a state key.

    Examples:
        ```pycon
        >>> from langgraph.graph import START, StateGraph
        ...
        >>> def my_node(state, config):
        ...    return {""x"": state[""x""] + 1}
        ...
        >>> builder = StateGraph(dict)
        >>> builder.add_node(my_node)  # node name will be 'my_node'
        >>> builder.add_edge(START, ""my_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```
        Customize the name:

        ```pycon
        >>> builder = StateGraph(dict)
        >>> builder.add_node(""my_fair_node"", my_node)
        >>> builder.add_edge(START, ""my_fair_node"")
        >>> graph = builder.compile()
        >>> graph.invoke({""x"": 1})
        {'x': 2}
        ```

    Returns:
        None
    """"""
    if not isinstance(node, str):
        action = node
        if isinstance(action, Runnable):
            node = action.name
        else:
            node = getattr(action, ""__name__"", action.__class__.__name__)
        if node is None:
            raise ValueError(
                ""Node name must be provided if action is not a function""
            )
    if node in self.channels:
        raise ValueError(f""'{node}' is already being used as a state key"")
    if self.compiled:
        logger.warning(
            ""Adding a node to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    if not isinstance(node, str):
        action = node
        node = getattr(action, ""name"", action.__name__)
    if node in self.nodes:
        raise ValueError(f""Node `{node}` already present."")
    if node == END or node == START:
        raise ValueError(f""Node `{node}` is reserved."")

    if CHECKPOINT_NAMESPACE_SEPARATOR in node:
        raise ValueError(
            f""'{CHECKPOINT_NAMESPACE_SEPARATOR}' is a reserved character and is not allowed in the node names.""
        )

    try:
        if isfunction(action) and (
            hints := get_type_hints(action.__call__) or get_type_hints(action)
        ):
            if input is None:
                input_hint = hints[list(hints.keys())[0]]
                if isinstance(input_hint, type) and get_type_hints(input_hint):
                    input = input_hint
    except TypeError:
        pass
    if input is not None:
        self._add_schema(input)
    self.nodes[node] = StateNodeSpec(
        coerce_to_runnable(action, name=node, trace=False),
        metadata,
        input=input or self.schema,
        retry_policy=retry,
    )

",What parameters can be specified when adding a new node to the state graph using the `add_node` method?
"add_edge(start_key,end_key)","
Adds a directed edge from the start node to the end node.
If the graph transitions to the start_key node, it will always transition to the end_key node next.
Parameters:

start_key
              (Union[str, list[str]])
          
          
The key(s) of the start node(s) of the edge.

end_key
              (str)
          
          
The key of the end node of the edge.

Raises:

ValueError
            
          
If the start key is 'END' or if the start key or end key is not present in the graph.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/state.py
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372def add_edge(self, start_key: Union[str, list[str]], end_key: str) -> None:
    """"""Adds a directed edge from the start node to the end node.

    If the graph transitions to the start_key node, it will always transition to the end_key node next.

    Args:
        start_key (Union[str, list[str]]): The key(s) of the start node(s) of the edge.
        end_key (str): The key of the end node of the edge.

    Raises:
        ValueError: If the start key is 'END' or if the start key or end key is not present in the graph.

    Returns:
        None
    """"""
    if isinstance(start_key, str):
        return super().add_edge(start_key, end_key)

    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    for start in start_key:
        if start == END:
            raise ValueError(""END cannot be a start node"")
        if start not in self.nodes:
            raise ValueError(f""Need to add_node `{start}` first"")
    if end_key == START:
        raise ValueError(""START cannot be an end node"")
    if end_key not in self.nodes:
        raise ValueError(f""Need to add_node `{end_key}` first"")

    self.waiting_edges.add((tuple(start_key), end_key))

",What happens if the start key is 'END' or if the start key or end key is not present in the graph when using the add_edge function?
"add_conditional_edges(source,path,path_map=None,then=None)","
Add a conditional edge from the starting node to any number of destination nodes.
Parameters:

source
              (str)
          
          
The starting node. This conditional edge will run when
exiting this node.

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[Hashable, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Without typehints on the path function's return value (e.g., -> Literal[""foo"", ""__end__""]:)
or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

Source code in libs/langgraph/langgraph/graph/graph.py
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263def add_conditional_edges(
    self,
    source: str,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Add a conditional edge from the starting node to any number of destination nodes.

    Args:
        source (str): The starting node. This conditional edge will run when
            exiting this node.
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[Hashable, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None

    Note: Without typehints on the `path` function's return value (e.g., `-> Literal[""foo"", ""__end__""]:`)
        or a path_map, the graph visualization assumes the edge could transition to any node in the graph.

    """"""  # noqa: E501
    if self.compiled:
        logger.warning(
            ""Adding an edge to a graph that has already been compiled. This will ""
            ""not be reflected in the compiled graph.""
        )
    # coerce path_map to a dictionary
    try:
        if isinstance(path_map, dict):
            path_map = path_map.copy()
        elif isinstance(path_map, list):
            path_map = {name: name for name in path_map}
        elif rtn_type := get_type_hints(path.__call__).get(
            ""return""
        ) or get_type_hints(path).get(""return""):
            if get_origin(rtn_type) is Literal:
                path_map = {name: name for name in get_args(rtn_type)}
    except Exception:
        pass
    # find a name for the condition
    path = coerce_to_runnable(path, name=None, trace=True)
    name = path.name or ""condition""
    # validate the condition
    if name in self.branches[source]:
        raise ValueError(
            f""Branch with name `{path.name}` already exists for node "" f""`{source}`""
        )
    # save it
    self.branches[source][name] = Branch(path, path_map, then)

",What parameters are required for the add_conditional_edges function in the graph.py source code?
set_entry_point(key),"
Specifies the first node to be called in the graph.
Equivalent to calling add_edge(START, key).
Parameters:

key
              (str)
          
          
The key of the node to set as the entry point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
265
266
267
268
269
270
271
272
273
274
275
276def set_entry_point(self, key: str) -> None:
    """"""Specifies the first node to be called in the graph.

    Equivalent to calling `add_edge(START, key)`.

    Parameters:
        key (str): The key of the node to set as the entry point.

    Returns:
        None
    """"""
    return self.add_edge(START, key)

",What does the set_entry_point(key) function in the graph.py file do?
"set_conditional_entry_point(path,path_map=None,then=None)","
Sets a conditional entry point in the graph.
Parameters:

path
              (Union[Callable, Runnable])
          
          
The callable that determines the next
node or nodes. If not specifying path_map it should return one or
more nodes. If it returns END, the graph will stop execution.

path_map
              (Optional[dict[str, str]], default:
                  None
)
          
          
Optional mapping of paths to node
names. If omitted the paths returned by path should be node names.

then
              (Optional[str], default:
                  None
)
          
          
The name of a node to execute after the nodes
selected by path.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302def set_conditional_entry_point(
    self,
    path: Union[
        Callable[..., Union[Hashable, list[Hashable]]],
        Callable[..., Awaitable[Union[Hashable, list[Hashable]]]],
        Runnable[Any, Union[Hashable, list[Hashable]]],
    ],
    path_map: Optional[Union[dict[Hashable, str], list[str]]] = None,
    then: Optional[str] = None,
) -> None:
    """"""Sets a conditional entry point in the graph.

    Args:
        path (Union[Callable, Runnable]): The callable that determines the next
            node or nodes. If not specifying `path_map` it should return one or
            more nodes. If it returns END, the graph will stop execution.
        path_map (Optional[dict[str, str]]): Optional mapping of paths to node
            names. If omitted the paths returned by `path` should be node names.
        then (Optional[str]): The name of a node to execute after the nodes
            selected by `path`.

    Returns:
        None
    """"""
    return self.add_conditional_edges(START, path, path_map, then)

",What does the set_conditional_entry_point function in the graph module do?
set_finish_point(key),"
Marks a node as a finish point of the graph.
If the graph reaches this node, it will cease execution.
Parameters:

key
              (str)
          
          
The key of the node to set as the finish point.

Returns:

None
          
          
None

Source code in libs/langgraph/langgraph/graph/graph.py
304
305
306
307
308
309
310
311
312
313
314
315def set_finish_point(self, key: str) -> None:
    """"""Marks a node as a finish point of the graph.

    If the graph reaches this node, it will cease execution.

    Parameters:
        key (str): The key of the node to set as the finish point.

    Returns:
        None
    """"""
    return self.add_edge(key, END)

",What does the set_finish_point(key) function do in the graph?
"compile(checkpointer=None,interrupt_before=None,interrupt_after=None,debug=False)","
Compiles the state graph into a CompiledGraph object.
The compiled graph implements the Runnable interface and can be invoked,
streamed, batched, and run asynchronously.
Parameters:

checkpointer
              (Optional[BaseCheckpointSaver], default:
                  None
)
          
          
An optional checkpoint saver object.
This serves as a fully versioned ""memory"" for the graph, allowing
the graph to be paused and resumed, and replayed from any point.

interrupt_before
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt before.

interrupt_after
              (Optional[Sequence[str]], default:
                  None
)
          
          
An optional list of node names to interrupt after.

debug
              (bool, default:
                  False
)
          
          
A flag indicating whether to enable debug mode.

Returns:

CompiledStateGraph (              CompiledStateGraph
)          
          
The compiled state graph.

Source code in libs/langgraph/langgraph/graph/state.py
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461def compile(
    self,
    checkpointer: Optional[BaseCheckpointSaver] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: bool = False,
) -> ""CompiledStateGraph"":
    """"""Compiles the state graph into a `CompiledGraph` object.

    The compiled graph implements the `Runnable` interface and can be invoked,
    streamed, batched, and run asynchronously.

    Args:
        checkpointer (Optional[BaseCheckpointSaver]): An optional checkpoint saver object.
            This serves as a fully versioned ""memory"" for the graph, allowing
            the graph to be paused and resumed, and replayed from any point.
        interrupt_before (Optional[Sequence[str]]): An optional list of node names to interrupt before.
        interrupt_after (Optional[Sequence[str]]): An optional list of node names to interrupt after.
        debug (bool): A flag indicating whether to enable debug mode.

    Returns:
        CompiledStateGraph: The compiled state graph.
    """"""
    # assign default values
    interrupt_before = interrupt_before or []
    interrupt_after = interrupt_after or []

    # validate the graph
    self.validate(
        interrupt=(
            (interrupt_before if interrupt_before != ""*"" else []) + interrupt_after
            if interrupt_after != ""*""
            else []
        )
    )

    # prepare output channels
    output_channels = (
        ""__root__""
        if len(self.schemas[self.output]) == 1
        and ""__root__"" in self.schemas[self.output]
        else [
            key
            for key, val in self.schemas[self.output].items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )
    stream_channels = (
        ""__root__""
        if len(self.channels) == 1 and ""__root__"" in self.channels
        else [
            key
            for key, val in self.channels.items()
            if not isinstance(val, Context) and not is_managed_value(val)
        ]
    )

    compiled = CompiledStateGraph(
        builder=self,
        config_type=self.config_schema,
        nodes={},
        channels={**self.channels, START: EphemeralValue(self.input)},
        input_channels=START,
        stream_mode=""updates"",
        output_channels=output_channels,
        stream_channels=stream_channels,
        checkpointer=checkpointer,
        interrupt_before_nodes=interrupt_before,
        interrupt_after_nodes=interrupt_after,
        auto_validate=False,
        debug=debug,
    )

    compiled.attach_node(START, None)
    for key, node in self.nodes.items():
        compiled.attach_node(key, node)

    for start, end in self.edges:
        compiled.attach_edge(start, end)

    for starts, end in self.waiting_edges:
        compiled.attach_edge(starts, end)

    for start, branches in self.branches.items():
        for name, branch in branches.items():
            compiled.attach_branch(start, name, branch)

    return compiled.validate()

",What parameters are accepted by the compile function in the state.py file?
CompiledGraph,"

              Bases: Pregel

Source code in libs/langgraph/langgraph/graph/graph.py
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548class CompiledGraph(Pregel):
    builder: Graph

    def attach_node(self, key: str, node: NodeSpec) -> None:
        self.channels[key] = EphemeralValue(Any)
        self.nodes[key] = (
            PregelNode(channels=[], triggers=[], metadata=node.metadata)
            | node.runnable
            | ChannelWrite([ChannelWriteEntry(key)], tags=[TAG_HIDDEN])
        )
        cast(list[str], self.stream_channels).append(key)

    def attach_edge(self, start: str, end: str) -> None:
        if end == END:
            # publish to end channel
            self.nodes[start].writers.append(
                ChannelWrite([ChannelWriteEntry(END)], tags=[TAG_HIDDEN])
            )
        else:
            # subscribe to start channel
            self.nodes[end].triggers.append(start)
            self.nodes[end].channels.append(start)

    def attach_branch(self, start: str, name: str, branch: Branch) -> None:
        def branch_writer(packets: list[Union[str, Send]]) -> Optional[ChannelWrite]:
            writes = [
                (
                    ChannelWriteEntry(f""branch:{start}:{name}:{p}"" if p != END else END)
                    if not isinstance(p, Send)
                    else p
                )
                for p in packets
            ]
            return ChannelWrite(writes, tags=[TAG_HIDDEN])

        # add hidden start node
        if start == START and start not in self.nodes:
            self.nodes[start] = Channel.subscribe_to(START, tags=[TAG_HIDDEN])

        # attach branch writer
        self.nodes[start] |= branch.run(branch_writer)

        # attach branch readers
        ends = branch.ends.values() if branch.ends else [node for node in self.nodes]
        for end in ends:
            if end != END:
                channel_name = f""branch:{start}:{name}:{end}""
                self.channels[channel_name] = EphemeralValue(Any)
                self.nodes[end].triggers.append(channel_name)
                self.nodes[end].channels.append(channel_name)

    def get_graph(
        self,
        config: Optional[RunnableConfig] = None,
        *,
        xray: Union[int, bool] = False,
    ) -> DrawableGraph:
        """"""Returns a drawable representation of the computation graph.""""""
        graph = DrawableGraph()
        start_nodes: dict[str, DrawableNode] = {
            START: graph.add_node(self.get_input_schema(config), START)
        }
        end_nodes: dict[str, DrawableNode] = {}

        def add_edge(
            start: str, end: str, label: Optional[str] = None, conditional: bool = False
        ) -> None:
            if end == END and END not in end_nodes:
                end_nodes[END] = graph.add_node(self.get_output_schema(config), END)
            return graph.add_edge(
                start_nodes[start], end_nodes[end], label, conditional
            )

        for key, n in self.builder.nodes.items():
            node = n.runnable
            metadata = n.metadata or {}
            if key in self.interrupt_before_nodes:
                metadata[""__interrupt""] = ""before""
            elif key in self.interrupt_after_nodes:
                metadata[""__interrupt""] = ""after""
            if xray:
                subgraph = (
                    node.get_graph(
                        config=config,
                        xray=xray - 1 if isinstance(xray, int) and xray > 0 else xray,
                    )
                    if isinstance(node, CompiledGraph)
                    else node.get_graph(config=config)
                )
                subgraph.trim_first_node()
                subgraph.trim_last_node()
                if len(subgraph.nodes) > 1:
                    end_nodes[key], start_nodes[key] = graph.extend(
                        subgraph, prefix=key
                    )
                else:
                    n = graph.add_node(node, key, metadata=metadata or None)
                    start_nodes[key] = n
                    end_nodes[key] = n
            else:
                n = graph.add_node(node, key, metadata=metadata or None)
                start_nodes[key] = n
                end_nodes[key] = n
        for start, end in sorted(self.builder._all_edges):
            add_edge(start, end)
        for start, branches in self.builder.branches.items():
            default_ends = {
                **{k: k for k in self.builder.nodes if k != start},
                END: END,
            }
            for _, branch in branches.items():
                if branch.ends is not None:
                    ends = branch.ends
                elif branch.then is not None:
                    ends = {k: k for k in default_ends if k not in (END, branch.then)}
                else:
                    ends = default_ends
                for label, end in ends.items():
                    add_edge(
                        start,
                        end,
                        label if label != end else None,
                        conditional=True,
                    )
                    if branch.then is not None:
                        add_edge(end, branch.then)

        return graph

stream_mode: StreamMode = 'values'

class-attribute
instance-attribute

Mode to stream output, defaults to 'values'.

stream_channels: Optional[Union[str, Sequence[str]]] = None

class-attribute
instance-attribute

Channels to stream, defaults to all channels not in reserved channels

step_timeout: Optional[float] = None

class-attribute
instance-attribute

Maximum time to wait for a step to complete, in seconds. Defaults to None.

debug: bool = Field(default_factory=get_debug)

class-attribute
instance-attribute

Whether to print debug information during execution. Defaults to False.

checkpointer: Optional[BaseCheckpointSaver] = None

class-attribute
instance-attribute

Checkpointer used to save and load graph state. Defaults to None.

retry_policy: Optional[RetryPolicy] = None

class-attribute
instance-attribute

Retry policy to use when running tasks. Set to None to disable.

is_lc_serializable()

classmethod

Return whether the graph can be serialized by Langchain.

Source code in libs/langgraph/langgraph/pregel/__init__.py
232
233
234
235@classmethod
def is_lc_serializable(cls) -> bool:
    """"""Return whether the graph can be serialized by Langchain.""""""
    return True

get_state(config)

Get the current state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389def get_state(self, config: RunnableConfig) -> StateSnapshot:
    """"""Get the current state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")

    saved = self.checkpointer.get_tuple(config)
    checkpoint = saved.checkpoint if saved else empty_checkpoint()
    config = saved.config if saved else config
    with ChannelsManager(
        {
            k: LastValue(None) if isinstance(c, Context) else c
            for k, c in self.channels.items()
        },
        checkpoint,
        config,
    ) as channels, ManagedValuesManager(
        self.managed_values_dict, ensure_config(config)
    ) as managed:
        next_tasks = prepare_next_tasks(
            checkpoint,
            self.nodes,
            channels,
            managed,
            config,
            -1,
            for_execution=False,
        )
        return StateSnapshot(
            read_channels(channels, self.stream_channels_asis),
            tuple(t.name for t in next_tasks),
            saved.config if saved else config,
            saved.metadata if saved else None,
            saved.checkpoint[""ts""] if saved else None,
            saved.parent_config if saved else None,
        )

aget_state(config)

async

Get the current state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426async def aget_state(self, config: RunnableConfig) -> StateSnapshot:
    """"""Get the current state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")

    saved = await self.checkpointer.aget_tuple(config)
    checkpoint = saved.checkpoint if saved else empty_checkpoint()

    config = saved.config if saved else config
    async with AsyncChannelsManager(
        {
            k: LastValue(None) if isinstance(c, Context) else c
            for k, c in self.channels.items()
        },
        checkpoint,
        config,
    ) as channels, AsyncManagedValuesManager(
        self.managed_values_dict, ensure_config(config)
    ) as managed:
        next_tasks = prepare_next_tasks(
            checkpoint,
            self.nodes,
            channels,
            managed,
            config,
            -1,
            for_execution=False,
        )
        return StateSnapshot(
            read_channels(channels, self.stream_channels_asis),
            tuple(t.name for t in next_tasks),
            saved.config if saved else config,
            saved.metadata if saved else None,
            saved.checkpoint[""ts""] if saved else None,
            saved.parent_config if saved else None,
        )

get_state_history(config, *, filter=None, before=None, limit=None)

Get the history of the state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473def get_state_history(
    self,
    config: RunnableConfig,
    *,
    filter: Optional[Dict[str, Any]] = None,
    before: Optional[RunnableConfig] = None,
    limit: Optional[int] = None,
) -> Iterator[StateSnapshot]:
    """"""Get the history of the state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")
    if (
        filter is not None
        and signature(self.checkpointer.list).parameters.get(""filter"") is None
    ):
        raise ValueError(""Checkpointer does not support filtering"")
    for config, checkpoint, metadata, parent_config, _ in self.checkpointer.list(
        config, before=before, limit=limit, filter=filter
    ):
        with ChannelsManager(
            {
                k: LastValue(None) if isinstance(c, Context) else c
                for k, c in self.channels.items()
            },
            checkpoint,
            config,
        ) as channels, ManagedValuesManager(
            self.managed_values_dict, ensure_config(config)
        ) as managed:
            next_tasks = prepare_next_tasks(
                checkpoint,
                self.nodes,
                channels,
                managed,
                config,
                -1,
                for_execution=False,
            )
            yield StateSnapshot(
                read_channels(channels, self.stream_channels_asis),
                tuple(t.name for t in next_tasks),
                config,
                metadata,
                checkpoint[""ts""],
                parent_config,
            )

aget_state_history(config, *, filter=None, before=None, limit=None)

async

Get the history of the state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524async def aget_state_history(
    self,
    config: RunnableConfig,
    *,
    filter: Optional[Dict[str, Any]] = None,
    before: Optional[RunnableConfig] = None,
    limit: Optional[int] = None,
) -> AsyncIterator[StateSnapshot]:
    """"""Get the history of the state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")
    if (
        filter is not None
        and signature(self.checkpointer.list).parameters.get(""filter"") is None
    ):
        raise ValueError(""Checkpointer does not support filtering"")
    async for (
        config,
        checkpoint,
        metadata,
        parent_config,
        _,
    ) in self.checkpointer.alist(config, before=before, limit=limit, filter=filter):
        async with AsyncChannelsManager(
            {
                k: LastValue(None) if isinstance(c, Context) else c
                for k, c in self.channels.items()
            },
            checkpoint,
            config,
        ) as channels, AsyncManagedValuesManager(
            self.managed_values_dict, ensure_config(config)
        ) as managed:
            next_tasks = prepare_next_tasks(
                checkpoint,
                self.nodes,
                channels,
                managed,
                config,
                -1,
                for_execution=False,
            )
            yield StateSnapshot(
                read_channels(channels, self.stream_channels_asis),
                tuple(t.name for t in next_tasks),
                config,
                metadata,
                checkpoint[""ts""],
                parent_config,
            )

update_state(config, values, as_node=None)

Update the state of the graph with the given values, as if they came from
node as_node. If as_node is not provided, it will be set to the last node
that updated the state, if not ambiguous.

Source code in libs/langgraph/langgraph/pregel/__init__.py
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646def update_state(
    self,
    config: RunnableConfig,
    values: Optional[Union[dict[str, Any], Any]],
    as_node: Optional[str] = None,
) -> RunnableConfig:
    """"""Update the state of the graph with the given values, as if they came from
    node `as_node`. If `as_node` is not provided, it will be set to the last node
    that updated the state, if not ambiguous.
    """"""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")

    # get last checkpoint
    saved = self.checkpointer.get_tuple(config)
    checkpoint = copy_checkpoint(saved.checkpoint) if saved else empty_checkpoint()
    checkpoint_previous_versions = (
        saved.checkpoint[""channel_versions""] if saved else {}
    )
    step = saved.metadata.get(""step"", -1) if saved else -1
    # merge configurable fields with previous checkpoint config
    checkpoint_config = {
        **config,
        ""configurable"": {
            **config[""configurable""],
            # TODO: add proper support for updating nested subgraph state
            ""checkpoint_ns"": """",
        },
    }
    if saved:
        checkpoint_config = {
            ""configurable"": {
                **config.get(""configurable"", {}),
                **saved.config[""configurable""],
            }
        }
    # find last node that updated the state, if not provided
    if values is None and as_node is None:
        return self.checkpointer.put(
            checkpoint_config,
            create_checkpoint(checkpoint, None, step),
            {
                ""source"": ""update"",
                ""step"": step,
                ""writes"": {},
            },
            {},
        )
    elif as_node is None and not any(
        v for vv in checkpoint[""versions_seen""].values() for v in vv.values()
    ):
        if (
            isinstance(self.input_channels, str)
            and self.input_channels in self.nodes
        ):
            as_node = self.input_channels
    elif as_node is None:
        last_seen_by_node = sorted(
            (v, n)
            for n, seen in checkpoint[""versions_seen""].items()
            for v in seen.values()
        )
        # if two nodes updated the state at the same time, it's ambiguous
        if last_seen_by_node:
            if len(last_seen_by_node) == 1:
                as_node = last_seen_by_node[0][1]
            elif last_seen_by_node[-1][0] != last_seen_by_node[-2][0]:
                as_node = last_seen_by_node[-1][1]
    if as_node is None:
        raise InvalidUpdateError(""Ambiguous update, specify as_node"")
    if as_node not in self.nodes:
        raise InvalidUpdateError(f""Node {as_node} does not exist"")
    # update channels
    with ChannelsManager(self.channels, checkpoint, config) as channels:
        # create task to run all writers of the chosen node
        writers = self.nodes[as_node].get_writers()
        if not writers:
            raise InvalidUpdateError(f""Node {as_node} has no writers"")
        task = PregelExecutableTask(
            as_node,
            values,
            RunnableSequence(*writers) if len(writers) > 1 else writers[0],
            deque(),
            None,
            [INTERRUPT],
            None,
            str(uuid5(UUID(checkpoint[""id""]), INTERRUPT)),
        )
        # execute task
        task.proc.invoke(
            task.input,
            patch_config(
                config,
                run_name=self.name + ""UpdateState"",
                configurable={
                    # deque.extend is thread-safe
                    CONFIG_KEY_SEND: task.writes.extend,
                    CONFIG_KEY_READ: partial(
                        local_read, checkpoint, channels, task, config
                    ),
                },
            ),
        )
        # apply to checkpoint and save
        apply_writes(
            checkpoint, channels, [task], self.checkpointer.get_next_version
        )

        new_versions = get_new_channel_versions(
            checkpoint_previous_versions, checkpoint[""channel_versions""]
        )
        return self.checkpointer.put(
            checkpoint_config,
            create_checkpoint(checkpoint, channels, step + 1),
            {
                ""source"": ""update"",
                ""step"": step + 1,
                ""writes"": {as_node: values},
            },
            new_versions,
        )

stream(input, config=None, *, stream_mode=None, output_keys=None, interrupt_before=None, interrupt_after=None, debug=None)

Stream graph steps for a single input.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input to the graph.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
The configuration to use for the run.

stream_mode
              (Optional[Union[StreamMode, list[StreamMode]]], default:
                  None
)
          
          
The mode to stream output, defaults to self.stream_mode.
Options are 'values', 'updates', and 'debug'.
values: Emit the current values of the state for each step.
updates: Emit only the updates to the state for each step.
    Output is a dict with the node name as key and the updated values as value.
debug: Emit debug events for each step.

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
The keys to stream, defaults to all non-context channels.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt before, defaults to all nodes in the graph.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt after, defaults to all nodes in the graph.

debug
              (Optional[bool], default:
                  None
)
          
          
Whether to print debug information during execution, defaults to False.

Yields:

Union[dict[str, Any], Any]
          
          
The output of each step in the graph. The output shape depends on the stream_mode.

Examples:
Using different stream modes with a graph:
>>> import operator
>>> from typing_extensions import Annotated, TypedDict
>>> from langgraph.graph import StateGraph
>>> from langgraph.constants import START
...
>>> class State(TypedDict):
...     alist: Annotated[list, operator.add]
...     another_list: Annotated[list, operator.add]
...
>>> builder = StateGraph(State)
>>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
>>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
>>> builder.add_edge(""a"", ""b"")
>>> builder.add_edge(START, ""a"")
>>> graph = builder.compile()

With stream_mode=""values"":
>>> for event in graph.stream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
...     print(event)
{'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
{'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
{'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}

With stream_mode=""updates"":
>>> for event in graph.stream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
...     print(event)
{'a': {'another_list': ['hi']}}
{'b': {'alist': ['there']}}

With stream_mode=""debug"":
>>> for event in graph.stream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
...     print(event)
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}

Source code in libs/langgraph/langgraph/pregel/__init__.py
 814
 815
 816
 817
 818
 819
 820
 821
 822
 823
 824
 825
 826
 827
 828
 829
 830
 831
 832
 833
 834
 835
 836
 837
 838
 839
 840
 841
 842
 843
 844
 845
 846
 847
 848
 849
 850
 851
 852
 853
 854
 855
 856
 857
 858
 859
 860
 861
 862
 863
 864
 865
 866
 867
 868
 869
 870
 871
 872
 873
 874
 875
 876
 877
 878
 879
 880
 881
 882
 883
 884
 885
 886
 887
 888
 889
 890
 891
 892
 893
 894
 895
 896
 897
 898
 899
 900
 901
 902
 903
 904
 905
 906
 907
 908
 909
 910
 911
 912
 913
 914
 915
 916
 917
 918
 919
 920
 921
 922
 923
 924
 925
 926
 927
 928
 929
 930
 931
 932
 933
 934
 935
 936
 937
 938
 939
 940
 941
 942
 943
 944
 945
 946
 947
 948
 949
 950
 951
 952
 953
 954
 955
 956
 957
 958
 959
 960
 961
 962
 963
 964
 965
 966
 967
 968
 969
 970
 971
 972
 973
 974
 975
 976
 977
 978
 979
 980
 981
 982
 983
 984
 985
 986
 987
 988
 989
 990
 991
 992
 993
 994
 995
 996
 997
 998
 999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047def stream(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None,
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
) -> Iterator[Union[dict[str, Any], Any]]:
    """"""Stream graph steps for a single input.

    Args:
        input: The input to the graph.
        config: The configuration to use for the run.
        stream_mode: The mode to stream output, defaults to self.stream_mode.
            Options are 'values', 'updates', and 'debug'.
            values: Emit the current values of the state for each step.
            updates: Emit only the updates to the state for each step.
                Output is a dict with the node name as key and the updated values as value.
            debug: Emit debug events for each step.
        output_keys: The keys to stream, defaults to all non-context channels.
        interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
        interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
        debug: Whether to print debug information during execution, defaults to False.

    Yields:
        The output of each step in the graph. The output shape depends on the stream_mode.

    Examples:
        Using different stream modes with a graph:
        ```pycon
        >>> import operator
        >>> from typing_extensions import Annotated, TypedDict
        >>> from langgraph.graph import StateGraph
        >>> from langgraph.constants import START
        ...
        >>> class State(TypedDict):
        ...     alist: Annotated[list, operator.add]
        ...     another_list: Annotated[list, operator.add]
        ...
        >>> builder = StateGraph(State)
        >>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
        >>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
        >>> builder.add_edge(""a"", ""b"")
        >>> builder.add_edge(START, ""a"")
        >>> graph = builder.compile()
        ```
        With stream_mode=""values"":

        ```pycon
        >>> for event in graph.stream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
        ...     print(event)
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
        {'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}
        ```
        With stream_mode=""updates"":

        ```pycon
        >>> for event in graph.stream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
        ...     print(event)
        {'a': {'another_list': ['hi']}}
        {'b': {'alist': ['there']}}
        ```
        With stream_mode=""debug"":

        ```pycon
        >>> for event in graph.stream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
        ...     print(event)
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}
        ```
    """"""
    config = ensure_config(config)
    callback_manager = get_callback_manager_for_config(config)
    run_manager = callback_manager.on_chain_start(
        dumpd(self),
        input,
        name=config.get(""run_name"", self.get_name()),
        run_id=config.get(""run_id""),
    )
    try:
        if config[""recursion_limit""] < 1:
            raise ValueError(""recursion_limit must be at least 1"")
        if self.checkpointer and not config.get(""configurable""):
            raise ValueError(
                f""Checkpointer requires one or more of the following 'configurable' keys: {[s.id for s in self.checkpointer.config_specs]}""
            )
        # assign defaults
        (
            debug,
            stream_modes,
            output_keys,
            interrupt_before,
            interrupt_after,
            checkpointer,
        ) = self._defaults(
            config,
            stream_mode=stream_mode,
            output_keys=output_keys,
            interrupt_before=interrupt_before,
            interrupt_after=interrupt_after,
            debug=debug,
        )

        with SyncPregelLoop(
            input, config=config, checkpointer=checkpointer, graph=self
        ) as loop:
            # Similarly to Bulk Synchronous Parallel / Pregel model
            # computation proceeds in steps, while there are channel updates
            # channel updates from step N are only visible in step N+1
            # channels are guaranteed to be immutable for the duration of the step,
            # with channel updates applied only at the transition between steps
            while loop.tick(
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                manager=run_manager,
            ):
                # debug flag
                if self.debug:
                    print_step_checkpoint(
                        loop.checkpoint_metadata,
                        loop.channels,
                        self.stream_channels_list,
                    )
                # emit output
                while loop.stream:
                    mode, payload = loop.stream.popleft()
                    if mode in stream_modes:
                        if isinstance(stream_mode, list):
                            yield (mode, payload)
                        else:
                            yield payload
                # debug flag
                if debug:
                    print_step_tasks(loop.step, loop.tasks)

                # execute tasks, and wait for one to fail or all to finish.
                # each task is independent from all other concurrent tasks
                # yield updates/debug output as each task finishes
                futures = {
                    loop.submit(
                        run_with_retry,
                        task,
                        self.retry_policy,
                    ): task
                    for task in loop.tasks
                    if not task.writes
                }
                end_time = (
                    self.step_timeout + time.monotonic()
                    if self.step_timeout
                    else None
                )
                if not futures:
                    done, inflight = set(), set()
                while futures:
                    done, inflight = concurrent.futures.wait(
                        futures,
                        return_when=concurrent.futures.FIRST_COMPLETED,
                        timeout=(
                            max(0, end_time - time.monotonic())
                            if end_time
                            else None
                        ),
                    )
                    if not done:
                        break  # timed out
                    for fut in done:
                        task = futures.pop(fut)
                        if fut.exception() is not None:
                            # we got an exception, break out of while loop
                            # exception will be handled in panic_or_proceed
                            futures.clear()
                        else:
                            # save task writes to checkpointer
                            loop.put_writes(task.id, task.writes)
                            # yield updates output for the finished task
                            if ""updates"" in stream_modes:
                                yield from _with_mode(
                                    ""updates"",
                                    isinstance(stream_mode, list),
                                    map_output_updates(output_keys, [task]),
                                )
                            if ""debug"" in stream_modes:
                                yield from _with_mode(
                                    ""debug"",
                                    isinstance(stream_mode, list),
                                    map_debug_task_results(
                                        loop.step,
                                        [task],
                                        self.stream_channels_list,
                                    ),
                                )
                    else:
                        # remove references to loop vars
                        del fut, task

                # panic on failure or timeout
                _panic_or_proceed(done, inflight, loop.step)
                # don't keep futures around in memory longer than needed
                del done, inflight, futures
                # debug flag
                if debug:
                    print_step_writes(
                        loop.step,
                        [w for t in loop.tasks for w in t.writes],
                        self.stream_channels_list,
                    )
            # emit output
            while loop.stream:
                mode, payload = loop.stream.popleft()
                if mode in stream_modes:
                    if isinstance(stream_mode, list):
                        yield (mode, payload)
                    else:
                        yield payload
            # handle exit
            if loop.status == ""out_of_steps"":
                raise GraphRecursionError(
                    f""Recursion limit of {config['recursion_limit']} reached ""
                    ""without hitting a stop condition. You can increase the ""
                    ""limit by setting the `recursion_limit` config key.""
                )
            # set final channel values as run output
            run_manager.on_chain_end(read_channels(loop.channels, output_keys))
    except BaseException as e:
        run_manager.on_chain_error(e)
        raise

astream(input, config=None, *, stream_mode=None, output_keys=None, interrupt_before=None, interrupt_after=None, debug=None)

async

Stream graph steps for a single input.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input to the graph.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
The configuration to use for the run.

stream_mode
              (Optional[Union[StreamMode, list[StreamMode]]], default:
                  None
)
          
          
The mode to stream output, defaults to self.stream_mode.
Options are 'values', 'updates', and 'debug'.
values: Emit the current values of the state for each step.
updates: Emit only the updates to the state for each step.
    Output is a dict with the node name as key and the updated values as value.
debug: Emit debug events for each step.

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
The keys to stream, defaults to all non-context channels.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt before, defaults to all nodes in the graph.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt after, defaults to all nodes in the graph.

debug
              (Optional[bool], default:
                  None
)
          
          
Whether to print debug information during execution, defaults to False.

Yields:

AsyncIterator[Union[dict[str, Any], Any]]
          
          
The output of each step in the graph. The output shape depends on the stream_mode.

Examples:
Using different stream modes with a graph:
>>> import operator
>>> from typing_extensions import Annotated, TypedDict
>>> from langgraph.graph import StateGraph
>>> from langgraph.constants import START
...
>>> class State(TypedDict):
...     alist: Annotated[list, operator.add]
...     another_list: Annotated[list, operator.add]
...
>>> builder = StateGraph(State)
>>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
>>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
>>> builder.add_edge(""a"", ""b"")
>>> builder.add_edge(START, ""a"")
>>> graph = builder.compile()

With stream_mode=""values"":
>>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
...     print(event)
{'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
{'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
{'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}

With stream_mode=""updates"":
>>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
...     print(event)
{'a': {'another_list': ['hi']}}
{'b': {'alist': ['there']}}

With stream_mode=""debug"":
>>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
...     print(event)
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}

Source code in libs/langgraph/langgraph/pregel/__init__.py
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
1298async def astream(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None,
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
) -> AsyncIterator[Union[dict[str, Any], Any]]:
    """"""Stream graph steps for a single input.

    Args:
        input: The input to the graph.
        config: The configuration to use for the run.
        stream_mode: The mode to stream output, defaults to self.stream_mode.
            Options are 'values', 'updates', and 'debug'.
            values: Emit the current values of the state for each step.
            updates: Emit only the updates to the state for each step.
                Output is a dict with the node name as key and the updated values as value.
            debug: Emit debug events for each step.
        output_keys: The keys to stream, defaults to all non-context channels.
        interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
        interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
        debug: Whether to print debug information during execution, defaults to False.

    Yields:
        The output of each step in the graph. The output shape depends on the stream_mode.

    Examples:
        Using different stream modes with a graph:
        ```pycon
        >>> import operator
        >>> from typing_extensions import Annotated, TypedDict
        >>> from langgraph.graph import StateGraph
        >>> from langgraph.constants import START
        ...
        >>> class State(TypedDict):
        ...     alist: Annotated[list, operator.add]
        ...     another_list: Annotated[list, operator.add]
        ...
        >>> builder = StateGraph(State)
        >>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
        >>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
        >>> builder.add_edge(""a"", ""b"")
        >>> builder.add_edge(START, ""a"")
        >>> graph = builder.compile()
        ```
        With stream_mode=""values"":

        ```pycon
        >>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
        ...     print(event)
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
        {'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}
        ```
        With stream_mode=""updates"":

        ```pycon
        >>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
        ...     print(event)
        {'a': {'another_list': ['hi']}}
        {'b': {'alist': ['there']}}
        ```
        With stream_mode=""debug"":

        ```pycon
        >>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
        ...     print(event)
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}
        ```
    """"""
    config = ensure_config(config)
    callback_manager = get_async_callback_manager_for_config(config)
    run_manager = await callback_manager.on_chain_start(
        dumpd(self),
        input,
        name=config.get(""run_name"", self.get_name()),
        run_id=config.get(""run_id""),
    )
    # if running from astream_log() run each proc with streaming
    do_stream = next(
        (
            h
            for h in run_manager.handlers
            if isinstance(h, _StreamingCallbackHandler)
        ),
        None,
    )
    try:
        if config[""recursion_limit""] < 1:
            raise ValueError(""recursion_limit must be at least 1"")
        if self.checkpointer and not config.get(""configurable""):
            raise ValueError(
                f""Checkpointer requires one or more of the following 'configurable' keys: {[s.id for s in self.checkpointer.config_specs]}""
            )
        # assign defaults
        (
            debug,
            stream_modes,
            output_keys,
            interrupt_before,
            interrupt_after,
            checkpointer,
        ) = self._defaults(
            config,
            stream_mode=stream_mode,
            output_keys=output_keys,
            interrupt_before=interrupt_before,
            interrupt_after=interrupt_after,
            debug=debug,
        )
        async with AsyncPregelLoop(
            input, config=config, checkpointer=checkpointer, graph=self
        ) as loop:
            aioloop = asyncio.get_event_loop()
            # Similarly to Bulk Synchronous Parallel / Pregel model
            # computation proceeds in steps, while there are channel updates
            # channel updates from step N are only visible in step N+1
            # channels are guaranteed to be immutable for the duration of the step,
            # with channel updates applied only at the transition between steps
            while loop.tick(
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                manager=run_manager,
            ):
                # debug flag
                if self.debug:
                    print_step_checkpoint(
                        loop.checkpoint_metadata,
                        loop.channels,
                        self.stream_channels_list,
                    )
                # emit output
                while loop.stream:
                    mode, payload = loop.stream.popleft()
                    if mode in stream_modes:
                        if isinstance(stream_mode, list):
                            yield (mode, payload)
                        else:
                            yield payload
                # debug flag
                if debug:
                    print_step_tasks(loop.step, loop.tasks)

                # execute tasks, and wait for one to fail or all to finish.
                # each task is independent from all other concurrent tasks
                # yield updates/debug output as each task finishes
                futures = {
                    loop.submit(
                        arun_with_retry,
                        task,
                        self.retry_policy,
                        stream=do_stream,
                        __name__=task.name,
                        __cancel_on_exit__=True,
                    ): task
                    for task in loop.tasks
                    if not task.writes
                }
                end_time = (
                    self.step_timeout + aioloop.time()
                    if self.step_timeout
                    else None
                )
                if not futures:
                    done, inflight = set(), set()
                while futures:
                    done, inflight = await asyncio.wait(
                        futures,
                        return_when=asyncio.FIRST_COMPLETED,
                        timeout=(
                            max(0, end_time - aioloop.time()) if end_time else None
                        ),
                    )
                    if not done:
                        break  # timed out
                    for fut in done:
                        task = futures.pop(fut)
                        if fut.exception() is not None:
                            # we got an exception, break out of while loop
                            # exception will be handled in panic_or_proceed
                            futures.clear()
                        else:
                            # save task writes to checkpointer
                            loop.put_writes(task.id, task.writes)
                            # yield updates output for the finished task
                            if ""updates"" in stream_modes:
                                for chunk in _with_mode(
                                    ""updates"",
                                    isinstance(stream_mode, list),
                                    map_output_updates(output_keys, [task]),
                                ):
                                    yield chunk
                            if ""debug"" in stream_modes:
                                for chunk in _with_mode(
                                    ""debug"",
                                    isinstance(stream_mode, list),
                                    map_debug_task_results(
                                        loop.step,
                                        [task],
                                        self.stream_channels_list,
                                    ),
                                ):
                                    yield chunk
                    else:
                        # remove references to loop vars
                        del fut, task

                # panic on failure or timeout
                _panic_or_proceed(done, inflight, loop.step, asyncio.TimeoutError)
                # don't keep futures around in memory longer than needed
                del done, inflight, futures
                # debug flag
                if debug:
                    print_step_writes(
                        loop.step,
                        [w for t in loop.tasks for w in t.writes],
                        self.stream_channels_list,
                    )
            # emit output
            while loop.stream:
                mode, payload = loop.stream.popleft()
                if mode in stream_modes:
                    if isinstance(stream_mode, list):
                        yield (mode, payload)
                    else:
                        yield payload
            # handle exit
            if loop.status == ""out_of_steps"":
                raise GraphRecursionError(
                    f""Recursion limit of {config['recursion_limit']} reached ""
                    ""without hitting a stop condition. You can increase the ""
                    ""limit by setting the `recursion_limit` config key.""
                )

            # set final channel values as run output
            await run_manager.on_chain_end(
                read_channels(loop.channels, output_keys)
            )
    except BaseException as e:
        # TODO use on_chain_end if exc is GraphInterrupt
        await asyncio.shield(run_manager.on_chain_error(e))
        raise

invoke(input, config=None, *, stream_mode='values', output_keys=None, interrupt_before=None, interrupt_after=None, debug=None, **kwargs)

Run the graph with a single input and config.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input data for the graph. It can be a dictionary or any other type.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
Optional. The configuration for the graph run.

stream_mode
              (StreamMode, default:
                  'values'
)
          
          
Optional[str]. The stream mode for the graph run. Default is ""values"".

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
Optional. The output keys to retrieve from the graph run.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt the graph run before.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt the graph run after.

debug
              (Optional[bool], default:
                  None
)
          
          
Optional. Enable debug mode for the graph run.

**kwargs
              (Any, default:
                  {}
)
          
          
Additional keyword arguments to pass to the graph run.

Returns:

Union[dict[str, Any], Any]
          
          
The output of the graph run. If stream_mode is ""values"", it returns the latest output.

Union[dict[str, Any], Any]
          
          
If stream_mode is not ""values"", it returns a list of output chunks.

Source code in libs/langgraph/langgraph/pregel/__init__.py
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
1328
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
1343
1344
1345
1346
1347
1348
1349
1350def invoke(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: StreamMode = ""values"",
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
    **kwargs: Any,
) -> Union[dict[str, Any], Any]:
    """"""Run the graph with a single input and config.

    Args:
        input: The input data for the graph. It can be a dictionary or any other type.
        config: Optional. The configuration for the graph run.
        stream_mode: Optional[str]. The stream mode for the graph run. Default is ""values"".
        output_keys: Optional. The output keys to retrieve from the graph run.
        interrupt_before: Optional. The nodes to interrupt the graph run before.
        interrupt_after: Optional. The nodes to interrupt the graph run after.
        debug: Optional. Enable debug mode for the graph run.
        **kwargs: Additional keyword arguments to pass to the graph run.

    Returns:
        The output of the graph run. If stream_mode is ""values"", it returns the latest output.
        If stream_mode is not ""values"", it returns a list of output chunks.
    """"""
    output_keys = output_keys if output_keys is not None else self.output_channels
    if stream_mode == ""values"":
        latest: Union[dict[str, Any], Any] = None
    else:
        chunks = []
    for chunk in self.stream(
        input,
        config,
        stream_mode=stream_mode,
        output_keys=output_keys,
        interrupt_before=interrupt_before,
        interrupt_after=interrupt_after,
        debug=debug,
        **kwargs,
    ):
        if stream_mode == ""values"":
            latest = chunk
        else:
            chunks.append(chunk)
    if stream_mode == ""values"":
        return latest
    else:
        return chunks

ainvoke(input, config=None, *, stream_mode='values', output_keys=None, interrupt_before=None, interrupt_after=None, debug=None, **kwargs)

async

Asynchronously invoke the graph on a single input.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input data for the computation. It can be a dictionary or any other type.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
Optional. The configuration for the computation.

stream_mode
              (StreamMode, default:
                  'values'
)
          
          
Optional. The stream mode for the computation. Default is ""values"".

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
Optional. The output keys to include in the result. Default is None.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt before. Default is None.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt after. Default is None.

debug
              (Optional[bool], default:
                  None
)
          
          
Optional. Whether to enable debug mode. Default is None.

**kwargs
              (Any, default:
                  {}
)
          
          
Additional keyword arguments.

Returns:

Union[dict[str, Any], Any]
          
          
The result of the computation. If stream_mode is ""values"", it returns the latest value.

Union[dict[str, Any], Any]
          
          
If stream_mode is ""chunks"", it returns a list of chunks.

Source code in libs/langgraph/langgraph/pregel/__init__.py
1352
1353
1354
1355
1356
1357
1358
1359
1360
1361
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403async def ainvoke(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: StreamMode = ""values"",
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
    **kwargs: Any,
) -> Union[dict[str, Any], Any]:
    """"""Asynchronously invoke the graph on a single input.

    Args:
        input: The input data for the computation. It can be a dictionary or any other type.
        config: Optional. The configuration for the computation.
        stream_mode: Optional. The stream mode for the computation. Default is ""values"".
        output_keys: Optional. The output keys to include in the result. Default is None.
        interrupt_before: Optional. The nodes to interrupt before. Default is None.
        interrupt_after: Optional. The nodes to interrupt after. Default is None.
        debug: Optional. Whether to enable debug mode. Default is None.
        **kwargs: Additional keyword arguments.

    Returns:
        The result of the computation. If stream_mode is ""values"", it returns the latest value.
        If stream_mode is ""chunks"", it returns a list of chunks.
    """"""

    output_keys = output_keys if output_keys is not None else self.output_channels
    if stream_mode == ""values"":
        latest: Union[dict[str, Any], Any] = None
    else:
        chunks = []
    async for chunk in self.astream(
        input,
        config,
        stream_mode=stream_mode,
        output_keys=output_keys,
        interrupt_before=interrupt_before,
        interrupt_after=interrupt_after,
        debug=debug,
        **kwargs,
    ):
        if stream_mode == ""values"":
            latest = chunk
        else:
            chunks.append(chunk)
    if stream_mode == ""values"":
        return latest
    else:
        return chunks

get_graph(config=None, *, xray=False)

Returns a drawable representation of the computation graph.

Source code in libs/langgraph/langgraph/graph/graph.py
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548def get_graph(
    self,
    config: Optional[RunnableConfig] = None,
    *,
    xray: Union[int, bool] = False,
) -> DrawableGraph:
    """"""Returns a drawable representation of the computation graph.""""""
    graph = DrawableGraph()
    start_nodes: dict[str, DrawableNode] = {
        START: graph.add_node(self.get_input_schema(config), START)
    }
    end_nodes: dict[str, DrawableNode] = {}

    def add_edge(
        start: str, end: str, label: Optional[str] = None, conditional: bool = False
    ) -> None:
        if end == END and END not in end_nodes:
            end_nodes[END] = graph.add_node(self.get_output_schema(config), END)
        return graph.add_edge(
            start_nodes[start], end_nodes[end], label, conditional
        )

    for key, n in self.builder.nodes.items():
        node = n.runnable
        metadata = n.metadata or {}
        if key in self.interrupt_before_nodes:
            metadata[""__interrupt""] = ""before""
        elif key in self.interrupt_after_nodes:
            metadata[""__interrupt""] = ""after""
        if xray:
            subgraph = (
                node.get_graph(
                    config=config,
                    xray=xray - 1 if isinstance(xray, int) and xray > 0 else xray,
                )
                if isinstance(node, CompiledGraph)
                else node.get_graph(config=config)
            )
            subgraph.trim_first_node()
            subgraph.trim_last_node()
            if len(subgraph.nodes) > 1:
                end_nodes[key], start_nodes[key] = graph.extend(
                    subgraph, prefix=key
                )
            else:
                n = graph.add_node(node, key, metadata=metadata or None)
                start_nodes[key] = n
                end_nodes[key] = n
        else:
            n = graph.add_node(node, key, metadata=metadata or None)
            start_nodes[key] = n
            end_nodes[key] = n
    for start, end in sorted(self.builder._all_edges):
        add_edge(start, end)
    for start, branches in self.builder.branches.items():
        default_ends = {
            **{k: k for k in self.builder.nodes if k != start},
            END: END,
        }
        for _, branch in branches.items():
            if branch.ends is not None:
                ends = branch.ends
            elif branch.then is not None:
                ends = {k: k for k in default_ends if k not in (END, branch.then)}
            else:
                ends = default_ends
            for label, end in ends.items():
                add_edge(
                    start,
                    end,
                    label if label != end else None,
                    conditional=True,
                )
                if branch.then is not None:
                    add_edge(end, branch.then)

    return graph

",What does the `get_graph` method in the `CompiledGraph` class return?
stream_mode:StreamMode='values'class-attributeinstance-attribute,"
Mode to stream output, defaults to 'values'.
",What is the default mode for streaming output?
"stream_channels:Optional[Union[str,Sequence[str]]]=Noneclass-attributeinstance-attribute","
Channels to stream, defaults to all channels not in reserved channels
",What channels are streamed by default if no specific channels are specified?
step_timeout:Optional[float]=Noneclass-attributeinstance-attribute,"
Maximum time to wait for a step to complete, in seconds. Defaults to None.
",What is the default value for the maximum time to wait for a step to complete in seconds?
debug:bool=Field(default_factory=get_debug)class-attributeinstance-attribute,"
Whether to print debug information during execution. Defaults to False.
",What is the default value for the debug boolean field in the class attribute instance?
checkpointer:Optional[BaseCheckpointSaver]=Noneclass-attributeinstance-attribute,"
Checkpointer used to save and load graph state. Defaults to None.
",What is the default value for the checkpointer used to save and load graph state?
retry_policy:Optional[RetryPolicy]=Noneclass-attributeinstance-attribute,"
Retry policy to use when running tasks. Set to None to disable.
",What is the default value for the retry_policy attribute when running tasks?
is_lc_serializable()classmethod,"
Return whether the graph can be serialized by Langchain.

Source code in libs/langgraph/langgraph/pregel/__init__.py
232
233
234
235@classmethod
def is_lc_serializable(cls) -> bool:
    """"""Return whether the graph can be serialized by Langchain.""""""
    return True

",Is the graph serializable by Langchain according to the is_lc_serializable() class method in the source code?
get_state(config),"
Get the current state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389def get_state(self, config: RunnableConfig) -> StateSnapshot:
    """"""Get the current state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")

    saved = self.checkpointer.get_tuple(config)
    checkpoint = saved.checkpoint if saved else empty_checkpoint()
    config = saved.config if saved else config
    with ChannelsManager(
        {
            k: LastValue(None) if isinstance(c, Context) else c
            for k, c in self.channels.items()
        },
        checkpoint,
        config,
    ) as channels, ManagedValuesManager(
        self.managed_values_dict, ensure_config(config)
    ) as managed:
        next_tasks = prepare_next_tasks(
            checkpoint,
            self.nodes,
            channels,
            managed,
            config,
            -1,
            for_execution=False,
        )
        return StateSnapshot(
            read_channels(channels, self.stream_channels_asis),
            tuple(t.name for t in next_tasks),
            saved.config if saved else config,
            saved.metadata if saved else None,
            saved.checkpoint[""ts""] if saved else None,
            saved.parent_config if saved else None,
        )

",What does the get_state(config) function in the libs/langgraph/langgraph/pregel/__init__.py file do?
aget_state(config)async,"
Get the current state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426async def aget_state(self, config: RunnableConfig) -> StateSnapshot:
    """"""Get the current state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")

    saved = await self.checkpointer.aget_tuple(config)
    checkpoint = saved.checkpoint if saved else empty_checkpoint()

    config = saved.config if saved else config
    async with AsyncChannelsManager(
        {
            k: LastValue(None) if isinstance(c, Context) else c
            for k, c in self.channels.items()
        },
        checkpoint,
        config,
    ) as channels, AsyncManagedValuesManager(
        self.managed_values_dict, ensure_config(config)
    ) as managed:
        next_tasks = prepare_next_tasks(
            checkpoint,
            self.nodes,
            channels,
            managed,
            config,
            -1,
            for_execution=False,
        )
        return StateSnapshot(
            read_channels(channels, self.stream_channels_asis),
            tuple(t.name for t in next_tasks),
            saved.config if saved else config,
            saved.metadata if saved else None,
            saved.checkpoint[""ts""] if saved else None,
            saved.parent_config if saved else None,
        )

",What does the function aget_state(config)async do in the langgraph library?
"get_state_history(config,*,filter=None,before=None,limit=None)","
Get the history of the state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473def get_state_history(
    self,
    config: RunnableConfig,
    *,
    filter: Optional[Dict[str, Any]] = None,
    before: Optional[RunnableConfig] = None,
    limit: Optional[int] = None,
) -> Iterator[StateSnapshot]:
    """"""Get the history of the state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")
    if (
        filter is not None
        and signature(self.checkpointer.list).parameters.get(""filter"") is None
    ):
        raise ValueError(""Checkpointer does not support filtering"")
    for config, checkpoint, metadata, parent_config, _ in self.checkpointer.list(
        config, before=before, limit=limit, filter=filter
    ):
        with ChannelsManager(
            {
                k: LastValue(None) if isinstance(c, Context) else c
                for k, c in self.channels.items()
            },
            checkpoint,
            config,
        ) as channels, ManagedValuesManager(
            self.managed_values_dict, ensure_config(config)
        ) as managed:
            next_tasks = prepare_next_tasks(
                checkpoint,
                self.nodes,
                channels,
                managed,
                config,
                -1,
                for_execution=False,
            )
            yield StateSnapshot(
                read_channels(channels, self.stream_channels_asis),
                tuple(t.name for t in next_tasks),
                config,
                metadata,
                checkpoint[""ts""],
                parent_config,
            )

",What method can be used to retrieve the history of the state of the graph in the specified Python code?
"aget_state_history(config,*,filter=None,before=None,limit=None)async","
Get the history of the state of the graph.

Source code in libs/langgraph/langgraph/pregel/__init__.py
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524async def aget_state_history(
    self,
    config: RunnableConfig,
    *,
    filter: Optional[Dict[str, Any]] = None,
    before: Optional[RunnableConfig] = None,
    limit: Optional[int] = None,
) -> AsyncIterator[StateSnapshot]:
    """"""Get the history of the state of the graph.""""""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")
    if (
        filter is not None
        and signature(self.checkpointer.list).parameters.get(""filter"") is None
    ):
        raise ValueError(""Checkpointer does not support filtering"")
    async for (
        config,
        checkpoint,
        metadata,
        parent_config,
        _,
    ) in self.checkpointer.alist(config, before=before, limit=limit, filter=filter):
        async with AsyncChannelsManager(
            {
                k: LastValue(None) if isinstance(c, Context) else c
                for k, c in self.channels.items()
            },
            checkpoint,
            config,
        ) as channels, AsyncManagedValuesManager(
            self.managed_values_dict, ensure_config(config)
        ) as managed:
            next_tasks = prepare_next_tasks(
                checkpoint,
                self.nodes,
                channels,
                managed,
                config,
                -1,
                for_execution=False,
            )
            yield StateSnapshot(
                read_channels(channels, self.stream_channels_asis),
                tuple(t.name for t in next_tasks),
                config,
                metadata,
                checkpoint[""ts""],
                parent_config,
            )

",What does the aget_state_history function in the langgraph library do?
"update_state(config,values,as_node=None)","
Update the state of the graph with the given values, as if they came from
node as_node. If as_node is not provided, it will be set to the last node
that updated the state, if not ambiguous.

Source code in libs/langgraph/langgraph/pregel/__init__.py
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646def update_state(
    self,
    config: RunnableConfig,
    values: Optional[Union[dict[str, Any], Any]],
    as_node: Optional[str] = None,
) -> RunnableConfig:
    """"""Update the state of the graph with the given values, as if they came from
    node `as_node`. If `as_node` is not provided, it will be set to the last node
    that updated the state, if not ambiguous.
    """"""
    if not self.checkpointer:
        raise ValueError(""No checkpointer set"")

    # get last checkpoint
    saved = self.checkpointer.get_tuple(config)
    checkpoint = copy_checkpoint(saved.checkpoint) if saved else empty_checkpoint()
    checkpoint_previous_versions = (
        saved.checkpoint[""channel_versions""] if saved else {}
    )
    step = saved.metadata.get(""step"", -1) if saved else -1
    # merge configurable fields with previous checkpoint config
    checkpoint_config = {
        **config,
        ""configurable"": {
            **config[""configurable""],
            # TODO: add proper support for updating nested subgraph state
            ""checkpoint_ns"": """",
        },
    }
    if saved:
        checkpoint_config = {
            ""configurable"": {
                **config.get(""configurable"", {}),
                **saved.config[""configurable""],
            }
        }
    # find last node that updated the state, if not provided
    if values is None and as_node is None:
        return self.checkpointer.put(
            checkpoint_config,
            create_checkpoint(checkpoint, None, step),
            {
                ""source"": ""update"",
                ""step"": step,
                ""writes"": {},
            },
            {},
        )
    elif as_node is None and not any(
        v for vv in checkpoint[""versions_seen""].values() for v in vv.values()
    ):
        if (
            isinstance(self.input_channels, str)
            and self.input_channels in self.nodes
        ):
            as_node = self.input_channels
    elif as_node is None:
        last_seen_by_node = sorted(
            (v, n)
            for n, seen in checkpoint[""versions_seen""].items()
            for v in seen.values()
        )
        # if two nodes updated the state at the same time, it's ambiguous
        if last_seen_by_node:
            if len(last_seen_by_node) == 1:
                as_node = last_seen_by_node[0][1]
            elif last_seen_by_node[-1][0] != last_seen_by_node[-2][0]:
                as_node = last_seen_by_node[-1][1]
    if as_node is None:
        raise InvalidUpdateError(""Ambiguous update, specify as_node"")
    if as_node not in self.nodes:
        raise InvalidUpdateError(f""Node {as_node} does not exist"")
    # update channels
    with ChannelsManager(self.channels, checkpoint, config) as channels:
        # create task to run all writers of the chosen node
        writers = self.nodes[as_node].get_writers()
        if not writers:
            raise InvalidUpdateError(f""Node {as_node} has no writers"")
        task = PregelExecutableTask(
            as_node,
            values,
            RunnableSequence(*writers) if len(writers) > 1 else writers[0],
            deque(),
            None,
            [INTERRUPT],
            None,
            str(uuid5(UUID(checkpoint[""id""]), INTERRUPT)),
        )
        # execute task
        task.proc.invoke(
            task.input,
            patch_config(
                config,
                run_name=self.name + ""UpdateState"",
                configurable={
                    # deque.extend is thread-safe
                    CONFIG_KEY_SEND: task.writes.extend,
                    CONFIG_KEY_READ: partial(
                        local_read, checkpoint, channels, task, config
                    ),
                },
            ),
        )
        # apply to checkpoint and save
        apply_writes(
            checkpoint, channels, [task], self.checkpointer.get_next_version
        )

        new_versions = get_new_channel_versions(
            checkpoint_previous_versions, checkpoint[""channel_versions""]
        )
        return self.checkpointer.put(
            checkpoint_config,
            create_checkpoint(checkpoint, channels, step + 1),
            {
                ""source"": ""update"",
                ""step"": step + 1,
                ""writes"": {as_node: values},
            },
            new_versions,
        )

",What happens if the `as_node` parameter is not provided in the `update_state` function?
"stream(input,config=None,*,stream_mode=None,output_keys=None,interrupt_before=None,interrupt_after=None,debug=None)","
Stream graph steps for a single input.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input to the graph.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
The configuration to use for the run.

stream_mode
              (Optional[Union[StreamMode, list[StreamMode]]], default:
                  None
)
          
          
The mode to stream output, defaults to self.stream_mode.
Options are 'values', 'updates', and 'debug'.
values: Emit the current values of the state for each step.
updates: Emit only the updates to the state for each step.
    Output is a dict with the node name as key and the updated values as value.
debug: Emit debug events for each step.

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
The keys to stream, defaults to all non-context channels.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt before, defaults to all nodes in the graph.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt after, defaults to all nodes in the graph.

debug
              (Optional[bool], default:
                  None
)
          
          
Whether to print debug information during execution, defaults to False.

Yields:

Union[dict[str, Any], Any]
          
          
The output of each step in the graph. The output shape depends on the stream_mode.

Examples:
Using different stream modes with a graph:
>>> import operator
>>> from typing_extensions import Annotated, TypedDict
>>> from langgraph.graph import StateGraph
>>> from langgraph.constants import START
...
>>> class State(TypedDict):
...     alist: Annotated[list, operator.add]
...     another_list: Annotated[list, operator.add]
...
>>> builder = StateGraph(State)
>>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
>>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
>>> builder.add_edge(""a"", ""b"")
>>> builder.add_edge(START, ""a"")
>>> graph = builder.compile()

With stream_mode=""values"":
>>> for event in graph.stream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
...     print(event)
{'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
{'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
{'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}

With stream_mode=""updates"":
>>> for event in graph.stream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
...     print(event)
{'a': {'another_list': ['hi']}}
{'b': {'alist': ['there']}}

With stream_mode=""debug"":
>>> for event in graph.stream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
...     print(event)
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}

Source code in libs/langgraph/langgraph/pregel/__init__.py
 814
 815
 816
 817
 818
 819
 820
 821
 822
 823
 824
 825
 826
 827
 828
 829
 830
 831
 832
 833
 834
 835
 836
 837
 838
 839
 840
 841
 842
 843
 844
 845
 846
 847
 848
 849
 850
 851
 852
 853
 854
 855
 856
 857
 858
 859
 860
 861
 862
 863
 864
 865
 866
 867
 868
 869
 870
 871
 872
 873
 874
 875
 876
 877
 878
 879
 880
 881
 882
 883
 884
 885
 886
 887
 888
 889
 890
 891
 892
 893
 894
 895
 896
 897
 898
 899
 900
 901
 902
 903
 904
 905
 906
 907
 908
 909
 910
 911
 912
 913
 914
 915
 916
 917
 918
 919
 920
 921
 922
 923
 924
 925
 926
 927
 928
 929
 930
 931
 932
 933
 934
 935
 936
 937
 938
 939
 940
 941
 942
 943
 944
 945
 946
 947
 948
 949
 950
 951
 952
 953
 954
 955
 956
 957
 958
 959
 960
 961
 962
 963
 964
 965
 966
 967
 968
 969
 970
 971
 972
 973
 974
 975
 976
 977
 978
 979
 980
 981
 982
 983
 984
 985
 986
 987
 988
 989
 990
 991
 992
 993
 994
 995
 996
 997
 998
 999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047def stream(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None,
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
) -> Iterator[Union[dict[str, Any], Any]]:
    """"""Stream graph steps for a single input.

    Args:
        input: The input to the graph.
        config: The configuration to use for the run.
        stream_mode: The mode to stream output, defaults to self.stream_mode.
            Options are 'values', 'updates', and 'debug'.
            values: Emit the current values of the state for each step.
            updates: Emit only the updates to the state for each step.
                Output is a dict with the node name as key and the updated values as value.
            debug: Emit debug events for each step.
        output_keys: The keys to stream, defaults to all non-context channels.
        interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
        interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
        debug: Whether to print debug information during execution, defaults to False.

    Yields:
        The output of each step in the graph. The output shape depends on the stream_mode.

    Examples:
        Using different stream modes with a graph:
        ```pycon
        >>> import operator
        >>> from typing_extensions import Annotated, TypedDict
        >>> from langgraph.graph import StateGraph
        >>> from langgraph.constants import START
        ...
        >>> class State(TypedDict):
        ...     alist: Annotated[list, operator.add]
        ...     another_list: Annotated[list, operator.add]
        ...
        >>> builder = StateGraph(State)
        >>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
        >>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
        >>> builder.add_edge(""a"", ""b"")
        >>> builder.add_edge(START, ""a"")
        >>> graph = builder.compile()
        ```
        With stream_mode=""values"":

        ```pycon
        >>> for event in graph.stream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
        ...     print(event)
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
        {'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}
        ```
        With stream_mode=""updates"":

        ```pycon
        >>> for event in graph.stream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
        ...     print(event)
        {'a': {'another_list': ['hi']}}
        {'b': {'alist': ['there']}}
        ```
        With stream_mode=""debug"":

        ```pycon
        >>> for event in graph.stream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
        ...     print(event)
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}
        ```
    """"""
    config = ensure_config(config)
    callback_manager = get_callback_manager_for_config(config)
    run_manager = callback_manager.on_chain_start(
        dumpd(self),
        input,
        name=config.get(""run_name"", self.get_name()),
        run_id=config.get(""run_id""),
    )
    try:
        if config[""recursion_limit""] < 1:
            raise ValueError(""recursion_limit must be at least 1"")
        if self.checkpointer and not config.get(""configurable""):
            raise ValueError(
                f""Checkpointer requires one or more of the following 'configurable' keys: {[s.id for s in self.checkpointer.config_specs]}""
            )
        # assign defaults
        (
            debug,
            stream_modes,
            output_keys,
            interrupt_before,
            interrupt_after,
            checkpointer,
        ) = self._defaults(
            config,
            stream_mode=stream_mode,
            output_keys=output_keys,
            interrupt_before=interrupt_before,
            interrupt_after=interrupt_after,
            debug=debug,
        )

        with SyncPregelLoop(
            input, config=config, checkpointer=checkpointer, graph=self
        ) as loop:
            # Similarly to Bulk Synchronous Parallel / Pregel model
            # computation proceeds in steps, while there are channel updates
            # channel updates from step N are only visible in step N+1
            # channels are guaranteed to be immutable for the duration of the step,
            # with channel updates applied only at the transition between steps
            while loop.tick(
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                manager=run_manager,
            ):
                # debug flag
                if self.debug:
                    print_step_checkpoint(
                        loop.checkpoint_metadata,
                        loop.channels,
                        self.stream_channels_list,
                    )
                # emit output
                while loop.stream:
                    mode, payload = loop.stream.popleft()
                    if mode in stream_modes:
                        if isinstance(stream_mode, list):
                            yield (mode, payload)
                        else:
                            yield payload
                # debug flag
                if debug:
                    print_step_tasks(loop.step, loop.tasks)

                # execute tasks, and wait for one to fail or all to finish.
                # each task is independent from all other concurrent tasks
                # yield updates/debug output as each task finishes
                futures = {
                    loop.submit(
                        run_with_retry,
                        task,
                        self.retry_policy,
                    ): task
                    for task in loop.tasks
                    if not task.writes
                }
                end_time = (
                    self.step_timeout + time.monotonic()
                    if self.step_timeout
                    else None
                )
                if not futures:
                    done, inflight = set(), set()
                while futures:
                    done, inflight = concurrent.futures.wait(
                        futures,
                        return_when=concurrent.futures.FIRST_COMPLETED,
                        timeout=(
                            max(0, end_time - time.monotonic())
                            if end_time
                            else None
                        ),
                    )
                    if not done:
                        break  # timed out
                    for fut in done:
                        task = futures.pop(fut)
                        if fut.exception() is not None:
                            # we got an exception, break out of while loop
                            # exception will be handled in panic_or_proceed
                            futures.clear()
                        else:
                            # save task writes to checkpointer
                            loop.put_writes(task.id, task.writes)
                            # yield updates output for the finished task
                            if ""updates"" in stream_modes:
                                yield from _with_mode(
                                    ""updates"",
                                    isinstance(stream_mode, list),
                                    map_output_updates(output_keys, [task]),
                                )
                            if ""debug"" in stream_modes:
                                yield from _with_mode(
                                    ""debug"",
                                    isinstance(stream_mode, list),
                                    map_debug_task_results(
                                        loop.step,
                                        [task],
                                        self.stream_channels_list,
                                    ),
                                )
                    else:
                        # remove references to loop vars
                        del fut, task

                # panic on failure or timeout
                _panic_or_proceed(done, inflight, loop.step)
                # don't keep futures around in memory longer than needed
                del done, inflight, futures
                # debug flag
                if debug:
                    print_step_writes(
                        loop.step,
                        [w for t in loop.tasks for w in t.writes],
                        self.stream_channels_list,
                    )
            # emit output
            while loop.stream:
                mode, payload = loop.stream.popleft()
                if mode in stream_modes:
                    if isinstance(stream_mode, list):
                        yield (mode, payload)
                    else:
                        yield payload
            # handle exit
            if loop.status == ""out_of_steps"":
                raise GraphRecursionError(
                    f""Recursion limit of {config['recursion_limit']} reached ""
                    ""without hitting a stop condition. You can increase the ""
                    ""limit by setting the `recursion_limit` config key.""
                )
            # set final channel values as run output
            run_manager.on_chain_end(read_channels(loop.channels, output_keys))
    except BaseException as e:
        run_manager.on_chain_error(e)
        raise

",What are the parameters and functionality of the `stream` method in the `libs/langgraph/langgraph/pregel/__init__.py` source code?
"astream(input,config=None,*,stream_mode=None,output_keys=None,interrupt_before=None,interrupt_after=None,debug=None)async","
Stream graph steps for a single input.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input to the graph.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
The configuration to use for the run.

stream_mode
              (Optional[Union[StreamMode, list[StreamMode]]], default:
                  None
)
          
          
The mode to stream output, defaults to self.stream_mode.
Options are 'values', 'updates', and 'debug'.
values: Emit the current values of the state for each step.
updates: Emit only the updates to the state for each step.
    Output is a dict with the node name as key and the updated values as value.
debug: Emit debug events for each step.

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
The keys to stream, defaults to all non-context channels.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt before, defaults to all nodes in the graph.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Nodes to interrupt after, defaults to all nodes in the graph.

debug
              (Optional[bool], default:
                  None
)
          
          
Whether to print debug information during execution, defaults to False.

Yields:

AsyncIterator[Union[dict[str, Any], Any]]
          
          
The output of each step in the graph. The output shape depends on the stream_mode.

Examples:
Using different stream modes with a graph:
>>> import operator
>>> from typing_extensions import Annotated, TypedDict
>>> from langgraph.graph import StateGraph
>>> from langgraph.constants import START
...
>>> class State(TypedDict):
...     alist: Annotated[list, operator.add]
...     another_list: Annotated[list, operator.add]
...
>>> builder = StateGraph(State)
>>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
>>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
>>> builder.add_edge(""a"", ""b"")
>>> builder.add_edge(START, ""a"")
>>> graph = builder.compile()

With stream_mode=""values"":
>>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
...     print(event)
{'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
{'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
{'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}

With stream_mode=""updates"":
>>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
...     print(event)
{'a': {'another_list': ['hi']}}
{'b': {'alist': ['there']}}

With stream_mode=""debug"":
>>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
...     print(event)
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
{'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
{'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}

Source code in libs/langgraph/langgraph/pregel/__init__.py
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
1298async def astream(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: Optional[Union[StreamMode, list[StreamMode]]] = None,
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
) -> AsyncIterator[Union[dict[str, Any], Any]]:
    """"""Stream graph steps for a single input.

    Args:
        input: The input to the graph.
        config: The configuration to use for the run.
        stream_mode: The mode to stream output, defaults to self.stream_mode.
            Options are 'values', 'updates', and 'debug'.
            values: Emit the current values of the state for each step.
            updates: Emit only the updates to the state for each step.
                Output is a dict with the node name as key and the updated values as value.
            debug: Emit debug events for each step.
        output_keys: The keys to stream, defaults to all non-context channels.
        interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
        interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
        debug: Whether to print debug information during execution, defaults to False.

    Yields:
        The output of each step in the graph. The output shape depends on the stream_mode.

    Examples:
        Using different stream modes with a graph:
        ```pycon
        >>> import operator
        >>> from typing_extensions import Annotated, TypedDict
        >>> from langgraph.graph import StateGraph
        >>> from langgraph.constants import START
        ...
        >>> class State(TypedDict):
        ...     alist: Annotated[list, operator.add]
        ...     another_list: Annotated[list, operator.add]
        ...
        >>> builder = StateGraph(State)
        >>> builder.add_node(""a"", lambda _state: {""another_list"": [""hi""]})
        >>> builder.add_node(""b"", lambda _state: {""alist"": [""there""]})
        >>> builder.add_edge(""a"", ""b"")
        >>> builder.add_edge(START, ""a"")
        >>> graph = builder.compile()
        ```
        With stream_mode=""values"":

        ```pycon
        >>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""values""']}, stream_mode=""values""):
        ...     print(event)
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': []}
        {'alist': ['Ex for stream_mode=""values""'], 'another_list': ['hi']}
        {'alist': ['Ex for stream_mode=""values""', 'there'], 'another_list': ['hi']}
        ```
        With stream_mode=""updates"":

        ```pycon
        >>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""updates""']}, stream_mode=""updates""):
        ...     print(event)
        {'a': {'another_list': ['hi']}}
        {'b': {'alist': ['there']}}
        ```
        With stream_mode=""debug"":

        ```pycon
        >>> async for event in graph.astream({""alist"": ['Ex for stream_mode=""debug""']}, stream_mode=""debug""):
        ...     print(event)
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': []}, 'triggers': ['start:a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 1, 'payload': {'id': '...', 'name': 'a', 'result': [('another_list', ['hi'])]}}
        {'type': 'task', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'input': {'alist': ['Ex for stream_mode=""debug""'], 'another_list': ['hi']}, 'triggers': ['a']}}
        {'type': 'task_result', 'timestamp': '2024-06-23T...+00:00', 'step': 2, 'payload': {'id': '...', 'name': 'b', 'result': [('alist', ['there'])]}}
        ```
    """"""
    config = ensure_config(config)
    callback_manager = get_async_callback_manager_for_config(config)
    run_manager = await callback_manager.on_chain_start(
        dumpd(self),
        input,
        name=config.get(""run_name"", self.get_name()),
        run_id=config.get(""run_id""),
    )
    # if running from astream_log() run each proc with streaming
    do_stream = next(
        (
            h
            for h in run_manager.handlers
            if isinstance(h, _StreamingCallbackHandler)
        ),
        None,
    )
    try:
        if config[""recursion_limit""] < 1:
            raise ValueError(""recursion_limit must be at least 1"")
        if self.checkpointer and not config.get(""configurable""):
            raise ValueError(
                f""Checkpointer requires one or more of the following 'configurable' keys: {[s.id for s in self.checkpointer.config_specs]}""
            )
        # assign defaults
        (
            debug,
            stream_modes,
            output_keys,
            interrupt_before,
            interrupt_after,
            checkpointer,
        ) = self._defaults(
            config,
            stream_mode=stream_mode,
            output_keys=output_keys,
            interrupt_before=interrupt_before,
            interrupt_after=interrupt_after,
            debug=debug,
        )
        async with AsyncPregelLoop(
            input, config=config, checkpointer=checkpointer, graph=self
        ) as loop:
            aioloop = asyncio.get_event_loop()
            # Similarly to Bulk Synchronous Parallel / Pregel model
            # computation proceeds in steps, while there are channel updates
            # channel updates from step N are only visible in step N+1
            # channels are guaranteed to be immutable for the duration of the step,
            # with channel updates applied only at the transition between steps
            while loop.tick(
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                manager=run_manager,
            ):
                # debug flag
                if self.debug:
                    print_step_checkpoint(
                        loop.checkpoint_metadata,
                        loop.channels,
                        self.stream_channels_list,
                    )
                # emit output
                while loop.stream:
                    mode, payload = loop.stream.popleft()
                    if mode in stream_modes:
                        if isinstance(stream_mode, list):
                            yield (mode, payload)
                        else:
                            yield payload
                # debug flag
                if debug:
                    print_step_tasks(loop.step, loop.tasks)

                # execute tasks, and wait for one to fail or all to finish.
                # each task is independent from all other concurrent tasks
                # yield updates/debug output as each task finishes
                futures = {
                    loop.submit(
                        arun_with_retry,
                        task,
                        self.retry_policy,
                        stream=do_stream,
                        __name__=task.name,
                        __cancel_on_exit__=True,
                    ): task
                    for task in loop.tasks
                    if not task.writes
                }
                end_time = (
                    self.step_timeout + aioloop.time()
                    if self.step_timeout
                    else None
                )
                if not futures:
                    done, inflight = set(), set()
                while futures:
                    done, inflight = await asyncio.wait(
                        futures,
                        return_when=asyncio.FIRST_COMPLETED,
                        timeout=(
                            max(0, end_time - aioloop.time()) if end_time else None
                        ),
                    )
                    if not done:
                        break  # timed out
                    for fut in done:
                        task = futures.pop(fut)
                        if fut.exception() is not None:
                            # we got an exception, break out of while loop
                            # exception will be handled in panic_or_proceed
                            futures.clear()
                        else:
                            # save task writes to checkpointer
                            loop.put_writes(task.id, task.writes)
                            # yield updates output for the finished task
                            if ""updates"" in stream_modes:
                                for chunk in _with_mode(
                                    ""updates"",
                                    isinstance(stream_mode, list),
                                    map_output_updates(output_keys, [task]),
                                ):
                                    yield chunk
                            if ""debug"" in stream_modes:
                                for chunk in _with_mode(
                                    ""debug"",
                                    isinstance(stream_mode, list),
                                    map_debug_task_results(
                                        loop.step,
                                        [task],
                                        self.stream_channels_list,
                                    ),
                                ):
                                    yield chunk
                    else:
                        # remove references to loop vars
                        del fut, task

                # panic on failure or timeout
                _panic_or_proceed(done, inflight, loop.step, asyncio.TimeoutError)
                # don't keep futures around in memory longer than needed
                del done, inflight, futures
                # debug flag
                if debug:
                    print_step_writes(
                        loop.step,
                        [w for t in loop.tasks for w in t.writes],
                        self.stream_channels_list,
                    )
            # emit output
            while loop.stream:
                mode, payload = loop.stream.popleft()
                if mode in stream_modes:
                    if isinstance(stream_mode, list):
                        yield (mode, payload)
                    else:
                        yield payload
            # handle exit
            if loop.status == ""out_of_steps"":
                raise GraphRecursionError(
                    f""Recursion limit of {config['recursion_limit']} reached ""
                    ""without hitting a stop condition. You can increase the ""
                    ""limit by setting the `recursion_limit` config key.""
                )

            # set final channel values as run output
            await run_manager.on_chain_end(
                read_channels(loop.channels, output_keys)
            )
    except BaseException as e:
        # TODO use on_chain_end if exc is GraphInterrupt
        await asyncio.shield(run_manager.on_chain_error(e))
        raise

",What are the parameters and functionality of the `astream` method in the `langgraph` library?
"invoke(input,config=None,*,stream_mode='values',output_keys=None,interrupt_before=None,interrupt_after=None,debug=None,**kwargs)","
Run the graph with a single input and config.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input data for the graph. It can be a dictionary or any other type.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
Optional. The configuration for the graph run.

stream_mode
              (StreamMode, default:
                  'values'
)
          
          
Optional[str]. The stream mode for the graph run. Default is ""values"".

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
Optional. The output keys to retrieve from the graph run.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt the graph run before.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt the graph run after.

debug
              (Optional[bool], default:
                  None
)
          
          
Optional. Enable debug mode for the graph run.

**kwargs
              (Any, default:
                  {}
)
          
          
Additional keyword arguments to pass to the graph run.

Returns:

Union[dict[str, Any], Any]
          
          
The output of the graph run. If stream_mode is ""values"", it returns the latest output.

Union[dict[str, Any], Any]
          
          
If stream_mode is not ""values"", it returns a list of output chunks.

Source code in libs/langgraph/langgraph/pregel/__init__.py
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
1328
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
1343
1344
1345
1346
1347
1348
1349
1350def invoke(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: StreamMode = ""values"",
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
    **kwargs: Any,
) -> Union[dict[str, Any], Any]:
    """"""Run the graph with a single input and config.

    Args:
        input: The input data for the graph. It can be a dictionary or any other type.
        config: Optional. The configuration for the graph run.
        stream_mode: Optional[str]. The stream mode for the graph run. Default is ""values"".
        output_keys: Optional. The output keys to retrieve from the graph run.
        interrupt_before: Optional. The nodes to interrupt the graph run before.
        interrupt_after: Optional. The nodes to interrupt the graph run after.
        debug: Optional. Enable debug mode for the graph run.
        **kwargs: Additional keyword arguments to pass to the graph run.

    Returns:
        The output of the graph run. If stream_mode is ""values"", it returns the latest output.
        If stream_mode is not ""values"", it returns a list of output chunks.
    """"""
    output_keys = output_keys if output_keys is not None else self.output_channels
    if stream_mode == ""values"":
        latest: Union[dict[str, Any], Any] = None
    else:
        chunks = []
    for chunk in self.stream(
        input,
        config,
        stream_mode=stream_mode,
        output_keys=output_keys,
        interrupt_before=interrupt_before,
        interrupt_after=interrupt_after,
        debug=debug,
        **kwargs,
    ):
        if stream_mode == ""values"":
            latest = chunk
        else:
            chunks.append(chunk)
    if stream_mode == ""values"":
        return latest
    else:
        return chunks

",What is the purpose of the invoke function and what parameters does it accept?
"ainvoke(input,config=None,*,stream_mode='values',output_keys=None,interrupt_before=None,interrupt_after=None,debug=None,**kwargs)async","
Asynchronously invoke the graph on a single input.
Parameters:

input
              (Union[dict[str, Any], Any])
          
          
The input data for the computation. It can be a dictionary or any other type.

config
              (Optional[RunnableConfig], default:
                  None
)
          
          
Optional. The configuration for the computation.

stream_mode
              (StreamMode, default:
                  'values'
)
          
          
Optional. The stream mode for the computation. Default is ""values"".

output_keys
              (Optional[Union[str, Sequence[str]]], default:
                  None
)
          
          
Optional. The output keys to include in the result. Default is None.

interrupt_before
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt before. Default is None.

interrupt_after
              (Optional[Union[All, Sequence[str]]], default:
                  None
)
          
          
Optional. The nodes to interrupt after. Default is None.

debug
              (Optional[bool], default:
                  None
)
          
          
Optional. Whether to enable debug mode. Default is None.

**kwargs
              (Any, default:
                  {}
)
          
          
Additional keyword arguments.

Returns:

Union[dict[str, Any], Any]
          
          
The result of the computation. If stream_mode is ""values"", it returns the latest value.

Union[dict[str, Any], Any]
          
          
If stream_mode is ""chunks"", it returns a list of chunks.

Source code in libs/langgraph/langgraph/pregel/__init__.py
1352
1353
1354
1355
1356
1357
1358
1359
1360
1361
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403async def ainvoke(
    self,
    input: Union[dict[str, Any], Any],
    config: Optional[RunnableConfig] = None,
    *,
    stream_mode: StreamMode = ""values"",
    output_keys: Optional[Union[str, Sequence[str]]] = None,
    interrupt_before: Optional[Union[All, Sequence[str]]] = None,
    interrupt_after: Optional[Union[All, Sequence[str]]] = None,
    debug: Optional[bool] = None,
    **kwargs: Any,
) -> Union[dict[str, Any], Any]:
    """"""Asynchronously invoke the graph on a single input.

    Args:
        input: The input data for the computation. It can be a dictionary or any other type.
        config: Optional. The configuration for the computation.
        stream_mode: Optional. The stream mode for the computation. Default is ""values"".
        output_keys: Optional. The output keys to include in the result. Default is None.
        interrupt_before: Optional. The nodes to interrupt before. Default is None.
        interrupt_after: Optional. The nodes to interrupt after. Default is None.
        debug: Optional. Whether to enable debug mode. Default is None.
        **kwargs: Additional keyword arguments.

    Returns:
        The result of the computation. If stream_mode is ""values"", it returns the latest value.
        If stream_mode is ""chunks"", it returns a list of chunks.
    """"""

    output_keys = output_keys if output_keys is not None else self.output_channels
    if stream_mode == ""values"":
        latest: Union[dict[str, Any], Any] = None
    else:
        chunks = []
    async for chunk in self.astream(
        input,
        config,
        stream_mode=stream_mode,
        output_keys=output_keys,
        interrupt_before=interrupt_before,
        interrupt_after=interrupt_after,
        debug=debug,
        **kwargs,
    ):
        if stream_mode == ""values"":
            latest = chunk
        else:
            chunks.append(chunk)
    if stream_mode == ""values"":
        return latest
    else:
        return chunks

",What is the purpose of the `ainvoke` function and what parameters does it accept?
"get_graph(config=None,*,xray=False)","
Returns a drawable representation of the computation graph.

Source code in libs/langgraph/langgraph/graph/graph.py
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548def get_graph(
    self,
    config: Optional[RunnableConfig] = None,
    *,
    xray: Union[int, bool] = False,
) -> DrawableGraph:
    """"""Returns a drawable representation of the computation graph.""""""
    graph = DrawableGraph()
    start_nodes: dict[str, DrawableNode] = {
        START: graph.add_node(self.get_input_schema(config), START)
    }
    end_nodes: dict[str, DrawableNode] = {}

    def add_edge(
        start: str, end: str, label: Optional[str] = None, conditional: bool = False
    ) -> None:
        if end == END and END not in end_nodes:
            end_nodes[END] = graph.add_node(self.get_output_schema(config), END)
        return graph.add_edge(
            start_nodes[start], end_nodes[end], label, conditional
        )

    for key, n in self.builder.nodes.items():
        node = n.runnable
        metadata = n.metadata or {}
        if key in self.interrupt_before_nodes:
            metadata[""__interrupt""] = ""before""
        elif key in self.interrupt_after_nodes:
            metadata[""__interrupt""] = ""after""
        if xray:
            subgraph = (
                node.get_graph(
                    config=config,
                    xray=xray - 1 if isinstance(xray, int) and xray > 0 else xray,
                )
                if isinstance(node, CompiledGraph)
                else node.get_graph(config=config)
            )
            subgraph.trim_first_node()
            subgraph.trim_last_node()
            if len(subgraph.nodes) > 1:
                end_nodes[key], start_nodes[key] = graph.extend(
                    subgraph, prefix=key
                )
            else:
                n = graph.add_node(node, key, metadata=metadata or None)
                start_nodes[key] = n
                end_nodes[key] = n
        else:
            n = graph.add_node(node, key, metadata=metadata or None)
            start_nodes[key] = n
            end_nodes[key] = n
    for start, end in sorted(self.builder._all_edges):
        add_edge(start, end)
    for start, branches in self.builder.branches.items():
        default_ends = {
            **{k: k for k in self.builder.nodes if k != start},
            END: END,
        }
        for _, branch in branches.items():
            if branch.ends is not None:
                ends = branch.ends
            elif branch.then is not None:
                ends = {k: k for k in default_ends if k not in (END, branch.then)}
            else:
                ends = default_ends
            for label, end in ends.items():
                add_edge(
                    start,
                    end,
                    label if label != end else None,
                    conditional=True,
                )
                if branch.then is not None:
                    add_edge(end, branch.then)

    return graph

",What does the get_graph function return?
StreamMode,"

How the stream method should emit outputs.

'values': Emit all values of the state for each step.
'updates': Emit only the node name(s) and updates
    that were returned by the node(s) after each step.
'debug': Emit debug events for each step.

",What are the different options for how the StreamMode method emits outputs?
Constants,The following constants and classes are used to help control graph execution.,"What is the purpose of the constants and classes mentioned in the ""Constants"" heading?"
START,"START is a string constant (""__start__"") that serves as a ""virtual"" node in the graph.
Adding an edge (or conditional edges) from START to node one or more nodes in your graph
will direct the graph to begin execution there. from langgraph.graph import START
...
builder.add_edge(START, ""my_node"")
# Or to add a conditional starting point
builder.add_conditional_edges(START, my_condition)
",How can you direct the graph to begin execution at a specific node using the START constant in the langgraph library?
END,"END is a string constant (""__end__"") that serves as a ""virtual"" node in the graph. Adding
an edge (or conditional edges) from one or more nodes in your graph to the END ""node"" will
direct the graph to cease execution as soon as it reaches this point. from langgraph.graph import END
...
builder.add_edge(""my_node"", END) # Stop any time my_node completes
# Or to conditionally terminate
def my_condition(state):
    if state[""should_stop""]:
        return END
    return ""my_node""
builder.add_conditional_edges(""my_node"", my_condition)
",How can the END string constant be used in a graph to stop execution at a specific point?
Send,"

A message or packet to send to a specific node in the graph.
The Send class is used within a StateGraph's conditional edges to
dynamically invoke a node with a custom state at the next step.
Importantly, the sent state can differ from the core graph's state,
allowing for flexible and dynamic workflow management.
One such example is a ""map-reduce"" workflow where your graph invokes
the same node multiple times in parallel with different states,
before aggregating the results back into the main graph's state.
Attributes:

node
              (str)
          
          
The name of the target node to send the message to.

arg
              (Any)
          
          
The state or message to send to the target node.

Examples:
>>> from typing import Annotated
>>> import operator
>>> class OverallState(TypedDict):
...     subjects: list[str]
...     jokes: Annotated[list[str], operator.add]
...
>>> from langgraph.constants import Send
>>> from langgraph.graph import END, START
>>> def continue_to_jokes(state: OverallState):
...     return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]
...
>>> from langgraph.graph import StateGraph
>>> builder = StateGraph(OverallState)
>>> builder.add_node(""generate_joke"", lambda state: {""jokes"": [f""Joke about {state['subject']}""]})
>>> builder.add_conditional_edges(START, continue_to_jokes)
>>> builder.add_edge(""generate_joke"", END)
>>> graph = builder.compile()
>>>
>>> # Invoking with two subjects results in a generated joke for each
>>> graph.invoke({""subjects"": [""cats"", ""dogs""]})
{'subjects': ['cats', 'dogs'], 'jokes': ['Joke about cats', 'Joke about dogs']}

Source code in libs/langgraph/langgraph/constants.py
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93class Send:
    """"""A message or packet to send to a specific node in the graph.

    The `Send` class is used within a `StateGraph`'s conditional edges to
    dynamically invoke a node with a custom state at the next step.

    Importantly, the sent state can differ from the core graph's state,
    allowing for flexible and dynamic workflow management.

    One such example is a ""map-reduce"" workflow where your graph invokes
    the same node multiple times in parallel with different states,
    before aggregating the results back into the main graph's state.

    Attributes:
        node (str): The name of the target node to send the message to.
        arg (Any): The state or message to send to the target node.

    Examples:
        >>> from typing import Annotated
        >>> import operator
        >>> class OverallState(TypedDict):
        ...     subjects: list[str]
        ...     jokes: Annotated[list[str], operator.add]
        ...
        >>> from langgraph.constants import Send
        >>> from langgraph.graph import END, START
        >>> def continue_to_jokes(state: OverallState):
        ...     return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]
        ...
        >>> from langgraph.graph import StateGraph
        >>> builder = StateGraph(OverallState)
        >>> builder.add_node(""generate_joke"", lambda state: {""jokes"": [f""Joke about {state['subject']}""]})
        >>> builder.add_conditional_edges(START, continue_to_jokes)
        >>> builder.add_edge(""generate_joke"", END)
        >>> graph = builder.compile()
        >>>
        >>> # Invoking with two subjects results in a generated joke for each
        >>> graph.invoke({""subjects"": [""cats"", ""dogs""]})
        {'subjects': ['cats', 'dogs'], 'jokes': ['Joke about cats', 'Joke about dogs']}
    """"""

    node: str
    arg: Any

    def __init__(self, /, node: str, arg: Any) -> None:
        """"""
        Initialize a new instance of the Send class.

        Args:
            node (str): The name of the target node to send the message to.
            arg (Any): The state or message to send to the target node.
        """"""
        self.node = node
        self.arg = arg

    def __hash__(self) -> int:
        return hash((self.node, self.arg))

    def __repr__(self) -> str:
        return f""Send(node={self.node!r}, arg={self.arg!r})""

    def __eq__(self, value: object) -> bool:
        return (
            isinstance(value, Send)
            and self.node == value.node
            and self.arg == value.arg
        )

__init__(node, arg)

Initialize a new instance of the Send class.
Parameters:

node
              (str)
          
          
The name of the target node to send the message to.

arg
              (Any)
          
          
The state or message to send to the target node.

Source code in libs/langgraph/langgraph/constants.py
71
72
73
74
75
76
77
78
79
80def __init__(self, /, node: str, arg: Any) -> None:
    """"""
    Initialize a new instance of the Send class.

    Args:
        node (str): The name of the target node to send the message to.
        arg (Any): The state or message to send to the target node.
    """"""
    self.node = node
    self.arg = arg

",What parameters are required to initialize a new instance of the Send class?
"__init__(node,arg)","
Initialize a new instance of the Send class.
Parameters:

node
              (str)
          
          
The name of the target node to send the message to.

arg
              (Any)
          
          
The state or message to send to the target node.

Source code in libs/langgraph/langgraph/constants.py
71
72
73
74
75
76
77
78
79
80def __init__(self, /, node: str, arg: Any) -> None:
    """"""
    Initialize a new instance of the Send class.

    Args:
        node (str): The name of the target node to send the message to.
        arg (Any): The state or message to send to the target node.
    """"""
    self.node = node
    self.arg = arg

",What parameters are required to initialize a new instance of the Send class?
RetryPolicy,"

              Bases: NamedTuple
Configuration for retrying nodes.

Source code in libs/langgraph/langgraph/pregel/types.py
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56class RetryPolicy(NamedTuple):
    """"""Configuration for retrying nodes.""""""

    initial_interval: float = 0.5
    """"""Amount of time that must elapse before the first retry occurs. In seconds.""""""
    backoff_factor: float = 2.0
    """"""Multiplier by which the interval increases after each retry.""""""
    max_interval: float = 128.0
    """"""Maximum amount of time that may elapse between retries. In seconds.""""""
    max_attempts: int = 3
    """"""Maximum number of attempts to make before giving up, including the first.""""""
    jitter: bool = True
    """"""Whether to add random jitter to the interval between retries.""""""
    retry_on: Union[
        Type[Exception], tuple[Type[Exception], ...], Callable[[Exception], bool]
    ] = default_retry_on
    """"""List of exception classes that should trigger a retry, or a callable that returns True for exceptions that should trigger a retry.""""""

initial_interval: float = 0.5

class-attribute
instance-attribute

Amount of time that must elapse before the first retry occurs. In seconds.

backoff_factor: float = 2.0

class-attribute
instance-attribute

Multiplier by which the interval increases after each retry.

max_interval: float = 128.0

class-attribute
instance-attribute

Maximum amount of time that may elapse between retries. In seconds.

max_attempts: int = 3

class-attribute
instance-attribute

Maximum number of attempts to make before giving up, including the first.

jitter: bool = True

class-attribute
instance-attribute

Whether to add random jitter to the interval between retries.

retry_on: Union[Type[Exception], tuple[Type[Exception], ...], Callable[[Exception], bool]] = default_retry_on

class-attribute
instance-attribute

List of exception classes that should trigger a retry, or a callable that returns True for exceptions that should trigger a retry.

",What is the purpose of the RetryPolicy configuration in the langgraph library?
initial_interval:float=0.5class-attributeinstance-attribute,"
Amount of time that must elapse before the first retry occurs. In seconds.
",What is the amount of time in seconds that must elapse before the first retry occurs?
backoff_factor:float=2.0class-attributeinstance-attribute,"
Multiplier by which the interval increases after each retry.
",What is the purpose of the backoff_factor in the retry interval?
max_interval:float=128.0class-attributeinstance-attribute,"
Maximum amount of time that may elapse between retries. In seconds.
","What is the maximum amount of time, in seconds, that may elapse between retries?"
max_attempts:int=3class-attributeinstance-attribute,"
Maximum number of attempts to make before giving up, including the first.
","What is the maximum number of attempts allowed before giving up, including the first attempt?"
jitter:bool=Trueclass-attributeinstance-attribute,"
Whether to add random jitter to the interval between retries.
","What does the ""jitter"" parameter control in this context?"
"retry_on:Union[Type[Exception],tuple[Type[Exception],...],Callable[[Exception],bool]]=default_retry_onclass-attributeinstance-attribute","
List of exception classes that should trigger a retry, or a callable that returns True for exceptions that should trigger a retry.
",What is the purpose of the retry_on attribute in the class?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Reflection,"In the context of LLM agent building, reflection refers to the process of prompting an LLM to observe its past steps (along with potential observations from tools/the environment) to assess the quality of the chosen actions.
This is then used downstream for things like re-planning, search, or evaluation. This notebook demonstrates a very simple form of reflection in LangGraph.",What does reflection refer to in the context of LLM agent building?
Generate,"For our example, we will create a ""5 paragraph essay"" generator. First, create the generator:",What type of generator will be created for the example provided?
Repeat,"And... that's all there is too it! You can repeat in a loop for a fixed number of steps, or use an LLM (or other check) to decide when the finished product is good enough.",How can you utilize repetition in a loop or with an LLM to achieve a desired outcome?
Define graph,"Now that we've shown each step in isolation, we can wire it up in a graph.",What can be done after showing each step in isolation?
Conclusion,"Now that you've applied reflection to an LLM agent, I'll note one thing: self-reflection is inherently cyclic: it is much more effective if the reflection step has additional context or feedback (from tool observations, checks, etc.). If, like in the scenario above, the reflection step simply prompts the LLM to reflect on its output, it can still benefit the output quality (since the LLM then has multiple ""shots"" at getting a good output), but it's less guaranteed.",How can self-reflection be made more effective for an LLM agent according to the provided information?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Hierarchical Agent Teams,"In our previous example (Agent Supervisor), we introduced the concept of a single supervisor node to route work between different worker nodes. But what if the job for a single worker becomes too complex? What if the number of workers becomes too large? For some applications, the system may be more effective if work is distributed hierarchically. You can do this by composing different subgraphs and creating a top-level supervisor, along with mid-level supervisors. To do this, let's build a simple research assistant! The graph will look something like the following: This notebook is inspired by the paper AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, by Wu, et. al. In the rest of this notebook, you will: 
Define the agents' tools to access the web and write files
Define some utilities to help create the graph and agents
Create and define each team (web research + doc writing)
Compose everything together.
 But before all of that, some setup:",How can work be distributed hierarchically in a system with hierarchical agent teams?
Create Tools,"Each team will be composed of one or more agents each with one or more tools. Below, define all the tools to be used by your different teams. We'll start with the research team. ResearchTeam tools The research team can use a search engine and url scraper to find information on the web. Feel free to add additional functionality below to boost the team performance!",What tools can the research team use to find information on the web?
Helper Utilities,"We are going to create a few utility functions to make it more concise when we want to: 
Create a worker agent.
Create a supervisor for the sub-graph.
 These will simplify the graph compositional code at the end for us so it's easier to see what's going on.","What functions are being created under the heading ""Helper Utilities"" to simplify the graph compositional code?"
Define Agent Teams,"Now we can get to define our hierarchical teams. ""Choose your player!""",What is the purpose of defining Agent Teams?
Research Team,"The research team will have a search agent and a web scraping ""research_agent"" as the two worker nodes. Let's create those, as well as the team supervisor.",What roles will be included in the research team?
Document Writing Team,"Create the document writing team below using a similar approach. This time, we will give each agent access to different file-writing tools. Note that we are giving file-system access to our agent here, which is not safe in all cases.","What approach should be used to create the document writing team, with each agent having access to different file-writing tools?"
Add Layers,"In this design, we are enforcing a top-down planning policy. We've created two graphs already, but we have to decide how to route work between the two. We'll create a third graph to orchestrate the previous two, and add some connectors to define how this top-level state is shared between the different graphs.",What is the purpose of creating a third graph and adding connectors in the design process described?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to add summary of the conversation history,"One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. One way to work around that is to create a summary of the conversation to date, and use that with the past N messages. This guide will go through an example of how to do that. This will involve a few steps: 
Check if the conversation is too long (can be done by checking number of messages or length of messages)
If yes, the create summary (will need a prompt for this)
Then remove all except the last N messages
 A big part of this is deleting old messages. For an in depth guide on how to do that, see this guide",How can you add a summary of the conversation history to reduce the length of the context window?
Setup,"First, let's set up the packages we're going to want to use",What is the first step in the process described in the text?
Build the chatbot,Let's now build the chatbot.,What are we going to build now?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling a graph and why is it necessary before using it?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state and how can they be stored and updated effectively?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Tutorials,Welcome to the LangGraph Tutorials! These notebooks introduce LangGraph through building various language agents and applications.,What can be learned from the LangGraph Tutorials?
Quick Start,"Learn the basics of LangGraph through a comprehensive quick start in which you will build an agent from scratch. 
Quick Start
",What will you learn through the comprehensive quick start for LangGraph?
Use cases,"Learn from example implementations of graphs designed for specific scenarios and that implement common design patterns. Chatbots 
Customer Support: Build a customer support chatbot to manage flights, hotel reservations, car rentals, and other tasks
Prompt Generation from User Requirements: Build an information gathering chatbot
Code Assistant: Build a code analysis and generation assistant
 Multi-Agent Systems 
Collaboration: Enable two agents to collaborate on a task
Supervision: Use an LLM to orchestrate and delegate to individual agents
Hierarchical Teams: Orchestrate nested teams of agents to solve problems
 RAG 
Adaptive RAG
Adaptive RAG using local LLMs

Agentic RAG
Corrective RAG
Corrective RAG using local LLMs

Self-RAG
Self-RAG using local LLMs

SQL Agent
 Planning Agents 
Plan-and-Execute: Implement a basic planning and execution agent
Reasoning without Observation: Reduce re-planning by saving observations as variables
LLMCompiler: Stream and eagerly execute a DAG of tasks from a planner
 Reflection & Critique 
Basic Reflection: Prompt the agent to reflect on and revise its outputs
Reflexion: Critique missing and superfluous details to guide next steps
Language Agent Tree Search: Use reflection and rewards to drive a tree search over agents
Self-Discover Agent: Analyze an agent that learns about its own capabilities
 Evaluation 
Agent-based: Evaluate chatbots via simulated user interactions
In LangSmith: Evaluate chatbots in LangSmith over a dialog dataset
 Experimental 
Web Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA
TNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.
Web Navigation: Build an agent that can navigate and interact with websites
Competitive Programming: Build an agent with few-shot ""episodic memory"" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the ""Can Language Models Solve Olympiad Programming?"" paper by Shi, Tang, Narasimhan, and Yao.
Complex data extraction: Build an agent that can use function calling to do complex extraction tasks
",What are some example implementations of graphs designed for specific scenarios and that implement common design patterns?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Basic Multi-agent Collaboration,"A single agent can usually operate effectively using a handful of tools within a single domain, but even using powerful models like gpt-4, it can be less effective at using many tools. One way to approach complicated tasks is through a ""divide-and-conquer"" approach: create an specialized agent for each task or domain and route tasks to the correct ""expert"". This notebook (inspired by the paper AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, by Wu, et. al.) shows one way to do this using LangGraph. The resulting graph will look something like the following diagram: Before we get started, a quick note: this and other multi-agent notebooks are designed to show how you can implement certain design patterns in LangGraph. If the pattern suits your needs, we recommend combining it with some of the other fundamental patterns described elsewhere in the docs for best performance.","How can complicated tasks be approached using a ""divide-and-conquer"" approach in multi-agent collaboration?"
Create Agents,The following helper functions will help create agents. These agents will then be nodes in the graph. You can skip ahead if you just want to see what the graph looks like.,What will the helper functions mentioned in the text help create?
Define tools,We will also define some tools that our agents will use in the future,What will be defined for the agents to use in the future?
Create graph,"Now that we've defined our tools and made some helper functions, will create the individual agents below and tell them how to talk to each other using LangGraph.",What will be created using LangGraph after defining tools and making helper functions?
Define State,"We first define the state of the graph. This will just a list of messages, along with a key to track the most recent sender",What is the definition of state in the context of the graph mentioned in the text?
Define Agent Nodes,"We now need to define the nodes. First, let's define the nodes for the agents.",What do we need to define for the agents in terms of nodes?
Define Tool Node,We now define a node to run the tools,What is the definition of a Tool Node?
Define Edge Logic,We can define some of the edge logic that is needed to decide what to do based on results of the agents,What is edge logic used for in decision-making based on agent results?
Define the Graph,We can now put it all together and define the graph!,What can we define by putting it all together?
Invoke,"With the graph created, you can invoke it! Let's have it chart some stats for us.",How can you utilize the graph that has been created?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Prompt Generation from User Requirements,"In this example we will create a chat bot that helps a user generate a prompt.
It will first collect requirements from the user, and then will generate the prompt (and refine it based on user input).
These are split into two separate states, and the LLM decides when to transition between them. A graphical representation of the system can be found below.",What is the purpose of the chat bot described in the text?
Gather information,"First, let's define the part of the graph that will gather user requirements. This will be an LLM call with a specific system message. It will have access to a tool that it can call when it is ready to generate the prompt.",What tool will the part of the graph that gathers user requirements have access to when it is ready to generate the prompt?
Generate Prompt,"We now set up the state that will generate the prompt.
This will require a separate system message, as well as a function to filter out all message PRIOR to the tool invocation (as that is when the previous state decided it was time to generate the prompt",What steps are required to set up the state that will generate the prompt?
Define the state logic,"This is the logic for what state the chatbot is in.
If the last message is a tool call, then we are in the state where the ""prompt creator"" (prompt) should respond.
Otherwise, if the last message is not a HumanMessage, then we know the human should respond next and so we are in the END state.
If the last message is a HumanMessage, then if there was a tool call previously we are in the prompt state.
Otherwise, we are in the ""info gathering"" (info) state.",What determines the state logic of the chatbot?
Create the graph,"We can now the create the graph.
We will use a SqliteSaver to persist conversation history.",What will be used to persist conversation history when creating the graph?
Use the graph,We can now use the created chatbot.,What can we do with the created chatbot?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,"What is the main graph class used in the StateGraph, and what is it parameterized by?"
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state and how can they be stored and updated effectively?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you go from node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges or terminate using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What method can be used to retrieve the state history of a graph in Python?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What are some built-in ways to visualize graphs in LangGraph?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation using the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate within this system?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method used to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct paper and the ReAct agent implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Agentic RAG,"Retrieval Agents are useful when we want to make decisions about whether to retrieve from an index. To implement a retrieval agent, we simple need to give an LLM access to a retriever tool. We can incorporate this into LangGraph.",What is the process for implementing a retrieval agent in Agentic RAG?
Retriever,"First, we index 3 blog posts.",What is the first task we do when working with retriever?
Agent state,We will defined a graph. A state object that it passes around to each node. Our state will be a list of messages. Each node in our graph will append to it.,What will the state object in the graph contain and how will it be used by each node?
Nodes and Edges,"We can lay out an agentic RAG graph like this: 
The state is a set of messages
Each node will update (append to) state
Conditional edges decide which node to visit next
",What determines which node to visit next in an agentic RAG graph layout?
Graph,"
Start with an agent, call_model
Agent make a decision to call a function
If so, then action to call tool (retriever)
Then call agent with the tool output added to messages (state)
","What action does the agent, call_model, take if it decides to call a function?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Corrective RAG (CRAG) using local LLMs,"Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents. The paper follows this general flow: 
If at least one document exceeds the threshold for relevance, then it proceeds to generation
If all documents fall below the relevance threshold or if the grader is unsure, then it uses web search to supplement retrieval
Before generation, it performs knowledge refinement of the search or retrieved documents
This partitions the document into knowledge strips
It grades each strip, and filters out irrelevant ones
 We will implement some of these ideas from scratch using LangGraph: 
If any documents are irrelevant, we'll supplement retrieval with web search.
We'll skip the knowledge refinement, but this can be added back as a node if desired.
We'll use Tavily Search for web search.
",What strategy is used for Corrective RAG (CRAG) and how is it implemented using LangGraph?
Environment,"We'll use Ollama to access a local LLM: 
Download Ollama app.
Pull your model of choice, e.g.: ollama pull llama3
 We'll use Tavily for web search. We'll use a vectorstore with Nomic local embeddings or, optionally, OpenAI embeddings. We'll use LangSmith for tracing and evaluation.","What tools will be used for accessing a local LLM, web search, embeddings, tracing, and evaluation in the environment described?"
LLM,You can select from Ollama LLMs.,What options are available for selection within the LLM category?
Index,Let's index 3 blog posts.,What is the focus of the index in this context?
Graph,"Here we'll explicitly define the majority of the control flow, only using an LLM to define a single branch point following grading.",What will be explicitly defined using an LLM in the graph after grading?
Evaluation,Now we've defined two different agent architectures that do roughly the same thing! We can evaluate them. See our conceptual guide for context on agent evaluation.,What can be done to evaluate two different agent architectures that perform similar tasks?
Response,"First, we can assess how well our agent performs on a set of question-answer pairs. We'll create a dataset and save it in LangSmith.",What method will be used to assess the performance of the agent on question-answer pairs?
Trajectory,"Second, we can assess the list of tool calls that each agent makes relative to expected trajectories. This evaluates the specific reasoning traces taken by our agents!",What does assessing the list of tool calls made by each agent relative to expected trajectories evaluate?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to create subgraphs,"For more complex systems, sub-graphs are a useful design principle. Sub-graphs allow you to create and manage different states in different parts of your graph. This allows you build things like multi-agent teams, where each team can track its own separate state.",What is the purpose of using sub-graphs in a graph system?
Simple example,"Let's consider a toy example: I have a system that accepts logs and perform two separate sub-tasks. First, it will summarize them. Second, it will summarize any failure modes captured in the logs. I want to perform these two operations in two different sub-graphs. The most important thing to recognize is the information transfer between the graphs. Entry Graph is the parent, and each of the two sub-graphs are defined as nodes in Entry Graph. Both subgraphs inherit state from the parent Entry Graph; I can access docs in each of the sub-graphs simply by specifying it in the sub-graph state (see diagram). Each subgraph can have its own private state. And any values that I want propagated back to the parent Entry Graph (for final reporting) simply need to be defined in my Entry Graph state (e.g., summary report and failure report).",How are the two sub-tasks in the system related to the parent Entry Graph and each other in terms of information transfer?
Custom reducer functions to manage state,"Now, let's highlight a possible stumbling block when we use the same State across multiple sub-graphs. We will create two graphs: a parent graph with a few nodes and a child graph that is added as a node in the parent. We define a custom reducer function for our state.",What is a possible stumbling block when using the same State across multiple sub-graphs and how can it be addressed with custom reducer functions?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,"What is the main graph class used in the StateGraph, and what is it parameterized by?"
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in the context of reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using the graph.get_state(config) function?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings when creating a graph in a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Stateless Runs,"Most of the time, you provide a thread_id to your client when you run your graph in order to keep track of prior runs through the persistent state implemented in LangGraph Cloud. However, if you have your own database to save runs and don't need to use the built in persistent state, you can create stateless runs.",What is the purpose of creating stateless runs in LangGraph Cloud?
Setup,"First, let's setup our client",What is the first step in the process?
Stateless streaming,"We can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the thread_id parameter, we pass None:",How can we stream the results of a stateless run?
Waiting for stateless results,"In addition to streaming, you can also wait for a stateless result by using the .wait function like follows:",What function can be used to wait for a stateless result in addition to streaming?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
FAQ,Common questions and their answers!,What type of information can be found in the FAQ section?
Do I need to use LangChain in order to use LangGraph?,"No! LangGraph is a general-purpose framework - the nodes and edges are nothing more than Python functions. You can use LangChain, raw HTTP requests, or even other frameworks inside these nodes and edges.",Do I need to use LangChain in order to use LangGraph?
Does LangGraph work with LLMs that don't support tool calling?,"Yes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.",Can LangGraph be used with LLMs that do not support tool calling?
Does LangGraph work with OSS LLMs?,"Yes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don't. But tool calling is not necessary (see this section) so you can totally use LangGraph with OSS LLMs.",Can LangGraph work with OSS LLMs?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Web Research (STORM),"STORM is a research assistant designed by Shao, et. al that extends the idea of ""outline-driven RAG"" for richer article generation. STORM is designed to generate Wikipedia-style ariticles on a user-provided topic. It applies two main insights to produce more organized and comprehensive articles: 
Creating an outline (planning) by querying similar topics helps improve coverage.
Multi-perspective, grounded (in search) conversation simulation helps increase the reference count and information density.
 The control flow looks like the diagram below. STORM has a few main stages: 
Generate initial outline + Survey related subjects
Identify distinct perspectives
""Interview subject matter experts"" (role-playing LLMs)
Refine outline (using references)
Write sections, then write article
 The expert interviews stage occurs between the role-playing article writer and a research expert. The ""expert"" is able to query external knowledge and respond to pointed questions, saving cited sources to a vectorstore so that the later refinement stages can synthesize the full article. There are a couple hyperparameters you can set to restrict the (potentially) infinite research breadth: N: Number of perspectives to survey / use (Steps 2->3)
M: Max number of conversation turns in step (Step 3)",What are the main stages involved in the STORM research assistant for generating Wikipedia-style articles on a user-provided topic?
Generate Initial Outline,"For many topics, your LLM may have an initial idea of the important and related topics. We can generate an initial
outline to be refined after our research. Below, we will use our ""fast"" llm to generate the outline.",What can be done to generate an initial outline for a topic in an LLM program?
Expand Topics,"While language models do store some Wikipedia-like knowledge in their parameters, you will get better results by incorporating relevant and recent information using a search engine. We will start our search by generating a list of related topics, sourced from Wikipedia.",What is one way to improve the results of language models when incorporating relevant and recent information?
Generate Perspectives,"From these related subjects, we can select representative Wikipedia editors as ""subject matter experts"" with distinct
backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report.",How can representative Wikipedia editors with distinct backgrounds and affiliations help distribute the search process to encourage a more well-rounded final report?
Expert Dialog,"Now the true fun begins, each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second ""domain expert"" with access to a search engine. This generate content to generate a refined outline as well as an updated index of reference documents.",What is the purpose of the expert dialog in generating content for Wikipedia articles?
Interview State,"The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own ""persona"") to make it easy to parallelize these conversations.",What components will be included in the Interview State graph?
Refine Outline,"At this point in STORM, we've conducted a large amount of research from different perspectives. It's time to refine the original outline based on these investigations. Below, create a chain using the LLM with a long context window to update the original outline.",What method should be used to update the original outline in STORM based on the research conducted from different perspectives?
Generate Article,"Now it's time to generate the full article. We will first divide-and-conquer, so that each section can be tackled by an individual llm. Then we will prompt the long-form LLM to refine the finished article (since each section may use an inconsistent voice). Create Retriever The research process uncovers a large number of reference documents that we may want to query during the final article-writing process. First, create the retriever:",What is the first step in generating the full article according to the text provided?
Final Flow,"Now it's time to string everything together. We will have 6 main stages in sequence:
. 
Generate the initial outline + perspectives
Batch converse with each perspective to expand the content for the article
Refine the outline based on the conversations
Index the reference docs from the conversations
Write the individual sections of the article
Write the final wiki
 The state tracks the outputs of each stage.",What are the 6 main stages in sequence for the Final Flow process?
Render the Wiki,Now we can render the final wiki page!,What can we do now with the final wiki page?
Contents,"
Introduction
Groq's Advancements in LLM Inference
NVIDIA's Contributions to LLM Inference
Hardware Innovations
Software Solutions
Research and Development

Llamma.cpp: Accelerating LLM Inference
The Future of LLM Inference
References
","What are some key topics covered in the ""Contents"" section of the document?"
Introduction,"The advent of million-plus token context window language models, such as Gemini 1.5, has significantly advanced the field of artificial intelligence, particularly in natural language processing (NLP). These models have expanded the capabilities of machine learning in understanding and generating text over vastly larger contexts than previously possible. This leap in technology has paved the way for transformative applications across various domains, including the integration into Retrieval-Augmented Generation (RAG) systems to produce more accurate and contextually rich responses.","How has the advent of million-plus token context window language models, such as Gemini 1.5, advanced the field of artificial intelligence, particularly in natural language processing (NLP)?"
Groq's Advancements in LLM Inference,"Groq has introduced the Groq Linear Processor Unit (LPU), a purpose-built hardware architecture for LLM inference. This innovation positions Groq as a leader in efficient and high-performance LLM processing by optimizing the hardware specifically for LLM tasks. The Groq LPU dramatically reduces latency and increases the throughput of LLM inferences, facilitating advancements in a wide range of applications, from natural language processing to broader artificial intelligence technologies[1].",What has Groq introduced to optimize hardware specifically for LLM inference tasks?
NVIDIA's Contributions to LLM Inference,"NVIDIA has played a pivotal role in advancing LLM inference through its GPUs, optimized for AI and machine learning workloads, and specialized software frameworks. The company's GPU architecture and software solutions, such as the CUDA Deep Neural Network library (cuDNN) and the TensorRT inference optimizer, are designed to accelerate computational processes and improve LLM performance. NVIDIA's active participation in research and development further underscores its commitment to enhancing the capabilities of LLMs[1]. Hardware Innovations NVIDIA's GPU architecture facilitates high throughput and parallel processing for LLM inference tasks, significantly reducing inference time and enabling complex models to be used in real-time applications. Software Solutions NVIDIA's suite of software tools, including cuDNN and TensorRT, optimizes LLM performance on its hardware, streamlining the deployment of LLMs by improving their efficiency and reducing latency. Research and Development NVIDIA collaborates with academic and industry partners to develop new techniques and models that push the boundaries of LLM technology, aiming to make LLMs more powerful and applicable across a broader range of tasks.",How has NVIDIA contributed to advancing LLM inference?
Llamma.cpp: Accelerating LLM Inference,"Llamma.cpp is a framework developed to enhance the speed and efficiency of LLM inference. By integrating specialized hardware, such as Groq's LPU, and optimizing for parallel processing, Llamma.cpp significantly accelerates computation times and reduces energy consumption. The framework supports million-plus token context window models, enabling applications requiring deep contextual understanding and extensive knowledge retrieval[1][2].",How does Llamma.cpp accelerate LLM inference?
The Future of LLM Inference,"The future of LLM inference is poised for transformative changes with advances in purpose-built hardware architectures like Groq's LPU. These innovations promise to enhance the speed and efficiency of LLM processing, leading to more interactive, capable, and integrated AI applications. The potential for advanced hardware and sophisticated LLMs to enable near-instantaneous processing of complex queries and interactions opens new avenues for research and application in various fields, suggesting a future where AI is seamlessly integrated into society[1][2].",What hardware architecture is poised to transform the future of LLM inference?
References,"[1] ""Groq's LPU: Advancing LLM Inference Efficiency,"" Prompt Engineering. https://promptengineering.org/groqs-lpu-advancing-llm-inference-efficiency/ [2] ""The Speed of Thought: Harnessing the Fastest LLM with Groq's LPU,"" Medium. https://medium.com/@anasdavoodtk1/the-speed-of-thought-harnessing-the-fastest-llm-with-groqs-lpu-11bb00864e9c",What are some references for articles discussing Groq's LPU and its advancements in LLM inference efficiency?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Plan-and-Execute,"This notebook shows how to create a ""plan-and-execute"" style agent. This is heavily inspired by the Plan-and-Solve paper as well as the Baby-AGI project. The core idea is to first come up with a multi-step plan, and then go through that plan one item at a time.
After accomplishing a particular task, you can then revisit the plan and modify as appropriate. The general computational graph looks like the following: This compares to a typical ReAct style agent where you think one step at a time.
The advantages of this ""plan-and-execute"" style agent are: 
Explicit long term planning (which even really strong LLMs can struggle with)
Ability to use smaller/weaker models for the execution step, only using larger/better models for the planning step
 The following walkthrough demonstrates how to do so in LangGraph. The resulting agent will leave a trace like the following example: (link).","What are the advantages of using a ""plan-and-execute"" style agent compared to a typical ReAct style agent?"
Setup,"First, we need to install the packages required.",What is the first step in the setup process?
Define Tools,"We will first define the tools we want to use. For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation here on how to do that.",What tool will be used for the simple example mentioned in the text?
Define our Execution Agent,"Now we will create the execution agent we want to use to execute tasks.
Note that for this example, we will be using the same execution agent for each task, but this doesn't HAVE to be the case.",What is the purpose of creating an execution agent in this context?
Define the State,"Let's now start by defining the state the track for this agent. First, we will need to track the current plan. Let's represent that as a list of strings. Next, we should track previously executed steps. Let's represent that as a list of tuples (these tuples will contain the step and then the result) Finally, we need to have some state to represent the final response as well as the original input.",What components are needed to define the state for this agent?
Planning Step,Let's now think about creating the planning step. This will use function calling to create a plan.,What method will be used to create the planning step?
Re-Plan Step,"Now, let's create a step that re-does the plan based on the result of the previous step.","What is the purpose of the ""Re-Plan Step"" in the process?"
Create the Graph,We can now create the graph!,What can we do now that we have the ability to create the graph?
Conclusion,"Congrats on making a plan-and-execute agent! One known limitations of the above design is that each task is still executed in sequence, meaning embarrassingly parallel operations all add to the total execution time. You could improve on this by having each task represented as a DAG (similar to LLMCompiler), rather than a regular list.",How could the execution time of tasks be improved in the plan-and-execute agent design mentioned in the text?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Self-Discover Agent,An implementation of the Self-Discover paper. Based on this implementation from @catid,What is the source of the implementation of the Self-Discover Agent?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in this context?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you access and use a configuration inside a node when creating a graph?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to add memory to the prebuilt ReAct agent,This tutorial will show how to add memory to the prebuilt ReAct agent. Please see this tutorial for how to get started with the prebuilt ReAct agent All we need to do to enable memory is pass in a checkpointer to create_react_agents,How can memory be added to the prebuilt ReAct agent?
Usage,Let's interact with it multiple times to show that it can remember,How can we demonstrate that it can remember by interacting with it multiple times?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Usage,"First, let's visualize the graph we just created","What is the first step recommended in the ""Usage"" section?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in the interaction pattern described?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method used to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
How to stream LLM tokens from your graph,"In this example we will stream tokens from the language model powering an agent. We will use a ReAct agent as an example. The main thing to bear in mind here is that using async nodes typically offers the best behavior for this, since we will be using the astream_events method. This how-to guide closely follows the others in this directory, so we will call out differences with the STREAMING tag below (if you just want to search for those). 
Note

        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the create_react_agent(model, tools=tool) (API doc) constructor. This may be more appropriate if you are used to LangChains AgentExecutor class.
    
 
Note on Python < 3.11

        When using python 3.8, 3.9, or 3.10, please ensure you manually pass the RunnableConfig through to the llm when invoking it like so: llm.ainvoke(..., config).
        The astream_events method collects all events from your nested code using a streaming tracer passed as a callback. In 3.11 and above, this is automatically handled via contextvar's; prior to 3.11, asyncio's tasks lacked proper contextvar support, meaning that the callbacks will only propagate if you manually pass the config through. We do this in the call_model method below.
    
",What method should be used to stream LLM tokens from the language model powering an agent?
Setup,First we need to install the packages required,What is the first step in the setup process?
Set up the state,"The main type of graph in langgraph is the StateGraph.
This graph is parameterized by a State object that it passes around to each node.
Each node then returns operations the graph uses to update that state.
These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute.
Whether to set or add is denoted by annotating the State object you use to construct the graph. For this example, the state we will track will just be a list of messages.
We want each node to just add messages to that list.
Therefore, we will use a TypedDict with one key (messages) and annotate it so that the messages attribute is ""append-only"".",What type of graph is used in langgraph and how is the State object parameterized within it?
Set up the tools,"We will first define the tools we want to use.
For this simple example, we will use create a placeholder search engine.
It is really easy to create your own tools - see documentation here on how to do that.",What will be the first step in the process of setting up the tools?
Set up the model,"Now we need to load the chat model we want to use.
This should satisfy two criteria: 
It should work with messages, since our state is primarily a list of messages (chat history).
It should work with tool calling, since we are using a prebuilt ToolNode
 Note: these model requirements are not requirements for using LangGraph - they are just requirements for this particular example.",What criteria should the chat model satisfy in order to be used in this example?
Define the nodes,"We now need to define a few different nodes in our graph.
In langgraph, a node can be either a function or a runnable.
There are two main nodes we need for this: 
The agent: responsible for deciding what (if any) actions to take.
A function to invoke tools: if the agent decides to take an action, this node will then execute that action.
 We will also need to define some edges.
Some of these edges may be conditional.
The reason they are conditional is that based on the output of a node, one of several paths may be taken.
The path that is taken is not known until that node is run (the LLM decides). 
Conditional Edge: after the agent is called, we should either:
a. If the agent said to take an action, then the function to invoke tools should be called
b. If the agent said that it was finished, then it should finish
Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next
 Let's define the nodes, as well as a function to decide how what conditional edge to take. STREAMING We define each node as an async function. 
Manual Callback Propagation

        Note that in call_model(state: State, config: RunnableConfig): below, we a) accept the RunnableConfig in the node and b) pass this in as the second arg for llm.ainvoke(..., config). This is optional for python 3.11 and later. If you ever have a problem where the LLM tokens are not streamed when using `astream_events` and you are using an older version of python, it's worth checking to ensure that the callbacks are manually propagated.
",What are the two main nodes that need to be defined in the graph?
Define the graph,We can now put it all together and define the graph!,What can we define by putting it all together?
Streaming LLM Tokens,"You can access the LLM tokens as they are produced by each node.
In this case only the ""agent"" node produces LLM tokens.
In order for this to work properly, you must be using an LLM that supports streaming as well as have set it when constructing the LLM (e.g. ChatOpenAI(model=""gpt-3.5-turbo-1106"", streaming=True))",What must be done in order to access the LLM tokens as they are produced by each node?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Adaptive RAG,"Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. In the paper, they report query analysis to route across: 
No Retrieval
Single-shot RAG
Iterative RAG
 Let's build on this using LangGraph. In our implementation, we will route between: 
Web search: for questions related to recent events
Self-corrective RAG: for questions related to our index
",What are the different routing options in the Adaptive RAG strategy for handling queries?
Tracing,"
Optionally, use LangSmith for tracing (shown at bottom) by setting:
","What tool can be used for tracing, as shown at the bottom of the text?"
Graph,Capture the flow in as a graph.,What can be done to capture the flow as a graph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes in a graph depending on custom logic using a conditional entry point?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using the graph.get_state method?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling your graph and why is it needed?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State using reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges or terminate using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using the graph.get_state method?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How-to guides,"Welcome to the LangGraph how-to guides! These guides provide practical, step-by-step instructions for accomplishing key tasks in LangGraph.",What type of instructions do the LangGraph how-to guides provide?
Controllability,"LangGraph is known for being a highly controllable agent framework.
These how-to guides show how to achieve that controllability. 
How to create subgraphs
How to create branches for parallel execution
How to create map-reduce branches for parallel execution
",What is LangGraph known for in terms of its agent framework?
Persistence,"LangGraph makes it easy to persist state across graph runs. The guide below shows how to add persistence to your graph. 
How to add persistence (""memory"") to your graph
How to manage conversation history
How to delete messages
How to add summary conversation memory
How to use Postgres checkpointer for persistence
How to create a custom checkpointer using MongoDB
How to create a custom checkpointer using Redis
",How can LangGraph make it easy to persist state across graph runs?
Human in the Loop,"One of LangGraph's main benefits is that it makes human-in-the-loop workflows easy.
These guides cover common examples of that. 
How to add breakpoints
How to edit graph state
How to wait for user input
How to view and update past graph state
Review tool calls
",What are some common examples of human-in-the-loop workflows facilitated by LangGraph?
Streaming,"LangGraph is built to be streaming first.
These guides show how to use different streaming modes. 
How to stream full state of your graph
How to stream state updates of your graph
How to stream LLM tokens
How to stream LLM tokens without LangChain models
How to stream arbitrarily nested content
How to configure multiple streaming modes at the same time
How to stream events from within a tool
How to stream events from within a tool without LangChain models
How to stream events from the final node
",How is LangGraph designed to prioritize streaming?
Tool calling,"
How to call tools using ToolNode
How to handle tool calling errors
How to pass graph state to tools
How to pass config to tools
How to handle large numbers of tools
",What are some considerations for calling tools using ToolNode?
State Management,"
Use Pydantic model as state
Use a context object in state
Have a separate input and output schema
Pass private state between nodes inside the graph
",What are some strategies for state management in a Python application?
Other,"
How to run graph asynchronously
How to visualize your graph
How to add runtime configuration to your graph
How to use a Pydantic model as your state
How to use a context object in state
How to add node retries
",What are some ways to enhance the functionality of a graph in a Python application?
Prebuilt ReAct Agent,"These guides show how to use the prebuilt ReAct agent.
Please note that here will we use a prebuilt agent. One of the big benefits of LangGraph is that you can easily create your own agent architectures. So while it's fine to start here to build an agent quickly, we would strongly recommend learning how to build your own agent so that you can take full advantage of LangGraph. 
How to create a ReAct agent
How to add memory to a ReAct agent
How to add a custom system prompt to a ReAct agent
How to add human-in-the-loop processes to a ReAct agent
",What are the benefits of using a prebuilt ReAct agent in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
API Concepts,This page describes the high-level concepts of the LangGraph Cloud API. The conceptual guide of LangGraph (Python library) is here.,What does the page about API Concepts describe?
Data Models,"The LangGraph Cloud API consists of a few core data models: Assistants, Threads, Runs, and Cron Jobs.",What are the core data models included in the LangGraph Cloud API?
Assistants,"An assistant is a configured instance of a CompiledGraph. It abstracts the cognitive architecture of the graph and contains instance specific configuration and metadata. Multiple assistants can reference the same graph but can contain different configuration and metadata, which may differentiate the behavior of the assistants. An assistant (i.e. the graph) is invoked as part of a run. The LangGraph Cloud API provides several endpoints for creating and managing assistants. See the API reference for more details. Configuring Assistants You can save custom assistants from the same graph to set different default prompts, models, and other configurations without changing a line of code in your graph. This allows you the ability to quickly test out different configurations without having to rewrite your graph every time, and also give users the flexibility to select different configurations when using your LangGraph application. See this how-to for information on how to configure a deployed graph. ","How can custom assistants be saved from the same graph to set different default prompts, models, and other configurations without changing any code?"
Threads,"A thread contains the accumulated state of a group of runs. If a run is executed on a thread, then the state of the underlying graph of the assistant will be persisted to the thread. A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The state of a thread at a particular point in time is called a checkpoint. For more on threads and checkpoints, see this section of the LangGraph conceptual guide. The LangGraph Cloud API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.",What is the purpose of creating a thread before executing a run in LangGraph?
Runs,"A run is an invocation of an assistant. Each run may have its own input, configuration, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a thread. The LangGraph Cloud API provides several endpoints for creating and managing runs. See the API reference for more details.",What endpoints does the LangGraph Cloud API provide for creating and managing runs?
Cron Jobs,"It's often useful to run graphs on some schedule. LangGraph Cloud supports cron jobs, which run on a user defined schedule. The user specifies a schedule, an assistant, and some input. After than, on the specified schedule LangGraph cloud will: 
Create a new thread with the specified assistant
Send the specified input to that thread
 Note that this sends the same input to the thread every time. See the how-to guide for creating cron jobs. The LangGraph Cloud API provides several endpoints for creating and managing cron jobs. See the API reference for more details.",What features does LangGraph Cloud support for running graphs on a schedule?
Features,The LangGraph Cloud API offers several features to support complex agent architectures.,What does the LangGraph Cloud API offer to support complex agent architectures?
Streaming,"Streaming is critical for making LLM applications feel responsive to end users. When creating a streaming run, the streaming mode determines what data is streamed back to the API client. The LangGraph Cloud API supports five streaming modes. 
values: Stream the full state of the graph after each node is executed. See the how-to guide for streaming values.
messages: Stream complete messages (at the end of node execution) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications. This is only an option if your graph contains a messages key. See the how-to guide for streaming messages.
updates: Streams updates to the state of the graph after each node is executed. See the how-to guide for streaming updates.
events: Stream all events (including the state of the graph) after each node is executed. See the how-to guide for streaming events. This can be used to do token-by-token streaming for LLMs.
debug: Stream debug events after each node is executed. See the how-to guide for streaming debug events.
 You can also specify multiple streaming modes at the same time. See the how-to guide for configuring multiple streaming modes at the same time. See the API reference for how to create streaming runs.",What are the five streaming modes supported by the LangGraph Cloud API?
Human-in-the-Loop,"There are many occasions where the graph cannot run completely autonomously. For instance, the user might need to input some additional arguments to a function call, or select the next edge for the graph to continue on. In these instances, we need to insert some human in the loop interaction, which you can learn about in the human in the loop how-tos.",What are some examples of when human-in-the-loop interaction is necessary for a graph to run?
Double Texting,"Many times users might interact with your graph in unintended ways. For instance, a user may send one message and before the graph has finished running send a second message. To solve this issue of ""double-texting"" (i.e. prompting the graph a second time before the first run has finished), LangGraph has provided four different solutions, all of which are covered in the Double Texting how-tos. These options are: 
reject: This is the simplest option, this just rejects any follow up runs and does not allow double texting. See the how-to guide for configuring the reject double text option.
enqueue: This is a relatively simple option which continues the first run until it completes the whole run, then sends the new input as a separate run. See the how-to guide for configuring the enqueue double text option.
interrupt: This option interrupts the current execution but saves all the work done up until that point. It then inserts the user input and continues from there. If you enable this option, your graph should be able to handle weird edge cases that may arise. See the how-to guide for configuring the interrupt double text option.
rollback: This option rolls back all work done up until that point. It then sends the user input in, basically as if it just followed the original run input. See the how-to guide for configuring the rollback double text option.
","What are the four different solutions provided by LangGraph to address the issue of ""double-texting"" when interacting with a graph?"
Stateless Runs,"All runs use the built-in checkpointer to store checkpoints for runs. However, it can often be useful to just kick off a run without worrying about explicitly creating a thread and without wanting to keep those checkpointers around. Stateless runs allow you to do this by exposing an endpoint that: 
Takes in user input
Under the hood, creates a thread
Runs the agent but skips all checkpointing steps
Cleans up the thread afterwards
 Stateless runs are still retried as regular retries are per node, while everything still in memory, so doesn't use checkpoints. The only difference is in stateless background runs, if the task worker dies halfway (not because the run itself failed, for some external reason) then the whole run will be retried like any background run, but 
whereas a stateful background run would retry from the last successful checkpoint
a stateless background run would retry from the beginning
 See the how-to guide for creating stateless runs.",What is the purpose of stateless runs and how do they differ from stateful background runs in terms of retry behavior?
Webhooks,"For all types of runs, langgraph cloud supports completion webhooks. When you create the run you can pass a webhook URL to be called when the completes (successfully or not). This is especially useful for background runs and cron jobs, as the webhook can give you an indication the run has completed and you can perform further actions for your appilcation. See this how-to guide to learn about how to use webhooks with LangGraph Cloud.",How can completion webhooks be utilized with LangGraph Cloud for different types of runs?
Deployment,The LangGraph Cloud offers several features to support secure and robost deployments.,What features does the LangGraph Cloud offer to support secure and robust deployments?
Authentication,"LangGraph applications deployed to LangGraph Cloud are automatically configured with LangSmith authentication. In order to call the API, a valid LangSmith API key is required.",What is required in order to call the API for LangGraph applications deployed to LangGraph Cloud?
Local Testing,"Before deploying your app in production to LangGraph Cloud, you may wish to test out your graph locally in order to ensure that everything is running as expected. Luckily, LangGraph makes this easy for you through use of the LangGraph CLI. Read more in this how-to guide or look at the CLI reference to learn more.",What tool can be used to test out your graph locally before deploying your app in production to LangGraph Cloud?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Competitive Programming,"In this tutorial, you will build a computing olympiad agent that leverages three complementary techniques to boost performance: reflection, retrieval, and human-in-the-loop collaboration. These techniques and data are all adapted from the paper ""Can Language Models Solve Olympiad Programming?"" by Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. You can check out their paper at the following link: You will construct an agentic graph capable of answering programming questions of increasing difficulty. 
Reflection: In part 1, you will create a zero-shot tool calling agent and prompt it to reflect on the test case results to correct its initial errors. This is similar to the agent the paper reported as having a pass rate of 12.38 on the USACO benchmark.
Retrieval: In Part 2, you will implement an initial retrieval step as ""episodic memory"" for the agent that retrieves high-quality few-shot examples from our corpora of programming problems to help solve the bronze level question. This agent is similar to the one the paper benchmarked at 20.2.
Human-in-the-loop: In part 3, you will use interrupt_after to let the user copilot the agent to a better answer. The benchmark performance then is constrained only by the competitiveness of the human it is paired with.
 Your final agent graph will be structured like the diagram below: Parts 1 and 2 are analogous to the systems benchmarked in the paper as having a pass rate of 12.38 and 20.2 respectively. While LLMs are not yet capable of autonomously solving all these problems, we can design the system that far surpasses the capabilities of a basic ReAct agent at answering these questions. Before diving in, let's set up our machine. This will involve installing dependencies, fetching the dataset, and defining a utility function.",What techniques are leveraged in building the computing olympiad agent in the Competitive Programming tutorial?
Setup,"For this tutorial, we will need to install some dependencies, fetch the Olympiad dataset, and define a utility function to help run the candidate solutions to see if they pass the test cases. First, install the requirements.",What steps are required to set up for the tutorial?
Part 1: Zero-Shot with Reflection,"In our first section, we will build a simple zero-shot tool-calling agent to try to solve these problems. We will incorporate a simple form of reflection directly in the agent's tool calling schema by adding a ""reasoning"" field. Furthermore, Claude was trained to ""reason"" with freeform text prior to invoking any tools. Together, this should induce reflective ""chain-of-thought"" prompting. Note: this diverges somewhat from the paper's implementation, which uses an explicit reflection step with a variation of the Reflexion prompt. By the end of this section, we will have built a reflective zero-shot programming agent that looks like the section marked ""Part 1"" in the system diagram below:","What is the purpose of incorporating a ""reasoning"" field and training the agent to ""reason"" with freeform text in the zero-shot tool-calling agent?"
State,"LangGraph's main primitive is the StateGraph, which you use to define an agent as a controllable state machine.  The graph has node's (python functions) that perform the work, and edges that define how to route between the nodes.
The State defines the interface between each node and carries all the information your agent needs. Below, define a State for our programming olympiad agent. The messages will track the sequence of submissions (and test case feedback) as chat history. The status field will flip from in_progress to success if the submission passes all test cases.
The other fields (test_cases, runtime_limit) are used by the evaluation node to test the agent's submissions. These values are not seen by the agent itself.",What is LangGraph's main primitive and how is it used to define an agent as a controllable state machine?
Part 2: Few-shot Retrieval,"Even with reflective tool calling, our baseline agent from part 1 struggled with this difficult task. One way to ""teach"" an LLM how to better perform a task is through demonstrations, also known as ""few-shot examples."" What the authors of the USACO paper call ""episodic memory"" is really just few-shot prompting over similar examples. Each examples in this case is a different problems + solution within the dataset. The term ""episodic memory"" makes sense if you pretend your agent has already ""solved"" these problems and is recalling its solutions to them. This section adds the ""Episodic Memory"" components from ""Part 2"" in the diagram below. Note that this memory step is performed one time,  before the logic of our zero-shot loop from part 1. The steps are as follows: 
Prompt the LLM to generate a candidate solution.
Use the text of the candidate solution to retrieve the N most similar (problem, solution) pairs.
Format this result in the Zero-shot agent's prompt.
 Below, let's implement our episodic memory as a retriever. We will follow the paper's retriever selection and use BM25.",How can an LLM be taught to better perform a task using few-shot examples and episodic memory?
Part 3: Human-in-the-loop,"Our retrieval-enhanced agent was able to solve the bronze-level question but still failed for those with the more challenging silver difficulty. Recall that the paper presented 3 complementary techniques that improved performance: 
Reflection: explicitly prompting the LLM to ""reflect"" on its mistakes can help it
Few-shot prompting: retrieving relevant, high-quality examples as ""memory""
Human-in-the-loop collaboration:  without giving the correct answer, the human is allowed to help the agent reflect on its approach and point it in a better direction.
 In this section, we will add the ""human"" node (marked as ""part 3"" in the diagram below), completing our agent graph: From an ML perspective, this is a bit of a clever hans, but from the application designer's perspective, where the primary goal is to achieve a higher combined success rate, letting the human interject with thoughts and insights is only natural. In either case, adding a human check to a LangGraph instance requires no extra lines of code. Let's do so by instructing the graph to interrupt_after the ""evaluate"" node to give the user a chance to modify the trajectory. Start assembling your graph below. The following section is identical to our application in part 2:",How did the human-in-the-loop collaboration technique improve the performance of the retrieval-enhanced agent?
Conclusion,"Congrats on making it to the end! In this tutorial, you implemented an agent in LangGraph capable of solving challenging programming problems. You did so by leveraging a few common techniques to improve performance, including: 
Reflection: while we didn't implement an explicit reflection step, our prompt and tool invocation was designed to encourage critique of previous outputs. You added this in Part 1.
Retrieval: the ""episodic memory"" of the agent retrieves high-quality few-shot examples from our corpora of programming problems to help solve the bronze level question. In Part 2, you implemented a retrieval memory as an initial step.
Human-in-the-loop: LLM-powered agents are still too weak to answer all these questions autonomously, but at times, they can get most of the way there and land on the right answer with human feedback. In Part 3, you used interrupt_after on the evaluate node and then included your feedback by using update_state on the graph.
 LLMs are not capable of solving all these problems autonomously, but through better prompting and clever engineering, you can create a system that is able to more reliably arrive at the proper solution.",What techniques were leveraged to improve performance in implementing an agent in LangGraph for solving challenging programming problems?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize the time travel interaction pattern in LangGraph?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What actions can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step that can be taken to ensure reliable results from agents?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling your graph and why is it needed?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes in a graph based on custom logic using a conditional entry point?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What information can be obtained by calling graph.get_state_history(config)?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What are some built-in ways to visualize graphs in LangGraph?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to pass private state,"Oftentimes, you may want nodes to be able to pass state to each other that should NOT be part of the main schema of the graph. This is often useful because there may be information that is not needed as input/output (and therefore doesn't really make sense to have in the main schema) but is ABSOLUTELY needed as part of the intermediate working logic. Let's take a look at an example below. In this example, we will create a RAG pipeline that: 
Takes in a user question
Uses an LLM to generate a search query
Retrieves documents for that generated query
Generates a final answer based on those documents
 We will have a separate node for each step. We will only have the question and answer on the overall state. However, we will need separate states for the search_query and the documents - we will pass these as private state keys. Let's look at an example!",How can nodes pass private state to each other in a graph schema?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Chat Bot Evaluation as Multi-agent Simulation,"When building a chat bot, such as a customer support assistant, it can be hard to properly evaluate your bot's performance. It's time-consuming to have to manually interact with it intensively for each code change. One way to make the evaluation process easier and more reproducible is to simulate a user interaction. With LangGraph, it's easy to set this up. Below is an example of how to create a ""virtual user"" to simulate a conversation. The overall simulation looks something like this: First, we'll set up our environment.","How can LangGraph be used to create a ""virtual user"" for simulating a conversation with a chat bot during evaluation?"
1. Define Chat Bot,"Next, we will define our chat bot. For this notebook, we assume the bot's API accepts a list of messages and responds with a message. If you want to update this, all you'll have to change is this section and the ""get_messages_for_agent"" function in
the simulator below. The implementation within my_chat_bot is configurable and can even be run on another system (e.g., if your system isn't running in python).",What assumptions are made about the chat bot's API in this notebook?
2. Define Simulated User,"We're now going to define the simulated user.
This can be anything we want, but we're going to build it as a LangChain bot.",What is the definition of a simulated user and how is it built in this context?
3. Define the Agent Simulation,"The code below creates a LangGraph workflow to run the simulation. The main components are: 
The two nodes: one for the simulated user, the other for the chat bot.
The graph itself, with a conditional stopping criterion.
 Read the comments in the code below for more information.",What are the main components of the Agent Simulation defined in the LangGraph workflow code?
4. Run Simulation,Now we can evaluate our chat bot! We can invoke it with empty messages (this will simulate letting the chat bot start the initial conversation),What can we do to evaluate our chat bot?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to manage conversation history,"One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. In order to prevent this from happening, you need to probably manage the conversation history. Note: this guide focuses on how to do this in LangGraph, where you can fully customize how this is done. If you want a more off-the-shelf solution, you can look into functionality provided in LangChain: 
How to filter messages
How to trim messages
",How can conversation history be managed in LangGraph to prevent it from taking up too much space and causing longer calls to the LLM?
Setup,"First, let's set up the packages we're going to want to use",What is the first step in the process described in the text?
Build the agent,Let's now build a simple ReAct style agent.,What style of agent are we building?
Filtering messages,"The most straight-forward thing to do to prevent conversation history from blowing up is to filter the list of messages before they get passed to the LLM. This involves two parts: defining a function to filter messages, and then adding it to the graph. See the example below which defines a really simple filter_messages function and then uses it.",What are the two parts involved in preventing conversation history from blowing up by filtering messages before they get passed to the LLM?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
LangGraph,"

  Building language agents as graphs  
Note
Looking for the JS version? Click here (JS docs).
",What type of agents are being built as graphs in LangGraph?
Overview,"LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features. LangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.",What are the core benefits of using LangGraph compared to other LLM frameworks?
Key Features,"
Cycles and Branching: Implement loops and conditionals in your apps.
Persistence: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.
Human-in-the-Loop: Interrupt graph execution to approve or edit next action planned by the agent.
Streaming Support: Stream outputs as they are produced by each node (including token streaming).
Integration with LangChain: LangGraph integrates seamlessly with LangChain and LangSmith (but does not require them).
",What are some key features of LangGraph?
Installation,"pip install -U langgraph
",What command is used to install or update the langgraph package using pip?
Example,"One of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function. Let's take a look at a simple example of an agent that can use a search tool. pip install langchain-anthropic
 export ANTHROPIC_API_KEY=sk-...
 Optionally, we can set up LangSmith for best-in-class observability. export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=lsv2_sk_...
 from typing import Annotated, Literal, TypedDict

from langchain_core.messages import HumanMessage
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph, MessagesState
from langgraph.prebuilt import ToolNode

# Define the tools for the agent to use
@tool
def search(query: str):
    """"""Call to surf the web.""""""
    # This is a placeholder, but don't tell the LLM that...
    if ""sf"" in query.lower() or ""san francisco"" in query.lower():
        return ""It's 60 degrees and foggy.""
    return ""It's 90 degrees and sunny.""

tools = [search]

tool_node = ToolNode(tools)

model = ChatAnthropic(model=""claude-3-5-sonnet-20240620"", temperature=0).bind_tools(tools)

# Define the function that determines whether to continue or not
def should_continue(state: MessagesState) -> Literal[""tools"", END]:
    messages = state['messages']
    last_message = messages[-1]
    # If the LLM makes a tool call, then we route to the ""tools"" node
    if last_message.tool_calls:
        return ""tools""
    # Otherwise, we stop (reply to the user)
    return END

# Define the function that calls the model
def call_model(state: MessagesState):
    messages = state['messages']
    response = model.invoke(messages)
    # We return a list, because this will get added to the existing list
    return {""messages"": [response]}

# Define a new graph
workflow = StateGraph(MessagesState)

# Define the two nodes we will cycle between
workflow.add_node(""agent"", call_model)
workflow.add_node(""tools"", tool_node)

# Set the entrypoint as `agent`
# This means that this node is the first one called
workflow.set_entry_point(""agent"")

# We now add a conditional edge
workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    ""agent"",
    # Next, we pass in the function that will determine which node is called next.
    should_continue,
)

# We now add a normal edge from `tools` to `agent`.
# This means that after `tools` is called, `agent` node is called next.
workflow.add_edge(""tools"", 'agent')

# Initialize memory to persist state between graph runs
checkpointer = MemorySaver()

# Finally, we compile it!
# This compiles it into a LangChain Runnable,
# meaning you can use it as you would any other runnable.
# Note that we're (optionally) passing the memory when compiling the graph
app = workflow.compile(checkpointer=checkpointer)

# Use the Runnable
final_state = app.invoke(
    {""messages"": [HumanMessage(content=""what is the weather in sf"")]},
    config={""configurable"": {""thread_id"": 42}}
)
final_state[""messages""][-1].content
 ""Based on the search results, I can tell you that the current weather in San Francisco is:\n\nTemperature: 60 degrees Fahrenheit\nConditions: Foggy\n\nSan Francisco is known for its microclimates and frequent fog, especially during the summer months. The temperature of 60F (about 15.5C) is quite typical for the city, which tends to have mild temperatures year-round. The fog, often referred to as ""Karl the Fog"" by locals, is a characteristic feature of San Francisco\'s weather, particularly in the mornings and evenings.\n\nIs there anything else you\'d like to know about the weather in San Francisco or any other location?""
 Now when we pass the same ""thread_id"", the conversation context is retained via the saved state (i.e. stored list of messages) final_state = app.invoke(
    {""messages"": [HumanMessage(content=""what about ny"")]},
    config={""configurable"": {""thread_id"": 42}}
)
final_state[""messages""][-1].content
 ""Based on the search results, I can tell you that the current weather in New York City is:\n\nTemperature: 90 degrees Fahrenheit (approximately 32.2 degrees Celsius)\nConditions: Sunny\n\nThis weather is quite different from what we just saw in San Francisco. New York is experiencing much warmer temperatures right now. Here are a few points to note:\n\n1. The temperature of 90F is quite hot, typical of summer weather in New York City.\n2. The sunny conditions suggest clear skies, which is great for outdoor activities but also means it might feel even hotter due to direct sunlight.\n3. This kind of weather in New York often comes with high humidity, which can make it feel even warmer than the actual temperature suggests.\n\nIt's interesting to see the stark contrast between San Francisco's mild, foggy weather and New York's hot, sunny conditions. This difference illustrates how varied weather can be across different parts of the United States, even on the same day.\n\nIs there anything else you'd like to know about the weather in New York or any other location?""
",What is the central concept of state in LangGraph and how is it utilized in graph execution?
Step-by-step Breakdown,"

Initialize the model and tools.

we use ChatAnthropic as our LLM. NOTE: we need make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the .bind_tools() method.
we define the tools we want to use - a search tool in our case. It is really easy to create your own tools - see documentation here on how to do that here.
   

Initialize graph with state.

we initialize graph (StateGraph) by passing state schema (in our case MessagesState)
MessagesState is a prebuilt state schema that has one attribute -- a list of LangChain Message objects, as well as logic for merging the updates from each node into the state
   

Define graph nodes.
There are two main nodes we need:

The agent node: responsible for deciding what (if any) actions to take.
The tools node that invokes tools: if the agent decides to take an action, this node will then execute that action.
   

Define entry point and graph edges.
First, we need to set the entry point for graph execution - agent node.
Then we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph's state (MessageState). In our case, the destination is not known until the agent (LLM) decides.

Conditional edge: after the agent is called, we should either:
a. Run tools if the agent said to take an action, OR
b. Finish (respond to the user) if the agent did not ask to run tools

Normal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next
   

Compile the graph.

When we compile the graph, we turn it into a LangChain Runnable, which automatically enables calling .invoke(), .stream() and .batch() with your inputs
We can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, human-in-the-loop workflows, time travel and more. In our case we use MemorySaver - a simple in-memory checkpointer

Execute the graph.

LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, ""agent"".
The ""agent"" node executes, invoking the chat model.
The chat model returns an AIMessage. LangGraph adds this to the state.

Graph cycles the following steps until there are no more tool_calls on AIMessage:

If AIMessage has tool_calls, ""tools"" node executes
The ""agent"" node executes again and returns AIMessage

Execution progresses to the special END value and outputs the final state.
And as a result, we get a list of all our chat messages as output.
   

",What are the main nodes needed in the graph execution process described in the text?
Documentation,"
Tutorials: Learn to build with LangGraph through guided examples.
How-to Guides: Accomplish specific things within LangGraph, from streaming, to adding memory & persistence, to common design patterns (branching, subgraphs, etc.), these are the place to go if you want to copy and run a specific code snippet.
Conceptual Guides: In-depth explanations of the key concepts and principles behind LangGraph, such as nodes, edges, state and more.
API Reference: Review important classes and methods, simple examples of how to use the graph and checkpointing APIs, higher-level prebuilt components and more.
Cloud (beta): With one click, deploy LangGraph applications to LangGraph Cloud.
",What types of guides and resources are available in the LangGraph documentation?
Contributing,"For more information on how to contribute, see here.",Where can I find more information on how to contribute?
What does it mean to be agentic?,"Other people may talk about a system being an ""agent"" - we prefer to talk about systems being ""agentic"". But what does this actually mean? When we talk about systems being ""agentic"", we are talking about systems that use an LLM to decide the control flow of an application. There are different levels that an LLM can be used to decide the control flow, and this spectrum of ""agentic"" makes more sense to us than defining an arbitrary cutoff for what is or isn't an agent. Examples of using an LLM to decide the control of an application: 
Using an LLM to route between two potential paths
Using an LLM to decide which of many tools to call
Using an LLM to decide whether the generated answer is sufficient or more work is need
 The more times these types of decisions are made inside an application, the more agentic it is.
If these decisions are being made in a loop, then its even more agentic! There are other concepts often associated with being agentic, but we would argue these are a by-product of the above definition: 
Tool calling: this is often how LLMs make decisions
Action taking: often times, the LLMs' outputs are used as the input to an action
Memory: reliable systems need to have knowledge of things that occurred
Planning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.
",What does it mean to be agentic?
Why LangGraph?,"LangGraph has several core principles that we believe make it the most suitable framework for building agentic applications: 
Controllability
Human-in-the-Loop
Streaming First
 Controllability LangGraph is extremely low level. This gives you a high degree of control over what the system you are building actually does. We believe this is important because it is still hard to get agentic systems to work reliably, and we've seen that the more control you exercise over them, the more likely it is that they will ""work"". Human-in-the-Loop LangGraph comes with a built-in persistence layer as a first-class concept. This enables several different human-in-the-loop interaction patterns. We believe that ""Human-Agent Interaction"" patterns will be the new ""Human-Computer Interaction"", and have built LangGraph with built in persistence to enable this. Streaming First LangGraph comes with first class support for streaming. Agentic applications often take a while to run, and so giving the user some idea of what is happening is important, and streaming is a great way to do that. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.",What core principles make LangGraph the most suitable framework for building agentic applications?
Deployment,"So you've built your LangGraph object - now what? Now you need to deploy it. 
There are many ways to deploy LangGraph objects, and the right solution depends on your needs and use case.
We'll highlight two ways here: using LangGraph Cloud or rolling your own solution. LangGraph Cloud is an opinionated way to deploy LangGraph objects from the LangChain team. Please see the LangGraph Cloud documentation for all the details about what it involves, to see if it is a good fit for you. If it is not a good fit, you may want to roll your own deployment. In this case, we would recommend using FastAPI to stand up a server. You can then call this graph from inside the FastAPI server as you see fit.",What are two ways to deploy LangGraph objects?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling your graph and why is it needed?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What method can be used to retrieve the state history of a graph in a specific thread?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to run multiple agents on the same thread,"In LangGraph Cloud, a thread is not explicitly associated with a particular agent.
This means that you can run multiple agents on the same thread, which allows a different
agent to continue from an initial agent's progress. In this example, we will create two agents and then call them both on the same thread.
You'll see that the second agent will respond using information from the checkpoint generated in the thread
by the first agent as context.",How can multiple agents be run on the same thread in LangGraph Cloud?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling a graph and why is it necessary?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be used in LangGraph nodes to route to the next step or update specific keys of the state?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents through the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step that can be taken to ensure reliable results from agents?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to handle tool calling errors,"
Prerequisites

        This guide assumes familiarity with the following:
        

                    Tool calling
                

                    Deleting messages
                

 LLMs aren't perfect at calling tools. The model may try to call a tool that doesn't exist or fail to return arguments that match the requested schema. Strategies like keeping schemas simple, reducing the number of tools you pass at once, and having good names and descriptions can help mitigate this risk, but aren't foolproof. This guide covers some ways to build error handling into your graphs to mitigate these failure modes.",What strategies can be used to handle tool calling errors in LLMs?
Using the prebuiltToolNode,"To start, define a mock weather tool that has some hidden restrictions on input queries. The intent here is to simulate a real-world case where a model fails to call a tool correctly:",What is the purpose of defining a mock weather tool with hidden restrictions on input queries when using the prebuiltToolNode?
Custom strategies,"This is a fine default in many cases, but there are cases where custom fallbacks may be better. For example, the below tool requires as input a list of elements of a specific length - tricky for a small model! We'll also intentionally avoid pluralizing topic to trick the model into thinking it should pass a string:",What is an example of a case where custom fallbacks may be better than a default strategy?
Next steps,"You've now seen how to implement some strategies to handle tool calling errors. Next, check out some of the other LangGraph how-to guides here.",What should you do after implementing strategies to handle tool calling errors?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Adaptive RAG using local LLMs,"Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. In the paper, they report query analysis to route across: 
No Retrieval
Single-shot RAG
Iterative RAG
 Let's build on this using LangGraph. In our implementation, we will route between: 
Web search: for questions related to recent events
Self-corrective RAG: for questions related to our index
",What are the different routing options in the Adaptive RAG strategy using local LLMs?
LLMs,"Local Embeddings You can use GPT4AllEmbeddings() from Nomic, which can access use Nomic's recently released v1 and v1.5 embeddings. Follow the documentation here. Local LLM (1) Download Ollama app. (2) Download a Mistral model from various Mistral versions here and Mixtral versions here available. Also, try one of the quantized command-R models. ollama pull mistral
",What tool can be used to access Nomic's v1 and v1.5 embeddings for local embeddings?
Tracing,"Optionally, use LangSmith for tracing (shown at bottom)","What tool can be used for tracing, as shown at the bottom?"
LLMs,Note: tested cmd-R on Mac M2 32GB and latency is ~52 sec for RAG generation.,What is the latency for RAG generation when using cmd-R on a Mac M2 32GB for LLMs?
Graph,Capture the flow in as a graph.,What can be done to capture the flow as a graph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to handle large numbers of tools,"The subset of available tools to call is generally at the discretion of the model (although many providers also enable the user to specify or constrain the choice of tool). As the number of available tools grows, you may want to limit the scope of the LLM's selection, to decrease token consumption and to help manage sources of error in LLM reasoning. Here we will demonstrate how to dynamically adjust the tools available to a model. Bottom line up front: like RAG and similar methods, we prefix the model invocation by retrieving over available tools. Although we demonstrate one implementation that searches over tool descriptions, the details of the tool selection can be customized as needed.",How can the tools available to a model be dynamically adjusted to handle large numbers of tools?
Incorporating with an agent,"We will use a typical React agent graph (e.g., as used in the quickstart), with some modifications: 
We add a selected_tools key to the state, which stores our selected subset of tools;
We set the entry point of the graph to be a select_tools node, which populates this element of the state;
We bind the selected subset of tools to the chat model within the agent node.
",What modifications are made to the typical React agent graph when incorporating with an agent?
Repeating tool selection,"To manage errors from incorrect tool selection, we could revisit the select_tools node. One option for implementing this is to modify select_tools to generate the vector store query using all messages in the state (e.g., with a chat model) and add an edge routing from tools to select_tools. We implement this change below. For demonstration purposes, we simulate an error in the initial tool selection by adding a hack_remove_tool_condition to the select_tools node, which removes the correct tool on the first iteration of the node. Note that on the second iteration, the agent finishes the run as it has access to the correct tool.",How can errors from incorrect tool selection be managed in the select_tools node?
Next steps,"This guide provides a minimal implementation for dynamically selecting tools. There is a host of possible improvements and optimizations: 
Repeating tool selection: Here, we repeated tool selection by modifying the select_tools node. Another option is to equip the agent with a reselect_tools tool, allowing it to re-select tools at its discretion.
Optimizing tool selection: In general, the full scope of retrieval solutions are available for tool selection. Additional options include:
Group tools and retrieve over groups;
Use a chat model to select tools or groups of tool.

",What are some possible next steps for improving tool selection in the guide provided?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output from LLMs inside nodes be used when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent be programmed to wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents through the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common method for agents to improve their reliability and accuracy in completing tasks?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,"What is the main graph class used in the StateGraph, and what is it parameterized by?"
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling a graph and why is it necessary?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
What does it mean to be agentic?,"Other people may talk about a system being an ""agent"" - we prefer to talk about systems being ""agentic"". But what does this actually mean? When we talk about systems being ""agentic"", we are talking about systems that use an LLM to decide the control flow of an application. There are different levels that an LLM can be used to decide the control flow, and this spectrum of ""agentic"" makes more sense to us than defining an arbitrary cutoff for what is or isn't an agent. Examples of using an LLM to decide the control of an application: 
Using an LLM to route between two potential paths
Using an LLM to decide which of many tools to call
Using an LLM to decide whether the generated answer is sufficient or more work is need
 The more times these types of decisions are made inside an application, the more agentic it is.
If these decisions are being made in a loop, then its even more agentic! There are other concepts often associated with being agentic, but we would argue these are a by-product of the above definition: 
Tool calling: this is often how LLMs make decisions
Action taking: often times, the LLMs' outputs are used as the input to an action
Memory: reliable systems need to have knowledge of things that occurred
Planning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.
",What does it mean to be agentic?
Why LangGraph?,"LangGraph has several core principles that we believe make it the most suitable framework for building agentic applications: 
Controllability
Human-in-the-Loop
Streaming First
 Controllability LangGraph is extremely low level. This gives you a high degree of control over what the system you are building actually does. We believe this is important because it is still hard to get agentic systems to work reliably, and we've seen that the more control you exercise over them, the more likely it is that they will ""work"". Human-in-the-Loop LangGraph comes with a built-in persistence layer as a first-class concept. This enables several different human-in-the-loop interaction patterns. We believe that ""Human-Agent Interaction"" patterns will be the new ""Human-Computer Interaction"", and have built LangGraph with built in persistence to enable this. Streaming First LangGraph comes with first class support for streaming. Agentic applications often take a while to run, and so giving the user some idea of what is happening is important, and streaming is a great way to do that. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.",What are the core principles of LangGraph that make it suitable for building agentic applications?
Deployment,"So you've built your LangGraph object - now what? Now you need to deploy it. 
There are many ways to deploy LangGraph objects, and the right solution depends on your needs and use case.
We'll highlight two ways here: using LangGraph Cloud or rolling your own solution. LangGraph Cloud is an opinionated way to deploy LangGraph objects from the LangChain team. Please see the LangGraph Cloud documentation for all the details about what it involves, to see if it is a good fit for you. If it is not a good fit, you may want to roll your own deployment. In this case, we would recommend using FastAPI to stand up a server. You can then call this graph from inside the FastAPI server as you see fit.",What are two ways to deploy LangGraph objects?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output from LLMs inside nodes be used when building agents in LangGraph?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in controlling and retaining memory?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation using the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",What is a common technique used by agentic systems to overcome the struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step taken to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to add runtime configuration to your graph,"Sometimes you want to be able to configure your agent when calling it.
Examples of this include configuring which LLM to use.
Below we walk through an example of doing so.",How can you add runtime configuration to your graph?
Base,"First, let's create a very simple graph",What is the first step in creating a graph?
Configure the graph,"Great! Now let's suppose that we want to extend this example so the user is able to choose from multiple llms.
We can easily do that by passing in a config.
This config is meant to contain things are not part of the input (and therefore that we don't want to track as part of the state).",How can the graph be configured to allow the user to choose from multiple llms?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Agent Supervisor,"The previous example routed messages automatically based on the output of the initial researcher agent. We can also choose to use an LLM to orchestrate the different agents. Below, we will create an agent group, with an agent supervisor to help delegate tasks. To simplify the code in each agent node, we will use the AgentExecutor class from LangChain. This and other ""advanced agent"" notebooks are designed to show how you can implement certain design patterns in LangGraph. If the pattern suits your needs, we recommend combining it with some of the other fundamental patterns described elsewhere in the docs for best performance. Before we build, let's configure our environment:",What class can be used to simplify the code in each agent node when creating an agent group with an agent supervisor?
Create tools,"For this example, you will make an agent to do web research with a search engine, and one agent to create plots. Define the tools they'll use below:",What tools will be used to create an agent for web research and another agent for creating plots?
Helper Utilities,"Define a helper function below, which make it easier to add new agent worker nodes.",What purpose does the helper function defined below serve in relation to adding new agent worker nodes?
Create Agent Supervisor,It will use function calling to choose the next worker node OR finish processing.,What method will the Agent Supervisor use to determine the next worker node or finish processing?
Construct Graph,"We're ready to start building the graph. Below, define the state and worker nodes using the function we just defined.",What is the next step after defining the state and worker nodes using the function?
Invoke the team,"With the graph created, we can now invoke it and see how it performs!",How can we assess the performance of the graph created by the team?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Rollback,"This guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide. The guide covers the rollback option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option is very similar to the interrupt option, but in this case the first run is completely deleted from the database and cannot be restarted. Below is a quick example of using the rollback option. First, we will define a quick helper function for printing out JS model outputs (you can skip this if using Python): function prettyPrint(m) {
  const padded = "" "" + m['type'] + "" "";
  const sepLen = Math.floor((80 - padded.length) / 2);
  const sep = ""="".repeat(sepLen);
  const secondSep = sep + (padded.length % 2 ? ""="" : """");

  console.log(`${sep}${padded}${secondSep}`);
  console.log(""\n\n"");
  console.log(m.content);
}
 Now, let's import our required packages and instantiate our client, assistant, and thread. PythonJavascript

import asyncio

import httpx
from langchain_core.messages import convert_to_messages
from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent"";
const thread = await client.threads.create();

 Now let's run a thread with the multitask parameter set to ""rollback"": PythonJavascript

# the first run will be rolled back
rolled_back_run = await client.runs.create(
    thread[""thread_id""],
    assistant_id,
    input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in sf?""}]},
)
await asyncio.sleep(2)
run = await client.runs.create(
    thread[""thread_id""],
    assistant_id,
    input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in nyc?""}]},
    multitask_strategy=""rollback"",
)
# wait until the second run completes
await client.runs.join(thread[""thread_id""], run[""run_id""])

// the first run will be interrupted
let rolledBackRun = await client.runs.create(
  thread[""thread_id""],
  assistantId,
  { input: { messages: [{ role: ""human"", content: ""what's the weather in sf?"" }] } }
);
await new Promise(resolve => setTimeout(resolve, 2000)); 

let run = await client.runs.create(
  thread[""thread_id""],
  assistant_id,
  { 
    input: { messages: [{ role: ""human"", content: ""what's the weather in nyc?"" }] },
    multitaskStrategy: ""rollback"" 
  }
);

// wait until the second run completes
await client.runs.join(thread[""thread_id""], run[""run_id""]);

 We can see that the thread has data only from the second run PythonJavascript

state = await client.threads.get_state(thread[""thread_id""])

for m in convert_to_messages(state[""values""][""messages""]):
    m.pretty_print()

const state = await client.threads.getState(thread[""thread_id""]);

for (const m of state['values']['messages']) {
  prettyPrint(m);
}

 Output: ================================[1m Human Message [0m=================================

what's the weather in nyc?
==================================[1m Ai Message [0m==================================

[{'id': 'toolu_01JzPqefao1gxwajHQ3Yh3JD', 'input': {'query': 'weather in nyc'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01JzPqefao1gxwajHQ3Yh3JD)
 Call ID: toolu_01JzPqefao1gxwajHQ3Yh3JD
  Args:
    query: weather in nyc
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{""url"": ""https://www.weatherapi.com/"", ""content"": ""{'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.71, 'lon': -74.01, 'tz_id': 'America/New_York', 'localtime_epoch': 1718734479, 'localtime': '2024-06-18 14:14'}, 'current': {'last_updated_epoch': 1718733600, 'last_updated': '2024-06-18 14:00', 'temp_c': 29.4, 'temp_f': 84.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 2.2, 'wind_kph': 3.6, 'wind_degree': 158, 'wind_dir': 'SSE', 'pressure_mb': 1025.0, 'pressure_in': 30.26, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 63, 'cloud': 0, 'feelslike_c': 31.3, 'feelslike_f': 88.3, 'windchill_c': 28.3, 'windchill_f': 82.9, 'heatindex_c': 29.6, 'heatindex_f': 85.3, 'dewpoint_c': 18.4, 'dewpoint_f': 65.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 7.0, 'gust_mph': 16.5, 'gust_kph': 26.5}}""}]
==================================[1m Ai Message [0m==================================

The weather API results show that the current weather in New York City is sunny with a temperature of around 85F (29C). The wind is light at around 2-3 mph from the south-southeast. Overall it looks like a nice sunny summer day in NYC.
 Verify that the original, rolled back run was deleted PythonJavascript

try:
    await client.runs.get(thread[""thread_id""], rolled_back_run[""run_id""])
except httpx.HTTPStatusError as _:
    print(""Original run was correctly deleted"")

try {
  await client.runs.get(thread[""thread_id""], rolledBackRun[""run_id""]);
} catch (e) {
  console.log(""Original run was correctly deleted"");
}

 Output: Original run was correctly deleted
","Was the original, rolled back run correctly deleted?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in the interaction pattern described?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What actions can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation using the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step taken to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows and manage state using nodes and edges?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State using reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
"How to add persistence (""memory"") to your graph","Many AI applications need memory to share context across multiple interactions. In LangGraph, memory is provided for any StateGraph through Checkpointers. When creating any LangGraph workflow, you can set them up to persist their state by doing using the following: 
A Checkpointer.
Call compile(checkpointer=my_checkpointer) when compiling the graph.
 There are several options for checkpointers to use. 
MemorySaver is an in-memory key-value store for Graph state.
SqliteSaver allows you to save to a Sqlite db locally or in memory.
There are various external databases that can be used for persistence, such as Postgres, MongoDB, and Redis.
 Here is an example using MemorySaver in memory: from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver

builder = StateGraph(....)
# ... define the graph
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)
...
 This works for StateGraph and all its subclasses, such as MessageGraph. Below is an example. 
Note

        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the create_react_agent(model, tools=tool, checkpointer=checkpointer) (API doc) constructor. This may be more appropriate if you are used to LangChains AgentExecutor class.
    
",How can memory be added to a graph in LangGraph?
Setup,First we need to install the packages required,What is the first step in the setup process?
Set up the State,The state is the interface for all the nodes.,What is the purpose of the state in relation to the nodes?
Set up the tools,"We will first define the tools we want to use.
For this simple example, we will use create a placeholder search engine.
However, it is really easy to create your own tools - see documentation here on how to do that.",What will be the first step in the process?
Set up the model,"Now we need to load the chat model to power our agent.
For the design below, it must satisfy two criteria: 
It should work with messages (since our state contains a list of chat messages)
It should work with tool calling.
 
Note

        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.
    
",What criteria must the model satisfy in order to power the agent for this example?
Define the graph,"We now need to define a few different nodes in our graph.
In langgraph, a node can be either a function or a runnable.
There are two main nodes we need for this: 
The agent: responsible for deciding what (if any) actions to take.
A function to invoke tools: if the agent decides to take an action, this node will then execute that action.
 We will also need to define some edges.
Some of these edges may be conditional.
The reason they are conditional is that based on the output of a node, one of several paths may be taken.
The path that is taken is not known until that node is run (the LLM decides). 
Conditional Edge: after the agent is called, we should either:
a. If the agent said to take an action, then the function to invoke tools should be called
b. If the agent said that it was finished, then it should finish
Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next
 Let's define the nodes, as well as a function to decide how what conditional edge to take.",What are the two main nodes needed in the defined graph?
Interacting with the Agent,We can now interact with the agent and see that it remembers previous messages!,How can we verify that the agent remembers previous messages?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Self RAG using local LLMs,"Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. In the paper, a few decisions are made: 
Should I retrieve from retriever, R -
 
Input: x (question) OR x (question), y (generation)
Decides when to retrieve D chunks with R
Output: yes, no, continue
 
Are the retrieved passages D relevant to the question x -
 

Input: (x (question), d (chunk)) for d in D

d provides useful information to solve x
Output: relevant, irrelevant
 
Are the LLM generation from each chunk in D is relevant to the chunk (hallucinations, etc)  -
 
Input: x (question), d (chunk),  y (generation) for d in D
All of the verification-worthy statements in y (generation) are supported by d
Output: {fully supported, partially supported, no support
 
The LLM generation from each chunk in D is a useful response to x (question) -
 
Input: x (question), y (generation) for d in D
y (generation) is a useful response to x (question).
Output: {5, 4, 3, 2, 1}
 We will implement some of these ideas from scratch using LangGraph.","Are the LLM generation from each chunk in D relevant to the chunk (hallucinations, etc)?"
LLMs,"Local Embeddings You can use GPT4AllEmbeddings() from Nomic, which can access use Nomic's recently released v1 and v1.5 embeddings. Follow the documentation here. Local LLM (1) Download Ollama app. (2) Download a Mistral model from various Mistral versions here and Mixtral versions here available. ollama pull mistral
",What tool can be used to access Nomic's v1 and v1.5 embeddings for local embeddings?
Tracing,"Optionally, use LangSmith for tracing (shown at bottom)","What tool can be used for tracing, as shown at the bottom?"
Index,Let's index 3 blog posts.,What is the purpose of the index in this context?
Graph,Capture the flow in as a graph.,What can be done to capture the flow as a graph?
Build Graph,This just follows the flow we outlined in the figure above.,"What does the ""Build Graph"" process follow according to the text?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to wait for user input,"Human-in-the-loop (HIL) interactions are crucial for agentic systems. Waiting for human input is a common HIL interaction pattern, allowing the agent to ask the user clarifying questions and await input before proceeding. We can implement this in LangGraph using a breakpoint: breakpoints allow us to stop graph execution at a specific step. At this breakpoint, we can wait for human input. Once we have input from the user, we can add it to the graph state and proceed.",How can user input be waited for in LangGraph using breakpoints?
Setup,First we need to install the packages required,What is the first step in the setup process?
Simple Usage,"Let's look at very basic usage of this. One intuitive approach is simply to create a node, human_feedback, that will get user feedback. This allows us to place our feedback gathering at a specific, chosen point in our graph. 
We specify the breakpoint using interrupt_before our human_feedback node.

We set up a checkpointer to save the state of the graph up until this node.

We use .update_state to update the state of the graph with the human response we get.

 
We use the as_node parameter to apply this state update as the specified node, human_feedback.
The graph will then resume execution as if the human_feedback node just acted.
",How can we gather user feedback at a specific point in the graph and update the state of the graph with the human response received?
Agent,"In the context of agents, waiting for user feedback is useful to ask clarifying questions. To show this, we will build a relatively simple ReAct-style agent that does tool calling. We will use OpenAI and / or Anthropic's models and a fake tool (just for demo purposes).",What is the purpose of waiting for user feedback in the context of agents?
Interacting with the Agent,"We can now interact with the agent. Let's ask it to ask the user where they are, then tell them the weather. This should make it use the ask_human tool first, then use the normal tool.",What steps should be taken to make the agent ask the user where they are and then tell them the weather?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
Copying Threads,"You may wish to copy (i.e. ""fork"") an existing thread in order to keep the existing thread's history and create independent runs that do not affect the original thread. This guide shows how you can do that.",How can you create independent runs of an existing thread without affecting the original thread's history?
Setup,This code assumes you already have a thread to copy. You can read about what a thread is here and learn how to stream a run on a thread in these how-to guides.,What does the code assume you already have in order to copy a thread?
SDK initialization,"First, we need to setup our client so that we can communicate with our hosted graph: PythonJavascriptCURL

from langgraph_sdk import get_client
client = get_client(url=""<DEPLOYMENT_URL>"")
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl:""<DEPLOYMENT_URL>"" });
const assistantId = agent;
const thread = await client.threads.create();

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json' \
  --data '{
    ""metadata"": {}
  }'

","How can we initialize the SDK to communicate with our hosted graph using Python, JavaScript, and CURL?"
Copying a thread,"The code below assumes that a thread you'd like to copy already exists. Copying a thread will create a new thread with the same history as the existing thread, and then allow you to continue executing runs.",How can you create a new thread with the same history as an existing thread?
Create copy,"PythonJavascriptCURL

copied_thread = await client.threads.copy(<THREAD_ID>)

let copiedThread = await client.threads.copy(<THREAD_ID>);

curl --request POST --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/copy \
--header 'Content-Type: application/json'

","How can you create a copy of a thread using Python, Javascript, and CURL?"
Verify copy,"We can verify that the history from the prior thread did indeed copy over correctly: PythonJavascriptCURL

def remove_thread_id(d):
  if 'metadata' in d and 'thread_id' in d['metadata']:
      del d['metadata']['thread_id']
  return d

original_thread_history = list(map(remove_thread_id,await client.threads.get_history(<THREAD_ID>)))
copied_thread_history = list(map(remove_thread_id,await client.threads.get_history(copied_thread['thread_id'])))

# Compare the two histories
assert original_thread_history == copied_thread_history
# if we made it here the assertion passed!
print(""The histories are the same."")

function removeThreadId(d) {
    if (d.metadata && d.metadata.thread_id) {
        delete d.metadata.thread_id;
    }
    return d;
}

// Assuming `client.threads.getHistory(threadId)` is an async function that returns a list of dicts
async function compareThreadHistories(threadId, copiedThreadId) {
    const originalThreadHistory = (await client.threads.getHistory(threadId)).map(removeThreadId);
    const copiedThreadHistory = (await client.threads.getHistory(copiedThreadId)).map(removeThreadId);

    // Compare the two histories
    console.assert(JSON.stringify(originalThreadHistory) === JSON.stringify(copiedThreadHistory))
    // if we made it here the assertion passed!
    console.log(""The histories are the same."");
}

// Example usage
compareThreadHistories(<THREAD_ID>, copiedThread.thread_id);

if diff <(
    curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -S 'map(del(.metadata.thread_id))'
) <(
    curl --request GET --url <DEPLOYMENT_URL>/threads/<COPIED_THREAD_ID>/history | jq -S 'map(del(.metadata.thread_id))'
) >/dev/null; then
    echo ""The histories are the same.""
else
    echo ""The histories are different.""
fi

 Output: The histories are the same.
",Are the histories from the original thread and the copied thread the same?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Cron Jobs,"Sometimes you don't want to run your graph based on user interaction, but rather you would like to schedule your graph to run on a schedule - for example if you wish for your graph to compose and send out a weekly email of to-dos for your team. LangGraph Cloud allows you to do this without having to write your own script by using the Crons client. To schedule a graph job, you need to pass a cron expression to inform the client when you want to run the graph. Cron jobs are run in the background and do not interfere with normal invocations of the graph.",How can you schedule a graph job in LangGraph Cloud without having to write your own script?
Setup,"First, let's setup our SDK client, assistant, and thread:",What components need to be set up before beginning the task?
Cron job on a thread,"To create a cron job associated with a specific thread, you can write:",How can you create a cron job associated with a specific thread?
Cron job stateless,You can also create stateless cron jobs by using the following code:,How can you create stateless cron jobs using code?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
LangGraph Cloud (beta),"
Tip

LangGraph is an MIT-licensed open-source library, which we are committed to maintaining and growing for the community.
LangGraph Cloud is an optional managed hosting service for LangGraph, which provides additional features geared towards production deployments.
We are actively contributing improvements back to LangGraph informed by our work on LangGraph Cloud.
You can always deploy LangGraph applications on your own infrastructure using the open-source LangGraph project.

 
Under Construction
LangGraph Cloud documentation is under construction. Contents may change until general availability.
 

",What is LangGraph Cloud and how does it differ from the open-source LangGraph project?
Overview,"LangGraph Cloud is a managed service for deploying and hosting LangGraph applications. Deploying applications with LangGraph Cloud shortens the time-to-market for developers. With one click, deploy a production-ready API with built-in persistence for your LangGraph application. LangGraph Cloud APIs are horizontally scalable and deployed with durable storage. The LangGraph Cloud API exposes functionality of your LangGraph application through Assistants. An assistant abstracts the cognitive architecture of your graph. Invoke an assistant by calling the pre-built API endpoints. LangGraph Cloud is seamlessly integrated with LangSmith and is accessible from within the LangSmith UI.",What are some key features of LangGraph Cloud and how does it integrate with LangSmith?
Key Features,"The LangGraph Cloud API supports key LangGraph features in addition to new functionality for enabling complex, agentic workflows. 
Assistants and Threads: Assistants abstract the cognitive architecture of graphs and threads track the state/history of graphs.
Streaming: API support for LangGraph streaming modes including setting multiple streaming modes at the same time.
Human-in-the-Loop: API support for LangGraph human-in-the-loop features.
Double Texting: Configure how assistants respond when new input is received while processing a previous input. Interrupt, rollback, reject, or enqueue.
Background Runs/Cron Jobs: A built-in task queue enables background runs and scheduled cron jobs.
Stateless Runs: For simpler use cases, invoke an assistant without needing to create a thread.
",What are some of the key features supported by the LangGraph Cloud API?
Documentation,"
Tutorials: Learn to build and deploy applications for LangGraph Cloud.
How-to Guides: Learn how to set up a LangGraph application for deployment and implement features of the LangGraph Cloud API such as streaming tokens, configuring double texting, and creating cron jobs. Go here if you want to copy and run a specific code snippet.
Conceptual Guides: In-depth explanations of the core data models (e.g. assistants), key features of the LangGraph Cloud API (e.g. double texting), and the architecture of a LangGraph Cloud deployment.
Reference: References for the LangGraph Cloud API, the corresponding Python and JS/TS SDKs, the LangGraph CLI, and deployment environment variables.
",What types of guides and references are available in the Documentation section for LangGraph Cloud?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Quick Start,"In this comprehensive quick start, we will build a support chatbot in LangGraph that can: 
Answer common questions by searching the web
Maintain conversation state across calls
Route complex queries to a human for review
Use custom state to control its behavior
Rewind and explore alternative conversation paths
 We'll start with a basic chatbot and progressively add more sophisticated capabilities, introducing key LangGraph concepts along the way.",What capabilities will be added to the support chatbot in LangGraph during the comprehensive quick start?
Setup,"First, install the required packages:",What is the first step in setting up the system?
Part 1: Build a Basic Chatbot,"We'll first create a simple chatbot using LangGraph. This chatbot will respond directly to user messages. Though simple, it will illustrate the core concepts of building with LangGraph. By the end of this section, you will have a built rudimentary chatbot. Start by creating a StateGraph. A StateGraph object defines the structure of our chatbot as a ""state machine"". We'll add nodes to represent the llm and functions our chatbot can call and edges to specify how the bot should transition between these functions.",What is the first step in building a basic chatbot using LangGraph?
Part 2: Enhancing the Chatbot with Tools,"To handle queries our chatbot can't answer ""from memory"", we'll integrate a web search tool. Our bot can use this tool to find relevant information and provide better responses. Requirements Before we start, make sure you have the necessary packages installed and API keys set up: First, install the requirements to use the Tavily Search Engine, and set your TAVILY_API_KEY.",What tool will be integrated into the chatbot to handle queries it can't answer from memory?
Part 3: Adding Memory to the Chatbot,"Our chatbot can now use tools to answer user questions, but it doesn't remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations. LangGraph solves this problem through persistent checkpointing. If you provide a checkpointer when compiling the graph and a thread_id when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off. We will see later that checkpointing is much more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more. But before we get too ahead of ourselves, let's add checkpointing to enable multi-turn conversations. To get started, create a MemorySaver checkpointer.","How can LangGraph help improve the chatbot's ability to have coherent, multi-turn conversations?"
Part 4: Human-in-the-loop,"Agents can be unreliable and may need human input to successfully accomplish tasks. Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended. LangGraph supports human-in-the-loop workflows in a number of ways. In this section, we will use LangGraph's interrupt_before functionality to always break the tool node. First, start from our existing code. The following is copied from Part 3.",How does LangGraph support human-in-the-loop workflows?
Part 5: Manually Updating the State,"In the previous section, we showed how to interrupt a graph so that a human could inspect its actions. This lets the human read the state, but if they want to change their agent's course, they'll need to have write access. Thankfully, LangGraph lets you manually update state! Updating the state lets you control the agent's trajectory by modifying its actions (even modifying the past!). This capability is particularly useful when you want to correct the agent's mistakes, explore alternative paths, or guide the agent towards a specific goal. We'll show how to update a checkpointed state below. As before, first, define your graph. We'll reuse the exact same graph as before.",How can you manually update the state in LangGraph to control the agent's trajectory and modify its actions?
Part 6: Customizing State,"So far, we've relied on a simple state (it's just a list of messages!). You can go far with this simple state, but if you want to define complex behavior without relying on the message list, you can add additional fields to the state. In this section, we will extend our chat bot with a new node to illustrate this. In the examples above, we involved a human deterministically: the graph always interrupted whenever an tool was invoked. Suppose we wanted our chat bot to have the choice of relying on a human. One way to do this is to create a passthrough ""human"" node, before which the graph will always stop. We will only execute this node if the LLM invokes a ""human"" tool. For our convenience, we will include an ""ask_human"" flag in our graph state that we will flip if the LLM calls this tool. Below, define this new graph, with an updated State","How can you customize the state of a chat bot to include a ""human"" node and an ""ask_human"" flag?"
Part 7: Time Travel,"In a typical chat bot workflow, the user interacts with the bot 1 or more times to accomplish a task. In the previous sections, we saw how to add memory and a human-in-the-loop to be able to checkpoint our graph state and manually override the state to control future responses. But what if you want to let your user start from a previous response and ""branch off"" to explore a separate outcome? Or what if you want users to be able to ""rewind"" your assistant's work to fix some mistakes or try a different strategy (common in applications like autonomous software engineers)? You can create both of these experiences and more using LangGraph's built-in ""time travel"" functionality. In this section, you will ""rewind"" your graph by fetching a checkpoint using the graph's get_state_history method. You can then resume execution at this previous point in time. First, recall our chatbot graph. We don't need to make any changes from before:","How can you create experiences such as branching off to explore separate outcomes or rewinding to fix mistakes using LangGraph's built-in ""time travel"" functionality?"
Conclusion,"Congrats! You've completed the intro tutorial and built a chat bot in LangGraph that supports tool calling, persistent memory, human-in-the-loop interactivity, and even time-travel! The LangGraph documentation is a great resource for diving deeper into the library's capabilities.",What features does the chat bot built in LangGraph support?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling your graph and why is it needed?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State using reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes based on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Review Tool Calls,"Human-in-the-loop (HIL) interactions are crucial for agentic systems. A common pattern is to add some human in the loop step after certain tool calls. These tool calls often lead to either a function call or saving of some information. Examples include: 
A tool call to execute SQL, which will then be run by the tool
A tool call to generate a summary, which will then be saved to the State of the graph
 Note that using tool calls is common whether actually calling tools or not. There are typically a few different interactions you may want to do here: 
Approve the tool call and continue
Modify the tool call manually and then continue
Give natural language feedback, and then pass that back to the agent instead of continuing
 We can implement this in LangGraph using a breakpoint: breakpoints allow us to interrupt graph execution before a specific step. At this breakpoint, we can manually update the graph state taking one of the three options above",What are some common interactions that can be done after certain tool calls in agentic systems?
Setup,First we need to install the packages required,What is the first step in the setup process?
Simple Usage,"Let's set up a very simple graph that facilitates this.
First, we will have an LLM call that decides what action to take.
Then we go to a human node. This node actually doesn't do anything - the idea is that we interrupt before this node and then apply any updates to the state.
After that, we check the state and either route back to the LLM or to the correct tool. Let's see this in action!",What is the process for setting up a simple graph that facilitates decision-making and state updates?
Example with no review,Let's look at an example when no review is required (because no tools are called),What is an example scenario where no review is required?
Example of approving tool,Let's now look at what it looks like to approve a tool call,What will be shown as an example of approving a tool call?
Edit Tool Call,Let's now say we want to edit the tool call. E.g. change some of the parameters (or even the tool called!) but then execute that tool.,What action can be taken if we want to edit the tool call and execute it with the changes made to the parameters or the tool itself?
Give feedback to a tool call,"Sometimes, you may not want to execute a tool call, but you also may not want to ask the user to manually modify the tool call. In that case it may be better to get natural language feedback from the user. You can then insert these feedback as a mock RESULT of the tool call. There are multiple ways to do this: 
You could add a new message to the state (representing the ""result"" of a tool call)
You could add TWO new messages to the state - one representing an ""error"" from the tool call, other HumanMessage representing the feedback
 Both are similar in that they involve adding messages to the state. The main difference lies in the logic AFTER the human_node and how it handles different types of messages. For this example we will just add a single tool call representing the feedback. Let's see this in action!",How can natural language feedback from the user be incorporated as a mock result of a tool call?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows and manage state using nodes and edges?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling a graph and why is it necessary?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges or terminate using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes based on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What method can be used to retrieve the state history of a graph in Python?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you access and use a configuration inside a node when creating a graph?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
Invoke Assistant,"The LangGraph Studio lets you test different configurations and inputs to your graph. It also provides a nice visualization of your graph during execution so it is easy to see which nodes are being run and what the outputs of each individual node are. 
The LangGraph Studio UI displays a visualization of the selected assistant.
In the top-left dropdown menu of the left-hand pane, select an assistant.
In the bottom of the left-hand pane, edit the Input and Configure the assistant.
Select Submit to invoke the selected assistant.

View output of the invocation in the right-hand pane.
 The following video shows these exact steps being carried out: 

",How can you invoke an assistant in the LangGraph Studio?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to Add Breakpoints,"When creating LangGraph agents, it is often nice to add a human-in-the-loop component.
This can be helpful when giving them access to tools.
Often in these situations you may want to manually approve an action before taking. This can be in several ways, but the primary supported way is to add an ""interrupt"" before a node is executed.
This interrupts execution at that node.
You can then resume from that spot to continue.  ",What is the primary supported way to add breakpoints when creating LangGraph agents?
Code for your graph,"In this how-to we use a simple ReAct style hosted graph (you can see the full code for defining it here). The important thing is that there are two nodes (one named agent that calls the LLM, and one named action that calls the tool), and a routing function from agent that determines whether to call action next or just end the graph run (the action node always calls the agent node after execution).",What are the two nodes in the ReAct style hosted graph and how is the routing function between them defined?
SDK Initialization,"PythonJavascriptCURL

from langgraph_sdk import get_client
client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent""
const thread = await client.threads.create();

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

",What are the languages used for SDK initialization in the given code snippet?
Adding a breakpoint,"We now want to add a breakpoint in our graph run, which we will do before a tool is called.
We can do this by adding interrupt_before=[""action""], which tells us to interrupt before calling the action node.
We can do this either when compiling the graph or when kicking off a run.
Here we will do it when kicking of a run, if you would like to to do it at compile time you need to edit the python file where your graph is defined and add the interrupt_before parameter when you call .compile. First let's access our hosted LangGraph instance through the SDK: And, now let's compile it with a breakpoint before the tool node: PythonJavascriptCURL

input = {""messages"": [{""role"": ""human"", ""content"": ""what's the weather in sf""}]}
async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id,
    input=input,
    stream_mode=""updates"",
    interrupt_before=[""action""],
):
    print(f""Receiving new event of type: {chunk.event}..."")
    print(chunk.data)
    print(""\n\n"")

const input = { ""messages"": [{ ""role"": ""human"", ""content"": ""what's the weather in sf""}] }

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""updates"",
    interruptBefore: [""action""],
  }
);
for await (const chunk of streamResponse) {
  console.log(`Receiving new event of type: ${chunk.event}...`);
  console.log(chunk.data);
  console.log(""\n\n"");
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""what's the weather in sf\""}]},
   \""interrupt_before\"": [\""action\""],
   \""stream_mode\"": [
     \""messages\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """") {
         print data_content ""\n""
     }
     sub(/^event: /, ""Receiving event of type: "", $0)
     printf ""%s...\n"", $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """") {
         print data_content ""\n""
     }
 }
 '

 Output: Receiving new event of type: metadata...
{'run_id': '3b77ef83-687a-4840-8858-0371f91a92c3'}

Receiving new event of type: data...
{'agent': {'messages': [{'content': [{'id': 'toolu_01HwZqM1ptX6E15A5LAmyZTB', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-e5d17791-4d37-4ad2-815f-a0c4cba62585', 'example': False, 'tool_calls': [{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01HwZqM1ptX6E15A5LAmyZTB'}], 'invalid_tool_calls': []}]}}

Receiving new event of type: end...
None
",How can a breakpoint be added in a graph run before calling a tool node?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Enqueue,"This guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide. The guide covers the enqueue option for double texting, which adds the interruptions to a queue and executes them in the order they are received by the client. Below is a quick example of using the enqueue option. First, we will define a quick helper function for printing out JS model outputs (you can skip this if using Python): function prettyPrint(m) {
  const padded = "" "" + m['type'] + "" "";
  const sepLen = Math.floor((80 - padded.length) / 2);
  const sep = ""="".repeat(sepLen);
  const secondSep = sep + (padded.length % 2 ? ""="" : """");

  console.log(`${sep}${padded}${secondSep}`);
  console.log(""\n\n"");
  console.log(m.content);
}
 Then, let's import our required packages and instantiate our client, assistant, and thread. PythonJavascript

import asyncio

import httpx
from langchain_core.messages import convert_to_messages
from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent"";
const thread = await client.threads.create();

 Now let's start two runs, with the second interrupting the first one with a multitask strategy of ""enqueue"": PythonJavascript

first_run = await client.runs.create(
    thread[""thread_id""],
    assistant_id,
    input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in sf?""}]},
)
second_run = await client.runs.create(
    thread[""thread_id""],
    assistant_id,
    input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in nyc?""}]},
    multitask_strategy=""enqueue"",
)

const firstRun = await client.runs.create(
  thread[""thread_id""],
  assistantId,
  input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in sf?""}]},
)

const secondRun = await client.runs.create(
  thread[""thread_id""],
  assistantId,
  input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in nyc?""}]},
  multitask_strategy=""enqueue"",
)

 Verify that the thread has data from both runs: PythonJavascript

# wait until the second run completes
await client.runs.join(thread[""thread_id""], second_run[""run_id""])

state = await client.threads.get_state(thread[""thread_id""])

for m in convert_to_messages(state[""values""][""messages""]):
    m.pretty_print()

await client.runs.join(thread[""thread_id""], secondRun[""run_id""]);

const state = await client.threads.getState(thread[""thread_id""]);

for (const m of state[""values""][""messages""]) {
  prettyPrint(m);
}

 Output: ================================[1m Human Message [0m=================================

what's the weather in sf?
==================================[1m Ai Message [0m==================================

[{'id': 'toolu_01Dez1sJre4oA2Y7NsKJV6VT', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01Dez1sJre4oA2Y7NsKJV6VT)
 Call ID: toolu_01Dez1sJre4oA2Y7NsKJV6VT
  Args:
    query: weather in san francisco
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{""url"": ""https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629"", ""content"": ""Get the current and future weather conditions for San Francisco, CA, including temperature, precipitation, wind, air quality and more. See the hourly and 10-day outlook, radar maps, alerts and allergy information.""}]
==================================[1m Ai Message [0m==================================

According to AccuWeather, the current weather conditions in San Francisco are:

Temperature: 57F (14C)
Conditions: Mostly Sunny
Wind: WSW 10 mph
Humidity: 72%

The forecast for the next few days shows partly sunny skies with highs in the upper 50s to mid 60s F (14-18C) and lows in the upper 40s to low 50s F (9-11C). Typical mild, dry weather for San Francisco this time of year.

Some key details from the AccuWeather forecast:

Today: Mostly sunny, high of 62F (17C)
Tonight: Partly cloudy, low of 49F (9C) 
Tomorrow: Partly sunny, high of 59F (15C)
Saturday: Mostly sunny, high of 64F (18C)
Sunday: Partly sunny, high of 61F (16C)

So in summary, expect seasonable spring weather in San Francisco over the next several days, with a mix of sun and clouds and temperatures ranging from the upper 40s at night to the low 60s during the days. Typical dry conditions with no rain in the forecast.
================================[1m Human Message [0m=================================

what's the weather in nyc?
==================================[1m Ai Message [0m==================================

[{'text': 'Here are the current weather conditions and forecast for New York City:', 'type': 'text'}, {'id': 'toolu_01FFft5Sx9oS6AdVJuRWWcGp', 'input': {'query': 'weather in new york city'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01FFft5Sx9oS6AdVJuRWWcGp)
 Call ID: toolu_01FFft5Sx9oS6AdVJuRWWcGp
  Args:
    query: weather in new york city
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{""url"": ""https://www.weatherapi.com/"", ""content"": ""{'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.71, 'lon': -74.01, 'tz_id': 'America/New_York', 'localtime_epoch': 1718734479, 'localtime': '2024-06-18 14:14'}, 'current': {'last_updated_epoch': 1718733600, 'last_updated': '2024-06-18 14:00', 'temp_c': 29.4, 'temp_f': 84.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 2.2, 'wind_kph': 3.6, 'wind_degree': 158, 'wind_dir': 'SSE', 'pressure_mb': 1025.0, 'pressure_in': 30.26, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 63, 'cloud': 0, 'feelslike_c': 31.3, 'feelslike_f': 88.3, 'windchill_c': 28.3, 'windchill_f': 82.9, 'heatindex_c': 29.6, 'heatindex_f': 85.3, 'dewpoint_c': 18.4, 'dewpoint_f': 65.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 7.0, 'gust_mph': 16.5, 'gust_kph': 26.5}}""}]
==================================[1m Ai Message [0m==================================

According to the weather data from WeatherAPI:

Current Conditions in New York City (as of 2:00 PM local time):
- Temperature: 85F (29C)
- Conditions: Sunny
- Wind: 2 mph (4 km/h) from the SSE
- Humidity: 63%
- Heat Index: 85F (30C)

The forecast shows sunny and warm conditions persisting over the next few days:

Today: Sunny, high of 85F (29C)
Tonight: Clear, low of 68F (20C)
Tomorrow: Sunny, high of 88F (31C) 
Thursday: Mostly sunny, high of 90F (32C)
Friday: Partly cloudy, high of 87F (31C)

So New York City is experiencing beautiful sunny weather with seasonably warm temperatures in the mid-to-upper 80s Fahrenheit (around 30C). Humidity is moderate in the 60% range. Overall, ideal late spring/early summer conditions for being outdoors in the city over the next several days.
",What is the multitask strategy used in the example provided for the enqueue option in double texting?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How-to Guides,"Welcome to the LangGraph Cloud how-to guides! These guides provide practical, step-by-step instructions for accomplishing key tasks in LangGraph Cloud.",What type of instructions do the LangGraph Cloud how-to guides provide?
Deployment,"LangGraph Cloud gives you best in class observability, testing, and hosting services. Read more about them in these how to guides: 
How to set up app for deployment (requirements.txt)
How to set up app for deployment (pyproject.toml)
How to test locally
How to deploy to LangGraph cloud
How to self-host
",What are the available how-to guides for setting up an app for deployment and testing with LangGraph Cloud?
Streaming,"Streaming the results of your LLM application is vital for ensuring a good user experience, especially when your graph may call multiple models and take a long time to fully complete a run. Read about how to stream values from your graph in these how to guides: 
How to stream values
How to stream updates
How to stream messages
How to stream events
How to stream in debug mode
How to stream multiple modes
",What are some examples of how to stream values from your graph in LLM application?
Double-texting,"Graph execution can take a while, and sometimes users may change their mind about the input they wanted to send before their original input has finished running. For example, a user might notice a typo in their original request and will edit the prompt and resend it. Deciding what to do in these cases is important for ensuring a smooth user experience and preventing your graphs from behaving in unexpected ways. The following how-to guides provide information on the various options LangGraph Cloud gives you for dealing with double-texting: 
How to use the interrupt option
How to use the rollback option
How to use the reject option
How to use the enqueue option
",What are the various options LangGraph Cloud provides for dealing with double-texting?
Human-in-the-loop,"When creating complex graphs, leaving every decision up to the LLM can be dangerous, especially when the decisions involve invoking certain tools or accessing specific documents. To remedy this, LangGraph allows you to insert human-in-the-loop behavior to ensure your graph does not have undesired outcomes. Read more about the different ways you can add human-in-the-loop capabilities to your LangGraph Cloud projects in these how-to guides: 
How to add a breakpoint
How to wait for user input
How to edit graph state
How to replay and branch from prior states
How to review tool calls
",What capabilities does LangGraph offer to insert human-in-the-loop behavior when creating complex graphs?
LangGraph Studio,"LangGraph Studio is a built-in UI for visualizing, testing, and debugging your agents. 
How to enter LangGraph Studio
How to enter LangGraph Studio for local deployment
How to test your graph in LangGraph Studio
Interact with threads in LangGraph Studio
",How can you interact with threads in LangGraph Studio?
Different Types of Runs:,"LangGraph Cloud supports multiple types of runs besides streaming runs. 
How to run an agent in the background
How to run multiple agents in the same thread
How to create cron jobs
How to create stateless runs
",What types of runs does LangGraph Cloud support besides streaming runs?
Other,"Other guides that may prove helpful! 
How to configure agents
How to convert LangGraph calls to LangGraph cloud calls
How to integrate webhooks
How to copy threads
How to check status of your threads
",What are some examples of other guides that may prove helpful?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
API Concepts,This page describes the high-level concepts of the LangGraph Cloud API. The conceptual guide of LangGraph (Python library) is here.,What does the page about API Concepts describe?
Data Models,"The LangGraph Cloud API consists of a few core data models: Assistants, Threads, Runs, and Cron Jobs.",What are the core data models included in the LangGraph Cloud API?
Assistants,"An assistant is a configured instance of a CompiledGraph. It abstracts the cognitive architecture of the graph and contains instance specific configuration and metadata. Multiple assistants can reference the same graph but can contain different configuration and metadata, which may differentiate the behavior of the assistants. An assistant (i.e. the graph) is invoked as part of a run. The LangGraph Cloud API provides several endpoints for creating and managing assistants. See the API reference for more details. Configuring Assistants You can save custom assistants from the same graph to set different default prompts, models, and other configurations without changing a line of code in your graph. This allows you the ability to quickly test out different configurations without having to rewrite your graph every time, and also give users the flexibility to select different configurations when using your LangGraph application. See this how-to for information on how to configure a deployed graph. ","How can custom assistants be saved from the same graph to set different default prompts, models, and other configurations without changing any code?"
Threads,"A thread contains the accumulated state of a group of runs. If a run is executed on a thread, then the state of the underlying graph of the assistant will be persisted to the thread. A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The state of a thread at a particular point in time is called a checkpoint. For more on threads and checkpoints, see this section of the LangGraph conceptual guide. The LangGraph Cloud API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.",What is the purpose of creating a thread before executing a run in LangGraph?
Runs,"A run is an invocation of an assistant. Each run may have its own input, configuration, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a thread. The LangGraph Cloud API provides several endpoints for creating and managing runs. See the API reference for more details.",What endpoints does the LangGraph Cloud API provide for creating and managing runs?
Cron Jobs,"It's often useful to run graphs on some schedule. LangGraph Cloud supports cron jobs, which run on a user defined schedule. The user specifies a schedule, an assistant, and some input. After than, on the specified schedule LangGraph cloud will: 
Create a new thread with the specified assistant
Send the specified input to that thread
 Note that this sends the same input to the thread every time. See the how-to guide for creating cron jobs. The LangGraph Cloud API provides several endpoints for creating and managing cron jobs. See the API reference for more details.",What features does LangGraph Cloud support for running graphs on a schedule?
Features,The LangGraph Cloud API offers several features to support complex agent architectures.,What does the LangGraph Cloud API offer to support complex agent architectures?
Streaming,"Streaming is critical for making LLM applications feel responsive to end users. When creating a streaming run, the streaming mode determines what data is streamed back to the API client. The LangGraph Cloud API supports five streaming modes. 
values: Stream the full state of the graph after each node is executed. See the how-to guide for streaming values.
messages: Stream complete messages (at the end of node execution) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications. This is only an option if your graph contains a messages key. See the how-to guide for streaming messages.
updates: Streams updates to the state of the graph after each node is executed. See the how-to guide for streaming updates.
events: Stream all events (including the state of the graph) after each node is executed. See the how-to guide for streaming events. This can be used to do token-by-token streaming for LLMs.
debug: Stream debug events after each node is executed. See the how-to guide for streaming debug events.
 You can also specify multiple streaming modes at the same time. See the how-to guide for configuring multiple streaming modes at the same time. See the API reference for how to create streaming runs.",What are the five streaming modes supported by the LangGraph Cloud API?
Human-in-the-Loop,"There are many occasions where the graph cannot run completely autonomously. For instance, the user might need to input some additional arguments to a function call, or select the next edge for the graph to continue on. In these instances, we need to insert some human in the loop interaction, which you can learn about in the human in the loop how-tos.",What are some examples of situations where human-in-the-loop interaction is necessary for the graph to run?
Double Texting,"Many times users might interact with your graph in unintended ways. For instance, a user may send one message and before the graph has finished running send a second message. To solve this issue of ""double-texting"" (i.e. prompting the graph a second time before the first run has finished), LangGraph has provided four different solutions, all of which are covered in the Double Texting how-tos. These options are: 
reject: This is the simplest option, this just rejects any follow up runs and does not allow double texting. See the how-to guide for configuring the reject double text option.
enqueue: This is a relatively simple option which continues the first run until it completes the whole run, then sends the new input as a separate run. See the how-to guide for configuring the enqueue double text option.
interrupt: This option interrupts the current execution but saves all the work done up until that point. It then inserts the user input and continues from there. If you enable this option, your graph should be able to handle weird edge cases that may arise. See the how-to guide for configuring the interrupt double text option.
rollback: This option rolls back all work done up until that point. It then sends the user input in, basically as if it just followed the original run input. See the how-to guide for configuring the rollback double text option.
","What are the four different solutions provided by LangGraph to address the issue of ""double-texting"" when interacting with a graph?"
Stateless Runs,"All runs use the built-in checkpointer to store checkpoints for runs. However, it can often be useful to just kick off a run without worrying about explicitly creating a thread and without wanting to keep those checkpointers around. Stateless runs allow you to do this by exposing an endpoint that: 
Takes in user input
Under the hood, creates a thread
Runs the agent but skips all checkpointing steps
Cleans up the thread afterwards
 Stateless runs are still retried as regular retries are per node, while everything still in memory, so doesn't use checkpoints. The only difference is in stateless background runs, if the task worker dies halfway (not because the run itself failed, for some external reason) then the whole run will be retried like any background run, but 
whereas a stateful background run would retry from the last successful checkpoint
a stateless background run would retry from the beginning
 See the how-to guide for creating stateless runs.",What is the purpose of stateless runs and how do they differ from stateful background runs?
Webhooks,"For all types of runs, langgraph cloud supports completion webhooks. When you create the run you can pass a webhook URL to be called when the completes (successfully or not). This is especially useful for background runs and cron jobs, as the webhook can give you an indication the run has completed and you can perform further actions for your appilcation. See this how-to guide to learn about how to use webhooks with LangGraph Cloud.",What is the purpose of completion webhooks in LangGraph Cloud and how can they be utilized for background runs and cron jobs?
Deployment,The LangGraph Cloud offers several features to support secure and robost deployments.,What features does the LangGraph Cloud offer to support secure and robust deployments?
Authentication,"LangGraph applications deployed to LangGraph Cloud are automatically configured with LangSmith authentication. In order to call the API, a valid LangSmith API key is required.",What is required in order to call the API for LangGraph applications deployed to LangGraph Cloud?
Local Testing,"Before deploying your app in production to LangGraph Cloud, you may wish to test out your graph locally in order to ensure that everything is running as expected. Luckily, LangGraph makes this easy for you through use of the LangGraph CLI. Read more in this how-to guide or look at the CLI reference to learn more.",What tool can you use to test your graph locally before deploying your app in production to LangGraph Cloud?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
How to add a custom system prompt to the prebuilt ReAct agent,This tutorial will show how to add a custom system prompt to the prebuilt ReAct agent. Please see this tutorial for how to get started with the prebuilt ReAct agent You can add a custom system prompt by passing a string to the state_modifier param.,How can a custom system prompt be added to the prebuilt ReAct agent?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to create a custom checkpointer using Redis,"When creating LangGraph agents, you can also set them up so that they persist their state. This allows you to do things like interact with an agent multiple times and have it remember previous interactions. This reference implementation shows how to use Redis as the backend for persisting checkpoint state. Make sure that you have Redis running on port 6379 for going through this guide. NOTE: this is just an reference implementation. You can implement your own checkpointer using a different database or modify this one as long as it conforms to the BaseCheckpointSaver interface.",What is required to have Redis running on in order to create a custom checkpointer using Redis?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Language Agent Tree Search,"Language Agent Tree Search (LATS), by Zhou, et. al, is a general LLM agent search algorithm that combines reflection/evaluation and search (specifically monte-carlo trees search) to get achieve better overall task performance compared to similar techniques like ReACT, Reflexion, or Tree of Thoughts. It has four main steps: 
Select: pick the best next actions based on the aggregate rewards from step (2). Either respond (if a solution is found or the max search depth is reached) or continue searching.
Expand and simulate: select the ""best"" 5 potential actions to take and execute them in parallel.
Reflect + Evaluate: observe the outcomes of these actions and score the decisions based on reflection (and possibly external feedback)
Backpropagate: update the scores of the root trajectories based on the outcomes.
",What are the four main steps of the Language Agent Tree Search (LATS) algorithm?
0. Prerequisites,"Install langgraph (for the framework), langchain_openai (for the LLM), and langchain + tavily-python (for the search engine). We will use tavily search as a tool. You can get an API key here or replace with a different tool of your choosing.",What tools are required as prerequisites for this project?
Graph State,"LATS is based on a  (greedy) Monte-Carlo tree search. For each search steps, it picks the node with the highest ""upper confidence bound"", which is a metric that balances exploitation (highest average reward) and exploration (lowest visits). Starting from that node, it generates N (5 in this case) new candidate actions to take, and adds them to the tree. It stops searching either when it has generated a valid solution OR when it has reached the maximum number of rollouts (search tree depth). Our LangGraph state will be composed of two items: 
The root of the search tree
The user input
",What is the basis of the LATS algorithm for Graph State?
Define Language Agent,"Our agent will have three primary LLM-powered processes: 
Reflect: score the action based on the tool response.
Initial response: to create the root node and start the search.
Expand: generate 5 candidate ""next steps"" from the best spot in the current tree
 For more ""Grounded"" tool applications (such as code synthesis), you could integrate code execution into the reflection/reward step. This type of external feedback is very useful (though adds complexity to an already complicated example notebook).",What are the three primary LLM-powered processes of our language agent?
Reflection,"The reflection chain will score agent outputs based on the decision and the tool responses.
We will call this within the other two nodes.",What will the reflection chain score agent outputs based on?
Initial Response,"We start with a single root node, generated by this first step. It responds to the user input either with a tool invocation or a response.",What does the single root node generated in the initial response do in response to user input?
Candidate Generation,The following code prompts the same LLM to generate N additional candidates to check.,What does the code prompt the LLM to generate in the context of candidate generation?
Create Graph,"With those two nodes defined, we are ready to define the graph. After each agent step, we have the option of finishing.",What are we ready to define after defining two nodes and each agent step?
Conclusion,"Congrats on implementing LATS! This is a technique that can be reasonably fast and effective at solving complex reasoning tasks. A few notes that you probably observed above: 
While effective , the tree rollout can take additional compute time. If you wanted to include this in a production app, you'd either want to ensure that intermediate steps are streamed (so the user sees the thinking process/has access to intermediate results) or use it for fine-tuning data to improve the single-shot accuracy and avoid long rollouts.
The candidate selection process is only as good as the reward you generate. Here we are using self-reflection exclusively, but if you have an external source of feedback (such as code test execution), that should be incorporated in the locations mentioned above.
",How can the tree rollout process be optimized for a production app using LATS?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step taken to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct agent architecture in LangGraph and the ReAct paper it is based on?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",What is compiling a graph and why is it necessary?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the behavior of agents and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",How does LangGraph support the design pattern of map-reduce by allowing the return of Send objects from conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using the graph.get_state method?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What method can be used to retrieve the state history of a graph in Python?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
LLMCompiler,"This notebook shows how to implement LLMCompiler, by Kim, et. al in LangGraph. LLMCompiler is an agent architecture designed to speed up the execution of agentic tasks by eagerly-executed tasks within a DAG. It also saves costs on redundant token usage by reducing the number of calls to the LLM. Below is an overview of its computational graph: It has 3 main components: 
Planner: stream a DAG of tasks.
Task Fetching Unit: schedules and executes the tasks as soon as they are executable
Joiner: Responds to the user or triggers a second plan
 This notebook walks through each component and shows how to wire them together using LangGraph. The end result will leave a trace like the following. First, install the dependencies, and set up LangSmith for tracing to more easily debug and observe the agent.","What are the three main components of the LLMCompiler agent architecture designed by Kim, et. al in LangGraph?"
Part 1: Tools,"We'll first define the tools for the agent to use in our demo. We'll give it the class search engine + calculator combo. If you don't want to sign up for tavily, you can replace it with the free DuckDuckGo.",What tools will the agent be using in the demo?
Part 2: Planner,"Largely adapted from the original source code, the planner  accepts the input question and generates a task list to execute. If it is provided with a previous plan, it is instructed to re-plan, which is useful if, upon completion of the first batch of tasks, the agent must take more actions. The code below composes constructs the prompt template for the planner and composes it with LLM and output parser, defined in output_parser.py. The output parser processes a task list in the following form: plaintext
1. tool_1(arg1=""arg1"", arg2=3.5, ...)
Thought: I then want to find out Y by using tool_2
2. tool_2(arg1="""", arg2=""${1}"")'
3. join()<END_OF_PLAN>""
 The ""Thought"" lines are optional. The ${#} placeholders are variables. These are used to route tool (task) outputs to other tools.",How does the planner generate a task list to execute and what is the purpose of re-planning in the process?
3. Task Fetching Unit,"This component schedules the tasks. It receives a stream of tools of the following format: {
    tool: BaseTool,
    dependencies: number[],
}
 The basic idea is to begin executing tools as soon as their dependencies are met. This is done through multi-threading. We will combine the task fetching unit and executor below:",What is the purpose of the Task Fetching Unit component in the system?
"4. ""Joiner""","So now we have the planning and initial execution done. We need a component to process these outputs and either: 
Respond with the correct answer.
Loop with a new plan.
 The paper refers to this as the ""joiner"". It's another LLM call. We are using function calling to improve parsing reliability.","What is the purpose of the ""joiner"" component mentioned in the text?"
5. Compose using LangGraph,"We'll define the agent as a stateful graph, with the main nodes being: 
Plan and execute (the DAG from the first step above)
Join: determine if we should finish or replan
Recontextualize: update the graph state based on the output from the joiner
",What are the main nodes in the agent defined as a stateful graph when composing using LangGraph?
Conclusion,"Congrats on building your first LLMCompiler agent! I'll leave you with some known limitations to the implementation above: 
The planner output parsing format is fragile if your function requires more than 1 or 2 arguments. We could make it more robust by using streaming tool calling.
Variable substitution is fragile in the example above. It could be made more robust by using a fine-tuned model and a more robust syntax (using e.g., Lark or a tool calling schema)
The state can grow quite long if you require multiple re-planning runs. To handle, you could add a message compressor once you go above a certain token limit.
",What are some known limitations to the implementation of the LLMCompiler agent?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows and manage state using nodes and edges?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context of using TypedDict and Pydantic BaseModel?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the different types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What information can be obtained by calling graph.get_state_history(config)?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with it directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported by LangGraph for threads currently interrupted?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to create branches for parallel node execution,"Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.",How can branches be created for parallel node execution in LangGraph?
Parallel node fan-out and fan-in,"In this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. Note that LangGraph uses Annotated type to specify reducer functions for specific keys in the State: it maintains the original type (list) for type checking, but allows attaching the reducer function (add) to the type without changing the type itself.",How does LangGraph use Annotated type to specify reducer functions for specific keys in the State?
Parallel node fan-out and fan-in with extra steps,The above example showed how to fan-out and fan-in when each path was only one step. But what if one path had more than one step?,How can parallel node fan-out and fan-in be achieved with extra steps in one path?
Conditional Branching,"If your fan-out is not deterministic, you can use add_conditional_edges directly. If you have a known ""sink"" node that the conditional branches will route to afterwards, you can provide then=<final-node-name> when creating the conditional edges.",How can you handle non-deterministic fan-out in conditional branching?
Stable Sorting,"When fanned out, nodes are run in parallel as a single ""superstep"". The updates from each superstep are all applied to the state in sequence once the superstep has completed. If you need consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs (along with an identifying key) to a separate field in your state, then combine them in the ""sink"" node by adding regular edge's from each of the fanout nodes to the rendezvous point. For instance, suppose I want to order the outputs of the parallel step by ""reliability"".","How can you achieve consistent, predetermined ordering of updates from a parallel superstep in stable sorting?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to kick off background runs,"This guide covers how to kick off background runs for your agent.
This can be useful for long running jobs.",How can you kick off background runs for your agent according to the guide provided?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Interrupt,"This guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide. The guide covers the interrupt option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option does not delete the first run, but rather keeps it in the database but sets its status to interrupted. Below is a quick example of using the interrupt option. First, we will define a quick helper function for printing out JS model outputs (you can skip this if using Python): function prettyPrint(m) {
  const padded = "" "" + m['type'] + "" "";
  const sepLen = Math.floor((80 - padded.length) / 2);
  const sep = ""="".repeat(sepLen);
  const secondSep = sep + (padded.length % 2 ? ""="" : """");

  console.log(`${sep}${padded}${secondSep}`);
  console.log(""\n\n"");
  console.log(m.content);
}
 Now, let's import our required packages and instantiate our client, assistant, and thread. PythonJavascript

import asyncio

from langchain_core.messages import convert_to_messages
from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent"";
const thread = await client.threads.create();

 Now we can start our two runs and join the second on euntil it has completed: PythonJavascript

# the first run will be interrupted
interrupted_run = await client.runs.create(
    thread[""thread_id""],
    assistant_id,
    input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in sf?""}]},
)
await asyncio.sleep(2)
run = await client.runs.create(
    thread[""thread_id""],
    assistant_id,
    input={""messages"": [{""role"": ""human"", ""content"": ""what's the weather in nyc?""}]},
    multitask_strategychrom=""interrupt"",
)
# wait until the second run completes
await client.runs.join(thread[""thread_id""], run[""run_id""])

// the first run will be interrupted
let interruptedRun = await client.runs.create(
  thread[""thread_id""],
  assistantId,
  { input: { messages: [{ role: ""human"", content: ""what's the weather in sf?"" }] } }
);
await new Promise(resolve => setTimeout(resolve, 2000)); 

let run = await client.runs.create(
  thread[""thread_id""],
  assistantId,
  { 
    input: { messages: [{ role: ""human"", content: ""what's the weather in nyc?"" }] },
    multitaskStrategy: ""interrupt"" 
  }
);

// wait until the second run completes
await client.runs.join(thread[""thread_id""], run[""run_id""]);

 We can see that the thread has partial data from the first run + data from the second run PythonJavascript

state = await client.threads.get_state(thread[""thread_id""])

for m in convert_to_messages(state[""values""][""messages""]):
    m.pretty_print()

const state = await client.threads.getState(thread[""thread_id""]);

for (const m of state['values']['messages']) {
  prettyPrint(m);
}

 Output: ================================[1m Human Message [0m=================================

what's the weather in sf?
==================================[1m Ai Message [0m==================================

[{'id': 'toolu_01MjNtVJwEcpujRGrf3x6Pih', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01MjNtVJwEcpujRGrf3x6Pih)
 Call ID: toolu_01MjNtVJwEcpujRGrf3x6Pih
  Args:
    query: weather in san francisco
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{""url"": ""https://www.wunderground.com/hourly/us/ca/san-francisco/KCASANFR2002/date/2024-6-18"", ""content"": ""High 64F. Winds W at 10 to 20 mph. A few clouds from time to time. Low 49F. Winds W at 10 to 20 mph. Temp. San Francisco Weather Forecasts. Weather Underground provides local & long-range weather ...""}]
================================[1m Human Message [0m=================================

what's the weather in nyc?
==================================[1m Ai Message [0m==================================

[{'id': 'toolu_01KtE1m1ifPLQAx4fQLyZL9Q', 'input': {'query': 'weather in new york city'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01KtE1m1ifPLQAx4fQLyZL9Q)
 Call ID: toolu_01KtE1m1ifPLQAx4fQLyZL9Q
  Args:
    query: weather in new york city
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{""url"": ""https://www.accuweather.com/en/us/new-york/10021/june-weather/349727"", ""content"": ""Get the monthly weather forecast for New York, NY, including daily high/low, historical averages, to help you plan ahead.""}]
==================================[1m Ai Message [0m==================================

The search results provide weather forecasts and information for New York City. Based on the top result from AccuWeather, here are some key details about the weather in NYC:

- This is a monthly weather forecast for New York City for the month of June.
- It includes daily high and low temperatures to help plan ahead.
- Historical averages for June in NYC are also provided as a reference point.
- More detailed daily or hourly forecasts with precipitation chances, humidity, wind, etc. can be found by visiting the AccuWeather page.

So in summary, the search provides a convenient overview of the expected weather conditions in New York City over the next month to give you an idea of what to prepare for if traveling or making plans there. Let me know if you need any other details!
 Verify that the original, interrupted run was interrupted PythonJavascript

print((await client.runs.get(thread[""thread_id""], interrupted_run[""run_id""]))[""status""])

console.log((await client.runs.get(thread['thread_id'], interruptedRun[""run_id""]))[""status""])

 Output: 'interrupted'
","Was the original, interrupted run successfully interrupted?"
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to stream full state of your graph,"LangGraph Cloud supports multiple streaming modes. The main ones are: 
values: This streaming mode streams back values of the graph. This is the full state of the graph after each node is called.
updates: This streaming mode streams back updates to the graph. This is the update to the state of the graph after each node is called.
messages: This streaming mode streams back messages - both complete messages (at the end of a node) as well as tokens for any messages generated inside a node. This mode is primarily meant for powering chat applications.
 This guide covers stream_mode=""values"". First let's set up our client and thread: PythonJavascriptCURL

from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
# create thread
thread = await client.threads.create()
print(thread)

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
// create thread
const thread = await client.threads.create();
console.log(thread)

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

 Output: {'thread_id': 'bfc68029-1f7b-400f-beab-6f9032a52da4',
 'created_at': '2024-06-24T21:30:07.980789+00:00',
 'updated_at': '2024-06-24T21:30:07.980789+00:00',
 'metadata': {},
 'status': 'idle',
 'config': {}}
 Now we can stream by values, which streams the full state of the graph after each node has finished executing: PythonJavascriptCURL

input = {""messages"": [{""role"": ""human"", ""content"": ""what's the weather in la""}]}

# stream values
async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"", 
    input=input,
    stream_mode=""values""
):
    print(f""Receiving new event of type: {chunk.event}..."")
    print(chunk.data)
    print(""\n\n"")

const input = {""messages"": [{""role"": ""human"", ""content"": ""what's the weather in la""}]}

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    streamMode: ""values""
  }
);
for await (const chunk of streamResponse) {
  console.log(f""Receiving new event of type: {chunk.event}..."")
  console.log(chunk.data)
  console.log(""\n\n"")
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""what's the weather in la\""}]},
   \""stream_mode\"": [
     \""values\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """") {
         print data_content ""\n""
     }
     sub(/^event: /, ""Receiving event of type: "", $0)
     printf ""%s...\n"", $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """") {
         print data_content ""\n""
     }
 }
 ' 

 Output: Receiving new event of type: metadata...
{'run_id': 'f08791ce-0a3d-44e0-836c-ff62cd2e2786'}

Receiving new event of type: values...
{'messages': [{'role': 'human', 'content': 'what's the weather in la'}]}

Receiving new event of type: values...
{'messages': [{'content': 'what's the weather in la', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'faa15565-8823-4aa1-87af-e21b40526fae', 'example': False}, {'content': [{'id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g', 'input': {'query': 'weather in los angeles'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-3fe1db7a-6b8d-4d83-ba07-8657190ad811', 'example': False, 'tool_calls': [{'name': 'tavily_search_results_json', 'args': {'query': 'weather in los angeles'}, 'id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g'}], 'invalid_tool_calls': []}]}

Receiving new event of type: values...
{'messages': [{'content': 'what's the weather in la', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'faa15565-8823-4aa1-87af-e21b40526fae', 'example': False}, {'content': [{'id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g', 'input': {'query': 'weather in los angeles'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-3fe1db7a-6b8d-4d83-ba07-8657190ad811', 'example': False, 'tool_calls': [{'name': 'tavily_search_results_json', 'args': {'query': 'weather in los angeles'}, 'id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g'}], 'invalid_tool_calls': []}, {'content': '[{""url"": ""https://www.weatherapi.com/"", ""content"": ""{\'location\': {\'name\': \'Los Angeles\', \'region\': \'California\', \'country\': \'United States of America\', \'lat\': 34.05, \'lon\': -118.24, \'tz_id\': \'America/Los_Angeles\', \'localtime_epoch\': 1716310320, \'localtime\': \'2024-05-21 9:52\'}, \'current\': {\'last_updated_epoch\': 1716309900, \'last_updated\': \'2024-05-21 09:45\', \'temp_c\': 16.7, \'temp_f\': 62.1, \'is_day\': 1, \'condition\': {\'text\': \'Overcast\', \'icon\': \'//cdn.weatherapi.com/weather/64x64/day/122.png\', \'code\': 1009}, \'wind_mph\': 8.1, \'wind_kph\': 13.0, \'wind_degree\': 250, \'wind_dir\': \'WSW\', \'pressure_mb\': 1015.0, \'pressure_in\': 29.97, \'precip_mm\': 0.0, \'precip_in\': 0.0, \'humidity\': 65, \'cloud\': 100, \'feelslike_c\': 16.7, \'feelslike_f\': 62.1, \'vis_km\': 16.0, \'vis_miles\': 9.0, \'uv\': 5.0, \'gust_mph\': 12.5, \'gust_kph\': 20.2}}""}]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'tavily_search_results_json', 'id': '0d5dab31-5ff8-4ae2-a560-bc4bcba7c9d7', 'tool_call_id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g'}]}

Receiving new event of type: values...
{'messages': [{'content': 'what's the weather in la', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'faa15565-8823-4aa1-87af-e21b40526fae', 'example': False}, {'content': [{'id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g', 'input': {'query': 'weather in los angeles'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-3fe1db7a-6b8d-4d83-ba07-8657190ad811', 'example': False, 'tool_calls': [{'name': 'tavily_search_results_json', 'args': {'query': 'weather in los angeles'}, 'id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g'}], 'invalid_tool_calls': []}, {'content': '[{""url"": ""https://www.weatherapi.com/"", ""content"": ""{\'location\': {\'name\': \'Los Angeles\', \'region\': \'California\', \'country\': \'United States of America\', \'lat\': 34.05, \'lon\': -118.24, \'tz_id\': \'America/Los_Angeles\', \'localtime_epoch\': 1716310320, \'localtime\': \'2024-05-21 9:52\'}, \'current\': {\'last_updated_epoch\': 1716309900, \'last_updated\': \'2024-05-21 09:45\', \'temp_c\': 16.7, \'temp_f\': 62.1, \'is_day\': 1, \'condition\': {\'text\': \'Overcast\', \'icon\': \'//cdn.weatherapi.com/weather/64x64/day/122.png\', \'code\': 1009}, \'wind_mph\': 8.1, \'wind_kph\': 13.0, \'wind_degree\': 250, \'wind_dir\': \'WSW\', \'pressure_mb\': 1015.0, \'pressure_in\': 29.97, \'precip_mm\': 0.0, \'precip_in\': 0.0, \'humidity\': 65, \'cloud\': 100, \'feelslike_c\': 16.7, \'feelslike_f\': 62.1, \'vis_km\': 16.0, \'vis_miles\': 9.0, \'uv\': 5.0, \'gust_mph\': 12.5, \'gust_kph\': 20.2}}""}]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'tavily_search_results_json', 'id': '0d5dab31-5ff8-4ae2-a560-bc4bcba7c9d7', 'tool_call_id': 'toolu_01E5mSaZWm5rWJnCqmt63v4g'}, {'content': 'Based on the weather API results, the current weather in Los Angeles is overcast with a temperature of around 62F (17C). There are light winds from the west-southwest around 8-13 mph. The humidity is 65% and visibility is good at 9 miles. Overall, mild spring weather conditions in LA.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-4d6d4c23-5aad-4042-b0d9-19407a9e08e3', 'example': False, 'tool_calls': [], 'invalid_tool_calls': []}]}

Receiving new event of type: end...
None
 If we want to just get the final result, we can use this endpoint and just keep track of the last value we received PythonJavascriptCURL

final_answer = None
async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=input,
    stream_mode=""values""
):
    if chunk.event == ""values"":
        final_answer = chunk.data

let finalAnswer;
const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    streamMode: ""values""
  }
);
for await (const chunk of streamResponse) {
  finalAnswer = chunk.data;
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""what's the weather in la\""}]},
   \""stream_mode\"": [
     \""values\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^data:/ { 
     sub(/^data: /, """", $0)   
     data_content = $0          
 }    
 END {                                               
     if (data_content != """") {
         print data_content
     }
 }         
 '

 Output: {'messages': [{'content': 'what's the weather in la',
   'additional_kwargs': {},
   'response_metadata': {},
   'type': 'human',
   'name': None,
   'id': 'e78c2f94-d810-42fc-a399-11f6bb1b1092',
   'example': False},
  {'content': [{'id': 'toolu_01SBMoAGr4U9x3ibztm2UUom',
     'input': {'query': 'weather in los angeles'},
     'name': 'tavily_search_results_json',
     'type': 'tool_use'}],
   'additional_kwargs': {},
   'response_metadata': {},
   'type': 'ai',
   'name': None,
   'id': 'run-80767ab8-09fc-40ec-9e45-657ddef5e0b1',
   'example': False,
   'tool_calls': [{'name': 'tavily_search_results_json',
     'args': {'query': 'weather in los angeles'},
     'id': 'toolu_01SBMoAGr4U9x3ibztm2UUom'}],
   'invalid_tool_calls': []},
  {'content': '[{""url"": ""https://www.weatherapi.com/"", ""content"": ""{\'location\': {\'name\': \'Los Angeles\', \'region\': \'California\', \'country\': \'United States of America\', \'lat\': 34.05, \'lon\': -118.24, \'tz_id\': \'America/Los_Angeles\', \'localtime_epoch\': 1716310320, \'localtime\': \'2024-05-21 9:52\'}, \'current\': {\'last_updated_epoch\': 1716309900, \'last_updated\': \'2024-05-21 09:45\', \'temp_c\': 16.7, \'temp_f\': 62.1, \'is_day\': 1, \'condition\': {\'text\': \'Overcast\', \'icon\': \'//cdn.weatherapi.com/weather/64x64/day/122.png\', \'code\': 1009}, \'wind_mph\': 8.1, \'wind_kph\': 13.0, \'wind_degree\': 250, \'wind_dir\': \'WSW\', \'pressure_mb\': 1015.0, \'pressure_in\': 29.97, \'precip_mm\': 0.0, \'precip_in\': 0.0, \'humidity\': 65, \'cloud\': 100, \'feelslike_c\': 16.7, \'feelslike_f\': 62.1, \'vis_km\': 16.0, \'vis_miles\': 9.0, \'uv\': 5.0, \'gust_mph\': 12.5, \'gust_kph\': 20.2}}""}]',
   'additional_kwargs': {},
   'response_metadata': {},
   'type': 'tool',
   'name': 'tavily_search_results_json',
   'id': 'af25e94a-c119-48c3-bbd3-096e42f472ac',
   'tool_call_id': 'toolu_01SBMoAGr4U9x3ibztm2UUom'},
  {'content': 'Based on the weather API results, the current weather in Los Angeles is overcast with a temperature of around 62F (17C). There are light winds from the west-southwest around 8-13 mph. The humidity is 65% and visibility is good at 9 miles. Overall, mild spring weather conditions in LA.',
   'additional_kwargs': {},
   'response_metadata': {},
   'type': 'ai',
   'name': None,
   'id': 'run-b90f0037-e56a-4f3b-ad92-00d10d079a9e',
   'example': False,
   'tool_calls': [],
   'invalid_tool_calls': []}]}
",How can you stream the full state of your graph after each node is called using LangGraph Cloud?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus?
How to use Pydantic model as state,"Every StateGraph is a state machine. When initializing, it accepts a state_schema that tells it the ""shape"" of its state and how to incorporate updates from the nodes into a shared representation of what work has been done. The state_schema can be any type, though we typically use a python-native TypedDict in our examples (or in the case of MessageGraph, a list). If you want to apply additional validation on state updates, you could instead opt for a pydantic BaseModel. In this example, we will create a ReAct agent using a pydantic base model as the state object. This means all nodes receive an instance of the model as their first arg, and validation is run before each node executes.",How can a Pydantic model be used as state in a StateGraph?
Setup,First we need to install the packages required,What is the first step in the setup process?
Set up the tools,"We will first define the tools we want to use.
For this simple example, we will use create a placeholder search engine.
However, it is really easy to create your own tools - see documentation here on how to do that.",What is the first step in the process described in the text?
Set up the model,"Now we need to load the chat model we want to use.
Importantly, this should satisfy two criteria: 
It should work with messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them.
It should work with OpenAI function calling. This means it should either be an OpenAI model or a model that exposes a similar interface.
 Note: these model requirements are not requirements for using LangGraph - they are just requirements for this one example.",What criteria must a chat model satisfy in order to be used in this example?
Define the agent state,"The main type of graph in langgraph is the StateGraph.
This graph is parameterized by a state object that it passes around to each node.
Each node then returns operations to update that state.
These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute.
Whether to set or add is denoted by annotating the state object you construct the graph with. For this example, the state we will track will just be a list of messages.
We want each node to just add messages to that list.
Therefore, we will use a pydantic.BaseModel with one key (messages) and annotate it so that the messages attribute is treated as ""append-only"".",What type of graph is used in langgraph and how is the state object parameterized in it?
Define the nodes,"We now need to define a few different nodes in our graph.
In langgraph, a node can be either a function or a runnable.
There are two main nodes we need for this: 
The agent: responsible for deciding what (if any) actions to take.
A function to invoke tools: if the agent decides to take an action, this node will then execute that action.
 We will also need to define some edges.
Some of these edges may be conditional.
The reason they are conditional is that based on the output of a node, one of several paths may be taken.
The path that is taken is not known until that node is run (the LLM decides). 
Conditional Edge: after the agent is called, we should either:
a. If the agent said to take an action, then the function to invoke tools should be called
b. If the agent said that it was finished, then it should finish
Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next
 Let's define the nodes, as well as a function to decide how what conditional edge to take. MODIFICATION We define each node to receive the AgentState base model as its first argument.",What are the two main nodes that need to be defined in the graph?
Define the graph,We can now put it all together and define the graph!,What can we define by putting it all together?
Use it!,"We can now use it!
This now exposes the same interface as all other LangChain runnables.",What interface does it now expose?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Chat Bot Benchmarking using Simulation,"Building on our previous example, we can show how to use simulated conversations to benchmark your chat bot using LangSmith. First, we'll install the prerequisites.",How can simulated conversations be used to benchmark a chat bot using LangSmith?
Clone Dataset,"For our example, suppose you are developing a chat bot for customers of an airline.
We've prepared a red-teaming dataset to test your bot out on. Clone the data using the URL below.",What type of dataset is available for testing the chat bot for airline customers?
Define your assistant,"Next, define your assistant. You can put any logic in this function.",What can you do to define your assistant?
Create the Simulated User,"This bot will role-play as a customer of the airline. The dataset includes unique instructions for each data point that give it an objective.
It will try to trick your assistant over the course of the simulation.",What role will the simulated user play in the airline customer service simulation?
Create Simulation,"We've included a simple LangGraph simulation harness that will orchestrate the ""conversation"".","What type of simulation harness has been included for orchestrating the ""conversation""?"
Evaluate,We will use an LLM to evaluate whether or your assistant successfully resisted the red team attack.,What method will be used to evaluate whether or not the assistant successfully resisted the red team attack?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to edit graph state,"Human-in-the-loop (HIL) interactions are crucial for agentic systems. Manually updating the graph state a common HIL interaction pattern, allowing the human to edit actions (e.g., what tool is being called or how it is being called). We can implement this in LangGraph using a breakpoint: breakpoints allow us to interrupt graph execution before a specific step. At this breakpoint, we can manually update the graph state and then resume from that spot to continue.",What is a common HIL interaction pattern for editing the graph state in LangGraph?
Setup,First we need to install the packages required,What is the first step in the setup process?
Simple Usage,"Let's look at very basic usage of this. Below, we do three things: 
We specify the breakpoint using interrupt_before a specified step (node).

We set up a checkpointer to save the state of the graph up until this node.

We use .update_state to update the state of the graph.

",What are the three basic things done in the simple usage of this tool?
Agent,"In the context of agents, updating state is useful for things like editing tool calls. To show this, we will build a relatively simple ReAct-style agent that does tool calling. We will use Anthropic's models and a fake tool (just for demo purposes).",What is updating state useful for in the context of agents?
Interacting with the Agent,We can now interact with the agent and see that it stops before calling a tool.,What happens when we interact with the agent?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be utilized in LangGraph nodes when building agents?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in controlling and retaining memory?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",Why are human-in-the-loop interaction patterns necessary for agentic systems in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent be programmed to wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in LangGraph to look back at previous checkpoints and resume execution from a desired point?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation in agents?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step taken to ensure reliable results from agents and when might using an LLM not be necessary?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Use Webhooks,"You may wish to use webhooks in your client, especially when using async streams in case you want to update something in your service once the API call to LangGraph Cloud has finished running. To do so, you will need to expose an endpoint that can accept POST requests, and then pass it to your API request in the ""webhook"" parameter. Currently, the SDK has not exposed this endpoint but you can access it through curl commands as follows. The following endpoints accept webhook as a parameter: 
Create Run -> POST /thread/{thread_id}/runs
Create Thread Cron -> POST /thread/{thread_id}/runs/crons
Stream Run -> POST /thread/{thread_id}/runs/stream
Wait Run -> POST /thread/{thread_id}/runs/wait
Create Cron -> POST /runs/crons
Stream Run Stateless -> POST /runs/stream
Wait Run Stateless -> POST /runs/wait
 The following example uses a url from a public website that allows users to create free webhooks, but you should pass in the webhook that you wish to use.",What endpoints accept webhooks as a parameter in the LangGraph Cloud API?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",What are reducers and how are they used in updating the State in a graph?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes depending on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.","What method can be used to retrieve the state history of a graph, and what information should be included in the config parameter?"
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What changes can be made to the topology of a graph using LangGraph for threads at the end of the graph and for interrupted threads?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be set and used in a graph execution process?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
An agent for interacting with a SQL database,"In this tutorial, we will walk through how to build an agent that can answer questions about a SQL database. At a high level, the agent will: 
Fetch the available tables from the database
Decide which tables are relevant to the question
Fetch the DDL for the relevant tables
Generate a query based on the question and information from the DDL
Double-check the query for common mistakes using an LLM
Execute the query and return the results
Correct mistakes surfaced by the database engine until the query is successful
Formulate a response based on the results
 The end-to-end workflow will look something like below:",What is the workflow for building an agent that can interact with a SQL database to answer questions?
Set up environment,"We'll set up our environment variables for OpenAI, and optionally, to enable tracing with LangSmith.",What steps will be taken to set up the environment for OpenAI and potentially enable tracing with LangSmith?
Configure the database,"We will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the chinook database, which is a sample database that represents a digital media store.
Find more information about the database here. For convenience, we have hosted the database (Chinook.db) on a public GCS bucket.",What type of database will be used for this tutorial and what is the name of the sample database being loaded?
Utility functions,"We will define a few utility functions to help us with the agent implementation. Specifically, we will wrap a ToolNode with a fallback to handle errors and surface them to the agent.",What purpose do the utility functions serve in the agent implementation?
Define tools for the agent,"We will define a few tools that the agent will use to interact with the database. 
list_tables_tool: Fetch the available tables from the database
get_schema_tool: Fetch the DDL for a table
db_query_tool: Execute the query and fetch the results OR return an error message if the query fails
 For the first two tools, we will grab them from the SQLDatabaseToolkit, also available in the langchain_community package.",What are the tools available for the agent to interact with the database?
Define the workflow,"We will then define the workflow for the agent. The agent will first force-call the list_tables_tool to fetch the available tables from the database, then follow the steps mentioned at the beginning of the tutorial.",What steps will the agent follow to define the workflow?
Eval,"Now, we can evaluate this agent! We previously defined simple SQL agent as part of our LangSmith evaluation cookbooks, and evaluated responses to 5 questions about our database. We can compare this agent to our prior one on the same dataset. Agent evaluation can focus on 3 things: 
Response: The inputs are a prompt and a list of tools. The output is the agent response.
Single tool: As before, the inputs are a prompt and a list of tools. The output the tool call.
Trajectory: As before, the inputs are a prompt and a list of tools. The output is the list of tool calls
",What are the focus areas for evaluating the agent in the LangSmith evaluation cookbooks?
Response,We'll evaluate end-to-end responses of our agent relative to reference answers. Let's run response evaluation on the same dataset.,What will be evaluated in the response evaluation process on the dataset?
Trajectory,Let's run trajectory evaluation on this same dataset.,What type of evaluation will be conducted on the dataset mentioned in the text?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows using graphs?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you route to 1 or more edges optionally using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified in the code?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes based on custom logic using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What is the purpose of using Send objects in LangGraph for conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using the graph.get_state method?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What method can be used to retrieve the state history of a graph in Python?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state directly and what components are required to do so?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What built-in ways does LangGraph offer for visualizing graphs?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Review Tool Calls,"Human-in-the-loop (HIL) interactions are crucial for agentic systems. A common pattern is to add some human in the loop step after certain tool calls. These tool calls often lead to either a function call or saving of some information. Examples include: 
A tool call to execute SQL, which will then be run by the tool
A tool call to generate a summary, which will then be saved to the State of the graph
 Note that using tool calls is common whether actually calling tools or not. There are typically a few different interactions you may want to do here: 
Approve the tool call and continue
Modify the tool call manually and then continue
Give natural language feedback, and then pass that back to the agent instead of continuing
 We can implement this in LangGraph using a breakpoint: breakpoints allow us to interrupt graph execution before a specific step. At this breakpoint, we can manually update the graph state taking one of the three options above",What are some common interactions that can be done after certain tool calls in agentic systems?
Setup,"We are not going to show the full code for the graph we are hosting, but you can see it here if you want to. Once this graph is hosted, we are ready to invoke it and wait for user input. ",What do we need to do once the graph is hosted in order to be ready to invoke it and wait for user input?
SDK initialization,"First, we need to setup our client so that we can communicate with our hosted graph: PythonJavascript

from langgraph_sdk import get_client
client = get_client(url=<DEPLOYMENT_URL>)
assistant_id = ""agent""
thread = await client.threads.create()

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
const assistantId = ""agent"";
const thread = await client.threads.create();

",What is the first step required to communicate with the hosted graph in Python and JavaScript using the SDK?
Example with no review,"Let's look at an example when no review is required (because no tools are called) PythonJavascript

input = { 'messages':[{ ""role"":""user"", ""content"":""hi!"" }] }

async for chunk in client.runs.stream(
    thread[""thread_id""],
    assistant_id,
    input=input,
    stream_mode=""updates"",
    interrupt_before=[""action""],
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const input = {""messages"": [{ ""role"": ""human"", ""content"": ""hi!""}] }

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""updates"",
    interruptBefore: [""action""],
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

 Output: {'messages': [{'content': 'hi!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '39c51f14-2d5c-4690-883a-d940854b1845', 'example': False}]}
{'messages': [{'content': 'hi!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '39c51f14-2d5c-4690-883a-d940854b1845', 'example': False}, {'content': [{'text': ""Hello! Welcome. How can I assist you today? Is there anything specific you'd like to know or any information you're looking for?"", 'type': 'text', 'index': 0}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-d65e07fb-43ff-4d98-ab6b-6316191b9c8b', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 355, 'output_tokens': 31, 'total_tokens': 386}}]}
 If we check the state, we can see that it is finished PythonJavascript

state = await client.threads.get_state(thread[""thread_id""])

print(state['next'])

const state = await client.threads.getState(thread[""thread_id""]);

console.log(state.next);

 Output: []
",What is the state of the example when no review is required and no tools are called in PythonJavascript?
Example of approving tool,"Let's now look at what it looks like to approve a tool call. Note that we don't need to pass an interrupt to our streaming calls because the graph (defined here) was already compiled with an interrupt before the human_review_node. PythonJavascript

input = {""messages"": [{""role"": ""user"", ""content"": ""what's the weather in sf?""}]}

async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=input,
    stream_mode=""values"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const input = {""messages"": [{""role"": ""user"", ""content"": ""what's the weather in sf?""}]}

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""values"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

 Output: {'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '54e19d6e-89fa-44fb-b92c-12e7dd4ddf08', 'example': False}]}
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '54e19d6e-89fa-44fb-b92c-12e7dd4ddf08', 'example': False}, {'content': [{'text': ""Certainly! I can help you check the weather in San Francisco. To get this information, I'll use the weather search function. Let me do that for you right away."", 'type': 'text', 'index': 0}, {'id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-45a6b6c3-ac69-42a4-8957-d982203d6392', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 90, 'total_tokens': 450}}]}
 If we now check, we can see that it is waiting on human review: PythonJavascript

state = await client.threads.get_state(thread[""thread_id""])

print(state['next'])

const state = await client.threads.getState(thread[""thread_id""]);

console.log(state.next);

 Output: ['human_review_node']
 To approve the tool call, we can just continue the thread with no edits. To do this, we just create a new run with no inputs. PythonJavascript

async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=None,
    stream_mode=""values"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: undefined,
    streamMode: ""values"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

 Output: {'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '54e19d6e-89fa-44fb-b92c-12e7dd4ddf08', 'example': False}, {'content': [{'text': ""Certainly! I can help you check the weather in San Francisco. To get this information, I'll use the weather search function. Let me do that for you right away."", 'type': 'text', 'index': 0}, {'id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-45a6b6c3-ac69-42a4-8957-d982203d6392', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 90, 'total_tokens': 450}}, {'content': 'Sunny!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '826cd0f2-9cc6-46f0-b7df-daa6a05d13d2', 'tool_call_id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'artifact': None, 'status': 'success'}]}
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '54e19d6e-89fa-44fb-b92c-12e7dd4ddf08', 'example': False}, {'content': [{'text': ""Certainly! I can help you check the weather in San Francisco. To get this information, I'll use the weather search function. Let me do that for you right away."", 'type': 'text', 'index': 0}, {'id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-45a6b6c3-ac69-42a4-8957-d982203d6392', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 90, 'total_tokens': 450}}, {'content': 'Sunny!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '826cd0f2-9cc6-46f0-b7df-daa6a05d13d2', 'tool_call_id': 'toolu_015yrR3GMDXe6X8m2p9CsEDN', 'artifact': None, 'status': 'success'}, {'content': [{'text': ""\n\nGreat news! The weather in San Francisco is sunny today. It's a beautiful day in the city by the bay. Is there anything else you'd like to know about the weather or any other information I can help you with?"", 'type': 'text', 'index': 0}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-5d5fd0f1-a939-447e-801a-9aaa812322d3', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 464, 'output_tokens': 50, 'total_tokens': 514}}]}
",What is the process for approving a tool call without passing an interrupt to streaming calls?
Edit Tool Call,"Let's now say we want to edit the tool call. E.g. change some of the parameters (or even the tool called!) but then execute that tool. PythonJavascript

input = {""messages"": [{""role"": ""user"", ""content"": ""what's the weather in sf?""}]}

async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=input,
    stream_mode=""values"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const input = {""messages"": [{""role"": ""user"", ""content"": ""what's the weather in sf?""}]}

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""values"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

 Output: {'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'cec11391-84da-464b-bd2a-bd4f0d93b9ee', 'example': False}]}
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'cec11391-84da-464b-bd2a-bd4f0d93b9ee', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01SunSpDurNfcnXppWLPrtjC', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-6326da9f-6061-4e12-8586-482e32ab4cab', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_01SunSpDurNfcnXppWLPrtjC', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 80, 'total_tokens': 440}}]}
 To do this, we first need to update the state. We can do this by passing a message in with the same id of the message we want to overwrite. This will have the effect of replacing that old message. Note that this is only possible because of the reducer we are using that replaces messages with the same ID - read more about that here. PythonJavascript

# To get the ID of the message we want to replace, we need to fetch the current state and find it there.
state = await client.threads.get_state(thread['thread_id'])
print(""Current State:"")
print(state['values'])
print(""\nCurrent Tool Call ID:"")
current_content = state['values']['messages'][-1]['content']
current_id = state['values']['messages'][-1]['id']
tool_call_id = state['values']['messages'][-1]['tool_calls'][0]['id']
print(tool_call_id)

# We now need to construct a replacement tool call.
# We will change the argument to be `San Francisco, USA`
# Note that we could change any number of arguments or tool names - it just has to be a valid one
new_message = {
    ""role"": ""assistant"", 
    ""content"": current_content,
    ""tool_calls"": [
        {
            ""id"": tool_call_id,
            ""name"": ""weather_search"",
            ""args"": {""city"": ""San Francisco, USA""}
        }
    ],
    # This is important - this needs to be the same as the message you replacing!
    # Otherwise, it will show up as a separate message
    ""id"": current_id
}
await client.threads.update_state(
    # This is the config which represents this thread
    thread['thread_id'], 
    # This is the updated value we want to push
    {""messages"": [new_message]}, 
    # We push this update acting as our human_review_node
    as_node=""human_review_node""
)

print(""\nResuming Execution"")
# Let's now continue executing from here
async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=None,
    stream_mode=""values"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const state = await client.threads.getState(thread.thread_id);
console.log(""Current State:"");
console.log(state.values);

console.log(""\nCurrent Tool Call ID:"");
const lastMessage = state.values.messages[state.values.messages.length - 1];
const currentContent = lastMessage.content;
const currentId = lastMessage.id;
const toolCallId = lastMessage.tool_calls[0].id;
console.log(toolCallId);

// Construct a replacement tool call
const newMessage = {
    role: ""assistant"",
    content: currentContent,
    tool_calls: [
        {
            id: toolCallId,
            name: ""weather_search"",
            args: { city: ""San Francisco, USA"" }
        }
    ],
    // Ensure the ID is the same as the message you're replacing
    id: currentId
};

await client.threads.updateState(
    thread.thread_id,  // Thread ID
    {
    values: { ""messages"": [newMessage] },  // Updated message
    asNode: ""human_review_node""
    }  // Acting as human_review_node
);

console.log(""\nResuming Execution"");
// Continue executing from here
const streamResponseResumed = client.runs.stream(
thread[""thread_id""],
assistantId,
{
    input: undefined,
    streamMode: ""values"",
    interruptBefore: [""action""],
}
);
for await (const chunk of streamResponseResumed) {
if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
}
}

 Output: Current State:
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '8713d1fa-9b26-4eab-b768-dafdaac70590', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-ede13f26-daf5-4d8f-817a-7611075bbcf1', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 80, 'total_tokens': 440}}]}

Current Tool Call ID:
toolu_01VzagzsUGZsNMwW1wHkcw7h

Resuming Execution
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '8713d1fa-9b26-4eab-b768-dafdaac70590', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ede13f26-daf5-4d8f-817a-7611075bbcf1', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco, USA'}, 'id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'Sunny!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '7fc7d463-66bf-4555-9929-6af483de169b', 'tool_call_id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'artifact': None, 'status': 'success'}]}
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '8713d1fa-9b26-4eab-b768-dafdaac70590', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ede13f26-daf5-4d8f-817a-7611075bbcf1', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco, USA'}, 'id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'Sunny!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '7fc7d463-66bf-4555-9929-6af483de169b', 'tool_call_id': 'toolu_01VzagzsUGZsNMwW1wHkcw7h', 'artifact': None, 'status': 'success'}, {'content': [{'text': ""\n\nBased on the search result, the weather in San Francisco is sunny! It's a beautiful day in the city by the bay. Is there anything else you'd like to know about the weather or any other information I can help you with?"", 'type': 'text', 'index': 0}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-d90ce97a-39f9-4330-985e-67c5f351a0c5', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 455, 'output_tokens': 52, 'total_tokens': 507}}]}
",How can the tool call be edited and executed in Python/Javascript?
Give feedback to a tool call,"Sometimes, you may not want to execute a tool call, but you also may not want to ask the user to manually modify the tool call. In that case it may be better to get natural language feedback from the user. You can then insert these feedback as a mock RESULT of the tool call. There are multiple ways to do this: You could add a new message to the state (representing the ""result"" of a tool call)
You could add TWO new messages to the state - one representing an ""error"" from the tool call, other HumanMessage representing the feedback
Both are similar in that they involve adding messages to the state. The main difference lies in the logic AFTER the human_node and how it handles different types of messages. For this example we will just add a single tool call representing the feedback. Let's see this in action! PythonJavascript

input = {""messages"": [{""role"": ""user"", ""content"": ""what's the weather in sf?""}]}

async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=input,
    stream_mode=""values"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const input = {""messages"": [{""role"": ""user"", ""content"": ""what's the weather in sf?""}]}

const streamResponse = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: input,
    streamMode: ""values"",
  }
);
for await (const chunk of streamResponse) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

 Output: {'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c80f13d0-674d-4233-b6a0-3940509d3cf3', 'example': False}]}
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c80f13d0-674d-4233-b6a0-3940509d3cf3', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_016XyTdFA8NuPWeLyZPSzoM3', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-4911ac27-3d7c-4edf-a3ca-c2908e3922eb', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_016XyTdFA8NuPWeLyZPSzoM3', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 80, 'total_tokens': 440}}]}
 To do this, we first need to update the state. We can do this by passing a message in with the same tool call id of the tool call we want to respond to. Note that this is a different* ID from above PythonJavascript

# To get the ID of the message we want to replace, we need to fetch the current state and find it there.
state = await client.threads.get_state(thread['thread_id'])
print(""Current State:"")
print(state['values'])
print(""\nCurrent Tool Call ID:"")
tool_call_id = state['values']['messages'][-1]['tool_calls'][0]['id']
print(tool_call_id)

# We now need to construct a replacement tool call.
# We will change the argument to be `San Francisco, USA`
# Note that we could change any number of arguments or tool names - it just has to be a valid one
new_message = {
    ""role"": ""tool"", 
    # This is our natural language feedback
    ""content"": ""User requested changes: pass in the country as well"",
    ""name"": ""weather_search"",
    ""tool_call_id"": tool_call_id
}
await client.threads.update_state(
    # This is the config which represents this thread
    thread['thread_id'], 
    # This is the updated value we want to push
    {""messages"": [new_message]}, 
    # We push this update acting as our human_review_node
    as_node=""human_review_node""
)

print(""\nResuming execution"")
# Let's now continue executing from here
async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=None,
    stream_mode=""values"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const state = await client.threads.getState(thread.thread_id);
console.log(""Current State:"");
console.log(state.values);

console.log(""\nCurrent Tool Call ID:"");
const lastMessage = state.values.messages[state.values.messages.length - 1];
const toolCallId = lastMessage.tool_calls[0].id;
console.log(toolCallId);

// Construct a replacement tool call
const newMessage = {
    role: ""tool"",
    content: ""User requested changes: pass in the country as well"",
    name: ""weather_search"",
    tool_call_id: toolCallId,
};

await client.threads.updateState(
    thread.thread_id,  // Thread ID
    {
    values: { ""messages"": [newMessage] },  // Updated message
    asNode: ""human_review_node""
    }  // Acting as human_review_node
);

console.log(""\nResuming Execution"");
// Continue executing from here
const streamResponseEdited = client.runs.stream(
thread[""thread_id""],
assistantId,
{
    input: undefined,
    streamMode: ""values"",
    interruptBefore: [""action""],
}
);
for await (const chunk of streamResponseEdited) {
if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
}
}

 Output: Current State:
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3b2bbc38-d11b-49eb-80c0-c24a40dab5a8', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-c5a50900-abf5-4885-9cdb-da2bf0d892ac', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 80, 'total_tokens': 440}}]}

Current Tool Call ID:
toolu_01NNw18j57GEGPZvsa9f1wvX

Resuming execution
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3b2bbc38-d11b-49eb-80c0-c24a40dab5a8', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-c5a50900-abf5-4885-9cdb-da2bf0d892ac', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 80, 'total_tokens': 440}}, {'content': 'User requested changes: pass in the country as well', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '787288be-213c-4fd3-8503-4a009bdb1b00', 'tool_call_id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'artifact': None, 'status': 'success'}, {'content': [{'text': '\n\nI apologize for the oversight. It seems the function requires additional information. Let me try again with a more specific request.', 'type': 'text', 'index': 0}, {'id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco, USA""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-5c355a56-cfe3-4046-b49f-f5b09fc397ef', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco, USA'}, 'id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 461, 'output_tokens': 83, 'total_tokens': 544}}]}
 We can see that we now get to another breakpoint - because it went back to the model and got an entirely new prediction of what to call. Let's now approve this one and continue PythonJavascript

async for chunk in client.runs.stream(
    thread[""thread_id""],
    ""agent"",
    input=None,
    stream_mode=""values"",
):
    if chunk.data and chunk.event != ""metadata"": 
        print(chunk.data)

const streamResponseResumed = client.runs.stream(
  thread[""thread_id""],
  assistantId,
  {
    input: undefined,
    streamMode: ""values"",
  }
);
for await (const chunk of streamResponseResumed) {
  if (chunk.data && chunk.event !== ""metadata"") {
    console.log(chunk.data);
  }
}

 Output: {'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3b2bbc38-d11b-49eb-80c0-c24a40dab5a8', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-c5a50900-abf5-4885-9cdb-da2bf0d892ac', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 80, 'total_tokens': 440}}, {'content': 'User requested changes: pass in the country as well', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '787288be-213c-4fd3-8503-4a009bdb1b00', 'tool_call_id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'artifact': None, 'status': 'success'}, {'content': [{'text': '\n\nI apologize for the oversight. It seems the function requires additional information. Let me try again with a more specific request.', 'type': 'text', 'index': 0}, {'id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco, USA""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-5c355a56-cfe3-4046-b49f-f5b09fc397ef', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco, USA'}, 'id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 461, 'output_tokens': 83, 'total_tokens': 544}}, {'content': 'Sunny!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '3b857482-bca2-4a73-a9ab-1f35a3e43e5f', 'tool_call_id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'artifact': None, 'status': 'success'}]}
{'messages': [{'content': ""what's the weather in sf?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3b2bbc38-d11b-49eb-80c0-c24a40dab5a8', 'example': False}, {'content': [{'text': 'To get the weather information for San Francisco, I can use the weather_search function. Let me do that for you.', 'type': 'text', 'index': 0}, {'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-c5a50900-abf5-4885-9cdb-da2bf0d892ac', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 360, 'output_tokens': 80, 'total_tokens': 440}}, {'content': 'User requested changes: pass in the country as well', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '787288be-213c-4fd3-8503-4a009bdb1b00', 'tool_call_id': 'toolu_01NNw18j57GEGPZvsa9f1wvX', 'artifact': None, 'status': 'success'}, {'content': [{'text': '\n\nI apologize for the oversight. It seems the function requires additional information. Let me try again with a more specific request.', 'type': 'text', 'index': 0}, {'id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'input': {}, 'name': 'weather_search', 'type': 'tool_use', 'index': 1, 'partial_json': '{""city"": ""San Francisco, USA""}'}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'tool_use', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-5c355a56-cfe3-4046-b49f-f5b09fc397ef', 'example': False, 'tool_calls': [{'name': 'weather_search', 'args': {'city': 'San Francisco, USA'}, 'id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 461, 'output_tokens': 83, 'total_tokens': 544}}, {'content': 'Sunny!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'weather_search', 'id': '3b857482-bca2-4a73-a9ab-1f35a3e43e5f', 'tool_call_id': 'toolu_01YAbLBoKozJyRQnB8LUMpXC', 'artifact': None, 'status': 'success'}, {'content': [{'text': ""\n\nGreat news! The weather in San Francisco is sunny today. Is there anything else you'd like to know about the weather or any other information I can help you with?"", 'type': 'text', 'index': 0}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-6a857bb1-f65b-4b86-93d6-c025e003c777', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 557, 'output_tokens': 38, 'total_tokens': 595}}]}
",How can natural language feedback be used as a mock result of a tool call?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to stream events,"This guide covers how to stream events from your graph (stream_mode=""events""). Depending on the use case and user experience of your LangGraph application, your application may process event types differently. PythonJavascriptCURL

from langgraph_sdk import get_client

client = get_client(url=<DEPLOYMENT_URL>)
# create thread
thread = await client.threads.create()
print(thread)

import { Client } from ""@langchain/langgraph-sdk"";

const client = new Client({ apiUrl: <DEPLOYMENT_URL> });
// create thread
const thread = await client.threads.create();
console.log(thread)

curl --request POST \
  --url <DEPLOYMENT_URL>/threads \
  --header 'Content-Type: application/json'

 Output: {'thread_id': '3f4c64e0-f792-4a5e-aa07-a4404e06e0bd',
 'created_at': '2024-06-24T22:16:29.301522+00:00',
 'updated_at': '2024-06-24T22:16:29.301522+00:00',
 'metadata': {},
 'status': 'idle',
 'config': {}}
 Streaming events produces responses containing an event key (in addition to other keys such as data). See the LangChain Runnable.astream_events() reference for all event types. PythonJavascriptCURL

# create input
input = {
    ""messages"": [
        {
            ""role"": ""human"",
            ""content"": ""What's the weather in SF?"",
        }
    ]
}

# stream events
async for chunk in client.runs.stream(
    thread_id=thread[""thread_id""],
    assistant_id=""agent"",
    input=input,
    stream_mode=""events"",
):
    print(f""Receiving new event of type: {chunk.event}..."")
    print(chunk.data)
    print(""\n\n"")

// create input
const input = {
    ""messages"": [
        {
            ""role"": ""human"",
            ""content"": ""What's the weather in SF?"",
        }
    ]
}

// stream events
const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    streamMode: ""events""
  }
);
for await (const chunk of streamResponse) {
  console.log(f""Receiving new event of type: {chunk.event}..."")
  console.log(chunk.data)
  console.log(""\n\n"")
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""What's the weather in sf\""}]},
   \""stream_mode\"": [
     \""events\""
   ]
 }"" | \
 sed 's/\r$//' | \
 awk '
 /^event:/ {
     if (data_content != """") {
         print data_content ""\n""
     }
     sub(/^event: /, ""Receiving event of type: "", $0)
     printf ""%s...\n"", $0
     data_content = """"
 }
 /^data:/ {
     sub(/^data: /, """", $0)
     data_content = $0
 }
 END {
     if (data_content != """") {
         print data_content ""\n""
     }
 }
 ' 

 Output: Receiving new event of type: metadata...
{'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8'}

Receiving new event of type: events...
{'event': 'on_chain_start', 'data': {'input': {'messages': [{'role': 'human', 'content': ""What's the weather in SF?""}]}}, 'name': 'LangGraph', 'tags': [], 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'parent_ids': []}

Receiving new event of type: events...
{'event': 'on_chain_start', 'data': {}, 'name': 'agent', 'tags': ['graph:step:6'], 'run_id': '7bb08493-d507-4e28-b9e6-4a5eda9d04f0', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}]]}}, 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'run_id': 'cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'b', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': 'cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'e', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': 'cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'g', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': 'cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'i', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': 'cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': 'cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chat_model_end', 'data': {'output': {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, 'input': {'messages': [[{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}]]}}, 'run_id': 'cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chain_start', 'data': {'input': {'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}}, 'name': 'should_continue', 'tags': ['seq:step:3'], 'run_id': 'c7fe4d2d-3fb8-4e53-946d-03de13527853', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chain_end', 'data': {'output': 'tool', 'input': {'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}}, 'run_id': 'c7fe4d2d-3fb8-4e53-946d-03de13527853', 'name': 'should_continue', 'tags': ['seq:step:3'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '7bb08493-d507-4e28-b9e6-4a5eda9d04f0']}

Receiving new event of type: events...
{'event': 'on_chain_stream', 'run_id': '7bb08493-d507-4e28-b9e6-4a5eda9d04f0', 'name': 'agent', 'tags': ['graph:step:6'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0}, 'data': {'chunk': {'messages': [{'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chain_end', 'data': {'output': {'messages': [{'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}, 'input': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}], 'sleep': None}}, 'run_id': '7bb08493-d507-4e28-b9e6-4a5eda9d04f0', 'name': 'agent', 'tags': ['graph:step:6'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 6, 'langgraph_node': 'agent', 'langgraph_triggers': ['start:agent'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chain_start', 'data': {}, 'name': 'tool', 'tags': ['graph:step:7'], 'run_id': 'f044fd3d-7271-488f-b8aa-e01572ff9112', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 7, 'langgraph_node': 'tool', 'langgraph_triggers': ['branch:agent:should_continue:tool'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chain_stream', 'run_id': 'f044fd3d-7271-488f-b8aa-e01572ff9112', 'name': 'tool', 'tags': ['graph:step:7'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 7, 'langgraph_node': 'tool', 'langgraph_triggers': ['branch:agent:should_continue:tool'], 'langgraph_task_idx': 0}, 'data': {'chunk': {'messages': [{'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': None, 'tool_call_id': 'tool_call_id'}]}}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chain_end', 'data': {'output': {'messages': [{'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1c9a16d2-5f0a-4eba-a0d2-240484a4ce7e', 'tool_call_id': 'tool_call_id'}]}, 'input': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'sleep': None}}, 'run_id': 'f044fd3d-7271-488f-b8aa-e01572ff9112', 'name': 'tool', 'tags': ['graph:step:7'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 7, 'langgraph_node': 'tool', 'langgraph_triggers': ['branch:agent:should_continue:tool'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chain_start', 'data': {}, 'name': 'agent', 'tags': ['graph:step:8'], 'run_id': '1f4f95d0-0ce1-4061-85d4-946446bbd3e5', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1c9a16d2-5f0a-4eba-a0d2-240484a4ce7e', 'tool_call_id': 'tool_call_id'}]]}}, 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'run_id': '028a68fb-6435-4b46-a156-c3326f73985c', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '1f4f95d0-0ce1-4061-85d4-946446bbd3e5']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'e', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': '028a68fb-6435-4b46-a156-c3326f73985c', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '1f4f95d0-0ce1-4061-85d4-946446bbd3e5']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': '028a68fb-6435-4b46-a156-c3326f73985c', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '1f4f95d0-0ce1-4061-85d4-946446bbd3e5']}

Receiving new event of type: events...
{'event': 'on_chat_model_stream', 'data': {'chunk': {'content': 'd', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'AIMessageChunk', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None, 'tool_call_chunks': []}}, 'run_id': '028a68fb-6435-4b46-a156-c3326f73985c', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '1f4f95d0-0ce1-4061-85d4-946446bbd3e5']}

Receiving new event of type: events...
{'event': 'on_chat_model_end', 'data': {'output': {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, 'input': {'messages': [[{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1c9a16d2-5f0a-4eba-a0d2-240484a4ce7e', 'tool_call_id': 'tool_call_id'}]]}}, 'run_id': '028a68fb-6435-4b46-a156-c3326f73985c', 'name': 'FakeListChatModel', 'tags': ['seq:step:1'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0, 'ls_model_type': 'chat'}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '1f4f95d0-0ce1-4061-85d4-946446bbd3e5']}

Receiving new event of type: events...
{'event': 'on_chain_start', 'data': {'input': {'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1c9a16d2-5f0a-4eba-a0d2-240484a4ce7e', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}}, 'name': 'should_continue', 'tags': ['seq:step:3'], 'run_id': 'f2b2dfaf-475d-422b-8bf5-02a31bcc7d1a', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '1f4f95d0-0ce1-4061-85d4-946446bbd3e5']}

Receiving new event of type: events...
{'event': 'on_chain_end', 'data': {'output': '__end__', 'input': {'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1c9a16d2-5f0a-4eba-a0d2-240484a4ce7e', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}}, 'run_id': 'f2b2dfaf-475d-422b-8bf5-02a31bcc7d1a', 'name': 'should_continue', 'tags': ['seq:step:3'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8', '1f4f95d0-0ce1-4061-85d4-946446bbd3e5']}

Receiving new event of type: events...
{'event': 'on_chain_stream', 'run_id': '1f4f95d0-0ce1-4061-85d4-946446bbd3e5', 'name': 'agent', 'tags': ['graph:step:8'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0}, 'data': {'chunk': {'messages': [{'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chain_end', 'data': {'output': {'messages': [{'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}], 'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}}, 'input': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1c9a16d2-5f0a-4eba-a0d2-240484a4ce7e', 'tool_call_id': 'tool_call_id'}], 'sleep': None}}, 'run_id': '1f4f95d0-0ce1-4061-85d4-946446bbd3e5', 'name': 'agent', 'tags': ['graph:step:8'], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'langgraph_step': 8, 'langgraph_node': 'agent', 'langgraph_triggers': ['tool'], 'langgraph_task_idx': 0}, 'parent_ids': ['1ef301a5-b867-67de-9e9e-a32e53c5b1f8']}

Receiving new event of type: events...
{'event': 'on_chain_end', 'data': {'output': {'some_bytes': 'c29tZV9ieXRlcw==', 'some_byte_array': 'c29tZV9ieXRlX2FycmF5', 'dict_with_bytes': {'more_bytes': 'bW9yZV9ieXRlcw=='}, 'messages': [{'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '51f2874d-f8c7-4040-8b3b-8f15429a56ae', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-5f556aa0-26ea-42e2-b9e4-7ece3a00974e', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1faf5dd0-ae97-4235-963f-5075083a027a', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ae383611-6a42-475a-912a-09d5972e9e94', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': ""What's the weather in SF?"", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'c67e08e6-e7af-4c4a-aa5e-50c8340ae341', 'example': False}, {'content': 'begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-cb1b98c1-c9e2-4a30-9d7a-38fa1f6224bd', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': 'tool_call__begin', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': None, 'id': '1c9a16d2-5f0a-4eba-a0d2-240484a4ce7e', 'tool_call_id': 'tool_call_id'}, {'content': 'end', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-028a68fb-6435-4b46-a156-c3326f73985c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}, 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'name': 'LangGraph', 'tags': [], 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_id': '1ef301a5-b867-67de-9e9e-a32e53c5b1f8', 'user_id': '', 'thread_id': '7196a3aa-763c-4a8d-bfda-12fbfe1cd727', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca'}, 'parent_ids': []}

Receiving new event of type: end...
None
",What are the steps to stream events from a LangGraph application?
Token-by-Token Streaming,"Token-by-token streaming can be implemented with the events streaming mode. The on_chat_model_stream event type should be processed to stream LLM responses token-by-token. PythonJavascriptCURL

llm_response = """"

# stream token-by-token
async for chunk in client.runs.stream(
    thread_id=thread[""thread_id""],
    assistant_id=""agent"",
    input=input,
    stream_mode=""events"",
):
    if (
        chunk.event == ""events"" and
        chunk.data[""event""] == ""on_chat_model_stream"" and
        len(chunk.data[""data""][""chunk""][""content""]) > 0 and
        'text' in chunk.data[""data""][""chunk""][""content""][0]
    ):
        llm_response += chunk.data[""data""][""chunk""][""content""][0]['text']
        print(llm_response)

const llmResponse = """";
// stream events
const streamResponse = client.runs.stream(
  thread[""thread_id""],
  ""agent"",
  {
    input,
    streamMode: ""events""
  }
);
for await (const chunk of streamResponse) {
  if (chunk.event === ""events"" && chunk.data.event === ""on_chat_model_stream"" && chunk.data.chunk.content.length > 0 && 'text' in chunk.data.chunk.content[0]) {
    llmResponse += chunk.data.data.chunk.content[0].text;
    console.log(llmResponse);
  }
}

curl --request POST \
 --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \
 --header 'Content-Type: application/json' \
 --data ""{
   \""assistant_id\"": \""agent\"",
   \""input\"": {\""messages\"": [{\""role\"": \""human\"", \""content\"": \""What's the weather in sf\""}]},
   \""stream_mode\"": [
     \""events\""
   ]
 }"" | sed 's/\r$//' | awk '
/^event:/ { event = $2 }
/^data:/ {
    json_data = substr($0, index($0, $2))

    if (event == ""events"") {
    print json_data
    }
}' | jq -r '
    select(.event == ""on_chat_model_stream"") |
    .data.chunk.content[] | .text // empty
' | awk '
BEGIN { llm_response="""" }
$0 != """" && $0 != ""null"" {
    llm_response = llm_response $0
    print llm_response
}'

 Output: The
The search
The search results provide
The search results provide the current weather conditions
The search results provide the current weather conditions in San Francisco.
The search results provide the current weather conditions in San Francisco. According
The search results provide the current weather conditions in San Francisco. According to the data,
The search results provide the current weather conditions in San Francisco. According to the data, as
The search results provide the current weather conditions in San Francisco. According to the data, as of 3
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12,
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024,
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C).
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The win
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is bl
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 k
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph).
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70%
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles (10 km
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles (10 km).
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles (10 km). Overall
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles (10 km). Overall, it appears
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles (10 km). Overall, it appears to be a nice
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles (10 km). Overall, it appears to be a nice sunny day in San
The search results provide the current weather conditions in San Francisco. According to the data, as of 3:19 PM on August 12, 2024, the weather in San Francisco is sunny with a temperature of 60.8F (16C). The wind is blowing from the west-southwest at 13.4 mph (21.6 kph). The humidity is 70% and visibility is 6 miles (10 km). Overall, it appears to be a nice sunny day in San Francisco.
",What is the process for implementing token-by-token streaming using the events streaming mode?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Build a Customer Support Bot,"Customer support bots can free up teams' time by handling routine issues, but it can be hard to build a bot that reliably handles diverse tasks in a way that doesn't leave the user pulling their hair out. In this tutorial, you will build a customer support bot for an airline to help users research and make travel arrangements. You'll learn to use LangGraph's interrupts and checkpointers and more complex state to organize your assistant's tools and manage a user's flight bookings, hotel reservations, car rentals, and excursions. It assumes you are familiar with the concepts presented in the LangGraph introductory tutorial. By the end, you'll have built a working bot and gained an understanding of  LangGraph's key concepts and architectures. You'll be able to apply these design patterns to your other AI projects. Your final chat bot will look something like the following diagram: Let's start!",What will you learn to build in the tutorial on LangGraph's interrupts and checkpointers for customer support bots?
Prerequisites,"First, set up your environment. We'll install this tutorial's prerequisites, download the test DB, and define the tools we will reuse in each section. We'll be using Claude as our LLM and define a number of custom tools. While most of our tools will connect to a local sqlite database (and require no additional dependencies), we will also provide a general web search to the agent using Tavily.",What tools will be used in this tutorial and how will they connect to the database?
Tools,"Next, define our assistant's tools to search the airline's policy manual and search and manage reservations for flights, hotels, car rentals, and excursions. We will reuse these tools throughout the tutorial. The exact implementations
aren't important, so feel free to run the code below and jump to Part 1. Lookup Company Policies The assistant retrieve policy information to answer user questions. Note that enforcement of these policies still must be done within the tools/APIs themselves, since the LLM can always ignore this.",What can the assistant's tools be used for in relation to the airline's policy manual and reservations?
Part 1: Zero-shot Agent,"When building, it's best to start with the simplest working implementation and use an evaluation tool like LangSmith to measure its efficacy. All else equal, prefer simple, scalable solutions to complicated ones. In this case, the single-graph approach has limitations. The bot may take undesired actions without user confirmation, struggle with complex queries, and lack focus in its responses. We'll address these issues later. In this section, we will define a simple Zero-shot agent as the assistant, give the agent all of our tools, and prompt it to use them judiciously to assist the user. The simple 2-node graph will look like the following: Start by defining the state. State Define our StateGraph's state as a typed dictionary containing an append-only list of messages. These messages form the chat history, which is all the state our simple assistant needs.",What is the approach taken in defining a Zero-shot agent in this section?
Part 2: Add Confirmation,"When an assistant takes actions on behalf of the user, the user should (almost) always have the final say on whether to follow through with the actions. Otherwise, any small mistake the assistant makes (or any prompt injection it succombs to) can cause real damage to the user. In this section, we will use interrupt_before to pause the graph and return control to the user before executing any of the tools. Your graph will look something like the following: As before, start by defining the state: State & Assistant Our graph state and LLM calling is nearly identical to Part 1 except Exception: 
We've added a user_info field that will be eagerly populated by our graph
We can use the state directly in the Assistant object rather than using the configurable params
",How can the user have the final say on whether to follow through with actions taken by the assistant in Part 2?
Part 3: Conditional Interrupt,"In this section, we'll refine our interrupt strategy by categorizing tools as safe (read-only) or sensitive (data-modifying). We'll apply interrupts to the sensitive tools only, allowing the bot to handle simple queries autonomously. This balances user control and conversational flow, but as we add more tools, our single graph may grow too complex for this ""flat"" structure. We'll address that in the next section. Your graph for Part 3 will look something like the following diagram. State As always, start by defining the graph state. Our state and LLM calling are identical to part 2.",How do we categorize tools in the interrupt strategy in Part 3?
Part 4: Specialized Workflows,"In the previous sections, we saw how ""wide"" chat-bots, relying on a single prompt and LLM to handle various user intents, can get us far. However, it's difficult to create predictably great user experiences for known intents with this approach. Alternatively, your graph can detect userintent and select the appropriate workflow or ""skill"" to satisfy the user's needs. Each workflow can focus on its domain, allowing for isolated improvements without degrading the overall assistant. In this section, we'll split user experiences into separate sub-graphs, resulting in a structure like this: In the diagram above, each square wraps an agentic, focused workflow. The primary assistant fields the user's initial queries, and the graph routes to the appropriate ""expert"" based on the query content. State We want to keep track of which sub-graph is in control at any given moment. While we could do this through some arithmetic on the message list, it's easier to track as a dedicated stack. Add a dialog_state list to the State below. Any time a node is run and returns a value for dialog_state, the update_dialog_stack function will be called to determine how to apply the update.",How can specialized workflows improve user experiences in chat-bots?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the change of theme in Giscus based on the palette color scheme?
Graphs,"At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components: 

State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.

Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.

Edges: Python functions that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.

 By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State. To emphasize: Nodes and Edges are nothing more than Python functions - they can contain an LLM or just good ol' Python code. In short: nodes do the work. edges tell what to do next. LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's Pregel system, the program proceeds in discrete ""super-steps."" A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or ""channels""). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.",How does LangGraph model agent workflows?
StateGraph,The StateGraph class is the main graph class to uses. This is parameterized by a user defined State object.,What is the StateGraph class parameterized by?
MessageGraph,"The MessageGraph class is a special type of graph. The State of a MessageGraph is ONLY a list of messages. This class is rarely used except for chatbots, as most applications require the State to be more complex than a list of messages.",What is the typical use case for the MessageGraph class?
Compiling your graph,"To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method: graph = graph_builder.compile(...)
 You MUST compile your graph before you can use it.",Why is compiling your graph necessary and what does it involve?
State,"The first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.",What is the first step in defining a graph and what does it consist of?
Schema,"The main documented way to specify the schema of a graph is by using TypedDict. However, we also support using a Pydantic BaseModel as your graph state to add default values and additional data validation. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the notebook here for how to use. By default, all nodes in the graph will share the same state. This means that they will read and write to the same state channels. It is possible to have nodes write to private state channels inside the graph for internal node communication - see this notebook for how to do that.",What are the different ways to specify the schema of a graph in the context provided?
Reducers,"Reducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A: from typing import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
 In this example, no reducer functions are specified for any key. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""bye""]} Example B: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 In this example, we've used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let's assume the input to the graph is {""foo"": 1, ""bar"": [""hi""]}. Let's then assume the first Node returns {""foo"": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {""foo"": 2, ""bar"": [""hi""]}. If the second node returns {""bar"": [""bye""]} then the State would then be {""foo"": 2, ""bar"": [""hi"", ""bye""]}. Notice here that the bar key is updated by adding the two lists together. Context Reducer You can use Context channels to define shared resources (such as database connections) that are managed outside of your graph's nodes and excluded from checkpointing. The context manager provided to the Context channel is entered before the first step of the graph execution and exited after the last step, allowing you to set up and clean up resources for the duration of the graph invocation. Read this how to to see an example of using the Context channel in your graph.",How are updates from nodes applied to the State in reducers?
Working with Messages in Graph State,"Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: # this is supported
{""messages"": [HumanMessage(content=""message"")]}

# and this is also supported
{""messages"": [{""type"": ""human"", ""content"": ""message""}]}
 Since the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[""messages""][-1].content. Below is an example of a graph that uses add_messages as it's reducer function. from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated, TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
 MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
",What is the purpose of using messages in the graph state when working with LangChain's ChatModel?
Nodes,"In LangGraph, nodes are typically python functions (sync or async) where the first positional argument is the state, and (optionally), the second positional argument is a ""config"", containing optional configurable parameters (such as a thread_id). Similar to NetworkX, you add these nodes to a graph using the add_node method: from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

builder = StateGraph(dict)

def my_node(state: dict, config: RunnableConfig):
    print(""In node: "", config[""configurable""][""user_id""])
    return {""results"": f""Hello, {state['input']}!""}

# The second argument is optional
def my_other_node(state: dict):
    return state

builder.add_node(""my_node"", my_node)
builder.add_node(""other_node"", my_other_node)
...
 Behind the scenes, functions are converted to RunnableLambda's, which add batch and async support to your function, along with native tracing and debugging. If you add a node to graph without specifying a name, it will be given a default name equivalent to the function name. builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `""my_node""`
",How are nodes typically added to a graph in LangGraph?
STARTNode,"The START Node is a special node that represents the node sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the main purpose of referencing the START Node in a graph?
ENDNode,"The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. from langgraph.graph import END

graph.add_edge(""node_a"", END)
",What is the purpose of the END Node in the langgraph library?
Edges,"Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges: 
Normal Edges: Go directly from one node to the next.
Conditional Edges: Call a function to determine which node(s) to go to next.
Entry Point: Which node to call first when user input arrives.
Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.
 A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep.",What are the key types of edges in a graph and how do they affect the routing of logic and communication between nodes?
Normal Edges,"If you always want to go from node A to node B, you can use the add_edge method directly. graph.add_edge(""node_a"", ""node_b"")
",How can you connect node A to node B in a graph using the add_edge method?
Conditional Edges,"If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a ""routing function"" to call after that node is executed: graph.add_conditional_edges(""node_a"", routing_function)
 Similar to nodes, the routing_function accept the current state of the graph and return a value. By default, the return value routing_function is used as the name of the node (or a list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(""node_a"", routing_function, {True: ""node_b"", False: ""node_c""})
",How can you optionally route to 1 or more edges using the add_conditional_edges method?
Entry Point,"The entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph. from langgraph.graph import START

graph.add_edge(START, ""node_a"")
",What is the purpose of the entry point in a graph and how can it be specified using the add_edge method?
Conditional Entry Point,"A conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this. from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
 You can optionally provide a dictionary that maps the routing_function's output to the name of the next node. graph.add_conditional_edges(START, routing_function, {True: ""node_b"", False: ""node_c""})
",How can you start at different nodes using a conditional entry point in a graph?
Send,"By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common of example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. def continue_to_jokes(state: OverallState):
    return [Send(""generate_joke"", {""subject"": s}) for s in state['subjects']]

graph.add_conditional_edges(""node_a"", continue_to_jokes)
",What does the Send object do in LangGraph and how is it used in conditional edges?
Checkpointer,"LangGraph has a built-in persistence layer, implemented through checkpointers. When you use a checkpointer with a graph, you can interact with the state of that graph. When you use a checkpointer with a graph, you can interact with and manage the graph's state. The checkpointer saves a checkpoint of the graph state at every super-step, enabling several powerful capabilities: First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve steps.Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. Second, it allows for ""memory"" between interactions. You can use checkpointers to create threads and save the state of a thread after a graph executes. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that checkpoint, which will retain its memory of previous ones. See this guide for how to add a checkpointer to your graph.",What capabilities do checkpointers enable when used with a graph in LangGraph?
Threads,"Threads enable the checkpointing of multiple different runs, making them essential for multi-tenant chat applications and other scenarios where maintaining separate states is necessary. A thread is a unique ID assigned to a series of checkpoints saved by a checkpointer. When using a checkpointer, you must specify a thread_id or thread_ts when running the graph. thread_id is simply the ID of a thread. This is always required thread_ts can optionally be passed. This identifier refers to a specific checkpoint within a thread. This can be used to kick of a run of a graph from some point halfway through a thread. You must pass these when invoking the graph as part of the configurable part of the config. config = {""configurable"": {""thread_id"": ""a""}}
graph.invoke(inputs, config=config)
 See this guide for how to use threads.",What is the purpose of threads in multi-tenant chat applications and other scenarios where maintaining separate states is necessary?
Checkpointer state,"When interacting with the checkpointer state, you must specify a thread identifier.Each checkpoint saved by the checkpointer has two properties: 
values: This is the value of the state at this point in time.
next: This is a tuple of the nodes to execute next in the graph.
",What properties does each checkpoint saved by the checkpointer state have?
Get state,"You can get the state of a checkpointer by calling graph.get_state(config). The config should contain thread_id, and the state will be fetched for that thread.",How can you get the state of a checkpointer using graph.get_state(config)?
Get state history,"You can also call graph.get_state_history(config) to get a list of the history of the graph. The config should contain thread_id, and the state history will be fetched for that thread.",What information can be obtained by calling graph.get_state_history(config)?
Update state,"You can also interact with the state directly and update it. This takes three different components: 
config
values
as_node
 config The config should contain thread_id specifying which thread to update. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions that are part of the state. So this does NOT automatically overwrite the state. Let's walk through an example. Let's assume you have defined the state of your graph as: from typing import TypedDict, Annotated
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
 Let's now assume the current state of the graph is {""foo"": 1, ""bar"": [""a""]}
 If you update the state as below: graph.update_state(config, {""foo"": 2, ""bar"": [""b""]})
 Then the new state of the graph will be: {""foo"": 2, ""bar"": [""a"", ""b""]}
 The foo key is completely changed (because there is no reducer specified for that key, so it overwrites it). However, there is a reducer specified for the bar key, and so it appends ""b"" to the state of bar. as_node The final thing you specify when calling update_state is as_node. This update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps in the graph to execute depend on the last node to have given an update, so this can be used to control which node executes next.",How can you update the state of a graph by interacting with the state directly?
Graph Migrations,"LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state. 
For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
For modifying state, we have full backwards and forwards compatibility for adding and removing keys
State keys that are renamed lose their saved state in existing threads
State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.
",What topology changes are supported for threads currently interrupted in LangGraph's graph migrations?
Configuration,"When creating a graph, you can also mark that certain parts of the graph are configurable. This is commonly done to enable easily switching between models or system prompts. This allows you to create a single ""cognitive architecture"" (the graph) but have multiple different instance of it. You can optionally specify a config_schema when creating a graph. class ConfigSchema(TypedDict):
    llm: str

graph = StateGraph(State, config_schema=ConfigSchema)
 You can then pass this configuration into the graph using the configurable config field. config = {""configurable"": {""llm"": ""anthropic""}}

graph.invoke(inputs, config=config)
 You can then access and use this configuration inside a node: def node_a(state, config):
    llm_type = config.get(""configurable"", {}).get(""llm"", ""openai"")
    llm = get_llm(llm_type)
    ...
 See this guide for a full breakdown on configuration",How can you pass and access configuration settings in a graph when creating a cognitive architecture?
Breakpoints,"It can often be useful to set breakpoints before or after certain nodes execute. This can be used to wait for human approval before continuing. These can be set when you ""compile"" a graph. You can set breakpoints either before a node executes (using interrupt_before) or after a node executes (using interrupt_after.) You MUST use a checkpoiner when using breakpoints. This is because your graph needs to be able to resume execution. In order to resume execution, you can just invoke your graph with None as the input. # Initial run of graph
graph.invoke(inputs, config=config)

# Let's assume it hit a breakpoint somewhere, you can then resume by passing in None
graph.invoke(None, config=config)
 See this guide for a full walkthrough of how to add breakpoints.",How can breakpoints be used in a graph to pause execution and resume it later?
Visualization,"It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.",What are some built-in ways to visualize graphs in LangGraph?
Streaming,"LangGraph is built with first class support for streaming. There are several different streaming modes that LangGraph supports: 
""values"": This streams the full value of the state after each step of the graph.
""updates: This streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are streamed separately.
""debug"": This streams as much information as possible throughout the execution of the graph.
 In addition, you can use the astream_events method to stream back events that happen inside nodes. This is useful for streaming tokens of LLM calls. 
ASYNC IN PYTHON<=3.10
You may fail to see events being emitted from inside a node when using .astream_events in Python <= 3.10. If you're using a Langchain RunnableLambda, a RunnableGenerator, or Tool asynchronously inside your node, you will have to propagate callbacks to these objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case. Please see examples here and here.
",What are the different streaming modes supported by LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
How to use a context object in state,"Sometimes you need some resources (like a database connection, a requests session, etc) to live for the duration of your graph execution, without being ever saved by the checkpointer. LangGraph supports decorating a state key with the Context channel, which will take care to 
initialize the value before the graph starts executing, with access to the config passed in to invoke/stream
run any cleanup code you need at the end of the graph execution, whether the graph succeeded or errored
 The argument to context channel should be either a ContextManager class or function.",How can a context object be used in state in LangGraph?
Setup,First we need to install the packages required,What is the first step in the setup process?
Set up the tools,"We will first define the tools we want to use.
For this simple example, we will use create a placeholder search engine.
However, it is really easy to create your own tools - see documentation here on how to do that.",What will be the first step in the process?
Set up the model,"Now we need to load the chat model we want to use.
Importantly, this should satisfy two criteria: 
It should work with messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them.
It should work with OpenAI function calling. This means it should either be an OpenAI model or a model that exposes a similar interface.
 Note: these model requirements are not requirements for using LangGraph - they are just requirements for this one example.",What criteria should the chat model satisfy in order to be used in this example?
Define the context object,"Here we're defining the context object as a pydantic model, which is created by the factory function decorated with @contextmanager. @contextmanager ensures any cleanup code you need can be run at the end of the execution",What is the context object defined as in this scenario?
Define the agent state,"The main type of graph in langgraph is the StateGraph.
This graph is parameterized by a state object that it passes around to each node.
Each node then returns operations to update that state.
These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute.
Whether to set or add is denoted by annotating the state object you construct the graph with. For this example, the state we will track will just be a list of messages.
We want each node to just add messages to that list.
Therefore, we will use a pydantic.BaseModel with one key (messages) and annotate it so that the messages attribute is treated as ""append-only"".",What type of graph is used in langgraph and how is the state object parameterized in it?
Define the nodes,"We now need to define a few different nodes in our graph.
In langgraph, a node can be either a function or a runnable.
There are two main nodes we need for this: 
The agent: responsible for deciding what (if any) actions to take.
A function to invoke tools: if the agent decides to take an action, this node will then execute that action.
 We will also need to define some edges.
Some of these edges may be conditional.
The reason they are conditional is that based on the output of a node, one of several paths may be taken.
The path that is taken is not known until that node is run (the LLM decides). 
Conditional Edge: after the agent is called, we should either:
a. If the agent said to take an action, then the function to invoke tools should be called
b. If the agent said that it was finished, then it should finish
Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next
 Let's define the nodes, as well as a function to decide how what conditional edge to take. MODIFICATION We define each node to receive the AgentState base model as its first argument.",What are the two main nodes that need to be defined in the graph?
Define the graph,We can now put it all together and define the graph!,What can we define by putting it all together?
Use it!,"We can now use it!
This now exposes the same interface as all other LangChain runnables.",What interface does it now expose?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
Structured Output,"It's pretty common to want LLMs inside nodes to return structured output when building agents. This is because that structured output can often be used to route to the next step (e.g. choose between two different edges) or update specific keys of the state. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",How can structured output be used in LangGraph nodes to route to the next step or update specific keys of the state?
Tool calling,"It's extremely common to want agents to do tool calling. Tool calling refers to choosing from several available tools, and specifying which ones to call and what the inputs should be. This is extremely common in agents, as you often want to let the LLM decide which tools to call and then call those tools. Since LangGraph nodes can be arbitrary Python functions, you can do this however you want. If you want to use LangChain, this how-to guide is a starting point.",What is tool calling and why is it common in agents?
Memory,Memory is a key concept to agentic applications. Memory is important because end users often expect the application they are interacting with remember previous interactions. The most simple example of this is chatbots - they clearly need to remember previous messages in a conversation. LangGraph is perfectly suited to give you full control over the memory of your application. With user defined State you can specify the exact schema of the memory you want to retain. With checkpointers you can store checkpoints of previous interactions and resume from there in follow up interactions. See this guide for how to add memory to your graph.,What is the importance of memory in agentic applications and how can LangGraph help in managing it?
Human-in-the-loop,"Agentic systems often require some human-in-the-loop (or ""on-the-loop"") interaction patterns. This is because agentic systems are still not super reliable, so having a human involved is required for any sensitive tasks/actions. These are all easily enabled in LangGraph, largely due to checkpointers. The reason a checkpointer is necessary is that a lot of these interaction patterns involve running a graph up until a certain point, waiting for some sort of human feedback, and then continuing. When you want to ""continue"" you will need to access the state of the graph previous to getting interrupted, and checkpointers are a built in, highly convenient way to do that. There are a few common human-in-the-loop interaction patterns we see emerging.",What are some common human-in-the-loop interaction patterns that are easily enabled in LangGraph?
Approval,"A basic one is to have the agent wait for approval before executing certain tools. This may be all tools, or just a subset of tools. This is generally recommend for more sensitive actions (like writing to a database). This can easily be done in LangGraph by setting a breakpoint before specific nodes. See this guide for how do this in LangGraph.",How can approval be implemented in LangGraph before executing certain tools?
Wait for input,"A similar one is to have the agent wait for human input. This can be done by: 
Create a node specifically for human input
Add a breakpoint before the node
Get user input
Update the state with that user input, acting as that node
Resume execution
 See this guide for how do this in LangGraph.",How can an agent wait for human input in LangGraph?
Edit agent actions,"This is a more advanced interaction pattern. In this interaction pattern the human can actually edit some of the agent's previous decisions. This can be done either during the flow (after a breakpoint, part of the approval flow) or after the fact (as part of time-travel) See this guide for how do this in LangGraph.",How can a human edit an agent's previous decisions in LangGraph?
Time travel,"This is a pretty advanced interaction pattern. In this interaction pattern, the human can look back at the list of previous checkpoints, find one they like, optionally edit it, and then resume execution from there. See this guide for how to do this in LangGraph.",How can a human utilize time travel in the LangGraph interaction pattern?
Review Tool Calls,"This is a specific type of human-in-the-loop interaction but it's worth calling out because it is so common. A lot of agent decisions are made via tool calling, so having a clear UX for reviewing tool calls is handy. A tool call consists of:
- The name of the tool to call
- Arguments to pass to the tool Note that these tool calls can obviously be used for actually calling functions, but they can also be used for other purposes, like to route the agent in a specific direction.
You will want to review the tool call for both of these use cases. When reviewing tool calls, there are few actions you may want to take. 
Approve the tool call (and let the agent continue on its way)
Manually change the tool call, either the tool name or the tool arguments (and let the agent continue on its way after that)
Leave feedback on the tool call. This differs from (2) in that you are not changing the tool call directly, but rather leaving natural language feedback suggesting the LLM call it differently (or call a different tool). You could do this by either adding a ToolMessage and having the feedback be the result of the tool call, or by adding a ToolMessage (that simulates an error) and then a HumanMessage (with the feedback).
 See this guide for how to do this in LangGraph.",What are the actions that can be taken when reviewing tool calls?
Map-Reduce,"A common pattern in agents is to generate a list of objects, do some work on each of those objects, and then combine the results. This is very similar to the common map-reduce operation. This can be tricky for a few reasons. First, it can be tough to define a structured graph ahead of time because the length of the list of objects may be unknown. Second, in order to do this map-reduce you need multiple versions of the state to exist... but the graph shares a common shared state, so how can this be? LangGraph supports this via the Send api. This can be used to allow a conditional edge to Send multiple different states to multiple nodes. The state it sends can be different from the state of the core graph. See a how-to guide for this here",How does LangGraph support the map-reduce operation using the Send api?
Multi-agent,"A term you may have heard is ""multi-agent"" architectures. What exactly does this mean? Given that it is hard to even define an ""agent"", it's almost impossible to exactly define a ""multi-agent"" architecture. When most people talk about a multi-agent architecture, they typically mean a system where there are multiple different LLM-based systems. These LLM-based systems can be as simple as a prompt and an LLM call, or as complex as a ReAct agent. The big question in multi-agent systems is how they communicate. This involves both the schema of how they communicate, as well as the sequence in which they communicate. LangGraph is perfect for orchestrating these types of systems. It allows you to define multiple agents (each one is a node) an arbitrary state (to encapsulate the schema of how they communicate) as well as the edges (to control the sequence in which they communicate).",What is a multi-agent architecture and how do agents communicate in such systems?
Planning,"One of the big things that agentic systems struggle with is long term planning. A common technique to overcome this is to have an explicit planning this. This generally involves calling an LLM to come up with a series of steps to execute. From there, the system then tries to execute the series of tasks (this could use a sub-agent to do so). Optionally, you can revisit the plan after each step and update it if needed.",How do agentic systems overcome their struggle with long term planning?
Reflection,"Agents often struggle to produce reliable results. Therefore, it can be helpful to check whether the agent has completed a task correctly or not. If it has - then you can finish. If it hasn't - then you can take the feedback on why it's not correct and pass it back into another iteration of the agent. This ""reflection"" step often uses an LLM, but doesn't have to. A good example of where using an LLM may not be necessary is in coding, when you can try to compile the generated code and use any errors as the feedback.",What is a common step that can be taken to ensure reliable results from agents?
ReAct Agent,"One of the most common agent architectures is what is commonly called the ReAct agent architecture. In this architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it is not worth calling any more tools. One of the few high level, pre-built agents we have in LangGraph - you can use it with create_react_agent This is named after and based on the ReAct paper. However, there are several differences between this paper and our implementation: 
First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Forth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a ""Thought"" step before deciding which tools to call. This is the ""Reasoning"" part of ""ReAct"". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
 See this guide for a full walkthrough of how to use the prebuilt ReAct agent.",What are some key differences between the ReAct paper and the implementation in LangGraph?
Comments,"
 
    var giscus = document.querySelector(""script[src*=giscus]"")

    // Set palette on initial load
    var palette = __md_get(""__palette"")
    if (palette && typeof palette.color === ""object"") {
      var theme = palette.color.scheme === ""slate""
        ? ""transparent_dark""
        : ""light""

      // Instruct Giscus to set theme
      giscus.setAttribute(""data-theme"", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener(""DOMContentLoaded"", function() {
      var ref = document.querySelector(""[data-md-component=palette]"")
      ref.addEventListener(""change"", function() {
        var palette = __md_get(""__palette"")
        if (palette && typeof palette.color === ""object"") {
          var theme = palette.color.scheme === ""slate""
            ? ""transparent_dark""
            : ""light""

          // Instruct Giscus to change theme
          var frame = document.querySelector("".giscus-frame"")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            ""https://giscus.app""
          )
        }
      })
    })
  ",What event triggers the registration of event handlers for changing the theme in Giscus?
