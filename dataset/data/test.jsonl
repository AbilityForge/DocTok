{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens in LangSmith when you add, update, or delete examples in a dataset?"}, {"role": "assistant", "content": "Version datasets In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to a dataset when examples are added, updated, or deleted?"}, {"role": "assistant", "content": "Create a new version of a dataset Any time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved. By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the \"Examples\" tab, you can see the state of the dataset at that point in time. Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the \"latest\" version of the dataset. Also, by default the latest version of the dataset is shown in the \"Examples\" tab and experiments from all versions are shown in the \"Tests\" tab. In the \"Tests\" tab, you can see the results of tests run on the dataset at different versions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you tag a version of your dataset to mark important milestones?"}, {"role": "assistant", "content": "Tag a version You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history. For example, you might tag a version of your dataset as \"prod\" and use it to run tests against your LLM pipeline. Tagging can be done in the UI by clicking on \"+ Tag this version\" in the \"Examples\" tab. You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK: from langsmith import Clientfromt datetime import datetimeclient = Client()initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like \"prod\"client.update_dataset_tag(    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\") To run an evaluation on a particular tagged version of a dataset, you can follow this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will API keys prefixed with ls__ be disabled in favor of lsv2 style keys?"}, {"role": "assistant", "content": "LangSmith Release Notes noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith v0.6 during the week of June 17, 2024?"}, {"role": "assistant", "content": "Week of June 17, 2024 - LangSmith v0.6 LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced in LangSmith?"}, {"role": "assistant", "content": "New Features Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the performance and reliability of run rules and querying the Hub via the SDK?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the default port and Helm configuration in the recent infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes [Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were added in the recent admin changes?"}, {"role": "assistant", "content": "Admin changes Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.5.x and earlier following the release of v0.6?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and features does LangSmith v0.5 introduce?"}, {"role": "assistant", "content": "Week of May 13, 2024 - LangSmith v0.5 LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after the LangSmith Helm release v0.7 in August 2024?"}, {"role": "assistant", "content": "Breaking changes We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced, including role-based access controls and improvements in regression testing and production monitoring?"}, {"role": "assistant", "content": "New Features Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to improve performance and reliability in resource management?"}, {"role": "assistant", "content": "Performance and Reliability Changes Split ingest, session deletion, and automation jobs to execute within separate resource pools."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default storage size for Clickhouse persistence in LangSmith v0.4, and how can it be adjusted?"}, {"role": "assistant", "content": "Infrastructure changes As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes regarding access controls and API keys?"}, {"role": "assistant", "content": "Admin changes Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.4.x and earlier following the release of v0.5?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.4 during the week of March 25, 2024?"}, {"role": "assistant", "content": "Week of March 25, 2024 - LangSmith v0.4 LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes should be made to ensure backwards compatibility when updating to the new release?"}, {"role": "assistant", "content": "Breaking changes This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to enhance performance and reliability in the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the image repositories and Clickhouse persistence storage in the infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to the API key management and OAuth flow in the recent admin updates?"}, {"role": "assistant", "content": "Admin changes Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.3.x and earlier versions following the release of 0.4?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.3 during the week of February 21, 2024?"}, {"role": "assistant", "content": "Week of Februrary 21, 2024 - LangSmith v0.3 LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do to retain run data when migrating from LangSmith v0.1?"}, {"role": "assistant", "content": "Breaking changes This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the LangSmith UI regarding trace ingestion and display delay?"}, {"role": "assistant", "content": "Performance and Reliability Changes Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is available regarding admin changes?"}, {"role": "assistant", "content": "Admin changes nan"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.2.x and earlier versions following the release of 0.3?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features does LangSmith 0.2 include?"}, {"role": "assistant", "content": "Week of January 29, 2024 - LangSmith v0.2 LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What versions of langsmith-sdk are required for this release to support changes in pagination of API results?"}, {"role": "assistant", "content": "Requirements This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to make custom metadata in traces searchable after the change in search syntax?"}, {"role": "assistant", "content": "Breaking changes The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,}) and then search using has(metadata, '{\"user_name\": ...}')"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to the performance and reliability of the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What database service has been added to improve performance and scalability for storing run results in LangSmith?"}, {"role": "assistant", "content": "Infrastructure Changes Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the new maximum number of users allowed per organization for new organizations?"}, {"role": "assistant", "content": "Admin changes Increased the maximum number of users per organization from 5 to 100 for new organizations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of LangSmith 0.1.x following the release of version 0.2?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the recommended methods for sending traces to LangSmith, and what should be included in the request when using the LangSmith REST API?"}, {"role": "assistant", "content": "Trace using the LangSmith REST API It is HIGHLY recommended to use our Python or TypeScript SDKs to send traces to LangSmith.\nWe have designed these SDKs with several optimizations, including batching and backgrounding, to ensure that your application's performance is not impacted by sending traces to LangSmith.\nHowever, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application.\nThis guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation here for a full list of endpoints and request/response schemas. noteWhen using the LangSmith REST API, you will need to provide your API key in the request headers as \"x-api-key\".Additionally, you should IGNORE and not set the dotted_order and trace_id fields in the request body. These fields will be automatically generated by the system. The following example shows how you might leverage our API directly in Python. The same principles apply to other languages. import openaiimport osimport requestsfrom datetime import datetimefrom uuid import uuid4def post_run(run_id, name, run_type, inputs, parent_id=None):    \"\"\"Function to post a new run to the API.\"\"\"    data = {        \"id\": run_id.hex,        \"name\": name,        \"run_type\": run_type,        \"inputs\": inputs,        \"start_time\": datetime.utcnow().isoformat(),    }    if parent_id:        data[\"parent_run_id\"] = parent_id.hex    requests.post(        \"https://api.smith.langchain.com/runs\", # Update appropriately for self-hosted installations or the EU region        json=data,        headers=headers    )def patch_run(run_id, outputs):    \"\"\"Function to patch a run with outputs.\"\"\"    requests.patch(        f\"https://api.smith.langchain.com/runs/{run_id}\",        json={            \"outputs\": outputs,            \"end_time\": datetime.utcnow().isoformat(),        },        headers=headers,    )# Send your API Key in the request headersheaders = {\"x-api-key\": os.environ[\"LANGCHAIN_API_KEY\"]}# This can be a user input to your appquestion = \"Can you summarize this morning's meetings?\"# This can be retrieved in a retrieval stepcontext = \"During this morning's meeting, we solved all world conflict.\"messages = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"},    {\"role\": \"user\", \"content\": f\"Question: {question}\\\\nContext: {context}\"}]# Create parent runparent_run_id = uuid4()post_run(parent_run_id, \"Chat Pipeline\", \"chain\", {\"question\": question})# Create child runchild_run_id = uuid4()post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)# End runspatch_run(child_run_id, chat_completion.dict())patch_run(parent_run_id, {\"answer\": chat_completion.choices[0].message.content})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What SDKs can be used to manage prompts programmatically according to the provided information?"}, {"role": "assistant", "content": "Manage prompts programmatically You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.\nAll functionality going forward will live in the langsmith package."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What commands are used to install the LangChain and Langsmith packages in Python and TypeScript?"}, {"role": "assistant", "content": "Install packages PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you do not have the LANGCHAIN_API_KEY set for your current workspace?"}, {"role": "assistant", "content": "Configure environment variables If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=\"lsv2_...\" TerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create or update a prompt using the push prompt method in PythonLangChain or TypeScript?"}, {"role": "assistant", "content": "Push a prompt To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");url = prompts.push(\"joke-generator\", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.\nThis is useful for storing the model configuration you want to use with this prompt.\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"langchain-openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);prompts.push(\"joke-generator-with-model\", chain);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method do you use to pull a prompt from the LangChain Hub, and what information is required for pulling a public prompt?"}, {"role": "assistant", "content": "Pull a prompt To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { ChatOpenAI } from \"langchain-openai\";const prompt = prompts.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);chain.invoke({\"topic\": \"cats\"}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\nJust specify include_model when pulling the prompt.\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\nMake sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = prompts.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });chain.invoke({\"topic\": \"cats\"}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use prompts stored in LangSmith directly with a model provider's API?"}, {"role": "assistant", "content": "Use a prompt without LangChain If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.\nThese convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to list, delete, and like/unlike prompts in the LangSmith SDK?"}, {"role": "assistant", "content": "List, delete, and like prompts You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\nSee the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\"); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of annotation queues in LangSmith?"}, {"role": "assistant", "content": "Use annotation queues Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.\nWhile you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in creating an annotation queue and what settings can be configured for multiple annotators?"}, {"role": "assistant", "content": "Create an annotation queue To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.\nThen click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.\nYou can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as \"Done\" for it to be removed from the queue. If you check \"All workspace members review each run,\" then a run will remain in the queue until all workspace members have marked it \"Done\".Enable reservations on runs: We recommend enabling reservations.\nThis will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified \"reservation length\". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it \"Done\", the reservation will expire after the specified \"reservation length\".\nThe run is then released back into the queue and can be reserved by another reviewer. noteClicking \"Requeue at end\" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you assign runs to an annotation queue?"}, {"role": "assistant", "content": "Assign runs to an annotation queue To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.\nSelect multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.\nSet up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.\nTo learn more about how to capture user feedback from your LLM application, follow this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you review runs in an annotation queue?"}, {"role": "assistant", "content": "Review runs in an annotation queue To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.\nThen click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.\nYou can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to \"View run\". The keyboard shortcuts shown can help streamline the review process."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can users upload experiments run outside of LangSmith to view results in the LangSmith UI?"}, {"role": "assistant", "content": "Upload experiments run outside of LangSmith with the REST API Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required to upload an experiment using the /datasets/upload-experiment endpoint?"}, {"role": "assistant", "content": "Request body schema Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within\nthe experiment. Each object in the results represents a \"row\" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name\nrefer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset\nin LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  \"experiment_name\": \"string (required)\",  \"experiment_description\": \"string (optional)\",  \"experiment_start_time\": \"datetime (required)\",  \"experiment_end_time\": \"datetime (required)\",  \"dataset_id\": \"uuid (optional - an external dataset id, used to group experiments together)\",  \"dataset_name\": \"string (optional - must provide either dataset_id or dataset_name)\",  \"dataset_description\": \"string (optional)\",  \"experiment_metadata\": { // Object (any shape - optional)    \"key\": \"value\"  },  \"summary_experiment_scores\": [ // List of summary feedback objects (optional)    {      \"key\": \"string (required)\",      \"score\": \"number (optional)\",      \"value\": \"string (optional)\",      \"comment\": \"string (optional)\",      \"feedback_source\": { // Object (optional)        \"type\": \"string (required)\"      },      \"feedback_config\": { // Object (optional)        \"type\": \"string enum: continuous, categorical, or freeform\",        \"min\": \"number (optional)\",        \"max\": \"number (optional)\",        \"categories\": [ // List of feedback category objects (optional)            \"value\": \"number (required)\",            \"label\": \"string (optional)\"        ]      },      \"created_at\": \"datetime (optional - defaults to now)\",      \"modified_at\": \"datetime (optional - defaults to now)\",      \"correction\": \"Object or string (optional)\"    }  ],  \"results\": [ // List of experiment row objects (required)    {      \"row_id\": \"uuid (required)\",      \"inputs\": {     // Object (required - any shape). This will        \"key\": \"val\"  // be the input to both the run and the dataset example.      },      \"expected_outputs\": { // Object (optional - any shape).        \"key\": \"val\"        // These will be the outputs of the dataset examples.      },      \"actual_outputs\": { // Object (optional - any shape).        \"key\": \"val       // These will be the outputs of the runs.      },      \"evaluation_scores\": [ // List of feedback objects for the run (optional)        {            \"key\": \"string (required)\",            \"score\": \"number (optional)\",            \"value\": \"string (optional)\",            \"comment\": \"string (optional)\",            \"feedback_source\": { // Object (optional)                \"type\": \"string (required)\"            },            \"feedback_config\": { // Object (optional)                \"type\": \"string enum: continuous, categorical, or freeform\",                \"min\": \"number (optional)\",                \"max\": \"number (optional)\",                \"categories\": [ // List of feedback category objects (optional)                    \"value\": \"number (required)\",                    \"label\": \"string (optional)\"                ]            },            \"created_at\": \"datetime (optional - defaults to now)\",            \"modified_at\": \"datetime (optional - defaults to now)\",            \"correction\": \"Object or string (optional)\"        }      ],      \"start_time\": \"datetime (required)\", // The start/end times for the runs will be used to      \"end_time\": \"datetime (required)\",   // calculate latency. They must all fall between the      \"run_name\": \"string (optional)\",     // start and end times for the experiment.      \"error\": \"string (optional)\",      \"run_metadata\": { // Object (any shape - optional)        \"key\": \"value\"      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for uploading multiple experiments to the same dataset?"}, {"role": "assistant", "content": "Considerations You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together\nunder a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if\nyou only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key details of the experiment uploaded to LangSmith, including its name, description, and start and end times?"}, {"role": "assistant", "content": "Example request Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  \"experiment_name\": \"My external experiment\",  \"experiment_description\": \"An experiment uploaded to LangSmith\",  \"dataset_name\": \"my-external-dataset\",  \"summary_experiment_scores\": [    {      \"key\": \"summary_accuracy\",      \"score\": 0.9,      \"comment\": \"Great job!\"    }  ],  \"results\": [    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the weather in San Francisco today?\"      },      \"expected_outputs\": {        \"output\": \"Sorry, I am unable to provide information about the current weather.\"      },      \"actual_outputs\": {        \"output\": \"The weather is partly cloudy with a high of 65.\"      },      \"evaluation_scores\": [        {          \"key\": \"hallucination\",          \"score\": 1,          \"comment\": \"The chatbot made up the weather instead of identifying that \"                     \"they don't have enough info to answer the question. This is \"                     \"a hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:39\",      \"end_time\": \"2024-08-03T00:12:41\",      \"run_name\": \"Chatbot\"    },    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the square root of 49?\"      },      \"expected_outputs\": {        \"output\": \"The square root of 49 is 7.\"      },      \"actual_outputs\": {        \"output\": \"7.\"      },      \"evaluation_scores\": [       {          \"key\": \"hallucination\",          \"score\": 0,          \"comment\": \"The chatbot correctly identified the answer. This is not a \"                     \"hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:40\",      \"end_time\": \"2024-08-03T00:12:42\",      \"run_name\": \"Chatbot\"    }  ],  \"experiment_start_time\": \"2024-08-03T00:12:38\",  \"experiment_end_time\": \"2024-08-03T00:12:43\"}resp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\",    json=body,    headers={\"x-api-key\": os.environ[\"LANGCHAIN_API_KEY\"]})print(resp.json()) Below is the response received: {  \"dataset\": {    \"name\": \"my-external-dataset\",    \"description\": null,    \"created_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"data_type\": \"kv\",    \"inputs_schema_definition\": null,    \"outputs_schema_definition\": null,    \"externally_managed\": true,    \"id\": \"<<uuid>>\",    \"tenant_id\": \"<<uuid>>\",    \"example_count\": 0,    \"session_count\": 0,    \"modified_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"last_session_start_time\": null  },  \"experiment\": {    \"start_time\": \"2024-08-03T00:12:38\",    \"end_time\": \"2024-08-03T00:12:43+00:00\",    \"extra\": null,    \"name\": \"My external experiment\",    \"description\": \"An experiment uploaded to LangSmith\",    \"default_dataset_id\": null,    \"reference_dataset_id\": \"<<uuid>>\",    \"trace_tier\": \"longlived\",    \"id\": \"<<uuid>>\",    \"run_count\": null,    \"latency_p50\": null,    \"latency_p99\": null,    \"first_token_p50\": null,    \"first_token_p99\": null,    \"total_tokens\": null,    \"prompt_tokens\": null,    \"completion_tokens\": null,    \"total_cost\": null,    \"prompt_cost\": null,    \"completion_cost\": null,    \"tenant_id\": \"<<uuid>>\",    \"last_run_start_time\": null,    \"last_run_start_time_live\": null,    \"feedback_stats\": null,    \"session_feedback_stats\": null,    \"run_facets\": null,    \"error_rate\": null,    \"streaming_rate\": null,    \"test_run_number\": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.\nIf you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this\ninformation in the request body)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will you see after clicking on your newly-created dataset in the UI?"}, {"role": "assistant", "content": "View the experiment in the UI Now, login to the UI and click on your newly-created dataset! You should see a single experiment:\n Your examples will have been uploaded:\n Clicking on your experiment will bring you to the comparison view:\n As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using LangChain's off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.\nThese evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello Harrison\"),]dataset_name = \"Hello Set\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three QA evaluators mentioned, and which one is recommended based on meta-evals?"}, {"role": "assistant", "content": "Use question and answer (correctness) evaluators Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we recommend using \"cot_qa\", or Chain of Thought QA. Here is a trivial example that uses a \"cot_qa\" evaluator to evaluate a simple pipeline that prefixes the input with \"Hello\": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using \"criteria\" evaluators when ground truth reference labels are not available?"}, {"role": "assistant", "content": "Use criteria evaluators If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The \"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",    config={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent \"direction\" (i.e., higher is not necessarily better).\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for \"helpfulness\" means that the prediction was deemed to be helpful by the model.\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content, which, of course, is \"bad\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functions of the \"labeled_criteria\" and \"labeled_score_string\" evaluators in evaluating predictions against reference labels?"}, {"role": "assistant", "content": "Use labeled criteria evaluators If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators. The \"labeled_criteria\" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={        \"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of distance metrics used to measure similarity between a predicted string and a reference?"}, {"role": "assistant", "content": "Use string or embedding distance metrics To measure the similarity between a predicted string and a reference, you can use string distance metrics: The \"string_distance\" evaluator computes a normalized string edit distance between the prediction and referenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    \"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\": True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # \"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",      })evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries are required to customize the model used for LLM-based evaluators?"}, {"role": "assistant", "content": "Use a custom LLM in off-the-shelf evaluators You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a model that outputs multiple fields using LangChain's evaluators?"}, {"role": "assistant", "content": "Handle multiple input or output fields LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\". We want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    },    prepare_data=lambda run, example: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],        \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" + input[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components and examples of automation rules that can be set up in LangSmith?"}, {"role": "assistant", "content": "Set up automation rules While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users.\nLangSmith provides a powerful feature called automations that allow you to trigger certain actions on your trace data.\nAt a high level, automations are defined by a filter, sampling rate, and action. Automation rules can trigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an annotation queue, and triggering a webhook. An example of an automation you can set up can be \"trigger an online evaluation that grades on vagueness for all of my downvoted traces.\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps for creating an automation rule in LangSmith?"}, {"role": "assistant", "content": "Create a rule We will outline the steps for creating an automation rule in LangSmith below."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of rules that can be created and what actions are allowed for each?"}, {"role": "assistant", "content": "Step 1: Navigate to rule creation To create a rule, head click on Rules in the top right corner of any project details page, then scroll to the bottom and click on + Add Rule. Alternatively, you can access rules in settings by navigating to this link, click on + Add Rule, then Project Rule. noteThere are currently two types of rules you can create: Project Rule and Dataset Rule.Project Rule: This rule will apply to traces in the specified project. Actions allowed are adding to a dataset, adding to an annotation queue, running online evaluation, and triggering a webhook.Dataset Rule: This rule will apply to traces that are part of an experiment in the specified dataset. Actions allowed are only running an evaluator on the experiment results. To see this in action, you can follow this guide. Give your rule a name, for example \"my_rule\":"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a filter to filter traces in the project?"}, {"role": "assistant", "content": "Step 2: Define the filter You can create a filter as you normally would to filter traces in the project. For more information on filters, you can refer to this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you apply a new rule to past runs?"}, {"role": "assistant", "content": "(Optional) Step 3: Apply Rule to Past Runs When creating a new rule, you can apply the rule to past runs as well. To do this, select Apply to Past Runs checkbox and enter Backfill From date as the\nstart date to apply the rule. This will start from the Backfill From date and apply the run rules until it is caught up with the latest runs. Note that you will have to expand the date range for logs if you wanted to look at the progress of the backfill, see View logs for your automations for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of setting a sampling rate in automations?"}, {"role": "assistant", "content": "Step 4: Define the sampling rate You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the four actions that can be taken with an automation rule?"}, {"role": "assistant", "content": "Step 5: Define the action There are four actions you can take with an automation rule: Add to dataset: Add the inputs and outputs of the trace to a dataset.Add to annotation queue: Add the trace to an annotation queue.Run online evaluation: Run an online evaluation on the trace. For more information on online evaluations, you can refer to this guide.Trigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.Extend data retention: Extends the data retention period on matching traces that use base retention.\nNote that all other rules will also extend data retention on matching traces through the\nauto-upgrade mechanism,\nbut this rule takes no additional action."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view logs for your automations and what information do they provide?"}, {"role": "assistant", "content": "View logs for your automations You can view logs for your automations by going to Settings -> Rules and click on the Logs button in any row. You can also get to logs by clicking on Rules in the top right hand corner of any project details page, then clicking on See Logs for any rule. Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced.\nIf a particular rule execution has triggered an error, you can view the error message by hovering over the error icon. By default, rule logs only show results for runs that occurred in the last day. To see results for older runs, you can select Last 1 day and enter the desired date range.\nWhen applying a rule to past runs, the processing will start from the start date and go forward, so this would be needed to view logs while the backfill is proceeding."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens when you add a webhook URL on an automation action?"}, {"role": "assistant", "content": "Set up webhook notifications for rules When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is included in the webhook payload sent to your endpoint?"}, {"role": "assistant", "content": "Webhook payload The payload we send to your webhook endpoint contains \"rule_id\" this is the ID of the automation that sent this payload\"start_time\" and \"end_time\" these are the time boundaries where we found matching runs\"runs\" this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.\"feedback_stats\" this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below. \"feedback_stats\": {    \"about_langchain\": {        \"n\": 1,        \"avg\": 0.0,        \"show_feedback_arrow\": true,        \"values\": {}    },    \"category\": {        \"n\": 0,        \"avg\": null,        \"show_feedback_arrow\": true,        \"values\": {            \"CONCEPTUAL\": 1        }    },    \"user_score\": {        \"n\": 2,        \"avg\": 0.0,        \"show_feedback_arrow\": false,        \"values\": {}    },    \"vagueness\": {        \"n\": 1,        \"avg\": 0.0,        \"show_feedback_arrow\": true,        \"values\": {}    }}, fetching from S3 URLsDepending on how recent your runs are, the inputs_s3_urls and outputs_s3_urls fields may contain S3 URLs to the actual data instead of the data itself.The inputs and outputs can be fetched by the ROOT.presigned_url provided in inputs_s3_urls and outputs_s3_urls respectively. This is an example of the entire payload we send to your webhook endpoint: {  \"rule_id\": \"d75d7417-0c57-4655-88fe-1db3cda3a47a\",  \"start_time\": \"2024-04-05T01:28:54.734491+00:00\",  \"end_time\": \"2024-04-05T01:28:56.492563+00:00\",  \"runs\": [    {      \"status\": \"success\",      \"is_root\": true,      \"trace_id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"dotted_order\": \"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"run_type\": \"tool\",      \"modified_at\": \"2024-04-05T01:28:54.145062\",      \"tenant_id\": \"2ebda79f-2946-4491-a9ad-d642f49e0815\",      \"end_time\": \"2024-04-05T01:28:54.085649\",      \"name\": \"Search\",      \"start_time\": \"2024-04-05T01:28:54.085646\",      \"id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"session_id\": \"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5\",      \"parent_run_ids\": [],      \"child_run_ids\": null,      \"direct_child_run_ids\": null,      \"total_tokens\": 0,      \"completion_tokens\": 0,      \"prompt_tokens\": 0,      \"total_cost\": null,      \"completion_cost\": null,      \"prompt_cost\": null,      \"first_token_time\": null,      \"app_path\": \"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809\",      \"in_dataset\": false,      \"last_queued_at\": null,      \"inputs\": null,      \"inputs_s3_urls\": null,      \"outputs\": null,      \"outputs_s3_urls\": null,      \"extra\": null,      \"events\": null,      \"feedback_stats\": null,      \"serialized\": null,      \"share_token\": null    }  ]}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a recommended method to enhance the security of a webhook URL?"}, {"role": "assistant", "content": "Webhook Security We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications. An example would be https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure specific HTTP headers for a webhook?"}, {"role": "assistant", "content": "Webhook custom HTTP headers If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the Headers option next to the URL field and add your headers. noteHeaders are stored in encrypted format."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens if a webhook endpoint returns a 4xx status code?"}, {"role": "assistant", "content": "Webhook Delivery When delivering events to your webhook endpoint we follow these guidelines If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.Anything your endpoint returns in the body will be ignored"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to set up a Modal account and install the Modal SDK?"}, {"role": "assistant", "content": "Setup For an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here. First, create a Modal account. Then, locally install the Modal SDK: pip install modal To finish setting up your account, run the command: modal setup and follow the instructions"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the name of the secret that needs to be created in Modal for LangSmith authentication?"}, {"role": "assistant", "content": "Secrets Next, you will need to set up some secrets in Modal. First, LangSmith will need to authenticate to Modal by passing in a secret.\nThe easiest way to do this is to pass in a secret in the query parameters.\nTo validate this secret, we will need to add a secret in Modal to validate it.\nWe will do that by creating a Modal secret.\nYou can see instructions for secrets here.\nFor this purpose, let's call our secret ls-webhook and have it set an environment variable with the name LS_WEBHOOK. We can also set up a LangSmith secret - luckily there is already an integration template for this!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the Python file created in the service described?"}, {"role": "assistant", "content": "Service After that, you can create a Python file that will serve as your endpoint.\nAn example is below, with comments explaining what is going on: from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub(\"auth-example\", image=Image.debian_slim().pip_install(\"langsmith\"))@stub.function(    secrets=[Secret.from_name(\"ls-webhook\"), Secret.from_name(\"my-langsmith-secret\")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method=\"POST\")# We set up a `secret` query parameterdef f(data: dict, secret: str = Query(...)):    # You can import dependencies you don't have locally inside Modal funxtions    from langsmith import Client    # First, we validate the secret key we pass    import os    if secret != os.environ[\"LS_WEBHOOK\"]:        raise HTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=\"Incorrect bearer token\",            headers={\"WWW-Authenticate\": \"Bearer\"},        )    # This is where we put the logic for what should happen inside this webhook    ls_client = Client()    runs = data[\"runs\"]    ids = [r[\"id\"] for r in runs]    feedback = list(ls_client.list_feedback(run_ids=ids))    for r, f in zip(runs, feedback):        try:            ls_client.create_example(                inputs=r[\"inputs\"],                outputs={\"output\": f.correction},                dataset_name=\"classifier-github-issues\",            )        except Exception:            raise ValueError(f\"{r} and {f}\")    # Function body    return \"success!\" We can now deploy this easily with modal deploy ... (see docs here). You should now get something like:  Created objects.  Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py  Created mount PythonPackage:langsmith  Created f => https://hwchase17--auth-example-f.modal.run App deployed! View Deployment: https://modal.com/apps/hwchase17/auth-example The important thing to remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.\nNOTE: this is NOT the final deployment URL, make sure not to accidentally use that."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How should the function URL be formatted when adding it as a webhook, including the secret key?"}, {"role": "assistant", "content": "Hooking it up We can now take the function URL we create above and add it as a webhook.\nWe have to remember to also pass in the secret key as a query parameter.\nPutting it all together, it should look something like: https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET} Replace {SECRET} with the secret key you created to access the Modal service."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the destination project of your traces?"}, {"role": "assistant", "content": "Log traces to specific project You can change the destination project of your traces both statically through environment variables and dynamically at runtime."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set a custom project name for an application run in LangSmith?"}, {"role": "assistant", "content": "Set the destination project statically As mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-custom-project If the project specified does not exist, it will be created automatically when the first trace is ingested."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name dynamically at runtime when logging traces in an application?"}, {"role": "assistant", "content": "Set the destination project dynamically You can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application. noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(    run_type=\"llm\",    name=\"OpenAI Call Decorator\",    project_name=\"My Project\")def call_openai(    messages: list[dict], model: str = \"gpt-3.5-turbo\") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(    messages,    langsmith_extra={\"project_name\": \"My Overriden Project\"},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=messages,    langsmith_extra={\"project_name\": \"My Project\"},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(    run_type=\"llm\",    name=\"OpenAI Call RunTree\",    inputs={\"messages\": messages},    project_name=\"My Project\")chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from \"langsmith\";const client = new OpenAI();const messages = [    {role: \"system\", content: \"You are a helpful assistant.\"},    {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: \"llm\",    name: \"OpenAI Call Traceable\",    project_name: \"My Project\"});// Call the traceable functionawait traceableCallOpenAI(messages, \"gpt-3.5-turbo\");// Create and use a RunTree objectconst rt = new RunTree({    runType: \"llm\",    name: \"OpenAI Call RunTree\",    inputs: { messages },    project_name: \"My Project\"});// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must be taken to generate synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to filter runs for analysis and export in the SDK and API?"}, {"role": "assistant", "content": "Trace query syntax Using the list_runs method in the SDK or /runs/query endpoint in the API, you can filter runs to analyze and export."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different filtering options available for fetching runs in a project?"}, {"role": "assistant", "content": "Filter arguments KeysDescriptionproject_id / project_nameThe project(s) to fetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a specific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name / dataset_idFetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.reference_example_idFetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.errorFetch runs that errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other filtering arguments.filterFetch runs that match a given structured filter statement. See the guide below for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to return in the response. By default, all fields are returned.query (experimental)Natural language query, which translates your query into a filter statement."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What filtering capabilities does LangSmith's filter query language support for fetching runs?"}, {"role": "assistant", "content": "Filter query language LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs. The filtering grammar is based on common comparators on fields in the run object. Supported comparators include: gte (greater than or equal to)gt (greater than)lte (less than or equal to)lt (less than)eq (equal to)neq (not equal to)has (check if run contains a tag or metadata json blob)search (search for a substring in a string field) Additionally, you can combine multiple comparisons through and and or operators. These can be applied on fields of the run object, such as its id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and any associated feedback through feedback_key and feedback_score."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a prompt from a trace in the Trace UI?"}, {"role": "assistant", "content": "Open a prompt from a trace If you pull a prompt into your code and begin logging traces that use it, you can find a link to the prompt in the Trace UI. In the run that used the prompt, hover over the Prompt tag. Clicking on this will take you to the prompt. (If you used a LangChain Hub prompt, this tag will say Hub)\n] In the metadata of the run, you can see more details. Click on an individual prompt metadata value to filter your traces by that attribute. You can filter by prompt handle, prompt name, or prompt commit hash.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What components make up the LangSmith application, and how can it be run in a controlled cloud environment?"}, {"role": "assistant", "content": "Architectural overview Enterprise License RequiredSelf-Hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith can be run via Kubernetes (recommended) or Docker in a Cloud environment that you control. The LangSmith application consists of several components including 5 LangSmith servers and 3 stateful services: LangSmith FrontendLangSmith BackendLangSmith Platform BackendLangSmith PlaygroundLangSmith QueueClickHousePostgresRedis To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended storage service configuration for LangSmith in a production setting?"}, {"role": "assistant", "content": "Storage Services noteLangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services.\nIn a production setting, we strongly recommend using external Storage Services."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database management system is ClickHouse and what is its primary use for LangSmith?"}, {"role": "assistant", "content": "ClickHouse ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database system is PostgreSQL and how does LangSmith utilize it?"}, {"role": "assistant", "content": "PostgreSQL PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database is Redis and how does it enhance performance for operations like caching?"}, {"role": "assistant", "content": "Redis Redis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching. LangSmith uses Redis to back queuing/caching operations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What role does Nginx play in the LangSmith frontend?"}, {"role": "assistant", "content": "LangSmith Frontend The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the primary function of the LangSmith backend in relation to API requests?"}, {"role": "assistant", "content": "LangSmith Backend The backend is the primary entrypoint for API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and sdk, preparing traces for ingestion, and supporting the hub API."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functions does the LangSmith Queue perform in relation to incoming traces and feedback?"}, {"role": "assistant", "content": "LangSmith Queue The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the primary function of the LangSmith platform backend?"}, {"role": "assistant", "content": "LangSmith Platform Backend The platform backend is an internal service that primarily handles authentication and other high-volume tasks. The user should not need to interact with this service directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LangSmith Playground service?"}, {"role": "assistant", "content": "LangSmith Playground The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key features and pricing details for the different plans offered by LangSmith?"}, {"role": "assistant", "content": "Plans StartupsDeveloperPlusEnterpriseDesigned for early stage startups building AI applicationsDesigned for hobbyists who want to start their adventure soloEverything in Developer, plus team features and higher rate limitsDesigned for teams with more security, deployment, and support needsContact us to learn moreFree for 1 user5,000 free traces per monthAdditional traces billed starting @ 0.05/trace$39/user10,000 free traces per monthAdditional traces billed starting @ 0.05/traceCustomWhat to expect:We want all early stage companies to build with LangSmith. LangSmith for Startups offers discounted prices and a generous free, monthly trace allotment, so you can have the right tooling in place as you grow your business.Key features:1 Developer seatDebugging tracesDataset collectionTesting and evaluationPrompt managementMonitoringKey features:All features in Developer tierUp to 10 seatsHosted LangServe (beta)Higher rate limitsEmail supportKey features:All features in Plus tierSingle Sign On (SSO)Negotiable SLAsDeployment options in customers environmentCustom rate limitsTeam trainingsShared Slack channelArchitectural guidanceDedicated customer success manager"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in team seats and pricing between the Developer and Enterprise plans?"}, {"role": "assistant", "content": "Plan Comparison DeveloperPlusEnterpriseFeaturesDebugging TracesDataset CollectionHuman LabelingTesting and EvaluationPrompt ManagementHosted LangServe--MonitoringRole-Based Access Controls (RBAC)----TeamDeveloper Seats1 Free SeatMaximum 10 seats$39 per seat/month1Custom pricingUsageTraces2First 5k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)First 10k base traces and extended upgrades per month for freePay as you go thereafter:$0.50 per 1k base traces (14-day retention)Additional $4.50 per 1k extended traces (400-day retention)CustomMax ingested events / hour350,0003 / 250,000500,000CustomTotal trace size storage / hour4500MB3 / 2.5GB5GBCustomSecurity ControlsSingle Sign On--GoogleGitHubCustom SSODeploymentHosted in USHosted in USAdd-on for self-hosted deployment in customer's VPCSupportSupport ChannelsCommunityEmailEmailShared Slack ChannelShared Slack Channel----Team Training----Application Architectural Guidance----Dedicated Customer Success Manager----SLA----ProcurementBillingMonthly, self-serveCredit CardMonthly, self-serveCredit CardAnnual InvoiceACHCustom Terms and Data Privacy Agreement----Infosec Review----WorkspacesSingle, default Workspace under Personal OrganizationUp to 3 Workspaces per OrganizationUp to 10 Workspaces per Organization (contact support@langchain.dev for more)Organization Roles (User and Admin)-- 1 2 3 4"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will my usage of LangSmith become billable?"}, {"role": "assistant", "content": "Ive been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account? If youve been using LangSmith already, your usage will be billable starting in July. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by reaching out to sales@langchain.dev."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What plan should I choose if I am an individual developer working on small projects?"}, {"role": "assistant", "content": "Which plan is right for me? If youre an individual developer, the Developer plan is a great choice for small projects. For teams that want to collaborate in LangSmith, check out the Plus plan. If you are an early-stage startup building an AI application, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our Startup Contact Form for more details. If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our Sales Contact Form for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is considered a seat in an organization?"}, {"role": "assistant", "content": "What is a seat? A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does the term \"trace\" refer to in the context of an application chain or agent?"}, {"role": "assistant", "content": "What is a trace? A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an example of a single trace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of data are included in an ingested event sent to LangSmith?"}, {"role": "assistant", "content": "What is an ingested event? An ingested event is any distinct, trace-related data sent to LangSmith. This includes: Inputs, outputs and metadata sent at the start of a run step within a traceInputs, outputs and metadata sent at the end of a run step within a traceFeedback on run steps or traces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do I have if I've reached my rate or usage limits on LangSmith?"}, {"role": "assistant", "content": "Ive hit my rate or usage limits. What can I do? If youve consumed the monthly allotment of free traces in your account, you can add a credit card on the Developer and Plus plans to continue sending traces to LangSmith. If youve hit the rate limits on your tier, you can upgrade to a higher plan to get higher limits, or reach out to support@langchain.dev with questions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Can a Developer account be upgraded to the Plus or Enterprise plans?"}, {"role": "assistant", "content": "I have a developer account, can I upgrade my account to the Plus or Enterprise plan? Every user will have a unique personal account on the Developer plan. We cannot upgrade a Developer account to the Plus or Enterprise plans. If youre interested in working as a team, create a separate LangSmith Organization on the Plus plan. This plan can upgraded to the Enterprise plan at a later date."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the billing process for Seats Traces?"}, {"role": "assistant", "content": "How will billing work? Seats Traces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Can I set a spend limit on tracing in LangSmith?"}, {"role": "assistant", "content": "Can I limit how much I spend on tracing? You can set limits on the number of traces that can be sent to LangSmith per month on\nthe Plans and Billing settings page. noteWhile we do show you the dollar value of your usage limit for convenience, this limit evaluated\nin terms of number of traces instead of dollar amount. For example, if you are approved for our\nstartup plan tier where you are given a generous allotment of free traces, your usage limit will\nnot automatically change.You are not currently able to set a spend limit in the product."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I view the daily number of billable LangSmith traces for my organization this month?"}, {"role": "assistant", "content": "How can my track my usage so far this month? Under the Settings section for your Organization you will see subsection for Usage. There, you will able to see a graph of the daily nunber of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should customers on the Enterprise plan do if they have a question about their bill?"}, {"role": "assistant", "content": "I have a question about my bill... Customers on the Developer and Plus plan tiers should email support@langchain.dev. Customers on the Enterprise plan should contact their sales representative directly. Enterprise plan customers are billed annually by invoice."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of support are available for each plan offered?"}, {"role": "assistant", "content": "What can I expect from Support? On the Developer plan, community-based support is available on Discord. On the Plus plan, you will also receive preferential, email support at support@langchain.dev for LangSmith-related questions only and we'll do our best to respond within the next business day. On the Enterprise plan, youll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, well also support deployments and new releases with our infra engineering team on-call."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "In which regions can I choose to sign up for data storage?"}, {"role": "assistant", "content": "Where is my data stored? You may choose to sign up in either the US or EU region. See the cloud architecture reference for more details. If youre on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What security frameworks is LangSmith compliant with?"}, {"role": "assistant", "content": "Which security frameworks is LangSmith compliant with? We are SOC 2 Type II, GDPR, and HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com. Please note we only enter into BAAs with customers on our Enterprise plan."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Will LangSmith train on the data that I send?"}, {"role": "assistant", "content": "Will you train on the data that I send LangSmith? We will not train on your data, and you own all rights to your data. See LangSmith Terms of Service for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the first step to filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionality has been added to help users construct queries in natural language?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key aspects covered under the Usage and Billing section?"}, {"role": "assistant", "content": "Usage and Billing Data RetentionUsage LimitsRate Limits"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in generating synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can users upload experiments run outside of LangSmith to view results in the LangSmith UI?"}, {"role": "assistant", "content": "Upload experiments run outside of LangSmith with the REST API Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required to upload an experiment using the /datasets/upload-experiment endpoint?"}, {"role": "assistant", "content": "Request body schema Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within\nthe experiment. Each object in the results represents a \"row\" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name\nrefer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset\nin LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  \"experiment_name\": \"string (required)\",  \"experiment_description\": \"string (optional)\",  \"experiment_start_time\": \"datetime (required)\",  \"experiment_end_time\": \"datetime (required)\",  \"dataset_id\": \"uuid (optional - an external dataset id, used to group experiments together)\",  \"dataset_name\": \"string (optional - must provide either dataset_id or dataset_name)\",  \"dataset_description\": \"string (optional)\",  \"experiment_metadata\": { // Object (any shape - optional)    \"key\": \"value\"  },  \"summary_experiment_scores\": [ // List of summary feedback objects (optional)    {      \"key\": \"string (required)\",      \"score\": \"number (optional)\",      \"value\": \"string (optional)\",      \"comment\": \"string (optional)\",      \"feedback_source\": { // Object (optional)        \"type\": \"string (required)\"      },      \"feedback_config\": { // Object (optional)        \"type\": \"string enum: continuous, categorical, or freeform\",        \"min\": \"number (optional)\",        \"max\": \"number (optional)\",        \"categories\": [ // List of feedback category objects (optional)            \"value\": \"number (required)\",            \"label\": \"string (optional)\"        ]      },      \"created_at\": \"datetime (optional - defaults to now)\",      \"modified_at\": \"datetime (optional - defaults to now)\",      \"correction\": \"Object or string (optional)\"    }  ],  \"results\": [ // List of experiment row objects (required)    {      \"row_id\": \"uuid (required)\",      \"inputs\": {     // Object (required - any shape). This will        \"key\": \"val\"  // be the input to both the run and the dataset example.      },      \"expected_outputs\": { // Object (optional - any shape).        \"key\": \"val\"        // These will be the outputs of the dataset examples.      },      \"actual_outputs\": { // Object (optional - any shape).        \"key\": \"val       // These will be the outputs of the runs.      },      \"evaluation_scores\": [ // List of feedback objects for the run (optional)        {            \"key\": \"string (required)\",            \"score\": \"number (optional)\",            \"value\": \"string (optional)\",            \"comment\": \"string (optional)\",            \"feedback_source\": { // Object (optional)                \"type\": \"string (required)\"            },            \"feedback_config\": { // Object (optional)                \"type\": \"string enum: continuous, categorical, or freeform\",                \"min\": \"number (optional)\",                \"max\": \"number (optional)\",                \"categories\": [ // List of feedback category objects (optional)                    \"value\": \"number (required)\",                    \"label\": \"string (optional)\"                ]            },            \"created_at\": \"datetime (optional - defaults to now)\",            \"modified_at\": \"datetime (optional - defaults to now)\",            \"correction\": \"Object or string (optional)\"        }      ],      \"start_time\": \"datetime (required)\", // The start/end times for the runs will be used to      \"end_time\": \"datetime (required)\",   // calculate latency. They must all fall between the      \"run_name\": \"string (optional)\",     // start and end times for the experiment.      \"error\": \"string (optional)\",      \"run_metadata\": { // Object (any shape - optional)        \"key\": \"value\"      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for uploading multiple experiments to the same dataset?"}, {"role": "assistant", "content": "Considerations You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together\nunder a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if\nyou only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key details of the experiment uploaded to LangSmith, including its name, description, and start and end times?"}, {"role": "assistant", "content": "Example request Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  \"experiment_name\": \"My external experiment\",  \"experiment_description\": \"An experiment uploaded to LangSmith\",  \"dataset_name\": \"my-external-dataset\",  \"summary_experiment_scores\": [    {      \"key\": \"summary_accuracy\",      \"score\": 0.9,      \"comment\": \"Great job!\"    }  ],  \"results\": [    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the weather in San Francisco today?\"      },      \"expected_outputs\": {        \"output\": \"Sorry, I am unable to provide information about the current weather.\"      },      \"actual_outputs\": {        \"output\": \"The weather is partly cloudy with a high of 65.\"      },      \"evaluation_scores\": [        {          \"key\": \"hallucination\",          \"score\": 1,          \"comment\": \"The chatbot made up the weather instead of identifying that \"                     \"they don't have enough info to answer the question. This is \"                     \"a hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:39\",      \"end_time\": \"2024-08-03T00:12:41\",      \"run_name\": \"Chatbot\"    },    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the square root of 49?\"      },      \"expected_outputs\": {        \"output\": \"The square root of 49 is 7.\"      },      \"actual_outputs\": {        \"output\": \"7.\"      },      \"evaluation_scores\": [       {          \"key\": \"hallucination\",          \"score\": 0,          \"comment\": \"The chatbot correctly identified the answer. This is not a \"                     \"hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:40\",      \"end_time\": \"2024-08-03T00:12:42\",      \"run_name\": \"Chatbot\"    }  ],  \"experiment_start_time\": \"2024-08-03T00:12:38\",  \"experiment_end_time\": \"2024-08-03T00:12:43\"}resp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\",    json=body,    headers={\"x-api-key\": os.environ[\"LANGCHAIN_API_KEY\"]})print(resp.json()) Below is the response received: {  \"dataset\": {    \"name\": \"my-external-dataset\",    \"description\": null,    \"created_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"data_type\": \"kv\",    \"inputs_schema_definition\": null,    \"outputs_schema_definition\": null,    \"externally_managed\": true,    \"id\": \"<<uuid>>\",    \"tenant_id\": \"<<uuid>>\",    \"example_count\": 0,    \"session_count\": 0,    \"modified_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"last_session_start_time\": null  },  \"experiment\": {    \"start_time\": \"2024-08-03T00:12:38\",    \"end_time\": \"2024-08-03T00:12:43+00:00\",    \"extra\": null,    \"name\": \"My external experiment\",    \"description\": \"An experiment uploaded to LangSmith\",    \"default_dataset_id\": null,    \"reference_dataset_id\": \"<<uuid>>\",    \"trace_tier\": \"longlived\",    \"id\": \"<<uuid>>\",    \"run_count\": null,    \"latency_p50\": null,    \"latency_p99\": null,    \"first_token_p50\": null,    \"first_token_p99\": null,    \"total_tokens\": null,    \"prompt_tokens\": null,    \"completion_tokens\": null,    \"total_cost\": null,    \"prompt_cost\": null,    \"completion_cost\": null,    \"tenant_id\": \"<<uuid>>\",    \"last_run_start_time\": null,    \"last_run_start_time_live\": null,    \"feedback_stats\": null,    \"session_feedback_stats\": null,    \"run_facets\": null,    \"error_rate\": null,    \"streaming_rate\": null,    \"test_run_number\": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.\nIf you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this\ninformation in the request body)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will you see after clicking on your newly-created dataset in the UI?"}, {"role": "assistant", "content": "View the experiment in the UI Now, login to the UI and click on your newly-created dataset! You should see a single experiment:\n Your examples will have been uploaded:\n Clicking on your experiment will bring you to the comparison view:\n As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can an individual organization be deleted from a self-hosted instance of LangSmith?"}, {"role": "assistant", "content": "Deleting Organizations The LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table. This command using the Organization ID as an argument."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools and database connection details are required to prepare for running the migration script?"}, {"role": "assistant", "content": "Prerequisites Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/PostgreSQL clienthttps://www.postgresql.org/download/PostgreSQL database connection:HostPortUsernameIf using the bundled version, this is postgresPasswordIf using the bundled version, this is postgresDatabase nameIf using the bundled version, this is postgresClickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the PostgreSQL database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the postgresql service to your local machine.Run kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.Connectivity to the Clickhouse database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port 8443"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should be run to delete an organization using the deletion script?"}, {"role": "assistant", "content": "Running the deletion script for a single organization Run the following command to run the organization removal script: sh delete_organization.sh <postgres_url> <clickhouse_url> --organization_id <organization_id> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_organization.sh \"postgres://postgres:postgres@localhost:5432/postgres\" \"clickhouse://default:password@localhost:8123/default\" --organization_id 4ec70ec7-0808-416a-b836-7100aeec934b If you visit the Langsmith UI, you should now see organization is no longer present."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of logging to LangSmith?"}, {"role": "assistant", "content": "Tracing This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Run in the context of an LLM application?"}, {"role": "assistant", "content": "Runs A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of LangSmith, and how are runs related to it?"}, {"role": "assistant", "content": "Traces A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a project in the context of traces?"}, {"role": "assistant", "content": "Projects A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different ways to collect feedback on runs?"}, {"role": "assistant", "content": "Feedback Feedback allows you to score an individual run based on certain criteria.\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are tags used for in the LangSmith UI?"}, {"role": "assistant", "content": "Tags Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of metadata in relation to runs?"}, {"role": "assistant", "content": "Metadata Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How long does LangSmith retain trace data after it is inserted into the trace database?"}, {"role": "assistant", "content": "Data storage and retention For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you delete a trace from LangSmith before its expiration date?"}, {"role": "assistant", "content": "Deleting traces from LangSmith If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the \"Delete\" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What feature is available only to Enterprise customers for managing user permissions in a LangSmith workspace?"}, {"role": "assistant", "content": "Set up access control noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Read more about roles under admin concepts Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces LangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it.\nOnly users with the workspace:manage permission can manage access control settings for a workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can Organization Admins create custom roles in LangSmith?"}, {"role": "assistant", "content": "Create a role By default, LangSmith comes with a set of system roles: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) If these do not fit your access model, Organization Admins can create custom roles to suit your needs. To create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization. Click on the Create Role button to create a new role. You should see a form like the one below: Assign permissions for the different LangSmith resources that you want to control access to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you assign a role to a user in the Organization settings?"}, {"role": "assistant", "content": "Assign a role to a user Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page Each user will have a Role dropdown that you can use to assign a role to them. You can also invite new users with a given role."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with LangSmith?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What key topics are discussed in LangSmith?"}, {"role": "assistant", "content": "Concepts Explanations, clarification and discussion of key topics in LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different components related to administration in the given text?"}, {"role": "assistant", "content": "Admin OrganizationsWorkspacesUsersAPI keysPersonal Access Tokens (PATs)Service keysRolesOrganization rolesWorkspace roles"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What elements are associated with the concept of tracing?"}, {"role": "assistant", "content": "Tracing RunsTracesProjectsFeedbackTagsMetadata"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components involved in the evaluation process?"}, {"role": "assistant", "content": "Evaluation Datasets and examplesExperimentsEvaluators"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the types of prompts mentioned in the text?"}, {"role": "assistant", "content": "Prompts Prompt typesTemplate formats"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key aspects of usage and billing related to data retention and usage limits?"}, {"role": "assistant", "content": "Usage and Billing Data RetentionUsage Limits"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functionalities of custom evaluators in the LangSmith UI?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on multiple splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you prevent the logging of inputs and outputs in traces when using LangSmith?"}, {"role": "assistant", "content": "Prevent logging of sensitive data in traces In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend. If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application: LANGCHAIN_HIDE_INPUTS=trueLANGCHAIN_HIDE_OUTPUTS=true This works for both the LangSmith SDK (Python and TypeScript) and LangChain. You can also customize and override this behavior for a given Client instance. This can be done by setting the hide_inputs and hide_outputs parameters on the Client object (hideInputs and hideOutputs in TypeScript). For the example below, we will simply return an empty object for both hide_inputs and hide_outputs, but you can customize this to your needs. PythonTypeScriptimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(openai.Client())langsmith_client = Client(    hide_inputs=lambda inputs: {}, hide_outputs=lambda outputs: {})# The trace produced will have its metadata present, but the inputs will be hiddenopenai_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"Hello!\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have hidden inputs and outputsopenai_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"Hello!\"},    ],)import OpenAI from \"openai\";import { Client } from \"langsmith\";import { wrapOpenAI } from \"langsmith/wrappers\";const langsmithClient = new Client({  hideInputs: (inputs) => ({}),  hideOutputs: (outputs) => ({}),});// The trace produced will have its metadata present, but the inputs will be hiddenconst filteredOAIClient = wrapOpenAI(new OpenAI(), {    client: langsmithClient,});await filteredOAIClient.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: [      { role: \"system\", content: \"You are a helpful assistant.\" },      { role: \"user\", content: \"Hello!\" },    ],});const openaiClient = wrapOpenAI(new OpenAI());// The trace produced will not have hidden inputs and outputsawait openaiClient.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: [      { role: \"system\", content: \"You are a helpful assistant.\" },      { role: \"user\", content: \"Hello!\" },    ],});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the LangSmith SDK versions that support the rule-based masking of inputs and outputs?"}, {"role": "assistant", "content": "Rule-based masking of inputs and outputs infoThis feature is available in the following LangSmith SDK versions:Python: 0.1.81 and aboveTypeScript: 0.1.33 and above To mask specific data in inputs and outputs, you can use the create_anonymizer / createAnonymizer function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value. The anonymizer will be skipped for inputs if LANGCHAIN_HIDE_INPUTS = true. Same applies for outputs if LANGCHAIN_HIDE_OUTPUTS = true. However, if inputs or outputs are to be sent to client, the anonymizer method will take precedence over functions found in hide_inputs and hide_outputs. By default, the create_anonymizer will only look at maximum of 10 nesting levels deep, which can be configured via the max_depth parameter. PythonTypeScriptfrom langsmith.anonymizer import create_anonymizerfrom langsmith import Client, traceable# create anonymizer from list of regex patterns and replacement valuesanonymizer = create_anonymizer([    { \"pattern\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\", \"replace\": \"<email>\" },    { \"pattern\": r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\", \"replace\": \"<uuid>\" }])# or create anonymizer from a functionanonymizer = create_anonymizer(lambda text: r\"...\".sub(\"[value]\", text))client = Client(anonymizer=anonymizer)@traceable(client=client)def main(inputs: dict) -> dict:    ...import { createAnonymizer } from \"langsmith/anonymizer\"import { traceable } from \"langsmith/traceable\"import { Client } from \"langsmith\"// create anonymizer from list of regex patterns and replacement valuesconst anonymizer = createAnonymizer([    { pattern: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g, replace: \"<email>\" },    { pattern: /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g, replace: \"<uuid>\" }])// or create anonymizer from a functionconst anonymizer = createAnonymizer((value) => value.replace(\"...\", \"<value>\"))const client = new Client({ anonymizer })const main = traceable(async (inputs: any) => {    // ...}, { client }) Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing. noteImproving the performance of anonymizer API is on our roadmap! If you are encountering performance issues, please contact us at support@langchain.dev. Older versions of LangSmith SDKs can use the hide_inputs and hide_outputs parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well. PythonTypeScriptimport refrom langsmith import Client, traceable# Define the regex patterns for email addresses and UUIDsEMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\"UUID_REGEX = r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\"def replace_sensitive_data(data, depth=10):    if depth == 0:        return data    if isinstance(data, dict):        return {k: replace_sensitive_data(v, depth-1) for k, v in data.items()}    elif isinstance(data, list):        return [replace_sensitive_data(item, depth-1) for item in data]    elif isinstance(data, str):        data = re.sub(EMAIL_REGEX, \"<email-address>\", data)        data = re.sub(UUID_REGEX, \"<UUID>\", data)        return data    else:        return dataclient = Client(    hide_inputs=lambda inputs: replace_sensitive_data(inputs),    hide_outputs=lambda outputs: replace_sensitive_data(outputs))inputs = {\"role\": \"user\", \"content\": \"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.\"}outputs = {\"role\": \"assistant\", \"content\": \"Hi! I've noted your email as user@example.com and your ID as 123e4567-e89b-12d3-a456-426614174000.\"}@traceable(client=client)def child(inputs: dict) -> dict:    return outputs@traceable(client=client)def parent(inputs: dict) -> dict:    child_outputs = child(inputs)    return child_outputsparent(inputs)import { Client } from \"langsmith\";import { traceable } from \"langsmith/traceable\";// Define the regex patterns for email addresses and UUIDsconst EMAIL_REGEX = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g;const UUID_REGEX = /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g;function replaceSensitiveData(data: any, depth: number = 10): any {    if (depth === 0) return data;    if (typeof data === \"object\" && !Array.isArray(data)) {        const result: Record<string, any> = {};        for (const [key, value] of Object.entries(data)) {            result[key] = replaceSensitiveData(value, depth - 1);        }        return result;    } else if (Array.isArray(data)) {        return data.map(item => replaceSensitiveData(item, depth - 1));    } else if (typeof data === \"string\") {        return data.replace(EMAIL_REGEX, \"<email-address>\").replace(UUID_REGEX, \"<UUID>\");    } else {        return data;    }}const langsmithClient = new Client({    hideInputs: (inputs) => replaceSensitiveData(inputs),    hideOutputs: (outputs) => replaceSensitiveData(outputs)});const inputs = {    role: \"user\",    content: \"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.\"};const outputs = {    role: \"assistant\",    content: \"Hi! I've noted your email as <email-address> and your ID as <UUID>.\"};const child = traceable(async (inputs: any) => {    return outputs;}, { name: \"child\", client: langsmithClient });const parent = traceable(async (inputs: any) => {    const childOutputs = await child(inputs);    return childOutputs;}, { name: \"parent\", client: langsmithClient });await parent(inputs)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the purposes of the process_inputs and process_outputs parameters in the LangSmith SDK?"}, {"role": "assistant", "content": "Processing Inputs & Outputs for a Single Function infoThe process_outputs parameter is available in LangSmith SDK version 0.1.98 and above for Python. In addition to client-level input and output processing, LangSmith provides function-level processing through the process_inputs and process_outputs parameters of the @traceable decorator. These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function. Here's an example of how to use process_inputs and process_outputs: from langsmith import traceabledef process_inputs(inputs: dict) -> dict:    # inputs is a dictionary where keys are argument names and values are the provided arguments    # Return a new dictionary with processed inputs    return {        \"processed_key\": inputs.get(\"my_cool_key\", \"default\"),        \"length\": len(inputs.get(\"my_cool_key\", \"\"))    }def process_outputs(output: Any) -> dict:    # output is the direct return value of the function    # Transform the output into a dictionary    # In this case, \"output\" will be an integer    return {\"processed_output\": str(output)}@traceable(process_inputs=process_inputs, process_outputs=process_outputs)def my_function(my_cool_key: str) -> int:    # Function implementation    return len(my_cool_key)result = my_function(\"example\") In this example, process_inputs creates a new dictionary with processed input data, and process_outputs transforms the output into a specific format before logging to LangSmith. cautionIt's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data. For asynchronous functions, the usage is similar: @traceable(process_inputs=process_inputs, process_outputs=process_outputs)async def async_function(key: str) -> int:    # Async implementation    return len(key) These function-level processors take precedence over client-level processors (hide_inputs and hide_outputs) when both are defined."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must be taken to generate synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing across multiple services?"}, {"role": "assistant", "content": "Implement distributed tracing Sometimes, you need to trace a request across multiple services. LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (langsmith-trace and optional baggage for metadata/tags). Example client-server setup: Trace starts on clientContinues on server"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can distributed tracing be implemented in a Python application using the provided example?"}, {"role": "assistant", "content": "Distributed tracing in Python # client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasync def my_client_function():    headers = {}    async with httpx.AsyncClient(base_url=\"...\") as client:        if run_tree := get_current_run_tree():            # add langsmith-id to headers            headers.update(run_tree.to_headers())        return await client.post(\"/my-route\", headers=headers) Then the server (or other service) can continue the trace by passing the headers in as langsmith_extra: # server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasync def my_application():    ...app = FastAPI()  # Or Flask, Django, or any other framework@app.post(\"/my-route\")async def fake_route(request: Request):    # request.headers:  {\"langsmith-trace\": \"...\"}    # as well as optional metadata/tags in `baggage`    with tracing_context(parent=request.headers):        return await my_application() The example above uses the tracing_context context manager. You can also directly specify the parent run context in the langsmith_extra parameter of a method wrapped with @traceable. from langsmith.run_helpers import traceable, trace# ... same as above@app.post(\"/my-route\")async def fake_route(request: Request):    # request.headers:  {\"langsmith-trace\": \"...\"}    my_application(langsmith_extra={\"parent\": request.headers})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What version of langsmith is required for distributed tracing in TypeScript?"}, {"role": "assistant", "content": "Distributed tracing in TypeScript noteDistributed tracing in TypeScript requires langsmith version >=0.1.31 First, we obtain the current run tree from the client and convert it to langsmith-trace and baggage header values, which we can pass to the server: // client.mtsimport { getCurrentRunTree, traceable } from \"langsmith/traceable\";const client = traceable(  async () => {    const runTree = getCurrentRunTree();    return await fetch(\"...\", {      method: \"POST\",      headers: runTree.toHeaders(),    }).then((a) => a.text());  },  { name: \"client\" });await client(); Then, the server converts the headers back to a run tree, which it uses to further continue the tracing. To pass the newly created run tree to a traceable function, we can use the withRunTree helper, which will ensure the run tree is propagated within traceable invocations. Express.JSHono// server.mtsimport { RunTree } from \"langsmith\";import { traceable, withRunTree } from \"langsmith/traceable\";import express from \"express\";import bodyParser from \"body-parser\";const server = traceable(  (text: string) => `Hello from the server! Received \"${text}\"`,  { name: \"server\" });const app = express();app.use(bodyParser.text());app.post(\"/\", async (req, res) => {  const runTree = RunTree.fromHeaders(req.headers);  const result = await withRunTree(runTree, () => server(req.body));  res.send(result);});// server.mtsimport { RunTree } from \"langsmith\";import { traceable, withRunTree } from \"langsmith/traceable\";import { Hono } from \"hono\";const server = traceable(  (text: string) => `Hello from the server! Received \"${text}\"`,  { name: \"server\" });const app = new Hono();app.post(\"/\", async (c) => {  const body = await c.req.text();  const runTree = RunTree.fromHeaders(c.req.raw.headers);  const result = await withRunTree(runTree, () => server(body));  return c.body(result);});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to create and update a prompt in LangSmith?"}, {"role": "assistant", "content": "How-to guides: Prompts This section contains how-to guides related to prompt management in LangSmith.  Create a promptNavigate to the Prompts section in the left-hand sidebar or from the application homepage. Update a promptNavigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit. Manage prompts programmaticallyYou can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. Open a prompt from a traceIf you pull a prompt into your code and begin logging traces that use it, you can find a link to the prompt in the Trace UI. LangChain HubNavigate to the LangChain Hub section of the left-hand sidebar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can you find the Prompts section in the application?"}, {"role": "assistant", "content": "Create a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I edit a prompt in the application?"}, {"role": "assistant", "content": "Update a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What SDKs can be used to manage prompts programmatically?"}, {"role": "assistant", "content": "Manage prompts programmatically You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you find a link to a prompt in the Trace UI after logging traces that use it?"}, {"role": "assistant", "content": "Open a prompt from a trace If you pull a prompt into your code and begin logging traces that use it, you can find a link to the prompt in the Trace UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can I find the LangChain Hub?"}, {"role": "assistant", "content": "LangChain Hub Navigate to the LangChain Hub section of the left-hand sidebar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can the monitoring charts for each tracing project be accessed in LangSmith?"}, {"role": "assistant", "content": "Use monitoring charts LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default time period set for viewing monitors?"}, {"role": "assistant", "content": "Change the time period You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view specific subsets of runs in the monitor tab?"}, {"role": "assistant", "content": "Slice data by metadata or tag By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\nThis can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you investigate the runs causing spikes in errors or latency when using a monitoring dashboard?"}, {"role": "assistant", "content": "Drill down into specific subsets Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What feature is available only to Enterprise customers for managing user permissions in a LangSmith workspace?"}, {"role": "assistant", "content": "Set up access control noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Read more about roles under admin concepts Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces LangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it.\nOnly users with the workspace:manage permission can manage access control settings for a workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can Organization Admins create custom roles in LangSmith?"}, {"role": "assistant", "content": "Create a role By default, LangSmith comes with a set of system roles: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) If these do not fit your access model, Organization Admins can create custom roles to suit your needs. To create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization. Click on the Create Role button to create a new role. You should see a form like the one below: Assign permissions for the different LangSmith resources that you want to control access to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you assign a role to a user in the Organization settings?"}, {"role": "assistant", "content": "Assign a role to a user Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page Each user will have a Role dropdown that you can use to assign a role to them. You can also invite new users with a given role."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key fields included in the run data format used by LangSmith?"}, {"role": "assistant", "content": "Run (span) data format Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and runs LangSmith stores and processes trace data in a simple format that is easy to export and import. Many of these fields are optional or not important to know about but are included for completeness.\nThe bolded fields are the most important ones to know about. Field NameTypeDescriptionidUUIDUnique identifier for the span.namestringThe name associated with the run.inputsobjectA map or set of inputs provided to the run.run_typestringType of run, e.g., \"llm\", \"chain\", \"tool\".start_timedatetimeStart time of the run.end_timedatetimeEnd time of the run.extraobjectAny extra information run.errorstringError message if the run encountered an error.outputsobjectA map or set of outputs generated by the run.eventsarray of objectsA list of event objects associated with the run. This is relevant for runs executed with streaming.tagsarray of stringsTags or labels associated with the run.trace_idUUIDUnique identifier for the trace the run is a part of. This is also the id field of the root run of the tracedotted_orderstringCustom ordering string, hierarchical. Built with timestamp and unique identifiers.statusstringCurrent status of the run execution, e.g., \"error\", \"pending\", \"success\"child_run_idsarray of UUIDsList of IDs for all child runs.direct_child_run_idsarray of UUIDsList of IDs for direct children of this run.parent_run_idsarray of UUIDsList of IDs for all parent runs.feedback_statsobjectAggregations of feedback statistics for this runreference_example_idUUIDID of a reference example associated with the run. This is usually only present for evaluation runs.total_tokensintegerTotal number of tokens processed by the run.prompt_tokensintegerNumber of tokens in the prompt of the run.completion_tokensintegerNumber of tokens in the completion of the run.total_coststringTotal cost associated with processing the run.prompt_coststringCost associated with the prompt part of the run.completion_coststringCost associated with the completion of the run.first_token_timedatetimeTime when the first token was generated.session_idstringSession identifier for the run.in_datasetbooleanIndicates whether the run is included in a dataset.parent_run_idUUIDUnique identifier of the parent run.execution_order (deprecated)integerThe order in which this run was executed within the trace.serializedobjectSerialized state of the object executing the run if applicable.manifest_id (deprecated)UUIDIdentifier for a manifest associated with the span.manifest_s3_idUUIDS3 identifier for the manifest.inputs_s3_urlsobjectS3 URLs for the inputs.outputs_s3_urlsobjectS3 URLs for the outputs.price_model_idUUIDIdentifier for the pricing model applied to the run.app_pathstringApplication (UI) path for this run.last_queued_atdatetimeLast time the span was queued.share_tokenstringToken for sharing access to the run's data. Here is an example of a JSON representation of a run in the above format: {  \"id\": \"497f6eca-6276-4993-bfeb-53cbbbba6f08\",  \"name\": \"string\",  \"inputs\": {},  \"run_type\": \"llm\",  \"start_time\": \"2024-04-29T00:49:12.090000\",  \"end_time\": \"2024-04-29T00:49:12.459000\",  \"extra\": {},  \"error\": \"string\",  \"execution_order\": 1,  \"serialized\": {},  \"outputs\": {},  \"parent_run_id\": \"f8faf8c1-9778-49a4-9004-628cdb0047e5\",  \"manifest_id\": \"82825e8e-31fc-47d5-83ce-cd926068341e\",  \"manifest_s3_id\": \"0454f93b-7eb6-4b9d-a203-f1261e686840\",  \"events\": [{}],  \"tags\": [\"foo\"],  \"inputs_s3_urls\": {},  \"outputs_s3_urls\": {},  \"trace_id\": \"df570c03-5a03-4cea-8df0-c162d05127ac\",  \"dotted_order\": \"20240429T004912090000Z497f6eca-6276-4993-bfeb-53cbbbba6f08\",  \"status\": \"string\",  \"child_run_ids\": [\"497f6eca-6276-4993-bfeb-53cbbbba6f08\"],  \"direct_child_run_ids\": [\"497f6eca-6276-4993-bfeb-53cbbbba6f08\"],  \"parent_run_ids\": [\"f8faf8c1-9778-49a4-9004-628cdb0047e5\"],  \"feedback_stats\": {    \"correctness\": {      \"n\": 1,      \"avg\": 1.0    }  },  \"reference_example_id\": \"9fb06aaa-105f-4c87-845f-47d62ffd7ee6\",  \"total_tokens\": 0,  \"prompt_tokens\": 0,  \"completion_tokens\": 0,  \"total_cost\": \"string\",  \"prompt_cost\": \"string\",  \"completion_cost\": \"string\",  \"price_model_id\": \"0b5d9575-bec3-4256-b43a-05893b8b8440\",  \"first_token_time\": null,  \"session_id\": \"1ffd059c-17ea-40a8-8aef-70fd0307db82\",  \"app_path\": \"string\",  \"last_queued_at\": null,  \"in_dataset\": true,  \"share_token\": \"d0430ac3-04a1-4e32-a7ea-57776ad22c1c\"}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can user feedback be utilized to optimize a classifier?"}, {"role": "assistant", "content": "Optimize a classifier This tutorial walks through optimizing a classifier based on user a feedback.\nClassifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback.\nThat is exactly what we will do in this example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the bot being built in this example?"}, {"role": "assistant", "content": "The objective In this example, we will build a bot that classify GitHub issues based on their title.\nIt will take in a title and classify it into one of many different classes.\nThen, we will start to collect user feedback and use that to shape how this classifier performs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in setting up a function to classify GitHub issue titles and collect feedback on its performance?"}, {"role": "assistant", "content": "Getting started To get started, we will first set it up so that we send all traces to a specific project.\nWe can do this by setting an environment variable: import osos.environ[\"LANGCHAIN_PROJECT\"] = \"classifier\" We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it. import openaifrom langsmith import traceable, Clientimport uuidclient = openai.Client()available_topics = [    \"bug\",    \"improvement\",    \"new_feature\",    \"documentation\",    \"integration\",]prompt_template = \"\"\"Classify the type of the issue as one of {topics}.Issue: {text}\"\"\"@traceable(    run_type=\"chain\",    name=\"Classifier\",)def topic_classifier(    topic: str):    return client.chat.completions.create(        model=\"gpt-3.5-turbo\",        temperature=0,        messages=[            {                \"role\": \"user\",                \"content\": prompt_template.format(                    topics=','.join(available_topics),                    text=topic,                )            }        ],    ).choices[0].message.content We can then start to interact with it.\nWhen interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function.\nWe do this so we can attach feedback later on. Here's how we can invoke the application: run_id = uuid.uuid4()topic_classifier(    \"fix bug in LCEL\",    langsmith_extra={\"run_id\": run_id}) Here's how we can attach feedback after.\nWe can collect feedback in two forms. First, we can collect \"positive\" feedback - this is for examples that the model got right. ls_client = Client()run_id = uuid.uuid4()topic_classifier(    \"fix bug in LCEL\",    langsmith_extra={\"run_id\": run_id})ls_client.create_feedback(    run_id,    key=\"user-score\",    score=1.0,) Next, we can focus on collecting feedback that corresponds to a \"correction\" to the generation.\nIn this example the model will classify it as a bug, whereas I really want this to be classified as documentation. ls_client = Client()run_id = uuid.uuid4()topic_classifier(    \"fix bug in documentation\",    langsmith_extra={\"run_id\": run_id})ls_client.create_feedback(    run_id,    key=\"correction\",    correction=\"documentation\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of feedback automations being set up for the dataset classifier-github-issues?"}, {"role": "assistant", "content": "Set up automations We can now set up automations to move examples with feedback of some form into a dataset.\nWe will set up two automations, one for positive feedback and the other for negative feedback. The first will take all runs with positive feedback and automatically add them to a dataset.\nThe logic behind this is that any run with positive feedback we can use as a good example in future iterations.\nLet's create a dataset called classifier-github-issues to add this data to. The second will take all runs with a correction and use a webhook to add them to a dataset.\nWhen creating this webhook, we will select the option to \"Use Corrections\".\nThis option will make it so that when creating a dataset from a run, rather than using the output of the run\nas the gold-truth output of the datapoint, it will use the correction."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the function `create_example_string` in the updated application code?"}, {"role": "assistant", "content": "Update the application We can now update our code to pull down the dataset we are sending runs to.\nOnce we pull it down, we can create a string with the examples in it.\nWe can then put this string as part of the prompt! ### NEW CODE #### Initialize the LangSmith Client so we can use to get the datasetls_client = Client()# Create a function that will take in a list of examples and format them into a stringdef create_example_string(examples):    final_strings = []    for e in examples:        final_strings.append(f\"Input: {e.inputs['topic']}\\n> {e.outputs['output']}\")    return \"\\n\\n\".join(final_strings)### NEW CODE ###client = openai.Client()available_topics = [    \"bug\",    \"improvement\",    \"new_feature\",    \"documentation\",    \"integration\",]prompt_template = \"\"\"Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>\"\"\"@traceable(    run_type=\"chain\",    name=\"Classifier\",)def topic_classifier(    topic: str):    # We can now pull down the examples from the dataset    # We do this inside the function so it always get the most up-to-date examples,    # But this can be done outside and cached for speed if desired    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))  # <- New Code    example_string = create_example_string(examples)    return client.chat.completions.create(        model=\"gpt-3.5-turbo\",        temperature=0,        messages=[            {                \"role\": \"user\",                \"content\": prompt_template.format(                    topics=','.join(available_topics),                    text=topic,                    examples=example_string,                )            }        ],    ).choices[0].message.content If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as documentation ls_client = Client()run_id = uuid.uuid4()topic_classifier(    \"address bug in documentation\",    langsmith_extra={\"run_id\": run_id})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using the most semantically similar examples in the context of semantic search?"}, {"role": "assistant", "content": "Semantic search over examples One additional thing we can do is only use the most semantically similar examples.\nThis is useful when you start to build up a lot of examples. In order to do this, we can first define an example to find the k most similar examples: import numpy as npdef find_similar(examples, topic, k=5):    inputs = [e.inputs['topic'] for e in examples] + [topic]    embedds = client.embeddings.create(input=inputs, model=\"text-embedding-3-small\")    embedds = [e.embedding for e in embedds.data]    embedds = np.array(embedds)    args = np.argsort(-embedds.dot(embedds[-1])[:-1])[:5]    examples = [examples[i] for i in args]    return examples We can then use that in the application ls_client = Client()def create_example_string(examples):    final_strings = []    for e in examples:        final_strings.append(f\"Input: {e.inputs['topic']}\\n> {e.outputs['output']}\")    return \"\\n\\n\".join(final_strings)client = openai.Client()available_topics = [    \"bug\",    \"improvement\",    \"new_feature\",    \"documentation\",    \"integration\",]prompt_template = \"\"\"Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>\"\"\"@traceable(    run_type=\"chain\",    name=\"Classifier\",)def topic_classifier(    topic: str):    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))    examples = find_similar(examples, topic)    example_string = create_example_string(examples)    return client.chat.completions.create(        model=\"gpt-3.5-turbo\",        temperature=0,        messages=[            {                \"role\": \"user\",                \"content\": prompt_template.format(                    topics=','.join(available_topics),                    text=topic,                    examples=example_string,                )            }        ],    ).choices[0].message.content"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Langsmith Python client?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith in Python and TypeScript?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features are supported when setting up online evaluations in LangSmith?"}, {"role": "assistant", "content": "Set up online evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:Set up automation rules Online evaluations is a powerful LangSmith feature that allows you to run an LLM-as-a-judge evaluator on a set of your production traces. They are implemented as a possible action in an automation rule. Currently, we provide support for specifying a prompt template, a model, and a set of criteria to evaluate the runs on. After entering rules setup, you can select Online Evaluation from the list of possible actions:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What action must be selected to access the panel for configuring online evaluations?"}, {"role": "assistant", "content": "Configure online evaluations When selection Online Evaluation as an action in an automation, you are presented with a panel from which you can configure online evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What models are currently supported for selection in the dropdown?"}, {"role": "assistant", "content": "Model You can choose any model available in the dropdown. Currently, we support OpenAI, AzureOpenAI, and models hosted on Fireworks.\nIn order to set the API keys to use for these invocations, click on the Secrets & API Keys button and add the necessary keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for selecting or creating a prompt in the prompt template?"}, {"role": "assistant", "content": "Prompt template For the prompt template, you can select a suggested evaluator prompt, create a new prompt, or choose a prompt from the LangChain Hub. Suggested evaluator prompts We provide a list of pre-existing prompts that cater to common evaluator use cases. Create your own prompt We provide a base template from which you can form your own prompt. Pull a prompt from the LangChain Hub You can pull any structured prompt, private or public. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses.\nIf the prompt is your own, you can edit it in the playground and commit the version.\nIf the prompt is someone else's, you can fork the prompt, make your edits in the playground, and then pull in your new prompt to the online evaluator. When you choose a hub prompt for your online evaluator, the prompt will be locked to the commit version it was at rule creation. If you want to update the prompt, you can go to edit the online evaluator and re-select the prompt in the dropdown."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you map variables in your evaluator prompt based on recent runs?"}, {"role": "assistant", "content": "Mapping variables Prompts can be crafted with any variable name you choose. To map what is passed into your evaluator prompt from your runs or experiments, use the variable mapping inputs. There's a dropdown with suggestions provided based on the schema of your recent runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does the preview feature show when evaluating a prompt?"}, {"role": "assistant", "content": "Preview Previewing the prompt will show you an example of what the formatted prompt will look like. This preview pulls the input and output of the most recent run. In the case of a dataset evaluator, the preview will also pull reference output from an example in your dataset. noteYou can configure an evaluation prompt that doesn't match the schema of your recent runs, but the dropdown suggestions and preview function won't work as expected."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for naming a schema in the output schema?"}, {"role": "assistant", "content": "Output schema An evaluator will attach metadata tags to a run. These tags will have a name and a value. You can configure these in the Schema section.\nThe names and the descriptions of the fields will be passed into the prompt. Behind the scenes, we use tool calling to coerce the output of the LLM into the score you specify. noteThe name of the schema cannot have spaces since it is used as the name of a tool."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three evaluation strategies for LLM agents discussed in the tutorial?"}, {"role": "assistant", "content": "Evaluate an agent In this tutorial, we will walk through 3 evaluation strategies LLM agents, building on the conceptual points shared in our evaluation guide. Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. First, we will build an agent using LangGraph."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables are being set up for OpenAI and Langchain in the provided text?"}, {"role": "assistant", "content": "Set up environment We'll set up our environment variables for OpenAI and  %pip install --upgrade --quiet langchain langsmith langchain-community langchain-experimental langgraph import getpassimport osdef _set_env(var: str):    if not os.environ.get(var):        os.environ[var] = getpass.getpass(f\"{var}: \")_set_env(\"OPENAI_API_KEY\")os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Update appropriately for self-hosted installations or the EU region_set_env(\"LANGCHAIN_API_KEY\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database is being created for the tutorial, and what is the name of the sample database used?"}, {"role": "assistant", "content": "Configure the database We will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the chinook database, which is a sample database that represents a digital media store.\nFind more information about the database here. For convenience, we have hosted the database (Chinook.db) on a public GCS bucket. import requestsurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"response = requests.get(url)if response.status_code == 200:    # Open a local file in binary write mode    with open(\"Chinook.db\", \"wb\") as file:        # Write the content of the response (the file) to the local file        file.write(response.content)    print(\"File downloaded and saved as Chinook.db\")else:    print(f\"Failed to download the file. Status code: {response.status_code}\") We will use a handy SQL database wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results. We will also use the langchain_openai package to interact with the OpenAI API for language models later in the tutorial. from langchain_community.utilities import SQLDatabasedb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")print(db.dialect)print(db.get_usable_table_names())db.run(\"SELECT * FROM Artist LIMIT 10;\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of agent will be used for working with SQL?"}, {"role": "assistant", "content": "SQL Agent We'll use a LangGraph agent with access to a set of tools for working with SQL:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What model is being used in the LLM example provided in the text?"}, {"role": "assistant", "content": "LLM from langchain_openai import ChatOpenAIllm=ChatOpenAI(model=\"gpt-4o\",temperature=0)experiment_prefix=\"sql-agent-gpt4o\"metadata = \"Chinook, gpt-4o base-case-agent\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools are used to check SQL queries and their results before execution?"}, {"role": "assistant", "content": "Tools We'll use SQL toolkit as well as some custom tools to check the query before executing it and check the query result from the database to confirm it is not empty or irrelevant to the question. import jsonfrom langchain_community.agent_toolkits import SQLDatabaseToolkitfrom langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.graph import END, MessageGraphfrom langgraph.prebuilt.tool_node import ToolNodefrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom langchain.agents import tool# SQL toolkittoolkit = SQLDatabaseToolkit(db=db, llm=llm)tools = toolkit.get_tools()# Query checkingquery_check_system = \"\"\"You are a SQL expert with a strong attention to detail.Double check the SQLite query for common mistakes, including:- Using NOT IN with NULL values- Using UNION when UNION ALL should have been used- Using BETWEEN for exclusive ranges- Data type mismatch in predicates- Properly quoting identifiers- Using the correct number of arguments for functions- Casting to the correct data type- Using the proper columns for joinsIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.Execute the correct query with the appropriate tool.\"\"\"query_check_prompt = ChatPromptTemplate.from_messages([(\"system\", query_check_system),(\"user\", \"{query}\")])query_check = query_check_prompt | llm@tooldef check_query_tool(query: str) -> str:    \"\"\"    Use this tool to double check if your query is correct before executing it.    \"\"\"    return query_check.invoke({\"query\": query}).content# Query result checkingquery_result_check_system = \"\"\"You are grading the result of a SQL query from a DB.- Check that the result is not empty.- If it is empty, instruct the system to re-try!\"\"\"query_result_check_prompt = ChatPromptTemplate.from_messages([(\"system\", query_result_check_system),(\"user\", \"{query_result}\")])query_result_check = query_result_check_prompt | llm@tooldef check_result(query_result: str) -> str:    \"\"\"    Use this tool to check the query result from the database to confirm it is not empty and is relevant.    \"\"\"    return query_result_check.invoke({\"query_result\": query_result}).contenttools.append(check_query_tool)tools.append(check_result)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the structure of the State TypedDict in the provided code?"}, {"role": "assistant", "content": "State from typing import Annotatedfrom typing_extensions import TypedDictfrom langgraph.graph.message import AnyMessage, add_messagesclass State(TypedDict):    messages: Annotated[list[AnyMessage], add_messages]"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the role of the Assistant class in the SQL Assistant implementation?"}, {"role": "assistant", "content": "SQL Assistant Use prompt based roughtly on what is shown here. from langchain_core.runnables import Runnable, RunnableConfig# Assistantclass Assistant:    def __init__(self, runnable: Runnable):        self.runnable = runnable    def __call__(self, state: State, config: RunnableConfig):        while True:            # Append to state            state = {**state}            # Invoke the tool-calling LLM            result = self.runnable.invoke(state)            # If it is a tool call -> response is valid            # If it has meaninful text -> response is valid            # Otherwise, we re-prompt it b/c response is not meaninful            if not result.tool_calls and (                not result.content                or isinstance(result.content, list)                and not result.content[0].get(\"text\")            ):                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]                state = {**state, \"messages\": messages}            else:                break        return {\"messages\": result}# Assistant runnablequery_gen_system = \"\"\"ROLE:You are an agent designed to interact with a SQL database. You have access to tools for interacting with the database.GOAL:Given an input question, create a syntactically correct SQLite query to run, then look at the results of the query and return the answer.INSTRUCTIONS:- Only use the below tools for the following operations.- Only use the information returned by the below tools to construct your final answer.- To start you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step.- Then you should query the schema of the most relevant tables.- Write your query based upon the schema of the tables. You MUST double check your query before executing it.- Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.- You can order the results by a relevant column to return the most interesting examples in the database.- Never query for all the columns from a specific table, only ask for the relevant columns given the question.- If you get an error while executing a query, rewrite the query and try again.- If the query returns a result, use check_result tool to check the query result.- If the query result result is empty, think about the table schema, rewrite the query, and try again.- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"query_gen_prompt = ChatPromptTemplate.from_messages([(\"system\", query_gen_system),(\"placeholder\", \"{messages}\")])assistant_runnable = query_gen_prompt | llm.bind_tools(tools)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What utility function is defined to wrap a ToolNode with a fallback for error handling in the agent implementation?"}, {"role": "assistant", "content": "Graph Utilities We will define a few utility functions to help us with the agent implementation. Specifically, we will wrap a ToolNode with a fallback to handle errors and surface them to the agent. from langchain_core.messages import ToolMessagefrom langchain_core.runnables import RunnableLambdadef create_tool_node_with_fallback(tools: list) -> dict:    return ToolNode(tools).with_fallbacks(        [RunnableLambda(handle_tool_error)], exception_key=\"error\"    )def _print_event(event: dict, _printed: set, max_length=1500):    current_state = event.get(\"dialog_state\")    if current_state:        print(f\"Currently in: \", current_state[-1])    message = event.get(\"messages\")    if message:        if isinstance(message, list):            message = message[-1]        if message.id not in _printed:            msg_repr = message.pretty_repr(html=True)            if len(msg_repr) > max_length:                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"            print(msg_repr)            _printed.add(message.id)def handle_tool_error(state) -> dict:    error = state.get(\"error\")    tool_calls = state[\"messages\"][-1].tool_calls    return {        \"messages\": [            ToolMessage(                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",                tool_call_id=tc[\"id\"],            )            for tc in tool_calls        ]    }"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What components are defined in the workflow for the agent in the graph?"}, {"role": "assistant", "content": "Graph We will then define the workflow for the agent. from langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.graph import END, StateGraphfrom langgraph.prebuilt import ToolNode, tools_condition# Graphbuilder = StateGraph(State)# Define nodes: these do the workbuilder.add_node(\"assistant\", Assistant(assistant_runnable))builder.add_node(\"tools\", create_tool_node_with_fallback(tools))# Define edges: these determine how the control flow movesbuilder.set_entry_point(\"assistant\")builder.add_conditional_edges(    \"assistant\",    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END    tools_condition,    # \"tools\" calls one of our tools. END causes the graph to terminate (and respond to the user)    {\"tools\": \"tools\", END: END},)builder.add_edge(\"tools\", \"assistant\")# The checkpointer lets the graph persist its statememory = SqliteSaver.from_conn_string(\":memory:\")graph = builder.compile(checkpointer=memory)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Which country's customers spent the most? And how much did they spend?"}, {"role": "assistant", "content": "Test questions = [\"Which country's customers spent the most? And how much did they spend?\",             \"How many albums does the artist Led Zeppelin have?\",             \"What was the most purchased track of 2017?\",             \"Which sales agent made the most in sales in 2009?\"] ## Invokeimport uuid_printed = set()thread_id = str(uuid.uuid4())config = {    \"configurable\": {        # Checkpoints are accessed by thread_id        \"thread_id\": thread_id,    }}msg = {\"messages\": (\"user\", questions[0])}messages = graph.invoke(msg,config)messages['messages'][-1].content## Stream_printed = set()thread_id = str(uuid.uuid4())config = {    \"configurable\": {        # Checkpoints are accessed by thread_id        \"thread_id\": thread_id,    }}events = graph.stream(    {\"messages\": (\"user\", questions[0])}, config, stream_mode=\"values\")for event in events:    _print_event(event, _printed)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three focus areas of agent evaluation?"}, {"role": "assistant", "content": "Eval Agent evaluation can focus on at least 3 things: Response: The inputs are a prompt and an optional list of tools. The output is the final agent response.Single step: As before, the inputs are a prompt and an optional list of tools. The output the tool call.Trajectory: As before, the inputs are a prompt and an optional list of tools. The output is the list of tool calls tipSee our evaluation guide for more details on Agent evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can we evaluate the overall performance of an agent on a task?"}, {"role": "assistant", "content": "Response evaluation We can evaluate how well an agent does overall on a task. This basically involves treating the agent as a black box and just evaluating whether it gets the job done or not. tipSee the full overview of agent response evaluation in our conceptual guide. Dataset First, create a dataset that evaluates end-to-end performance of the agent. We can take some questions related to the Chinook database from here. from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Which country's customers spent the most? And how much did they spend?\", \"The country whose customers spent the most is the USA, with a total expenditure of $523.06\"),    (\"What was the most purchased track of 2013?\", \"The most purchased track of 2013 was Hot Girl.\"),    (\"How many albums does the artist Led Zeppelin have?\",\"Led Zeppelin has 14 albums\"),    (\"What is the total price for the album Big Ones?\",\"The total price for the album 'Big Ones' is 14.85\"),    (\"Which sales agent made the most in sales in 2009?\", \"Steve Johnson made the most sales in 2009\"),]dataset_name = \"SQL Agent Response\"if not client.has_dataset(dataset_name=dataset_name):    dataset = client.create_dataset(dataset_name=dataset_name)    inputs, outputs = zip(        *[({\"input\": text}, {\"output\": label}) for text, label in examples]    )    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id) Run chain def predict_sql_agent_answer(example: dict):    \"\"\"Use this for answer evaluation\"\"\"    msg = {\"messages\": (\"user\", example[\"input\"])}    messages = graph.invoke(msg, config)    return {\"response\": messages['messages'][-1].content} Evaluator This can follow what we do for RAG where we compare the generated answer with the reference answer. from langchain import hubfrom langchain_openai import ChatOpenAI# Grade promptgrade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")def answer_evaluator(run, example) -> dict:    \"\"\"    A simple evaluator for RAG answer accuracy    \"\"\"    # Get question, ground truth answer, RAG chain answer    input_question = example.inputs[\"input\"]    reference = example.outputs[\"output\"]    prediction = run.outputs[\"response\"]    # LLM grader    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)    # Structured prompt    answer_grader = grade_prompt_answer_accuracy | llm    # Run evaluator    score = answer_grader.invoke({\"question\": input_question,                                  \"correct_answer\": reference,                                  \"student_answer\": prediction})    score = score[\"Score\"]    return {\"key\": \"answer_v_reference_score\", \"score\": score} Create evaluation from langsmith.evaluation import evaluateexperiment_results = evaluate(    predict_sql_agent_answer,    data=dataset_name,    evaluators=[answer_evaluator],    experiment_prefix=experiment_prefix + \"-response-v-reference\",    num_repetitions=3,    metadata={\"version\": metadata},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of single step evaluation in the context of agent actions?"}, {"role": "assistant", "content": "Single step evaluation Agents generally make multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate the individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. tipSee the full overview of single step evaluation in our conceptual guide. We can check a specific tool call using a custom evaluator: Here, we just invoke the assistant, assistant_runnable, with a prompt and check if the resulting tool call is as expected.Here, we are using a specialized agent where the tools are hard-coded (rather than passed with the dataset input).We specify the reference tool call for the step that we are evaluating, expected_tool_call. from langsmith.schemas import Example, Rundef predict_assistant(example: dict):    \"\"\"Invoke assistant for single tool call evaluation\"\"\"    msg = [ (\"user\", example[\"input\"]) ]    result = assistant_runnable.invoke({\"messages\":msg})    return {\"response\": result}def check_specific_tool_call(root_run: Run, example: Example) -> dict:    \"\"\"    Check if the first tool call in the response matches the expected tool call.    \"\"\"    # Exepected tool call    expected_tool_call = 'sql_db_list_tables'    # Run    response = root_run.outputs[\"response\"]    # Get tool call    try:        tool_call = getattr(response, 'tool_calls', [])[0]['name']    except (IndexError, KeyError):        tool_call = None    score = 1 if tool_call == expected_tool_call else 0    return {\"score\": score, \"key\": \"single_tool_call\"}experiment_results = evaluate(    predict_assistant,    data=dataset_name,    evaluators=[check_specific_tool_call],    experiment_prefix=experiment_prefix + \"-single-tool\",    num_repetitions=3,    metadata={\"version\": metadata},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to evaluate the trajectory of tool calls in the provided text?"}, {"role": "assistant", "content": "Trajectory We can check a trajectory of tool calls using custom evaluators: Here, we just invoke the agent, graph.invoke, with a prompt.Here, we are using a specialized agent where the tools are hard-coded (rather than passed with the dataset input).We extract the list of tools called, using find_tool_calls.Custom functions can process these tool calls in various user-defined ways.We can check if all expected tools are called in any order: contains_all_tool_calls_any_orderWe can check if all expected tools are called in order, allowing for insertion of tool calls: contains_all_tool_calls_in_orderWe can check if all expected tools are called in the exact order: contains_all_tool_calls_in_order_exact_match tipSee the full overview of single step evaluation in our conceptual guide. def predict_sql_agent_messages(example: dict):    \"\"\"Use this for answer evaluation\"\"\"    msg = {\"messages\": (\"user\", example[\"input\"])}    messages = graph.invoke(msg, config)    return {\"response\": messages}def find_tool_calls(messages):    \"\"\"    Find all tool calls in the messages returned    \"\"\"    tool_calls = [tc['name'] for m in messages['messages'] for tc in getattr(m, 'tool_calls', [])]    return tool_callsdef contains_all_tool_calls_any_order(root_run: Run, example: Example) -> dict:    \"\"\"    Check if all expected tools are called in any order.    \"\"\"    expected = ['sql_db_list_tables', 'sql_db_schema', 'sql_db_query_checker', 'sql_db_query', 'check_result']    messages = root_run.outputs[\"response\"]    tool_calls = find_tool_calls(messages)    # Optionally, log the tool calls -    #print(\"Here are my tool calls:\")    #print(tool_calls)    if set(expected) <= set(tool_calls):        score = 1    else:        score = 0    return {\"score\": int(score), \"key\": \"multi_tool_call_any_order\"}def contains_all_tool_calls_in_order(root_run: Run, example: Example) -> dict:    \"\"\"    Check if all expected tools are called in exact order.    \"\"\"    messages = root_run.outputs[\"response\"]    tool_calls = find_tool_calls(messages)    # Optionally, log the tool calls -    #print(\"Here are my tool calls:\")    #print(tool_calls)    it = iter(tool_calls)    expected = ['sql_db_list_tables', 'sql_db_schema', 'sql_db_query_checker', 'sql_db_query', 'check_result']    if all(elem in it for elem in expected):        score = 1    else:        score = 0    return {\"score\": int(score), \"key\": \"multi_tool_call_in_order\"}def contains_all_tool_calls_in_order_exact_match(root_run: Run, example: Example) -> dict:    \"\"\"    Check if all expected tools are called in exact order and without any additional tool calls.    \"\"\"    expected = ['sql_db_list_tables', 'sql_db_schema', 'sql_db_query_checker', 'sql_db_query', 'check_result']    messages = root_run.outputs[\"response\"]    tool_calls = find_tool_calls(messages)    # Optionally, log the tool calls -    #print(\"Here are my tool calls:\")    #print(tool_calls)    if tool_calls == expected:        score = 1    else:        score = 0    return {\"score\": int(score), \"key\": \"multi_tool_call_in_exact_order\"}experiment_results = evaluate(    predict_sql_agent_messages,    data=dataset_name,    evaluators=[contains_all_tool_calls_any_order,contains_all_tool_calls_in_order,contains_all_tool_calls_in_order_exact_match],    experiment_prefix=experiment_prefix + \"-trajectory\",    num_repetitions=3,    metadata={\"version\": metadata},) You can see the results from the evaluations logged to the dataset! https://smith.langchain.com/public/20808486-67c3-4e30-920b-6d49d6f2b6b8/d"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key features and functionalities related to managing datasets in the application?"}, {"role": "assistant", "content": "How-to guides: Datasets This section contains how-to guides related to working with datasets.  Manage datasets in the applicationBefore diving into this content, it might be helpful to read the following: Manage datasets programmaticallyYou can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them. Version datasetsIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. Share or unshare a dataset publiclySharing a dataset publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information. Dynamic few shot example selectionThis feature is currently in closed beta. Please sign up here for access"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do before managing datasets in the application?"}, {"role": "assistant", "content": "Manage datasets in the application Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens in LangSmith when you add, update, or delete examples in a dataset?"}, {"role": "assistant", "content": "Version datasets In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you ensure before sharing a dataset publicly?"}, {"role": "assistant", "content": "Share or unshare a dataset publicly Sharing a dataset publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I gain access to the dynamic few shot example selection feature?"}, {"role": "assistant", "content": "Dynamic few shot example selection This feature is currently in closed beta. Please sign up here for access"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in installing and configuring a self-hosted instance of LangSmith?"}, {"role": "assistant", "content": "Self-Hosting LangSmith Step-by-step guides that cover the installation, configuration, and scaling of your Self-Hosted LangSmith instance. Architectural overview: A high-level overview of the LangSmith architecture.Storage services: The storage services used by LangSmith.Services: The services that make up LangSmith.Installation: How to install LangSmith on your own infrastructure.Kubernetes: Deploy LangSmith on Kubernetes.Docker: Deploy LangSmith using Docker.Configuration: How to configure your self-hosted instance of LangSmith.SSO with OAuth2.0 and OIDC: Configure LangSmith to use OAuth2.0 and OIDC for SSO.Connect to an external ClickHouse database: Configure LangSmith to use an external ClickHouse database.Connect to an external Postgres database: Configure LangSmith to use an external Postgres database.Connect to an external Redis instance: Configure LangSmith to use an external Redis instance.Usage: How to use your self-hosted instance of LangSmith.Upgrades: How to upgrade your self-hosted instance of LangSmith.Release notes: The latest release notes for LangSmith.Week of June 17, 2024 - LangSmith v0.6: Release notes for version 0.6 of LangSmith.Week of May 13, 2024 - LangSmith v0.5: Release notes for version 0.5 of LangSmith.Week of March 25, 2024 - LangSmith v0.4: Release notes for version 0.4 of LangSmith.Week of Februrary 21, 2024 - LangSmith v0.3: Release notes for version 0.3 of LangSmith.Week of January 29, 2024 - LangSmith v0.2: Release notes for version 0.2 of LangSmith.FAQ: Frequently asked questions about LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you are using LangChain in relation to logging traces to LangSmith?"}, {"role": "assistant", "content": "Annotate code for tracing There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces using the @traceable decorator in Python and the traceable function in TypeScript?"}, {"role": "assistant", "content": "Use@traceable/traceable LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\nfrom langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful assistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What's a good name for a store that sells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(\"colorful socks\")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.\nimport { traceable } from \"langsmith/traceable\";import OpenAI from \"openai\";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: \"system\" as const,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: \"formatPrompt\" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: messages,            temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: \"parseOutput\" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(\"colorful socks\");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: \"runPipeline\" });await runPipeline();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces when using the wrap_openai or wrapOpenAI methods?"}, {"role": "assistant", "content": "Wrap the OpenAI client The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!\nThe wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"@traceable(name=\"Chat Pipeline\")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(\"Can you summarize this morning's meetings?\")import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return \"During this morning's meeting, we solved all world conflict.\";}, { name: \"Retrieve Context\", run_type: \"tool\" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: \"system\",            content:                \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",        },        { role: \"user\", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: \"Chat Pipeline\" });await chatPipeline(\"Can you summarize this morning's meetings?\");"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the RunTree API in LangSmith?"}, {"role": "assistant", "content": "Use theRunTreeAPI Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually\ncreate runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not\nnecessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \"Can you summarize this morning's meetings?\"# Create a top-level runpipeline = RunTree(    name=\"Chat Pipeline\",    run_type=\"chain\",    inputs={\"question\": question})# This can be retrieved in a retrieval stepcontext = \"During this morning's meeting, we solved all world conflict.\"messages = [    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}]# Create a child runchild_llm_run = pipeline.create_child(    name=\"OpenAI Call\",    run_type=\"llm\",    inputs={\"messages\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from \"openai\";import { RunTree } from \"langsmith\";// This can be a user input to your appconst question = \"Can you summarize this morning's meetings?\";const pipeline = new RunTree({    name: \"Chat Pipeline\",    run_type: \"chain\",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = \"During this morning's meeting, we solved all world conflict.\";const messages = [    { role: \"system\", content: \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { role: \"user\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: \"OpenAI Call\",    run_type: \"llm\",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using the trace context manager in Python?"}, {"role": "assistant", "content": "Use thetracecontext manager (Python only) In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {\"input\": \"Can you summarize this morning's meetings?\"}with trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:    output = chat_pipeline(\"Can you summarize this morning's meetings?\")    rt.end(outputs={\"output\": output})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the AI Query functionality mentioned in the text?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you prevent the logging of inputs and outputs in traces when using LangSmith?"}, {"role": "assistant", "content": "Prevent logging of sensitive data in traces In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend. If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application: LANGCHAIN_HIDE_INPUTS=trueLANGCHAIN_HIDE_OUTPUTS=true This works for both the LangSmith SDK (Python and TypeScript) and LangChain. You can also customize and override this behavior for a given Client instance. This can be done by setting the hide_inputs and hide_outputs parameters on the Client object (hideInputs and hideOutputs in TypeScript). For the example below, we will simply return an empty object for both hide_inputs and hide_outputs, but you can customize this to your needs. PythonTypeScriptimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(openai.Client())langsmith_client = Client(    hide_inputs=lambda inputs: {}, hide_outputs=lambda outputs: {})# The trace produced will have its metadata present, but the inputs will be hiddenopenai_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"Hello!\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have hidden inputs and outputsopenai_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"Hello!\"},    ],)import OpenAI from \"openai\";import { Client } from \"langsmith\";import { wrapOpenAI } from \"langsmith/wrappers\";const langsmithClient = new Client({  hideInputs: (inputs) => ({}),  hideOutputs: (outputs) => ({}),});// The trace produced will have its metadata present, but the inputs will be hiddenconst filteredOAIClient = wrapOpenAI(new OpenAI(), {    client: langsmithClient,});await filteredOAIClient.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: [      { role: \"system\", content: \"You are a helpful assistant.\" },      { role: \"user\", content: \"Hello!\" },    ],});const openaiClient = wrapOpenAI(new OpenAI());// The trace produced will not have hidden inputs and outputsawait openaiClient.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: [      { role: \"system\", content: \"You are a helpful assistant.\" },      { role: \"user\", content: \"Hello!\" },    ],});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the LangSmith SDK versions that support the rule-based masking of inputs and outputs?"}, {"role": "assistant", "content": "Rule-based masking of inputs and outputs infoThis feature is available in the following LangSmith SDK versions:Python: 0.1.81 and aboveTypeScript: 0.1.33 and above To mask specific data in inputs and outputs, you can use the create_anonymizer / createAnonymizer function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value. The anonymizer will be skipped for inputs if LANGCHAIN_HIDE_INPUTS = true. Same applies for outputs if LANGCHAIN_HIDE_OUTPUTS = true. However, if inputs or outputs are to be sent to client, the anonymizer method will take precedence over functions found in hide_inputs and hide_outputs. By default, the create_anonymizer will only look at maximum of 10 nesting levels deep, which can be configured via the max_depth parameter. PythonTypeScriptfrom langsmith.anonymizer import create_anonymizerfrom langsmith import Client, traceable# create anonymizer from list of regex patterns and replacement valuesanonymizer = create_anonymizer([    { \"pattern\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\", \"replace\": \"<email>\" },    { \"pattern\": r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\", \"replace\": \"<uuid>\" }])# or create anonymizer from a functionanonymizer = create_anonymizer(lambda text: r\"...\".sub(\"[value]\", text))client = Client(anonymizer=anonymizer)@traceable(client=client)def main(inputs: dict) -> dict:    ...import { createAnonymizer } from \"langsmith/anonymizer\"import { traceable } from \"langsmith/traceable\"import { Client } from \"langsmith\"// create anonymizer from list of regex patterns and replacement valuesconst anonymizer = createAnonymizer([    { pattern: /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g, replace: \"<email>\" },    { pattern: /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g, replace: \"<uuid>\" }])// or create anonymizer from a functionconst anonymizer = createAnonymizer((value) => value.replace(\"...\", \"<value>\"))const client = new Client({ anonymizer })const main = traceable(async (inputs: any) => {    // ...}, { client }) Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing. noteImproving the performance of anonymizer API is on our roadmap! If you are encountering performance issues, please contact us at support@langchain.dev. Older versions of LangSmith SDKs can use the hide_inputs and hide_outputs parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well. PythonTypeScriptimport refrom langsmith import Client, traceable# Define the regex patterns for email addresses and UUIDsEMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\"UUID_REGEX = r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\"def replace_sensitive_data(data, depth=10):    if depth == 0:        return data    if isinstance(data, dict):        return {k: replace_sensitive_data(v, depth-1) for k, v in data.items()}    elif isinstance(data, list):        return [replace_sensitive_data(item, depth-1) for item in data]    elif isinstance(data, str):        data = re.sub(EMAIL_REGEX, \"<email-address>\", data)        data = re.sub(UUID_REGEX, \"<UUID>\", data)        return data    else:        return dataclient = Client(    hide_inputs=lambda inputs: replace_sensitive_data(inputs),    hide_outputs=lambda outputs: replace_sensitive_data(outputs))inputs = {\"role\": \"user\", \"content\": \"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.\"}outputs = {\"role\": \"assistant\", \"content\": \"Hi! I've noted your email as user@example.com and your ID as 123e4567-e89b-12d3-a456-426614174000.\"}@traceable(client=client)def child(inputs: dict) -> dict:    return outputs@traceable(client=client)def parent(inputs: dict) -> dict:    child_outputs = child(inputs)    return child_outputsparent(inputs)import { Client } from \"langsmith\";import { traceable } from \"langsmith/traceable\";// Define the regex patterns for email addresses and UUIDsconst EMAIL_REGEX = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}/g;const UUID_REGEX = /[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}/g;function replaceSensitiveData(data: any, depth: number = 10): any {    if (depth === 0) return data;    if (typeof data === \"object\" && !Array.isArray(data)) {        const result: Record<string, any> = {};        for (const [key, value] of Object.entries(data)) {            result[key] = replaceSensitiveData(value, depth - 1);        }        return result;    } else if (Array.isArray(data)) {        return data.map(item => replaceSensitiveData(item, depth - 1));    } else if (typeof data === \"string\") {        return data.replace(EMAIL_REGEX, \"<email-address>\").replace(UUID_REGEX, \"<UUID>\");    } else {        return data;    }}const langsmithClient = new Client({    hideInputs: (inputs) => replaceSensitiveData(inputs),    hideOutputs: (outputs) => replaceSensitiveData(outputs)});const inputs = {    role: \"user\",    content: \"Hello! My email is user@example.com and my ID is 123e4567-e89b-12d3-a456-426614174000.\"};const outputs = {    role: \"assistant\",    content: \"Hi! I've noted your email as <email-address> and your ID as <UUID>.\"};const child = traceable(async (inputs: any) => {    return outputs;}, { name: \"child\", client: langsmithClient });const parent = traceable(async (inputs: any) => {    const childOutputs = await child(inputs);    return childOutputs;}, { name: \"parent\", client: langsmithClient });await parent(inputs)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the purposes of the process_inputs and process_outputs parameters in the LangSmith SDK?"}, {"role": "assistant", "content": "Processing Inputs & Outputs for a Single Function infoThe process_outputs parameter is available in LangSmith SDK version 0.1.98 and above for Python. In addition to client-level input and output processing, LangSmith provides function-level processing through the process_inputs and process_outputs parameters of the @traceable decorator. These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function. Here's an example of how to use process_inputs and process_outputs: from langsmith import traceabledef process_inputs(inputs: dict) -> dict:    # inputs is a dictionary where keys are argument names and values are the provided arguments    # Return a new dictionary with processed inputs    return {        \"processed_key\": inputs.get(\"my_cool_key\", \"default\"),        \"length\": len(inputs.get(\"my_cool_key\", \"\"))    }def process_outputs(output: Any) -> dict:    # output is the direct return value of the function    # Transform the output into a dictionary    # In this case, \"output\" will be an integer    return {\"processed_output\": str(output)}@traceable(process_inputs=process_inputs, process_outputs=process_outputs)def my_function(my_cool_key: str) -> int:    # Function implementation    return len(my_cool_key)result = my_function(\"example\") In this example, process_inputs creates a new dictionary with processed input data, and process_outputs transforms the output into a specific format before logging to LangSmith. cautionIt's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data. For asynchronous functions, the usage is similar: @traceable(process_inputs=process_inputs, process_outputs=process_outputs)async def async_function(key: str) -> int:    # Async implementation    return len(key) These function-level processors take precedence over client-level processors (hide_inputs and hide_outputs) when both are defined."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate the improvement of few-shot evaluators for LLM-based applications?"}, {"role": "assistant", "content": "Create few-shot evaluators Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary\noverhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows\nyou to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default maximum number of few-shot examples that can be used in the prompt when creating an evaluator?"}, {"role": "assistant", "content": "Create your evaluator tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators\nare only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,\nwe will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}\ntemplate variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name\nas your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,\nfew_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend\nto be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do when you disagree with the scores given by your evaluator?"}, {"role": "assistant", "content": "Make corrections Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will\nbegin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.\nThe outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access your corrections dataset?"}, {"role": "assistant", "content": "View your corrections dataset In order to view your corrections dataset, go to your rule and click \"Edit Rule\" (or \"Edit Evaluator\" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says \"View few-shot dataset\". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens in LangSmith when you add, update, or delete examples in a dataset?"}, {"role": "assistant", "content": "Version datasets In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to a dataset when examples are added, updated, or deleted?"}, {"role": "assistant", "content": "Create a new version of a dataset Any time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved. By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the \"Examples\" tab, you can see the state of the dataset at that point in time. Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the \"latest\" version of the dataset. Also, by default the latest version of the dataset is shown in the \"Examples\" tab and experiments from all versions are shown in the \"Tests\" tab. In the \"Tests\" tab, you can see the results of tests run on the dataset at different versions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you tag a version of your dataset to mark important milestones?"}, {"role": "assistant", "content": "Tag a version You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history. For example, you might tag a version of your dataset as \"prod\" and use it to run tests against your LLM pipeline. Tagging can be done in the UI by clicking on \"+ Tag this version\" in the \"Examples\" tab. You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK: from langsmith import Clientfromt datetime import datetimeclient = Client()initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like \"prod\"client.update_dataset_tag(    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\") To run an evaluation on a particular tagged version of a dataset, you can follow this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to adjust the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will API keys prefixed with ls__ be disabled in favor of lsv2 style keys?"}, {"role": "assistant", "content": "LangSmith Release Notes noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith v0.6 during the week of June 17, 2024?"}, {"role": "assistant", "content": "Week of June 17, 2024 - LangSmith v0.6 LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced in LangSmith?"}, {"role": "assistant", "content": "New Features Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the performance and reliability of run rules and querying the Hub via the SDK?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the default port and Helm configuration in the recent infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes [Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were added in the recent admin changes?"}, {"role": "assistant", "content": "Admin changes Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.5.x and earlier following the release of v0.6?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and features does LangSmith v0.5 introduce?"}, {"role": "assistant", "content": "Week of May 13, 2024 - LangSmith v0.5 LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after the LangSmith Helm release v0.7 in August 2024?"}, {"role": "assistant", "content": "Breaking changes We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced, including role-based access controls and improvements in regression testing and production monitoring?"}, {"role": "assistant", "content": "New Features Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to improve performance and reliability in resource management?"}, {"role": "assistant", "content": "Performance and Reliability Changes Split ingest, session deletion, and automation jobs to execute within separate resource pools."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default storage size for Clickhouse persistence in LangSmith v0.4, and how can it be adjusted?"}, {"role": "assistant", "content": "Infrastructure changes As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes regarding access controls and API keys?"}, {"role": "assistant", "content": "Admin changes Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.4.x and earlier following the release of v0.5?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.4 during the week of March 25, 2024?"}, {"role": "assistant", "content": "Week of March 25, 2024 - LangSmith v0.4 LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes should be made to ensure backwards compatibility when updating to the new release?"}, {"role": "assistant", "content": "Breaking changes This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to enhance performance and reliability in the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the image repositories and Clickhouse persistence storage in the infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to the API key management and OAuth flow in the recent admin updates?"}, {"role": "assistant", "content": "Admin changes Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.3.x and earlier versions following the release of 0.4?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.3 during the week of February 21, 2024?"}, {"role": "assistant", "content": "Week of Februrary 21, 2024 - LangSmith v0.3 LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do to retain run data when migrating from LangSmith v0.1?"}, {"role": "assistant", "content": "Breaking changes This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the LangSmith UI regarding trace ingestion and display delay?"}, {"role": "assistant", "content": "Performance and Reliability Changes Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is available regarding admin changes?"}, {"role": "assistant", "content": "Admin changes nan"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.2.x and earlier versions following the release of 0.3?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features does LangSmith 0.2 include?"}, {"role": "assistant", "content": "Week of January 29, 2024 - LangSmith v0.2 LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What versions of langsmith-sdk are required for this release to support changes in pagination of API results?"}, {"role": "assistant", "content": "Requirements This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to make custom metadata in traces searchable after the change in search syntax?"}, {"role": "assistant", "content": "Breaking changes The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,}) and then search using has(metadata, '{\"user_name\": ...}')"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to the performance and reliability of the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What database service has been added to improve performance and scalability for storing run results in LangSmith?"}, {"role": "assistant", "content": "Infrastructure Changes Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the new maximum number of users allowed per organization for new organizations?"}, {"role": "assistant", "content": "Admin changes Increased the maximum number of users per organization from 5 to 100 for new organizations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of LangSmith 0.1.x following the release of version 0.2?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features does LangSmith offer for capturing and annotating user feedback?"}, {"role": "assistant", "content": "How-to guides: Human feedback This section contains how-to guides related to human feedback.  Capture user feedback from your application to tracesBefore diving into this content, it might be helpful to read the following: Set up feedback criteriaBefore diving into this content, it might be helpful to read the following: Annotate traces and runs inlineLangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue. Use annotation queuesAnnotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can user feedback be captured from an application to traces?"}, {"role": "assistant", "content": "Capture user feedback from your application to traces Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should be done before engaging with the content related to setting up feedback criteria?"}, {"role": "assistant", "content": "Set up feedback criteria Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionality does LangSmith provide for annotating traces?"}, {"role": "assistant", "content": "Annotate traces and runs inline LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of annotation queues in LangSmith?"}, {"role": "assistant", "content": "Use annotation queues Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to adjust the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of annotation queues in LangSmith?"}, {"role": "assistant", "content": "Use annotation queues Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.\nWhile you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in creating an annotation queue and what settings can be configured for multiple annotators?"}, {"role": "assistant", "content": "Create an annotation queue To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.\nThen click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.\nYou can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as \"Done\" for it to be removed from the queue. If you check \"All workspace members review each run,\" then a run will remain in the queue until all workspace members have marked it \"Done\".Enable reservations on runs: We recommend enabling reservations.\nThis will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified \"reservation length\". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it \"Done\", the reservation will expire after the specified \"reservation length\".\nThe run is then released back into the queue and can be reserved by another reviewer. noteClicking \"Requeue at end\" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you assign runs to an annotation queue?"}, {"role": "assistant", "content": "Assign runs to an annotation queue To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.\nSelect multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.\nSet up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.\nTo learn more about how to capture user feedback from your LLM application, follow this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you review runs in an annotation queue?"}, {"role": "assistant", "content": "Review runs in an annotation queue To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.\nThen click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.\nYou can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to \"View run\". The keyboard shortcuts shown can help streamline the review process."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of logging to LangSmith?"}, {"role": "assistant", "content": "Tracing This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Run in the context of an LLM application?"}, {"role": "assistant", "content": "Runs A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace and how is it related to runs in the context of an operation?"}, {"role": "assistant", "content": "Traces A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a project in the context of traces?"}, {"role": "assistant", "content": "Projects A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different ways to collect feedback on runs?"}, {"role": "assistant", "content": "Feedback Feedback allows you to score an individual run based on certain criteria.\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are tags used for in the LangSmith UI?"}, {"role": "assistant", "content": "Tags Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of metadata in relation to runs?"}, {"role": "assistant", "content": "Metadata Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How long does LangSmith retain trace data after it is inserted into the trace database?"}, {"role": "assistant", "content": "Data storage and retention For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you delete a trace from LangSmith before its expiration date?"}, {"role": "assistant", "content": "Deleting traces from LangSmith If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the \"Delete\" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will infoWorkspaces be incrementally rolled out?"}, {"role": "assistant", "content": "Set up a workspace infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.\nWorkspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new workspace in your shared organization?"}, {"role": "assistant", "content": "Create a workspace To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.\nOnce your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.\nPlease see the pricing page for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What permissions do workspace admins have regarding user management in a workspace?"}, {"role": "assistant", "content": "Manage users infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.\nUsers may also be invited directly to one or more workspaces when they are invited to an organization."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options can be configured in the workspace settings page?"}, {"role": "assistant", "content": "Configure workspace settings Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with LangSmith?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will infoWorkspaces be incrementally rolled out?"}, {"role": "assistant", "content": "Set up a workspace infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.\nWorkspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new workspace in your shared organization?"}, {"role": "assistant", "content": "Create a workspace To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.\nOnce your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.\nPlease see the pricing page for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What permissions do workspace admins have regarding user management in a workspace?"}, {"role": "assistant", "content": "Manage users infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.\nUsers may also be invited directly to one or more workspaces when they are invited to an organization."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options can be configured in the workspace settings page?"}, {"role": "assistant", "content": "Configure workspace settings Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will API keys prefixed with ls__ be disabled in favor of lsv2 style keys?"}, {"role": "assistant", "content": "LangSmith Release Notes noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith v0.6 during the week of June 17, 2024?"}, {"role": "assistant", "content": "Week of June 17, 2024 - LangSmith v0.6 LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced in LangSmith?"}, {"role": "assistant", "content": "New Features Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the performance and reliability of run rules in the Hub SDK version 0.1.20 or greater?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the default port and Helm configuration in the recent infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes [Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were added in the recent admin changes?"}, {"role": "assistant", "content": "Admin changes Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.5.x and earlier following the release of v0.6?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and features does LangSmith v0.5 introduce?"}, {"role": "assistant", "content": "Week of May 13, 2024 - LangSmith v0.5 LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after the LangSmith Helm release v0.7 in August 2024?"}, {"role": "assistant", "content": "Breaking changes We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced, including role-based access controls and improvements in regression testing and production monitoring?"}, {"role": "assistant", "content": "New Features Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to improve performance and reliability in resource management?"}, {"role": "assistant", "content": "Performance and Reliability Changes Split ingest, session deletion, and automation jobs to execute within separate resource pools."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default storage size for Clickhouse persistence in LangSmith v0.4, and how can it be adjusted?"}, {"role": "assistant", "content": "Infrastructure changes As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes regarding access controls and API keys?"}, {"role": "assistant", "content": "Admin changes Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.4.x and earlier following the release of v0.5?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.4 during the week of March 25, 2024?"}, {"role": "assistant", "content": "Week of March 25, 2024 - LangSmith v0.4 LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes should be made to ensure backwards compatibility when updating to the new release?"}, {"role": "assistant", "content": "Breaking changes This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to enhance performance and reliability in the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the image repositories and Clickhouse persistence storage in the infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to the API key management and OAuth flow in the recent admin updates?"}, {"role": "assistant", "content": "Admin changes Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.3.x and earlier versions following the release of 0.4?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.3 during the week of February 21, 2024?"}, {"role": "assistant", "content": "Week of Februrary 21, 2024 - LangSmith v0.3 LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do to retain run data when migrating from LangSmith v0.1?"}, {"role": "assistant", "content": "Breaking changes This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the LangSmith UI regarding trace ingestion and display delay?"}, {"role": "assistant", "content": "Performance and Reliability Changes Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is available regarding admin changes?"}, {"role": "assistant", "content": "Admin changes nan"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.2.x and earlier versions following the release of 0.3?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features does LangSmith 0.2 include?"}, {"role": "assistant", "content": "Week of January 29, 2024 - LangSmith v0.2 LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What versions of langsmith-sdk are required for this release to support changes in pagination of API results?"}, {"role": "assistant", "content": "Requirements This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the search syntax for metadata in runs?"}, {"role": "assistant", "content": "Breaking changes The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,}) and then search using has(metadata, '{\"user_name\": ...}')"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to the performance and reliability of the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What database service has been added to improve performance and scalability for storing run results in LangSmith?"}, {"role": "assistant", "content": "Infrastructure Changes Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the new maximum number of users allowed per organization for new organizations?"}, {"role": "assistant", "content": "Admin changes Increased the maximum number of users per organization from 5 to 100 for new organizations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of LangSmith 0.1.x following the release of version 0.2?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can users upload experiments run outside of LangSmith to view results in the LangSmith UI?"}, {"role": "assistant", "content": "Upload experiments run outside of LangSmith with the REST API Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required to upload an experiment using the /datasets/upload-experiment endpoint?"}, {"role": "assistant", "content": "Request body schema Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within\nthe experiment. Each object in the results represents a \"row\" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name\nrefer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset\nin LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  \"experiment_name\": \"string (required)\",  \"experiment_description\": \"string (optional)\",  \"experiment_start_time\": \"datetime (required)\",  \"experiment_end_time\": \"datetime (required)\",  \"dataset_id\": \"uuid (optional - an external dataset id, used to group experiments together)\",  \"dataset_name\": \"string (optional - must provide either dataset_id or dataset_name)\",  \"dataset_description\": \"string (optional)\",  \"experiment_metadata\": { // Object (any shape - optional)    \"key\": \"value\"  },  \"summary_experiment_scores\": [ // List of summary feedback objects (optional)    {      \"key\": \"string (required)\",      \"score\": \"number (optional)\",      \"value\": \"string (optional)\",      \"comment\": \"string (optional)\",      \"feedback_source\": { // Object (optional)        \"type\": \"string (required)\"      },      \"feedback_config\": { // Object (optional)        \"type\": \"string enum: continuous, categorical, or freeform\",        \"min\": \"number (optional)\",        \"max\": \"number (optional)\",        \"categories\": [ // List of feedback category objects (optional)            \"value\": \"number (required)\",            \"label\": \"string (optional)\"        ]      },      \"created_at\": \"datetime (optional - defaults to now)\",      \"modified_at\": \"datetime (optional - defaults to now)\",      \"correction\": \"Object or string (optional)\"    }  ],  \"results\": [ // List of experiment row objects (required)    {      \"row_id\": \"uuid (required)\",      \"inputs\": {     // Object (required - any shape). This will        \"key\": \"val\"  // be the input to both the run and the dataset example.      },      \"expected_outputs\": { // Object (optional - any shape).        \"key\": \"val\"        // These will be the outputs of the dataset examples.      },      \"actual_outputs\": { // Object (optional - any shape).        \"key\": \"val       // These will be the outputs of the runs.      },      \"evaluation_scores\": [ // List of feedback objects for the run (optional)        {            \"key\": \"string (required)\",            \"score\": \"number (optional)\",            \"value\": \"string (optional)\",            \"comment\": \"string (optional)\",            \"feedback_source\": { // Object (optional)                \"type\": \"string (required)\"            },            \"feedback_config\": { // Object (optional)                \"type\": \"string enum: continuous, categorical, or freeform\",                \"min\": \"number (optional)\",                \"max\": \"number (optional)\",                \"categories\": [ // List of feedback category objects (optional)                    \"value\": \"number (required)\",                    \"label\": \"string (optional)\"                ]            },            \"created_at\": \"datetime (optional - defaults to now)\",            \"modified_at\": \"datetime (optional - defaults to now)\",            \"correction\": \"Object or string (optional)\"        }      ],      \"start_time\": \"datetime (required)\", // The start/end times for the runs will be used to      \"end_time\": \"datetime (required)\",   // calculate latency. They must all fall between the      \"run_name\": \"string (optional)\",     // start and end times for the experiment.      \"error\": \"string (optional)\",      \"run_metadata\": { // Object (any shape - optional)        \"key\": \"value\"      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for uploading multiple experiments to the same dataset?"}, {"role": "assistant", "content": "Considerations You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together\nunder a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if\nyou only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key details of the experiment uploaded to LangSmith, including its name, description, and start and end times?"}, {"role": "assistant", "content": "Example request Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  \"experiment_name\": \"My external experiment\",  \"experiment_description\": \"An experiment uploaded to LangSmith\",  \"dataset_name\": \"my-external-dataset\",  \"summary_experiment_scores\": [    {      \"key\": \"summary_accuracy\",      \"score\": 0.9,      \"comment\": \"Great job!\"    }  ],  \"results\": [    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the weather in San Francisco today?\"      },      \"expected_outputs\": {        \"output\": \"Sorry, I am unable to provide information about the current weather.\"      },      \"actual_outputs\": {        \"output\": \"The weather is partly cloudy with a high of 65.\"      },      \"evaluation_scores\": [        {          \"key\": \"hallucination\",          \"score\": 1,          \"comment\": \"The chatbot made up the weather instead of identifying that \"                     \"they don't have enough info to answer the question. This is \"                     \"a hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:39\",      \"end_time\": \"2024-08-03T00:12:41\",      \"run_name\": \"Chatbot\"    },    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the square root of 49?\"      },      \"expected_outputs\": {        \"output\": \"The square root of 49 is 7.\"      },      \"actual_outputs\": {        \"output\": \"7.\"      },      \"evaluation_scores\": [       {          \"key\": \"hallucination\",          \"score\": 0,          \"comment\": \"The chatbot correctly identified the answer. This is not a \"                     \"hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:40\",      \"end_time\": \"2024-08-03T00:12:42\",      \"run_name\": \"Chatbot\"    }  ],  \"experiment_start_time\": \"2024-08-03T00:12:38\",  \"experiment_end_time\": \"2024-08-03T00:12:43\"}resp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\",    json=body,    headers={\"x-api-key\": os.environ[\"LANGCHAIN_API_KEY\"]})print(resp.json()) Below is the response received: {  \"dataset\": {    \"name\": \"my-external-dataset\",    \"description\": null,    \"created_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"data_type\": \"kv\",    \"inputs_schema_definition\": null,    \"outputs_schema_definition\": null,    \"externally_managed\": true,    \"id\": \"<<uuid>>\",    \"tenant_id\": \"<<uuid>>\",    \"example_count\": 0,    \"session_count\": 0,    \"modified_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"last_session_start_time\": null  },  \"experiment\": {    \"start_time\": \"2024-08-03T00:12:38\",    \"end_time\": \"2024-08-03T00:12:43+00:00\",    \"extra\": null,    \"name\": \"My external experiment\",    \"description\": \"An experiment uploaded to LangSmith\",    \"default_dataset_id\": null,    \"reference_dataset_id\": \"<<uuid>>\",    \"trace_tier\": \"longlived\",    \"id\": \"<<uuid>>\",    \"run_count\": null,    \"latency_p50\": null,    \"latency_p99\": null,    \"first_token_p50\": null,    \"first_token_p99\": null,    \"total_tokens\": null,    \"prompt_tokens\": null,    \"completion_tokens\": null,    \"total_cost\": null,    \"prompt_cost\": null,    \"completion_cost\": null,    \"tenant_id\": \"<<uuid>>\",    \"last_run_start_time\": null,    \"last_run_start_time_live\": null,    \"feedback_stats\": null,    \"session_feedback_stats\": null,    \"run_facets\": null,    \"error_rate\": null,    \"streaming_rate\": null,    \"test_run_number\": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.\nIf you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this\ninformation in the request body)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will you see after clicking on your newly-created dataset in the UI?"}, {"role": "assistant", "content": "View the experiment in the UI Now, login to the UI and click on your newly-created dataset! You should see a single experiment:\n Your examples will have been uploaded:\n Clicking on your experiment will bring you to the comparison view:\n As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use your own custom models in the LangSmith playground?"}, {"role": "assistant", "content": "Use a custom model The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via LangServe, an open source library for serving LangChain applications.\nBehind the scenes, the playground will interact with your model server to generate responses."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of custom model implementations mentioned for the sample model server?"}, {"role": "assistant", "content": "Deploy a custom model server For your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server here\nWe highly recommend using the sample model server as a starting point. Depending on your model is an instruct-style or chat-style model, you will need to implement either custom_model.py or custom_chat_model.py respectively."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you make your model configurable in the LangSmith playground?"}, {"role": "assistant", "content": "Adding configurable fields It is often useful to configure your model with different parameters. These might include temperature, model_name, max_tokens, etc. To make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground. You can add configurable fields by implementing the with_configurable_fields function in the config.py file. You can def with_configurable_fields(self) -> Runnable:    \"\"\"Expose fields you want to be configurable in the playground. We will automatically expose these to the    playground. If you don't want to expose any fields, you can remove this method.\"\"\"    return self.configurable_fields(n=ConfigurableField(        id=\"n\",        name=\"Num Characters\",        description=\"Number of characters to return from the input prompt.\",    ))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use a deployed model server in the LangSmith Playground?"}, {"role": "assistant", "content": "Use the model in the LangSmith Playground Once you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the ChatCustomModel or the CustomModel provider for chat-style model or instruct-style models. Enter the URL. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters. If everything is set up correctly, you should see the model's response in the playground as well as the configurable fields specified in the with_configurable_fields. See how to store your model configuration for later use here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate the improvement of few-shot evaluators for LLM-based applications?"}, {"role": "assistant", "content": "Create few-shot evaluators Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary\noverhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows\nyou to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default maximum number of few-shot examples that can be used in the prompt when creating an evaluator?"}, {"role": "assistant", "content": "Create your evaluator tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators\nare only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,\nwe will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}\ntemplate variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name\nas your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,\nfew_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend\nto be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do when you disagree with the scores given by your evaluator?"}, {"role": "assistant", "content": "Make corrections Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will\nbegin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.\nThe outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access your corrections dataset?"}, {"role": "assistant", "content": "View your corrections dataset In order to view your corrections dataset, go to your rule and click \"Edit Rule\" (or \"Edit Evaluator\" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says \"View few-shot dataset\". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does LangSmith offer to help with writing good prompts?"}, {"role": "assistant", "content": "Prompts Writing good prompts is key to getting the best performance out of your applications. LangSmith provides ways to create, test, and manage prompts."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three types of prompt templates supported in LangSmith?"}, {"role": "assistant", "content": "Prompt types We support three types of prompt templates: StringPromptTemplateChatPromptTemplateStructuredPromptTemplate For detailed information about these templates, please refer to the LangChain documentation. In LangSmith, you can create prompts using the Playground. From the prompt view in the Playground, you can select either \"Chat-style prompt\" or \"Instruct-style prompt\" to get started. Chat-style prompt Chat prompts are used for chat-style models that accept a list of messages as an input and respond with an assistant message. A chat-style prompt is represented in LangSmith as a ChatPromptTemplate, which can contain multiple messages, each with prompt variables. You can also specify an output schema which is represented in LangSmith as a StructuredPromptTemplate. Instruct-style prompt An instruct-style prompt is represented as a StringPromptTemplate that gets formatted to a single string input for your model."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of template formats supported?"}, {"role": "assistant", "content": "Template formats We support two types of template formats: f-string and mustache. You can switch between these formats when editing prompts in the Playground."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is an f-string in Python and how is it used?"}, {"role": "assistant", "content": "F-string F-strings are a Python-specific string formatting method that allows you to embed expressions inside string literals, using curly braces {}.\nHere's an example of an f-string template: Hello, {name}!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some features of Mustache template syntax that differentiate it from f-strings?"}, {"role": "assistant", "content": "Mustache Mustache is a template syntax that allows you to embed variables inside double curly braces {{}}. NotePlease see the Mustache documentation for more detailed information on how to use Mustache templates. Here's an example of a mustache template: Hello, {{name}}! Mustache is more robust than f-strings since it supports more complex logic. You can use conditionals, loops, and access nested keys in mustache templates. Conditional Hello, {{#name}}{{name}}{{/name}}{{^name}}world{{/name}}! The template will output \"Hello, Bob!\" if \"Bob\" is provided as the name variable, and \"Hello, world!\" if the name variable is not provided. Loop {{#names}}{{name}}{{/names}} input: {  \"names\": [{ \"name\": \"Alice\" }, { \"name\": \"Bob\" }, { \"name\": \"Charlie\" }]} output: \"AliceBobCharlie\" Loop with nesting {{#people}}{{name}} is {{age}} years old. {{/people}} input: {  \"people\": [    { \"name\": \"Alice\", \"age\": 30 },    { \"name\": \"Bob\", \"age\": 25 },    { \"name\": \"Charlie\", \"age\": 35 }  ]} output: \"Alice is 30 years old. Bob is 25 years old. Charlie is 35 years old.\" Dot notation {{person.name}} is {{person.age}} years old. input: {  \"person\": {    \"name\": \"Alice\",    \"age\": 30  }} output: \"Alice is 30 years old.\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can an individual trace be deleted in LangSmith if the UI does not support it?"}, {"role": "assistant", "content": "Deleting Traces The LangSmith UI does not currently support the deletion of an invidual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs_history views) and the runs and feedback tables themselves. This command can either be run using a trace ID as an argument or using a file that is a list of trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools and credentials are required to run the delete_trace_by_id script with a Clickhouse database?"}, {"role": "assistant", "content": "Prerequisites Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/Clickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the Clickhouse database from the machine you will be running the delete_trace_by_id script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should be used to delete a trace by its ID in Clickhouse?"}, {"role": "assistant", "content": "Running the deletion script for a single trace Run the following command to run the trace deletion script using a single trace ID: sh delete_trace_by_id.sh <clickhouse_url> --trace_id <trace_id> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_trace_by_id.sh \"clickhouse://default:password@localhost:8123/default\" --trace_id 4ec70ec7-0808-416a-b836-7100aeec934b If you visit the Langsmith UI, you should now see specified trace ID is no longer present nor reflected in stats."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should be used to run the trace deletion script with a list of trace IDs from a file?"}, {"role": "assistant", "content": "Running the deletion script for a multiple traces from a file with one trace ID per line Run the following command to run the trace deletion script using a list of trace IDs: sh delete_trace_by_id.sh <clickhouse_url> --file <path/to/foo.txt> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_trace_by_id.sh \"clickhouse://default:password@localhost:8123/default\" --file path/to/traces.txt If you visit the Langsmith UI, you should now see all the specified traces have been removed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate the improvement of few-shot evaluators for LLM-based applications?"}, {"role": "assistant", "content": "Create few-shot evaluators Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary\noverhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows\nyou to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default maximum number of few-shot examples that can be used in the prompt when creating an evaluator?"}, {"role": "assistant", "content": "Create your evaluator tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators\nare only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,\nwe will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}\ntemplate variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name\nas your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,\nfew_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend\nto be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do when you disagree with the scores given by your evaluator?"}, {"role": "assistant", "content": "Make corrections Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will\nbegin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.\nThe outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access your corrections dataset?"}, {"role": "assistant", "content": "View your corrections dataset In order to view your corrections dataset, go to your rule and click \"Edit Rule\" (or \"Edit Evaluator\" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says \"View few-shot dataset\". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can user feedback be collected and attached to traces in an application using the LangSmith SDK or API?"}, {"role": "assistant", "content": "Capture user feedback from your application to traces Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and feedbackReference guide on feedback data format In many applications, but even more so for LLM applications, it is important to collect user feedback to understand how your application is performing in real-world scenarios.\nThe ability to observe user feedback along with trace data can be very powerful to drill down into the most interesting datapoints, then send those datapoints for further review, automatic evaluation, or even datasets.\nTo learn more about how to filter traces based on various attributes, including user feedback, see this guide LangSmith makes it easy to attach user feedback to traces.\nIt's often helpful to expose a simple mechanism (such as a thumbs-up, thumbs-down button) to collect user feedback for your application responses. You can then use the LangSmith SDK or API to send feedback for a trace. To get the run_id of a logged run, see this guide. noteYou can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.\nThis is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline. PythonTypeScriptfrom langsmith import Clientclient = Client()# ... Run your application and get the run_id...# This information can be the result of a user-facing feedback formclient.create_feedback(    run_id,    key=\"feedback-key\",    score=1.0,    comment=\"comment\",)import { Client } from \"langsmith\";const client = new Client();// ... Run your application and get the run_id...// This information can be the result of a user-facing feedback formawait client.createFeedback(    runId,    \"feedback-key\",    {        score: 1.0,        comment: \"comment\",    });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the options for configuring LangSmith to use an external ClickHouse database?"}, {"role": "assistant", "content": "Connect to an external ClickHouse database ClickHouse is a high-performance, column-oriented database system. It allows for fast ingestion of data and is optimized for analytical queries. LangSmith uses ClickHouse as the primary data store for traces and feedback. By default, self-hosted LangSmith will use an internal ClickHouse database that is bundled with the LangSmith instance. This is run as a stateful set in the same Kubernetes cluster as the LangSmith application or as a Docker container on the same host as the LangSmith application. However, you can configure LangSmith to use an external ClickHouse database for easier management and scaling.\nBy configuring an external ClickHouse database, you can manage backups, scaling, and other operational tasks for your database.\nWhile Clickhouse is not yet a native service in Azure, AWS, or Google Cloud, you can run LangSmith with an external ClickHouse database in the following ways: LangSmith-managed ClickHouse (beta)Provision a ClickHouse Cloud either directly or through a cloud provider marketplace:Azure MarketplaceGoogle Cloud MarketplaceAWS MarketplaceOn a VM in your cloud provider noteUsing the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC.\nHowever, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).Additionally, sensitive information can be configured to be not stored in Clickhouse. Please reach out to support@langchain.dev for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for using LangSmith with ClickHouse?"}, {"role": "assistant", "content": "Requirements A provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).A user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.We only support standalone ClickHouse (not clustered or replicated) or ClickHouse Cloud.We only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later. See the LangSmith release notes for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What parameters are required to configure an external ClickHouse database for LangSmith installation?"}, {"role": "assistant", "content": "Parameters You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include: Host: The hostname or IP address of the ClickHouse databaseHTTP Port: The port that the ClickHouse database listens on for HTTP connectionsNative Port: The port that the ClickHouse database listens on for native connectionsDatabase: The name of the ClickHouse database that LangSmith should useUsername: The username to use to connect to the ClickHouse databasePassword: The password to use to connect to the ClickHouse database"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure your LangSmith instance to use an external ClickHouse database?"}, {"role": "assistant", "content": "Configuration With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the config.yaml file for your LangSmith Helm Chart installation or the .env file for your Docker installation. HelmDocker  clickhouse:    external:      enabled: true      host: \"host\"      port: \"http port\"      nativePort: \"native port\"      user: \"default\"      password: \"password\"      database: \"default\"      tls: false    # In your .env fileCLICKHOUSE_HOST=langchain-clickhouse # Change to your Clickhouse host if using external Clickhouse. Otherwise, leave it as isCLICKHOUSE_USER=default # Change to your Clickhouse user if neededCLICKHOUSE_DB=default # Change to your Clickhouse database if neededCLICKHOUSE_PORT=8123 # Change to your Clickhouse port if neededCLICKHOUSE_TLS=false # Change to true if you are using TLS to connect to Clickhouse. Otherwise, leave it as isCLICKHOUSE_PASSWORD=password # Change to your Clickhouse password if neededCLICKHOUSE_NATIVE_PORT=9000 # Change to your Clickhouse native port if needed Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Python client in Langsmith?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith in Python and TypeScript?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made to evaluator scores in LangSmith?"}, {"role": "assistant", "content": "Audit evaluator scores LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you edit feedback details in the comparison view?"}, {"role": "assistant", "content": "In the comparison view In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the \"edit\" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under \"Make correction\".\nIf you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples\nin place of the few_shot_explanation prompt variable."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the feedback details and corrections view in the runs table?"}, {"role": "assistant", "content": "In the runs table In the runs table, find the \"Feedback\" column and click on the feedback tag to bring up the feedback details. Again, click the \"edit\" icon on the right to bring up the corrections view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made using the SDK's update_feedback function?"}, {"role": "assistant", "content": "In the SDK Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      \"score\": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can the monitoring charts for each tracing project be accessed in LangSmith?"}, {"role": "assistant", "content": "Use monitoring charts LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default time period set for viewing monitors?"}, {"role": "assistant", "content": "Change the time period You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view specific subsets of runs in the monitor tab?"}, {"role": "assistant", "content": "Slice data by metadata or tag By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\nThis can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you investigate the runs causing spikes in errors or latency observed in monitoring charts?"}, {"role": "assistant", "content": "Drill down into specific subsets Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the recommended methods for running evals, and what should you do if you cannot use the SDKs?"}, {"role": "assistant", "content": "Run evals with the REST API Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.\nHowever, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the code provided in the text?"}, {"role": "assistant", "content": "Create a dataset Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries - API Example\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in running a single experiment using the provided API?"}, {"role": "assistant", "content": "Run a single experiment First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { \"dataset\": dataset_id }resp = requests.get(    \"https://api.smith.langchain.com/api/v1/examples\",    params=params,    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"def run_completion_on_example(example, model_name, experiment_id):    \"\"\"Run completions on a list of examples.\"\"\"    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        \"\"\"Function to post a new run to the API.\"\"\"        data = {            \"id\": run_id.hex,            \"name\": name,            \"run_type\": run_type,            \"inputs\": inputs,            \"start_time\": datetime.utcnow().isoformat(),            \"reference_example_id\": example[\"id\"],            \"session_id\": experiment_id,        }        if parent_id:            data[\"parent_run_id\"] = parent_id.hex        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/runs\", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        \"\"\"Function to patch a run with outputs.\"\"\"        resp = requests.patch(            f\"https://api.smith.langchain.com/api/v1/runs/{run_id}\",            json={                \"outputs\": outputs,                \"end_time\": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    text = example[\"inputs\"][\"text\"]    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, \"LLM Pipeline\", \"chain\", {\"text\": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {\"label\": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (\"gpt-3.5-turbo\", \"gpt-4o-mini\")experiment_ids = []for model_name in model_names:    resp = requests.post(        \"https://api.smith.langchain.com/api/v1/sessions\",        json={            \"start_time\": datetime.utcnow().isoformat(),            \"reference_dataset_id\": str(dataset_id),            \"description\": \"An optional description for the experiment\",            \"name\": f\"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}\",  # A name for the experiment            \"extra\": {                \"metadata\": {\"foo\": \"bar\"},  # Optional metadata            },        },        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )    experiment = resp.json()    experiment_ids.append(experiment[\"id\"])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[\"id\"])    # Issue a patch request to \"end\" the experiment by updating the end_time    requests.patch(        f\"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}\",        json={\"end_time\": datetime.utcnow().isoformat()},        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for running a pairwise experiment and ranking the outputs?"}, {"role": "assistant", "content": "Run a pairwise experiment Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.\nFor more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/comparative\",    json={        \"experiment_ids\": experiment_ids,        \"name\": \"Toxicity detection - API Example - Comparative - \" + str(uuid4())[0:8],        \"description\": \"An optional description for the comparative experiment\",        \"extra\": {            \"metadata\": {\"foo\": \"bar\"},  # Optional metadata        },        \"reference_dataset_id\": str(dataset_id),    },    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[\"id\"]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f\"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative\",    params={\"id\": comparative_experiment_id},    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()[0]experiment_ids = [info[\"id\"] for info in comparative_experiment[\"experiments_info\"]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f\"https://api.smith.langchain.com/api/v1/runs/query\",    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]},    json={        \"session\": experiment_ids,        \"is_root\": True, # Only fetch root runs (spans) which contain the end outputs        \"select\": [\"id\", \"reference_example_id\", \"outputs\"],    }).json()runs = runs[\"runs\"]for run in runs:    example_id = run[\"reference_example_id\"]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f\"Example ID: {example_id}\")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f\"Run ID: {run['id']}\")        feedback = {            \"score\": 1 if i == 0 else 0,            \"run_id\": str(run[\"id\"]),            \"key\": \"ranked_preference\",            \"feedback_group_id\": str(feedback_group_id),            \"comparative_experiment_id\": comparative_experiment_id,        }        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/feedback\",            json=feedback,            headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}        )        resp.raise_for_status()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What SDKs can be used to manage prompts programmatically according to the provided information?"}, {"role": "assistant", "content": "Manage prompts programmatically You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.\nAll functionality going forward will live in the langsmith package."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What commands are used to install the LangChain and Langsmith packages in Python and TypeScript?"}, {"role": "assistant", "content": "Install packages PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you do not have the LANGCHAIN_API_KEY set for your current workspace?"}, {"role": "assistant", "content": "Configure environment variables If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=\"lsv2_...\" TerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create or update a prompt using the push prompt method in PythonLangChain or TypeScript?"}, {"role": "assistant", "content": "Push a prompt To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");url = prompts.push(\"joke-generator\", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.\nThis is useful for storing the model configuration you want to use with this prompt.\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"langchain-openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);prompts.push(\"joke-generator-with-model\", chain);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method do you use to pull a prompt from the LangChain Hub, and what information is required for pulling a public prompt?"}, {"role": "assistant", "content": "Pull a prompt To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { ChatOpenAI } from \"langchain-openai\";const prompt = prompts.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);chain.invoke({\"topic\": \"cats\"}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\nJust specify include_model when pulling the prompt.\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\nMake sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = prompts.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });chain.invoke({\"topic\": \"cats\"}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use prompts stored in LangSmith with a model provider's API?"}, {"role": "assistant", "content": "Use a prompt without LangChain If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.\nThese convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to list, delete, and like/unlike prompts in the LangSmith SDK?"}, {"role": "assistant", "content": "List, delete, and like prompts You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\nSee the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\"); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using LangChain's off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.\nThese evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello Harrison\"),]dataset_name = \"Hello Set\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three QA evaluators mentioned, and which one is recommended based on meta-evals?"}, {"role": "assistant", "content": "Use question and answer (correctness) evaluators Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we recommend using \"cot_qa\", or Chain of Thought QA. Here is a trivial example that uses a \"cot_qa\" evaluator to evaluate a simple pipeline that prefixes the input with \"Hello\": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using \"criteria\" evaluators when ground truth reference labels are not available?"}, {"role": "assistant", "content": "Use criteria evaluators If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The \"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",    config={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent \"direction\" (i.e., higher is not necessarily better).\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for \"helpfulness\" means that the prediction was deemed to be helpful by the model.\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content, which, of course, is \"bad\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functions of the \"labeled_criteria\" and \"labeled_score_string\" evaluators in evaluating predictions against reference labels?"}, {"role": "assistant", "content": "Use labeled criteria evaluators If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators. The \"labeled_criteria\" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={        \"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of distance metrics used to measure similarity between a predicted string and a reference?"}, {"role": "assistant", "content": "Use string or embedding distance metrics To measure the similarity between a predicted string and a reference, you can use string distance metrics: The \"string_distance\" evaluator computes a normalized string edit distance between the prediction and referenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    \"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\": True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # \"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",      })evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries are required to customize the model used for LLM-based evaluators?"}, {"role": "assistant", "content": "Use a custom LLM in off-the-shelf evaluators You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a model that outputs multiple fields using LangChain's evaluators?"}, {"role": "assistant", "content": "Handle multiple input or output fields LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\". We want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    },    prepare_data=lambda run, example: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],        \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" + input[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of LangSmith functional tests in relation to AI systems?"}, {"role": "assistant", "content": "Test LLM applications (Python only) LangSmith functional tests are assertions and expectations designed to quickly identify obvious bugs and regressions in your AI system. Relative to evaluations, tests typically are designed to be fast and cheap to run, focusing on specific functionality and edge cases.\nWe recommend using LangSmith to track any unit tests, end-to-end integration tests, or other specific assertions that touch an LLM or other non-deterministic part of your AI system.\nThese should run on every commit in your CI pipeline to catch regressions early. note@test currently requires langsmith python version >=0.1.74 (named @unit for versions >=0.1.42). If you are interested in unit testing functionality in TypeScript or other languages, please let us know at support@langchain.dev.  "}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you write a functional test using LangSmith?"}, {"role": "assistant", "content": "Write a @test To write a LangSmith functional test, decorate your test function with @test.\nIf you want to track the full nested trace of the system or component being tested, you can mark those functions with @traceable. For example: # my_app/main.pyfrom langsmith import traceable@traceable # Optionaldef generate_sql(user_query):    # Replace with your SQL generation logic    # e.g., my_llm(my_prompt.format(user_query))    return \"SELECT * FROM customers\" Then define your test: # tests/test_my_app.pyfrom langsmith import testfrom my_app.main import generate_sql@testdef test_sql_generation_select_all():    user_query = \"Get all users from the customers table\"    sql = generate_sql(user_query)    # LangSmith logs any exception raised by `assert` / `pytest.fail` / `raise` / etc.    # as a test failure    assert sql == \"SELECT * FROM customers\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does LangSmith do each time you run a test suite using pytest?"}, {"role": "assistant", "content": "Run tests You can use a standard testing framework such as pytest (docs) to run. For example: pytest tests/ Each time you run this test suite, LangSmith collects the pass/fail rate and other traces as a new TestSuiteResult, logging the pass rate (1 for pass, 0 for fail) over all the applicable tests. The test suite syncs to a corresponding dataset named after your package or github repository."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features does LangSmith provide to enhance testing with pytest?"}, {"role": "assistant", "content": "Going further @test is designed to stay out of your way and works well with familiar pytest features. For example: Defining inputs as fixtures Pytest fixtures let you define functions that serve as reusable inputs for your tests. LangSmith automatically syncs any test case inputs defined as fixtures. For example: import pytest@pytest.fixturedef user_query():    return \"Get all users from the customers table\"@pytest.fixturedef expected_sql():    return \"SELECT * FROM customers\"# output_keys indicate which test arguments to save as 'outputs' in the dataset (Optional)# Otherwise, all arguments are saved as 'inputs'@test(output_keys=[\"expected_sql\"])def test_sql_generation_with_fixture(user_query, expected_sql):    sql = generate_sql(user_query)    assert sql == expected_sql Parametrizing tests Parametrizing tests lets you run the same assertions across multiple sets of inputs. Use pytest's parametrize decorator to achieve this. For example: @test@pytest.mark.parametrize(    \"user_query, expected_sql\",    [        (\"Get all users from the customers table\", \"SELECT * FROM customers\"),        (\"Get all users from the orders table\", \"SELECT * FROM orders\"),    ],)def test_sql_generation_parametrized(user_query, expected_sql):    sql = generate_sql(user_query)    assert sql == expected_sql Note: as the parametrized list grows, you may consider using evaluate() instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset. Expectations LangSmith provides an expect utility to help define expectations about your LLM output. For example: from langsmith import expect@testdef test_sql_generation_select_all():    user_query = \"Get all users from the customers table\"    sql = generate_sql(user_query)    expect(sql).to_contain(\"customers\") This will log the binary \"expectation\" score to the experiment results, additionally asserting that the expectation is met possibly triggering a test failure. expect also provides \"fuzzy match\" methods. For example: @test@pytest.mark.parametrize(    \"query, expectation\",    [       (\"what's the capital of France?\", \"Paris\"),    ],)def test_embedding_similarity(query, expectation):    prediction = my_chatbot(query)    expect.embedding_distance(        # This step logs the distance as feedback for this run        prediction=prediction, expectation=expectation    # Adding a matcher (in this case, 'to_be_*\"), logs 'expectation' feedback    ).to_be_less_than(0.5) # Optional predicate to assert against    expect.edit_distance(        # This computes the normalized Damerau-Levenshtein distance between the two strings        prediction=prediction, expectation=expectation    # If no predicate is provided below, 'assert' isn't called, but the score is still logged    ) This test case will be assigned 4 scores: The embedding_distance between the prediction and the expectationThe binary expectation score (1 if cosine distance is less than 0.5, 0 if not)The edit_distance between the prediction and the expectationThe overall test pass/fail score (binary) The expect utility is modeled off of Jest's expect API, with some off-the-shelf functionality to make it easier to grade your LLMs. Dry-run mode If you want to run the tests without syncing the results to LangSmith, you can set LANGCHAIN_TEST_TRACKING=false in your environment. LANGCHAIN_TEST_TRACKING=false pytest tests/ The tests will run as normal, but the experiment logs will not be sent to LangSmith. Caching LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache results to disk. Any identical inputs will be loaded from the cache so you don't have to call out to your LLM provider unless there are changes to the model, prompt, or retrieved data. To enable caching, run with LANGCHAIN_TEST_CACHE=/my/cache/path. For example: LANGCHAIN_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests All requests will be cached to tests/cassettes and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well. Using watch mode With caching enabled, you can iterate quickly on your tests using watch mode without worrying about unnecessarily hitting your LLM provider. For example, using pytest-watch: pip install pytest-watchLANGCHAIN_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does the @test decorator do in the context of LangSmith examples?"}, {"role": "assistant", "content": "Explanations The @test test decorator converts any test into a parametrized LangSmith example. By default, all tests within a given file will be grouped as a single \"test suite\" with a corresponding dataset. You can configure which test suite a test belongs to by passing the test_suite_name parameter to @test. The following metrics are available off-the-shelf: FeedbackDescriptionExamplepassBinary pass/fail score, 1 for pass, 0 for failassert False # FailsexpectationBinary expectation score, 1 if expectation is met, 0 if notexpect(prediction).against(lambda x: re.search(r\"\\b[a-f\\d]{8}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{12}\\b\", x) )embedding_distanceCosine distance between two embeddingsexpect.embedding_distance(prediction=prediction, expectation=expectation)edit_distanceEdit distance between two stringsexpect.edit_distance(prediction=prediction, expectation=expectation) You can also log any arbitrary feeback within a unit test manually using the client. from langsmith import unit, Clientfrom langsmith.run_helpers import get_current_run_treeclient = Client()@testdef test_foo():    run_tree = get_current_run_tree()    client.create_feedback(run_id=run_tree.id, key=\"my_custom_feedback\", score=1)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionalities does the expect library provide for making assertions on test results and logging scores to LangSmith?"}, {"role": "assistant", "content": "expect expect makes it easy to make approximate assertions on test results and log scores to LangSmith.\nOff-the-shelf, it allows you to compute and compare embedding distances, edit distances, and make custom assertions on values. expect.embedding_distance(prediction, reference, *, config=None) Compute the embedding distance between the prediction and reference. This logs the embedding distance to LangSmith and returns a Matcher instance for making assertions on the distance value. By default, this uses the OpenAI API for computing embeddings. Parameters prediction (str): The predicted string to compare.reference (str): The reference string to compare against.config (Optional[EmbeddingConfig]): Optional configuration for the embedding distance evaluator. Supported options:encoder: A custom encoder function to encode the list of input strings to embeddings. Defaults to the OpenAI API.metric: The distance metric to use for comparison. Supported values: \"cosine\", \"euclidean\", \"manhattan\", \"chebyshev\", \"hamming\". Returns A Matcher instance for the embedding distance value. expect.edit_distance(prediction, reference, *, config=None) Compute the string distance between the prediction and reference. This logs the string distance (Damerau-Levenshtein) to LangSmith and returns a Matcher instance for making assertions on the distance value. This depends on the rapidfuzz package for string distance computation. Parameters prediction (str): The predicted string to compare.reference (str): The reference string to compare against.config (Optional[EditDistanceConfig]): Optional configuration for the string distance evaluator. Supported options:metric: The distance metric to use for comparison. Supported values: \"damerau_levenshtein\", \"levenshtein\", \"jaro\", \"jaro_winkler\", \"hamming\", \"indel\".normalize_score: Whether to normalize the score between 0 and 1. Returns A Matcher instance for the string distance value. expect.value(value) Create a Matcher instance for making assertions on the given value. Parameters value (Any): The value to make assertions on. Returns A Matcher instance for the given value. Matcher A class for making assertions on expectation values. to_be_less_than(value) Assert that the expectation value is less than the given value. to_be_greater_than(value)  Assert that the expectation value is greater than the given value. to_be_between(min_value, max_value) Assert that the expectation value is between the given min and max values. to_be_approximately(value, precision=2) Assert that the expectation value is approximately equal to the given value. to_equal(value) Assert that the expectation value equals the given value. to_contain(value) Assert that the expectation value contains the given value. against(func) Assert the expectation value against a custom function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the @test decorator in LangSmith?"}, {"role": "assistant", "content": "testAPI The @test decorator is used to mark a function as a test case for LangSmith. It ensures that the necessary example data is created and associated with the test function. The decorated function will be executed as a test case, and the results will be recorded and reported by LangSmith. @test(id=None, output_keys=None, client=None, test_suite_name=None) Create a test case in LangSmith. Parameters id (Optional[uuid.UUID]): A unique identifier for the test case. If not provided, an ID will be generated based on the test function's module and name.output_keys (Optional[Sequence[str]]): A list of keys to be considered as the output keys for the test case. These keys will be extracted from the test function's inputs and stored as the expected outputs.client (Optional[ls_client.Client]): An instance of the LangSmith client to be used for communication with the LangSmith service. If not provided, a default client will be used.test_suite_name (Optional[str]): The name of the test suite to which the test case belongs. If not provided, the test suite name will be determined based on the environment or the package name. Environment Variables LANGSMITH_TEST_TRACKING: Set this variable to the path of a directory to enable caching of test results. This is useful for re-running tests without re-executing the code. Requires the 'langsmith[vcr]' package."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in generating synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on multiple splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionality has been added to help users construct queries in natural language?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended approach for logging LLM traces from OpenAI models using the langsmith SDK?"}, {"role": "assistant", "content": "Log custom LLM traces noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.\nIn order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format must the inputs for chat-style models be in, and what are the acceptable output formats?"}, {"role": "assistant", "content": "Chat-style models For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.ls_model_name: The name of the model, eg \"gpt-3.5-turbo\", \"claude-3-opus-20240307\", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ]}# Can also use one of:# output = {#     \"message\": {#         \"role\": \"assistant\",#         \"content\": \"Sure, what time would you like to book the table for?\"#     }# }## output = {#     \"role\": \"assistant\",#     \"content\": \"Sure, what time would you like to book the table for?\"# }## output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" }];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\"      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: \"assistant\",//     content: \"Sure, what time would you like to book the table for?\"//   }// };//// const output = {//   role: \"assistant\",//   content: \"Sure, what time would you like to book the table for?\"// };//// const output = [\"assistant\", \"Sure, what time would you like to book the table for?\"];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the `_reduce_chunks` function in the context of streaming outputs?"}, {"role": "assistant", "content": "Stream outputs For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}@traceable(    run_type=\"llm\",    reduce_fn=_reduce_chunks,    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def my_streaming_chat_model(messages: list):    for chunk in [\"Hello, \" + messages[1][\"content\"]]:        yield {            \"choices\": [                {                    \"message\": {                        \"content\": chunk,                        \"role\": \"assistant\",                    }                }            ]        }list(    my_streaming_chat_model(        [            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},            {\"role\": \"user\", \"content\": \"polly the parrot\"},        ],    ))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can token counts be manually provided to LangSmith in a function's response?"}, {"role": "assistant", "content": "Manually provide token counts Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.\nMany models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.\nIf token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.\nIf using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ],    \"usage_metadata\": {        \"input_tokens\": 27,        \"output_tokens\": 13,        \"total_tokens\": 40,    },}@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for inputs and outputs in instruct-style models?"}, {"role": "assistant", "content": "Instruct-style models For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.\nThe same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def hello_llm(prompt: str):    return {        \"choices\": [            {\"text\": \"Hello, \" + prompt}        ],        \"usage_metadata\": {            \"input_tokens\": 4,            \"output_tokens\": 5,            \"total_tokens\": 9,        },    }hello_llm(\"polly the parrot\\n\")import { traceable } from \"langsmith/traceable\";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: \"Hello, \" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: \"llm\", name: \"hello_llm\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await helloLLM({ prompt: \"polly the parrot\\n\" }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended approach for logging LLM traces from OpenAI models using the langsmith SDK?"}, {"role": "assistant", "content": "Log custom LLM traces noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.\nIn order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format must the inputs for chat-style models be in, and what are the acceptable output formats?"}, {"role": "assistant", "content": "Chat-style models For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.ls_model_name: The name of the model, eg \"gpt-3.5-turbo\", \"claude-3-opus-20240307\", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ]}# Can also use one of:# output = {#     \"message\": {#         \"role\": \"assistant\",#         \"content\": \"Sure, what time would you like to book the table for?\"#     }# }## output = {#     \"role\": \"assistant\",#     \"content\": \"Sure, what time would you like to book the table for?\"# }## output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" }];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\"      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: \"assistant\",//     content: \"Sure, what time would you like to book the table for?\"//   }// };//// const output = {//   role: \"assistant\",//   content: \"Sure, what time would you like to book the table for?\"// };//// const output = [\"assistant\", \"Sure, what time would you like to book the table for?\"];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the `_reduce_chunks` function in the context of streaming outputs?"}, {"role": "assistant", "content": "Stream outputs For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}@traceable(    run_type=\"llm\",    reduce_fn=_reduce_chunks,    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def my_streaming_chat_model(messages: list):    for chunk in [\"Hello, \" + messages[1][\"content\"]]:        yield {            \"choices\": [                {                    \"message\": {                        \"content\": chunk,                        \"role\": \"assistant\",                    }                }            ]        }list(    my_streaming_chat_model(        [            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},            {\"role\": \"user\", \"content\": \"polly the parrot\"},        ],    ))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can token counts be manually provided to LangSmith in a function's response?"}, {"role": "assistant", "content": "Manually provide token counts Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.\nMany models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.\nIf token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.\nIf using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ],    \"usage_metadata\": {        \"input_tokens\": 27,        \"output_tokens\": 13,        \"total_tokens\": 40,    },}@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for inputs and outputs in instruct-style models?"}, {"role": "assistant", "content": "Instruct-style models For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.\nThe same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def hello_llm(prompt: str):    return {        \"choices\": [            {\"text\": \"Hello, \" + prompt}        ],        \"usage_metadata\": {            \"input_tokens\": 4,            \"output_tokens\": 5,            \"total_tokens\": 9,        },    }hello_llm(\"polly the parrot\\n\")import { traceable } from \"langsmith/traceable\";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: \"Hello, \" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: \"llm\", name: \"hello_llm\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await helloLLM({ prompt: \"polly the parrot\\n\" }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can images be logged and rendered in multimodal LLM runs using LangSmith?"}, {"role": "assistant", "content": "Log multimodal traces LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs. In order to log images, use wrap_openai/ wrapOpenAI in Python or TypeScript respectively and pass an image URL or base64 encoded image as part of the input. PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(OpenAI())response = client.chat.completions.create(  model=\"gpt-4-turbo\",  messages=[    {      \"role\": \"user\",      \"content\": [        {\"type\": \"text\", \"text\": \"Whats in this image?\"},        {          \"type\": \"image_url\",          \"image_url\": {            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",          },        },      ],    }  ],)print(response.choices[0])import OpenAI from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";// Wrap the OpenAI client to automatically log tracesconst wrappedClient = wrapOpenAI(new OpenAI());const response = await wrappedClient.chat.completions.create({    model: \"gpt-4-turbo\",    messages: [      {        role: \"user\",        content: [          { type: \"text\", text: \"Whats in this image?\" },          {            type: \"image_url\",            image_url: {              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",            },          },        ],      },    ],});console.log(response.choices[0]); The image will be rendered as part of the trace in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from the runs of your traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Langsmith client in Python?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of configuring LangSmith to use an external Redis instance?"}, {"role": "assistant", "content": "Connect to an external Redis database LangSmith uses Redis to back our queuing/caching operations. By default, LangSmith Self-Hosted will use an internal Redis instance.\nHowever, you can configure LangSmith to use an external Redis instance (strongly recommended in a production setting). By configuring an external Redis instance, you can more easily manage backups, scaling, and other operational tasks for your Redis instance."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for a Redis instance to be used with LangSmith?"}, {"role": "assistant", "content": "Requirements A provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like:Amazon ElastiCacheGoogle Cloud MemorystoreAzure Cache for RedisNote: We do only officially support Redis versions >= 5.We do not support Redis Cluster. Redis Cluster Not SupportedCertain tiers of managed Redis services may use Redis Cluster under the hood, but you can point to a single node in the cluster.\nFor example on Azure Cache for Redis, the Premium tier and above use Redis Cluster, so you will need to use a lower tier."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required to assemble a connection string for a Redis instance using redis-py?"}, {"role": "assistant", "content": "Connection String We use redis-py to connect to Redis. This library supports a variety of connection strings. You can find more information on the connection string format here. You will need to assemble the connection string for your Redis instance. This connection string should include the following information: HostDatabasePortURL params This will take the form of: \"redis://host:port/db?<url_params>\" An example connection string might look like: \"redis://langsmith-redis:6379/0\" To use SSL, you can use the rediss:// prefix. An example connection string with SSL might look like: \"rediss://langsmith-redis:6380/0?password=foo\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure your LangSmith instance to use an external Redis instance?"}, {"role": "assistant", "content": "Configuration With your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the values file for your LangSmith Helm Chart installation or the .env file for your Docker installation. HelmDocker  redis:    external:      enabled: true      connectionUrl: \"Your connection url\"    # In your .env fileREDIS_DATABASE_URI=\"Your connection url\"     Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Redis instance."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to adjust the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features are available in the LangSmith playground for using custom models?"}, {"role": "assistant", "content": "How-to guides: Playground This section contains how-to guides related to the LangSmith playground.  Use a custom modelThe LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via LangServe, an open source library for serving LangChain applications. Use custom TLS certificatesThis feature is only available for self-hosted enterprise customers, starting from version 0.6. Save settings configurationWithin the settings of the LangSmith playground, you can save your model configuration for later use."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you deploy your own custom models using the LangSmith playground?"}, {"role": "assistant", "content": "Use a custom model The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via LangServe, an open source library for serving LangChain applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Who can use custom TLS certificates and from which version is this feature available?"}, {"role": "assistant", "content": "Use custom TLS certificates This feature is only available for self-hosted enterprise customers, starting from version 0.6."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you save your model configuration in the LangSmith playground?"}, {"role": "assistant", "content": "Save settings configuration Within the settings of the LangSmith playground, you can save your model configuration for later use."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you compare traces in the trace view?"}, {"role": "assistant", "content": "Compare traces To compare traces, click on the Compare button in the upper right hand side of any trace view. This will show the trace run table. Select the trace you want to compare against original trace. The pane will open with both traces selected in a side by side comparison view. To stop comparing, close the pane or click on Stop comparing in the upper right hand side of the pane."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of configuring LangSmith to use an external Postgres database?"}, {"role": "assistant", "content": "Connect to an external Postgres database LangSmith uses a Postgres database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal Postgres database.\nHowever, you can configure LangSmith to use an external Postgres database (strongly recommended in a production setting). By configuring an external Postgres database, you can more easily manage backups, scaling, and other operational tasks for your database."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for setting up a Postgres database for a LangSmith instance?"}, {"role": "assistant", "content": "Requirements A provisioned Postgres database that your LangSmith instance will have network access to. We recommend using a managed Postgres service like:Amazon RDSGoogle Cloud SQLAzure Database for PostgreSQNote: We only officially support Postgres versions >= 14.A user with admin access to the Postgres database. This user will be used to create the necessary tables, indexes, and schemas.This user will also need to have the ability to create extensions in the database. We use/will try to install the btree_gin, btree_gist, pgcrypto, citext, and pg_trgm extensions.If using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required to create a connection string for a Postgres database?"}, {"role": "assistant", "content": "Connection String You will need to provide a connection string to your Postgres database. This connection string should include the following information: HostPortDatabaseUsernamePassword(Make sure to url encode this if there are any special characters)URL params This will take the form of: username:password@host:port/database?<url_params> An example connection string might look like: myuser:mypassword@myhost:5432/mydatabase?sslmode=disable Without url parameters, the connection string would look like: myuser:mypassword@myhost:5432/mydatabase"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure your LangSmith instance to use an external Postgres database?"}, {"role": "assistant", "content": "Configuration With your connection string in hand, you can configure your LangSmith instance to use an external Postgres database. You can do this by modifying the values file for your LangSmith Helm Chart installation or the .env file for your Docker installation. HelmDocker  postgres:    external:      enabled: true      connectionUrl: \"Your connection url\"    # In your .env filePOSTGRES_DATABASE_URI=\"Your connection url\"     Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Postgres database."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangGraph for tracing agentic workflows?"}, {"role": "assistant", "content": "Trace withLangGraph(Python and JS/TS) LangSmith smoothly integrates with LangGraph (Python and JS)\nto help you trace agentic workflows, whether you're using LangChain modules or other SDKs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What do you need to set in order to enable tracing when using LangChain modules within LangGraph?"}, {"role": "assistant", "content": "With LangChain If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing. This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangGraph library and the OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "0. Installation Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langgraphyarn add @langchain/openai @langchain/langgraphnpm install @langchain/openai @langchain/langgraphpnpm add @langchain/openai @langchain/langgraph"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with LangSmith?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the weather in San Francisco according to the LangChain runnable?"}, {"role": "assistant", "content": "2. Log a trace Once you've set up your environment, you can call LangChain runnables as normal.\nLangSmith will infer the proper tracing config: PythonTypeScriptfrom typing import Literalfrom langchain_core.messages import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain_core.tools import toolfrom langgraph.graph import StateGraph, MessagesStatefrom langgraph.prebuilt import ToolNode@tooldef search(query: str):    \"\"\"Call to surf the web.\"\"\"    if \"sf\" in query.lower() or \"san francisco\" in query.lower():        return \"It's 60 degrees and foggy.\"    return \"It's 90 degrees and sunny.\"tools = [search]tool_node = ToolNode(tools)model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:    messages = state['messages']    last_message = messages[-1]    if last_message.tool_calls:        return \"tools\"    return \"__end__\"def call_model(state: MessagesState):    messages = state['messages']    # Invoking `model` will automatically infer the correct tracing context    response = model.invoke(messages)    return {\"messages\": [response]}workflow = StateGraph(MessagesState)workflow.add_node(\"agent\", call_model)workflow.add_node(\"tools\", tool_node)workflow.add_edge(\"__start__\", \"agent\")workflow.add_conditional_edges(    \"agent\",    should_continue,)workflow.add_edge(\"tools\", 'agent')app = workflow.compile()final_state = app.invoke(    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},    config={\"configurable\": {\"thread_id\": 42}})final_state[\"messages\"][-1].contentimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";import { tool } from \"@langchain/core/tools\";import { z } from \"zod\";import { ChatOpenAI } from \"@langchain/openai\";import { StateGraph, StateGraphArgs } from \"@langchain/langgraph\";import { ToolNode } from \"@langchain/langgraph/prebuilt\";interface AgentState {  messages: HumanMessage[];}const graphState: StateGraphArgs<AgentState>[\"channels\"] = {  messages: {    reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),  },};const searchTool = tool(async ({ query }: { query: string }) => {  if (query.toLowerCase().includes(\"sf\") || query.toLowerCase().includes(\"san francisco\")) {    return \"It's 60 degrees and foggy.\"  }  return \"It's 90 degrees and sunny.\"}, {  name: \"search\",  description:    \"Call to surf the web.\",  schema: z.object({    query: z.string().describe(\"The query to use in your search.\"),  }),});const tools = [searchTool];const toolNode = new ToolNode<AgentState>(tools);const model = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).bindTools(tools);function shouldContinue(state: AgentState) {  const messages = state.messages;  const lastMessage = messages[messages.length - 1] as AIMessage;  if (lastMessage.tool_calls?.length) {    return \"tools\";  }  return \"__end__\";}async function callModel(state: AgentState) {  const messages = state.messages;  // Invoking `model` will automatically infer the correct tracing context  const response = await model.invoke(messages);  return { messages: [response] };}const workflow = new StateGraph<AgentState>({ channels: graphState })  .addNode(\"agent\", callModel)  .addNode(\"tools\", toolNode)  .addEdge(\"__start__\", \"agent\")  .addConditionalEdges(\"agent\", shouldContinue)  .addEdge(\"tools\", \"agent\");const app = workflow.compile();const finalState = await app.invoke(  { messages: [new HumanMessage(\"what is the weather in sf\")] },  { configurable: { thread_id: \"42\" } });finalState.messages[finalState.messages.length - 1].content; An example trace from running the above code looks like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to ensure that other SDKs or custom functions are properly integrated with LangGraph for tracing?"}, {"role": "assistant", "content": "Without LangChain If you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately\n(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).\nIf you do so, LangSmith will automatically nest traces from those wrapped methods. Here's an example. You can also see this page for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries need to be installed to use the LangGraph library and the OpenAI SDK for Python and JS?"}, {"role": "assistant", "content": "0. Installation Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below). pipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with LangSmith?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after setting up your environment to log a trace for custom functions or SDKs?"}, {"role": "assistant", "content": "2. Log a trace Once you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\nLangSmith will then infer the proper tracing config: PythonTypeScriptimport jsonimport openaiimport operatorfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaifrom typing import Annotated, Literal, TypedDictfrom langgraph.graph import StateGraphclass State(TypedDict):    messages: Annotated[list, operator.add]tool_schema = {    \"type\": \"function\",    \"function\": {        \"name\": \"search\",        \"description\": \"Call to surf the web.\",        \"parameters\": {            \"type\": \"object\",            \"properties\": {\"query\": {\"type\": \"string\"}},            \"required\": [\"query\"],        },    },}# Decorating the tool function will automatically trace it with the correct context@traceable(run_type=\"tool\", name=\"Search Tool\")def search(query: str):    \"\"\"Call to surf the web.\"\"\"    if \"sf\" in query.lower() or \"san francisco\" in query.lower():        return \"It's 60 degrees and foggy.\"    return \"It's 90 degrees and sunny.\"tools = [search]def call_tools(state):    function_name_to_function = {\"search\": search}    messages = state[\"messages\"]    tool_call = messages[-1][\"tool_calls\"][0]    function_name = tool_call[\"function\"][\"name\"]    function_arguments = tool_call[\"function\"][\"arguments\"]    arguments = json.loads(function_arguments)    function_response = function_name_to_function[function_name](**arguments)    tool_message = {        \"tool_call_id\": tool_call[\"id\"],        \"role\": \"tool\",        \"name\": function_name,        \"content\": function_response,    }    return {\"messages\": [tool_message]}wrapped_client = wrap_openai(openai.Client())def should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:    messages = state[\"messages\"]    last_message = messages[-1]    if last_message[\"tool_calls\"]:        return \"tools\"    return \"__end__\"def call_model(state: State):    messages = state[\"messages\"]    # Calling the wrapped client will automatically infer the correct tracing context    response = wrapped_client.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", tools=[tool_schema]    )    raw_tool_calls = response.choices[0].message.tool_calls    tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []    response_message = {        \"role\": \"assistant\",        \"content\": response.choices[0].message.content,        \"tool_calls\": tool_calls,    }    return {\"messages\": [response_message]}workflow = StateGraph(State)workflow.add_node(\"agent\", call_model)workflow.add_node(\"tools\", call_tools)workflow.add_edge(\"__start__\", \"agent\")workflow.add_conditional_edges(    \"agent\",    should_continue,)workflow.add_edge(\"tools\", 'agent')app = workflow.compile()final_state = app.invoke(    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})final_state[\"messages\"][-1][\"content\"]Note: The below example requires langsmith>=0.1.39 and @langchain/langgraph>=0.0.31\nimport OpenAI from \"openai\";import { StateGraph } from \"@langchain/langgraph\";import { wrapOpenAI } from \"langsmith/wrappers/openai\";import { traceable } from \"langsmith/traceable\";type GraphState = {  messages: OpenAI.ChatCompletionMessageParam[];};const wrappedClient = wrapOpenAI(new OpenAI({}));const toolSchema: OpenAI.ChatCompletionTool = {  type: \"function\",  function: {    name: \"search\",    description: \"Use this tool to query the web.\",    parameters: {      type: \"object\",      properties: {        query: {          type: \"string\",        },      },      required: [\"query\"],    }  }};// Wrapping the tool function will automatically trace it with the correct contextconst search = traceable(async ({ query }: { query: string }) => {  if (    query.toLowerCase().includes(\"sf\") ||    query.toLowerCase().includes(\"san francisco\")  ) {    return \"It's 60 degrees and foggy.\";  }  return \"It's 90 degrees and sunny.\"}, { run_type: \"tool\", name: \"Search Tool\" });const callTools = async ({ messages }: GraphState) => {  const mostRecentMessage = messages[messages.length - 1];  const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;  if (toolCalls === undefined || toolCalls.length === 0) {    throw new Error(\"No tool calls passed to node.\");  }  const toolNameMap = {    search,  };  const functionName = toolCalls[0].function.name;  const functionArguments = JSON.parse(toolCalls[0].function.arguments);  const response = await toolNameMap[functionName](functionArguments);  const toolMessage = {    tool_call_id: toolCalls[0].id,    role: \"tool\",    name: functionName,    content: response,  }  return { messages: [toolMessage] };}const callModel = async ({ messages }: GraphState) => {  // Calling the wrapped client will automatically infer the correct tracing context  const response = await wrappedClient.chat.completions.create({    messages,    model: \"gpt-4o-mini\",    tools: [toolSchema],  });  const responseMessage = {    role: \"assistant\",    content: response.choices[0].message.content,    tool_calls: response.choices[0].message.tool_calls ?? [],  };  return { messages: [responseMessage] };}const shouldContinue = ({ messages }: GraphState) => {  const lastMessage =    messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;  if (    lastMessage?.tool_calls !== undefined &&    lastMessage?.tool_calls.length > 0  ) {    return \"tools\";  }  return \"__end__\";}const workflow = new StateGraph<GraphState>({  channels: {    messages: {      reducer: (a: any, b: any) => a.concat(b),    }  }});const graph = workflow  .addNode(\"model\", callModel)  .addNode(\"tools\", callTools)  .addEdge(\"__start__\", \"model\")  .addConditionalEdges(\"model\", shouldContinue, {    tools: \"tools\",    __end__: \"__end__\",  })  .addEdge(\"tools\", \"model\")  .compile();await graph.invoke({  messages: [{ role: \"user\", content: \"what is the weather in sf\" }]}); An example trace from running the above code looks like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What plans allow for the update of business information, tax ID, and invoice email for a LangSmith organization?"}, {"role": "assistant", "content": "Update invoice email, tax id and, business information To update business information for your LangSmith organization, head to the Usage and Billing Plans and Billing noteBusiness information, tax id and invoice email can only be updated for the plus and start up plans. Free and developer plans cannot update this information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps should you follow to update the email address for receiving invoices?"}, {"role": "assistant", "content": "Update invoice email To update the email address where your invoices are sent, follow these steps: Navigate to the Plans and Billing tab.Locate the section beneath the payment method, where the current invoice email is displayed.Enter the new email address you want invoices to be sent to in the provided field.The new email address will be automatically saved. This ensures that all future invoices will be sent to the updated email address."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps should a business follow to update their business information and Tax ID for sales tax exemption?"}, {"role": "assistant", "content": "Update business information and tax id noteIn certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your Tax ID may qualify you for a sales tax exemption. To update your organization's business information, follow these steps: Navigate to the Plans and Billing tab.Below the invoice email section, you will find a checkbox labeled Business.Check the Business checkbox if your organization belongs to a business.A business information section will appear, allowing you to enter or update the following details:Business NameAddressTax ID for applicable jurisdictionsTax ID field will appear for applicable jurisdictions after you select a country.After entering the necessary information, click the Save button to save your changes. This ensures that your business information is up-to-date and accurate for billing and tax purposes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to set up an account and organization for LangSmith?"}, {"role": "assistant", "content": "How-to guides: Setup This section contains how-to guides related to setting up LangSmith.  Create an account and API keyCreate an account Set up an organizationBefore diving into this content, it might be helpful to read the following: Set up a workspaceWorkspaces will be incrementally rolled out being week of June 10, 2024. Set up billing for your LangSmith accountIf you are interested in the Enterprise plan, please contact sales. This guide is Update invoice email, tax id and, business informationTo update business information for your LangSmith organization, head to the Set up access controlRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the first step to obtain an API key?"}, {"role": "assistant", "content": "Create an account and API key Create an account"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do before diving into the content about setting up an organization?"}, {"role": "assistant", "content": "Set up an organization Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will workspaces be incrementally rolled out?"}, {"role": "assistant", "content": "Set up a workspace Workspaces will be incrementally rolled out being week of June 10, 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I set up billing for the Enterprise plan of my LangSmith account?"}, {"role": "assistant", "content": "Set up billing for your LangSmith account If you are interested in the Enterprise plan, please contact sales. This guide is"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I update the business information for my LangSmith organization?"}, {"role": "assistant", "content": "Update invoice email, tax id and, business information To update business information for your LangSmith organization, head to the"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Who should I contact if I am interested in the RBAC feature?"}, {"role": "assistant", "content": "Set up access control RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What components make up the LangSmith application, and how can it be run in a controlled cloud environment?"}, {"role": "assistant", "content": "Architectural overview Enterprise License RequiredSelf-Hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith can be run via Kubernetes (recommended) or Docker in a Cloud environment that you control. The LangSmith application consists of several components including 5 LangSmith servers and 3 stateful services: LangSmith FrontendLangSmith BackendLangSmith Platform BackendLangSmith PlaygroundLangSmith QueueClickHousePostgresRedis To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended storage service configuration for LangSmith in a production setting?"}, {"role": "assistant", "content": "Storage Services noteLangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services.\nIn a production setting, we strongly recommend using external Storage Services."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database management system is ClickHouse and what is its primary use for LangSmith?"}, {"role": "assistant", "content": "ClickHouse ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database system is PostgreSQL and how does LangSmith utilize it?"}, {"role": "assistant", "content": "PostgreSQL PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is Redis and how does LangSmith utilize it?"}, {"role": "assistant", "content": "Redis Redis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching. LangSmith uses Redis to back queuing/caching operations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What role does Nginx play in the LangSmith frontend?"}, {"role": "assistant", "content": "LangSmith Frontend The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions of the LangSmith backend?"}, {"role": "assistant", "content": "LangSmith Backend The backend is the primary entrypoint for API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and sdk, preparing traces for ingestion, and supporting the hub API."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functions does the LangSmith Queue perform to manage incoming traces and feedback?"}, {"role": "assistant", "content": "LangSmith Queue The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the primary function of the LangSmith platform backend?"}, {"role": "assistant", "content": "LangSmith Platform Backend The platform backend is an internal service that primarily handles authentication and other high-volume tasks. The user should not need to interact with this service directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LangSmith Playground service?"}, {"role": "assistant", "content": "LangSmith Playground The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two evaluation criteria mentioned for assessing the intermediate steps in a retrieval-augmented generation (RAG) pipeline?"}, {"role": "assistant", "content": "Evaluate on intermediate steps While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline. For example, for retrieval-augmented generation (RAG), you might want to Evaluate the retrieval step to ensure that the correct documents are retrieved w.r.t the input query.Evaluate the generation step to ensure that the correct answer is generated w.r.t the retrieved documents. In this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios. In order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the root_run/rootRun argument, which is a Run object that contains the intermediate steps of your pipeline."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps involved in the RAG pipeline for generating answers from Wikipedia?"}, {"role": "assistant", "content": "1. Define your LLM pipeline The below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents. PythonTypeScriptimport openaiimport wikipedia as wpfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai = wrap_openai(openai.Client())@traceabledef generate_wiki_search(question):    messages = [        {\"role\": \"system\", \"content\": \"Generate a search query to pass into wikipedia to answer the user's question. Return only the search query and nothing more. This will passed in directly to the wikipedia search engine.\"},        {\"role\": \"user\", \"content\": question}    ]    result = openai.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0)    return result.choices[0].message.content@traceable(run_type=\"retriever\")def retrieve(query):    results = []    for term in wp.search(query, results = 10):        try:            page = wp.page(term, auto_suggest=False)            results.append({                \"page_content\": page.summary,                \"type\": \"Document\",                \"metadata\": {\"url\": page.url}            })        except wp.DisambiguationError:            pass        if len(results) >= 2:            return results@traceabledef generate_answer(question, context):    messages = [        {\"role\": \"system\", \"content\": f\"Answer the user's question based ONLY on the content below:\\n\\n{context}\"},        {\"role\": \"user\", \"content\": question}    ]    result = openai.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0)    return result.choices[0].message.content@traceabledef rag_pipeline(question):    query = generate_wiki_search(question)    context = \"\\n\\n\".join([doc[\"page_content\"] for doc in retrieve(query)])    answer = generate_answer(question, context)    return answerimport OpenAI from \"openai\";import wiki from \"wikipedia\";import { Client } from \"langsmith\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openai = wrapOpenAI(new OpenAI());const generateWikiSearch = traceable(  async (input: { question: string }) => {    const messages = [      {        role: \"system\" as const,        content:          \"Generate a search query to pass into Wikipedia to answer the user's question. Return only the search query and nothing more. This will be passed in directly to the Wikipedia search engine.\",      },      { role: \"user\" as const, content: input.question },    ];        const chatCompletion = await openai.chat.completions.create({      model: \"gpt-3.5-turbo\",      messages: messages,      temperature: 0,    });        return chatCompletion.choices[0].message.content ?? \"\";  },  { name: \"generateWikiSearch\" });const retrieve = traceable(  async (input: { query: string; numDocuments: number }) => {    const { results } = await wiki.search(input.query, { limit: 10 });    const finalResults: Array<{      page_content: string;      type: \"Document\";      metadata: { url: string };    }> = [];      for (const result of results) {      if (finalResults.length >= input.numDocuments) {        // Just return the top 2 pages for now        break;      }      const page = await wiki.page(result.title, { autoSuggest: false });      const summary = await page.summary();      finalResults.push({        page_content: summary.extract,        type: \"Document\",        metadata: { url: page.fullurl },      });    }    return finalResults;  },  { name: \"retrieve\", run_type: \"retriever\" });const generateAnswer = traceable(  async (input: { question: string; context: string }) => {    const messages = [      {        role: \"system\" as const,        content: `Answer the user's question based only on the content below:\\n\\n${input.context}`,      },      { role: \"user\" as const, content: input.question },    ];      const chatCompletion = await openai.chat.completions.create({      model: \"gpt-3.5-turbo\",      messages: messages,      temperature: 0,    });    return chatCompletion.choices[0].message.content ?? \"\";  },  { name: \"generateAnswer\" });const ragPipeline = traceable(  async ({ question }: { question: string }, numDocuments: number = 2) => {    const query = await generateWikiSearch({ question });    const retrieverResults = await retrieve({ query, numDocuments });    const context = retrieverResults      .map((result) => result.page_content)      .join(\"\\n\\n\");    const answer = await generateAnswer({ question, context });    return answer;  },  { name: \"ragPipeline\" }); This pipeline will produce a trace that looks something like:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What examples are included in the dataset created to evaluate the pipeline?"}, {"role": "assistant", "content": "2. Create a dataset and examples to evaluate the pipeline We are building a very simple dataset with a couple of examples to evaluate the pipeline. PythonTypeScriptfrom langsmith import Clientclient = Client()examples = [    (\"What is LangChain?\", \"LangChain is an open-source framework for building applications using large language models.\"),    (\"What is LangSmith?\", \"LangSmith is an observability and evaluation tool for LLM products, built by LangChain Inc.\")]dataset_name = \"Wikipedia RAG\"if not client.has_dataset(dataset_name=dataset_name):    dataset = client.create_dataset(dataset_name=dataset_name)    inputs, outputs = zip(        *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples]    )    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const client = new Client();const examples = [  [    \"What is LangChain?\",    \"LangChain is an open-source framework for building applications using large language models.\",  ],  [    \"What is LangSmith?\",    \"LangSmith is an observability and evaluation tool for LLM products, built by LangChain Inc.\",  ],];const datasetName = \"Wikipedia RAG\";const inputs = examples.map(([input, _]) => ({ input }));const outputs = examples.map(([_, expected]) => ({ expected }));const dataset = await client.createDataset(datasetName);await client.createExamples({ datasetId: dataset.id, inputs, outputs });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two evaluators defined for assessing the relevance of retrieved documents and the hallucination of generated answers?"}, {"role": "assistant", "content": "3. Define your custom evaluators As mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w.r.t the input query and another that evaluates the hallucination of the generated answer w.r.t the retrieved documents.\nWe will be using LangChain LLM wrappers, along with with_structured_output to define the evaluator for hallucination. The key here is that the evaluator function should traverse the root_run / rootRun argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria. PythonTypeScriptfrom langsmith.evaluation import LangChainStringEvaluator, evaluatefrom langsmith.schemas import Example, Runfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.pydantic_v1 import BaseModel, Fielddef document_relevance(root_run: Run, example: Example) -> dict:    \"\"\"    A very simple evaluator that checks to see if the input of the retrieval step exists    in the retrieved docs.    \"\"\"    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"rag_pipeline\")    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve\")    page_contents = \"\\n\\n\".join(doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"])    score = retrieve_run.inputs[\"query\"] in page_contents    return {\"key\": \"simple_document_relevance\", \"score\": score}def hallucination(root_run: Run, example: Example) -> dict:    \"\"\"    A simple evaluator that checks to see the answer is grounded in the documents    \"\"\"    # Get documents and answer    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"rag_pipeline\")    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieve\")    page_contents = \"\\n\\n\".join(doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"])    generation = rag_pipeline_run.outputs[\"output\"]    # Data model    class GradeHallucinations(BaseModel):        \"\"\"Binary score for hallucination present in generation answer.\"\"\"        binary_score: int = Field(description=\"Answer is grounded in the facts, 1 or 0\")    # LLM with function call    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)    structured_llm_grader = llm.with_structured_output(GradeHallucinations)    # Prompt    system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n        Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.\"\"\"    hallucination_prompt = ChatPromptTemplate.from_messages(        [            (\"system\", system),            (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),        ]    )    hallucination_grader = hallucination_prompt | structured_llm_grader    score = hallucination_grader.invoke({\"documents\": page_contents, \"generation\": generation})    return {\"key\": \"answer_hallucination\", \"score\": int(score.binary_score)}import { EvaluationResult } from \"langsmith/evaluation\";import { Run, Example } from \"langsmith/schemas\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"@langchain/openai\";import { z } from \"zod\";function findNestedRun(run: Run, search: (run: Run) => boolean): Run | null {  const queue: Run[] = [run];  while (queue.length > 0) {    const currentRun = queue.shift()!;    if (search(currentRun)) return currentRun;    queue.push(...currentRun.child_runs);  }  return null;}// A very simple evaluator that checks to see if the input of the retrieval step exists// in the retrieved docs.function documentRelevance(rootRun: Run, example: Example): EvaluationResult {  const retrieveRun = findNestedRun(rootRun, (run) => run.name === \"retrieve\");  const docs: Array<{ page_content: string }> | undefined =    retrieveRun.outputs?.outputs;    const pageContents = docs?.map((doc) => doc.page_content).join(\"\\n\\n\");  const score = pageContents.includes(retrieveRun.inputs?.query);  return { key: \"simple_document_relevance\", score };}async function hallucination(  rootRun: Run,  example: Example): Promise<EvaluationResult> {  const rag = findNestedRun(rootRun, (run) => run.name === \"ragPipeline\");  const retrieve = findNestedRun(rootRun, (run) => run.name === \"retrieve\");    const docs: Array<{ page_content: string }> | undefined =    retrieve.outputs?.outputs;    const documents = docs?.map((doc) => doc.page_content).join(\"\\n\\n\");    const prompt = ChatPromptTemplate.fromMessages<{    documents: string;    generation: string;  }>([    [      \"system\",      [        `You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n`,        `Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.`,      ].join(\"\\n\"),    ],    [      \"human\",      \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\",    ],  ]);    const llm = new ChatOpenAI({    model: \"gpt-3.5-turbo-0125\",    temperature: 0,  }).withStructuredOutput(    z      .object({        binary_score: z          .number()          .describe(\"Answer is grounded in the facts, 1 or 0\"),      })      .describe(\"Binary score for hallucination present in generation answer.\")  );    const grader = prompt.pipe(llm);  const score = await grader.invoke({    documents,    generation: rag.outputs?.outputs,  });    return { key: \"answer_hallucination\", score: score.binary_score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will the experiment contain after running the evaluate function with the custom evaluators?"}, {"role": "assistant", "content": "4. Evaluate the pipeline Finally, we'll run evaluate with the custom evaluators defined above. PythonTypeScriptfrom langsmith.evaluation import evaluateexperiment_results = evaluate(    lambda inputs: rag_pipeline(inputs[\"input\"]),    data=dataset_name,    evaluators=[document_relevance, hallucination],    experiment_prefix=\"rag-wiki-oai\")import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => ragPipeline({ question: inputs.input }), {  data: datasetName,  evaluators: [hallucination, documentRelevance],  experimentPrefix: \"rag-wiki-oai\",}); The experiment will contain the results of the evaluation, including the scores and comments from the evaluators:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionality has been added to help users construct queries in natural language?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method for selecting the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will API keys prefixed with ls__ be disabled in favor of lsv2 style keys?"}, {"role": "assistant", "content": "LangSmith Release Notes noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith v0.6 during the week of June 17, 2024?"}, {"role": "assistant", "content": "Week of June 17, 2024 - LangSmith v0.6 LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced in LangSmith?"}, {"role": "assistant", "content": "New Features Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the performance and reliability of run rules and querying the Hub via the SDK?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the default port and Helm configuration in the recent infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes [Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes?"}, {"role": "assistant", "content": "Admin changes Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.5.x and earlier following the release of v0.6?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and features does LangSmith v0.5 introduce?"}, {"role": "assistant", "content": "Week of May 13, 2024 - LangSmith v0.5 LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after the LangSmith Helm release v0.7 in August 2024?"}, {"role": "assistant", "content": "Breaking changes We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced, including role-based access controls and improvements in regression testing and production monitoring?"}, {"role": "assistant", "content": "New Features Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to improve performance and reliability in resource management?"}, {"role": "assistant", "content": "Performance and Reliability Changes Split ingest, session deletion, and automation jobs to execute within separate resource pools."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default storage size for Clickhouse persistence in LangSmith v0.4, and how can it be adjusted?"}, {"role": "assistant", "content": "Infrastructure changes As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes regarding access controls and API keys?"}, {"role": "assistant", "content": "Admin changes Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.4.x and earlier following the release of v0.5?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.4 during the week of March 25, 2024?"}, {"role": "assistant", "content": "Week of March 25, 2024 - LangSmith v0.4 LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes should be made to ensure backwards compatibility when updating to the new release?"}, {"role": "assistant", "content": "Breaking changes This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to enhance performance and reliability in the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the image repositories and Clickhouse persistence storage in the infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to the API key management and OAuth flow in the recent admin updates?"}, {"role": "assistant", "content": "Admin changes Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.3.x and earlier versions following the release of 0.4?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.3 during the week of February 21, 2024?"}, {"role": "assistant", "content": "Week of Februrary 21, 2024 - LangSmith v0.3 LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do to retain run data when migrating from LangSmith v0.1?"}, {"role": "assistant", "content": "Breaking changes This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the LangSmith UI regarding trace ingestion and display delay?"}, {"role": "assistant", "content": "Performance and Reliability Changes Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is available regarding admin changes?"}, {"role": "assistant", "content": "Admin changes nan"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.2.x and earlier versions following the release of 0.3?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features does LangSmith 0.2 include?"}, {"role": "assistant", "content": "Week of January 29, 2024 - LangSmith v0.2 LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What versions of langsmith-sdk are required for this release to support changes in pagination of API results?"}, {"role": "assistant", "content": "Requirements This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the search syntax for metadata in runs?"}, {"role": "assistant", "content": "Breaking changes The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,}) and then search using has(metadata, '{\"user_name\": ...}')"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to the performance and reliability of the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What database service has been added to improve performance and scalability for storing run results in LangSmith?"}, {"role": "assistant", "content": "Infrastructure Changes Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the new maximum number of users allowed per organization for new organizations?"}, {"role": "assistant", "content": "Admin changes Increased the maximum number of users per organization from 5 to 100 for new organizations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of LangSmith 0.1.x following the release of version 0.2?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components and examples of automation rules that can be set up in LangSmith for processing production logs?"}, {"role": "assistant", "content": "Set up automation rules While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users.\nLangSmith provides a powerful feature called automations that allow you to trigger certain actions on your trace data.\nAt a high level, automations are defined by a filter, sampling rate, and action. Automation rules can trigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an annotation queue, and triggering a webhook. An example of an automation you can set up can be \"trigger an online evaluation that grades on vagueness for all of my downvoted traces.\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps for creating an automation rule in LangSmith?"}, {"role": "assistant", "content": "Create a rule We will outline the steps for creating an automation rule in LangSmith below."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of rules that can be created and what actions are allowed for each?"}, {"role": "assistant", "content": "Step 1: Navigate to rule creation To create a rule, head click on Rules in the top right corner of any project details page, then scroll to the bottom and click on + Add Rule. Alternatively, you can access rules in settings by navigating to this link, click on + Add Rule, then Project Rule. noteThere are currently two types of rules you can create: Project Rule and Dataset Rule.Project Rule: This rule will apply to traces in the specified project. Actions allowed are adding to a dataset, adding to an annotation queue, running online evaluation, and triggering a webhook.Dataset Rule: This rule will apply to traces that are part of an experiment in the specified dataset. Actions allowed are only running an evaluator on the experiment results. To see this in action, you can follow this guide. Give your rule a name, for example \"my_rule\":"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a filter to filter traces in the project?"}, {"role": "assistant", "content": "Step 2: Define the filter You can create a filter as you normally would to filter traces in the project. For more information on filters, you can refer to this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you apply a new rule to past runs?"}, {"role": "assistant", "content": "(Optional) Step 3: Apply Rule to Past Runs When creating a new rule, you can apply the rule to past runs as well. To do this, select Apply to Past Runs checkbox and enter Backfill From date as the\nstart date to apply the rule. This will start from the Backfill From date and apply the run rules until it is caught up with the latest runs. Note that you will have to expand the date range for logs if you wanted to look at the progress of the backfill, see View logs for your automations for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of setting a sampling rate in automations?"}, {"role": "assistant", "content": "Step 4: Define the sampling rate You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the four actions you can take with an automation rule?"}, {"role": "assistant", "content": "Step 5: Define the action There are four actions you can take with an automation rule: Add to dataset: Add the inputs and outputs of the trace to a dataset.Add to annotation queue: Add the trace to an annotation queue.Run online evaluation: Run an online evaluation on the trace. For more information on online evaluations, you can refer to this guide.Trigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.Extend data retention: Extends the data retention period on matching traces that use base retention.\nNote that all other rules will also extend data retention on matching traces through the\nauto-upgrade mechanism,\nbut this rule takes no additional action."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view logs for your automations and what information do they provide?"}, {"role": "assistant", "content": "View logs for your automations You can view logs for your automations by going to Settings -> Rules and click on the Logs button in any row. You can also get to logs by clicking on Rules in the top right hand corner of any project details page, then clicking on See Logs for any rule. Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced.\nIf a particular rule execution has triggered an error, you can view the error message by hovering over the error icon. By default, rule logs only show results for runs that occurred in the last day. To see results for older runs, you can select Last 1 day and enter the desired date range.\nWhen applying a rule to past runs, the processing will start from the start date and go forward, so this would be needed to view logs while the backfill is proceeding."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What strategies are suggested for optimizing tracing spend on LangSmith?"}, {"role": "assistant", "content": "Optimize tracing spend on LangSmith Recommended ReadingBefore diving into this content, it might be helpful to read the following:Data Retention Conceptual DocsUsage Limiting Conceptual Docs noteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\ncustom nature of billing. If you are on Enterprise plan and have questions about cost optimization,\nplease reach out to your sales rep or support@langchain.dev. This tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend\nand prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.\nConcepts can be transferred to your own organization."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How many workspaces does the organization have for its deployment stages?"}, {"role": "assistant", "content": "Problem Setup In this tutorial, we take an existing organization that has three workspaces, one for each deployment stage\n(Dev, Staging, and Prod):"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways LangSmith provides to understand current usage?"}, {"role": "assistant", "content": "Understand your current usage The first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\nand Invoices."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two usage metrics that LangSmith charges for, and how is LangSmith Traces usage measured?"}, {"role": "assistant", "content": "Usage Graph The usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\nspend (which we will see later on our draft invoice). We can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph. We see in the graph above that there are two usage metrics that LangSmith charges for: LangSmith Traces (Base Charge)LangSmith Traces (Extended Data Retention Upgrades). The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\nFor more details, see our data retention conceptual docs. Notice that these graphs look\nidentical, which will come into play later in the tutorial. LangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings. noteLangSmith's Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable. In the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\ngo to Settings -> Workspaces, and hover our mouse over the Workspace ID button to find the one with a matching ID. In\nthis case, it's the Prod workspace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information can be found in the Invoices tab regarding current month's spending and tracing charges?"}, {"role": "assistant", "content": "Invoices We understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so,\nwe head to the Invoices tab. The first invoice that will appear on screen is a draft of your current month's\ninvoice, which shows your running spend thus far this month. In the above GIF, we see that the charges for LangSmith Traces are broken up by \"tenant_id\" (i.e. Workspace ID), meaning we can track tracing spend\non each of our workspaces. In the first few days of June, the vast majority of the total spend of ~$2,000 is in our production\nworkspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades. These upgrades occur for two reasons: You use extended data retention tracing, meaning that, by default, your traces are retained for 400 daysYou use base data retention tracing, and use a feature that automatically extends the data retention of a trace (see our Auto-Upgrade conceptual docs) Given that the number of total traces per day is equal to the number of extended retention traces per day, it's most likely the\ncase that this org is using extended data retention tracing everywhere. As such, we start by optimizing our retention settings."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does managing data retention impact the cost of traces in LangSmith?"}, {"role": "assistant", "content": "Optimization 1: manage data retention LangSmith charges differently based on a trace's data retention (see our data retention conceptual docs),\nwhere short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, we will\nshow how to get optimal settings for data retention without sacrificing historical observability, and\nshow the effect it has on our bill."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to the organization level retention settings when modified for new projects in all workspaces?"}, {"role": "assistant", "content": "Change org level retention defaults for new projects We navigate to the Usage configuration tab, and look at our organization level retention settings. Modifying this setting affects all new projects that are\ncreated going forward in all workspaces in our org. noteFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd\nhave this defaulted to Base."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can we change the data retention settings for existing projects?"}, {"role": "assistant", "content": "Change project level retention defaults Our existing projects have not changed their data retention settings, so we can change these on the individual project pages. We navigate to Projects -> <your project name>, click the data retention drop down, and modify it to base retention. As\nwith the organization level setting, this will only affect retention (and pricing) for traces going forward."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What percentage of traces can be sampled for extended data retention using LangSmith's server side sampling feature?"}, {"role": "assistant", "content": "Keep around a percentage of traces for extended data retention We may not want all our traces to expire after 14 days if we care about historical debugging. As such, we can take advantage\nof LangSmith's built in ability to do server side sampling for extended data retention. Choosing the right percentage of runs to sample depends on your use case. We will arbitrarily pick 10% of runs here, but will\nleave it to the user to find the right value that balances collecting rare events and cost constraints. LangSmith automatically upgrades the data retention for any trace that matches a run rule in our automations product (see our run rules docs). On the\nprojects page, click Rules -> Add Rule, and configure the rule as follows: Run rules match on runs rather than traces. Runs are single units of work within an LLM application's API handling. Traces\nare end to end API calls (learn more about tracing concepts in LangSmith). This means a trace can\nbe thought of as a tree of runs making up an API call. When a run rule matches any run within a trace, the trace's full run tree\nupgrades to be retained for 400 days. Therefore, to make sure we have the proper sampling rate on traces, we take advantage of the\nfiltering functionality of run rules. We add add a filter condition to only match the \"root\" run in the run tree. This is distinct per trace, so our 10% sampling\nwill upgrade 10% of traces, rather 10% of runs, which could correspond to more than 10% of traces. If desired, we can optionally add\nany other filtering conditions required (e.g. specific tags/metadata attached to our traces) for more pointed data retention\nextension. For the sake of this tutorial, we will stick with the simplest condition, and leave more advanced filtering as an\nexercise to the user. noteIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run\nrule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset),\nand will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What was the cost reduction in spending over the last 7 days compared to the previous 4 days?"}, {"role": "assistant", "content": "See results after 7 days While the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. This translates to the invoice, where we've only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.\nThat's a cost reduction of nearly 75% per day!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two usage limits that LangSmith offers to help prevent future overspend?"}, {"role": "assistant", "content": "Optimization 2: limit usage In the previous section, we managed data retention settings to optimize existing spend. In this section, we will\nuse usage limits to prevent future overspend. LangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we've\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend. To set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\nwith a cost estimate: Lets start by setting limits on our production usage, since that is where the majority of spend comes from."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended total traces limit for a gen AI application expecting to double its load, based on current logging of 100,000-130,000 traces per day?"}, {"role": "assistant", "content": "Setting a good total traces limit Picking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\nclearly think about your assumptions before setting a limit. For example: Current Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,\nmeaning we log around 100,000-130,000 traces per dayExpected Growth in Load: We expect to double in size in the near future. From these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of: limit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month We click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows: noteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can setting a limit on high retention traces help reduce monthly costs for a business?"}, {"role": "assistant", "content": "Cutting maximum spend with an extended data retention limit If we are not a big enterprise, we may shudder at the ~$40k per month bill. We saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000. As we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive\ndata retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon. noteThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to\nuse this feature, please read more about its functionality here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What percentage of the production limit is set for usage in dev and staging environments for each workspace?"}, {"role": "assistant", "content": "Set dev/staging limits and view total spent limit across workspaces Following the same logic for our dev and staging environments, whe set limits at 10% of the production\nlimit on usage for each workspace. While this works with our usage pattern, setting good dev and staging limits may vary depending on\nyour use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\nwant to be more liberal with your usage limits to avoid test failures. Now that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces: With this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What strategies were discussed in the tutorial for managing costs?"}, {"role": "assistant", "content": "Summary In this tutorial, we learned how to: Cut down our existing costs with data retention policiesPrevent future overspend with usage limits If you have questions about further optimizing your spend, please reach out to support@langchain.dev."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you are using LangChain in relation to logging traces to LangSmith?"}, {"role": "assistant", "content": "Annotate code for tracing There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces using the @traceable decorator in Python and the traceable function in TypeScript?"}, {"role": "assistant", "content": "Use@traceable/traceable LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\nfrom langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful assistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What's a good name for a store that sells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(\"colorful socks\")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.\nimport { traceable } from \"langsmith/traceable\";import OpenAI from \"openai\";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: \"system\" as const,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: \"formatPrompt\" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: messages,            temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: \"parseOutput\" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(\"colorful socks\");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: \"runPipeline\" });await runPipeline();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces when using the wrap_openai or wrapOpenAI methods?"}, {"role": "assistant", "content": "Wrap the OpenAI client The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!\nThe wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"@traceable(name=\"Chat Pipeline\")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(\"Can you summarize this morning's meetings?\")import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return \"During this morning's meeting, we solved all world conflict.\";}, { name: \"Retrieve Context\", run_type: \"tool\" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: \"system\",            content:                \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",        },        { role: \"user\", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: \"Chat Pipeline\" });await chatPipeline(\"Can you summarize this morning's meetings?\");"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the RunTree API in LangSmith?"}, {"role": "assistant", "content": "Use theRunTreeAPI Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually\ncreate runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not\nnecessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \"Can you summarize this morning's meetings?\"# Create a top-level runpipeline = RunTree(    name=\"Chat Pipeline\",    run_type=\"chain\",    inputs={\"question\": question})# This can be retrieved in a retrieval stepcontext = \"During this morning's meeting, we solved all world conflict.\"messages = [    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}]# Create a child runchild_llm_run = pipeline.create_child(    name=\"OpenAI Call\",    run_type=\"llm\",    inputs={\"messages\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from \"openai\";import { RunTree } from \"langsmith\";// This can be a user input to your appconst question = \"Can you summarize this morning's meetings?\";const pipeline = new RunTree({    name: \"Chat Pipeline\",    run_type: \"chain\",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = \"During this morning's meeting, we solved all world conflict.\";const messages = [    { role: \"system\", content: \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { role: \"user\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: \"OpenAI Call\",    run_type: \"llm\",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using the trace context manager in Python?"}, {"role": "assistant", "content": "Use thetracecontext manager (Python only) In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {\"input\": \"Can you summarize this morning's meetings?\"}with trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:    output = chat_pipeline(\"Can you summarize this morning's meetings?\")    rt.end(outputs={\"output\": output})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of logging to LangSmith?"}, {"role": "assistant", "content": "Tracing This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Run in the context of an LLM application?"}, {"role": "assistant", "content": "Runs A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of LangSmith, and how are runs related to it?"}, {"role": "assistant", "content": "Traces A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a project in the context of traces?"}, {"role": "assistant", "content": "Projects A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different ways to collect feedback on runs?"}, {"role": "assistant", "content": "Feedback Feedback allows you to score an individual run based on certain criteria.\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are tags used for in the LangSmith UI?"}, {"role": "assistant", "content": "Tags Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of metadata in relation to runs?"}, {"role": "assistant", "content": "Metadata Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How long does LangSmith retain trace data after it is inserted into the trace database?"}, {"role": "assistant", "content": "Data storage and retention For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you delete a trace from LangSmith before its expiration date?"}, {"role": "assistant", "content": "Deleting traces from LangSmith If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the \"Delete\" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some methods for evaluating an LLM application as described in the how-to guides?"}, {"role": "assistant", "content": "How-to guides: Evaluation This section contains how-to guides related to evaluation.  Evaluate an LLM ApplicationBefore diving into this content, it might be helpful to read the following: Bind an evaluator to a dataset in the UIWhile you can specify evaluators to grade the results of your experiments programmatically (see this guide for more information), you can also bind evaluators to a dataset in the UI. Run an evaluation from the prompt playgroundWhile you can kick off experiments easily using the sdk, as outlined here, it's often useful to run experiments directly in the prompt playground. Evaluate on intermediate stepsWhile, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline. Use LangChain off-the-shelf evaluators (Python only)Before diving into this content, it might be helpful to read the following: Compare experiment resultsOftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. Evaluate an existing experimentCurrently, evaluate_existing is only supported in the Python SDK. Test LLM applications (Python only)LangSmith functional tests are assertions and expectations designed to quickly identify obvious bugs and regressions in your AI system. Relative to evaluations, tests typically are designed to be fast and cheap to run, focusing on specific functionality and edge cases. Run pairwise evaluationsBefore diving into this content, it might be helpful to read the following: Audit evaluator scoresLLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK. Create few-shot evaluatorsUsing LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary Fetch performance metrics for an experimentTracing projects and experiments use the same underlying data structure in our backend, which is called a \"session.\" Run evals with the REST APIBefore diving into this content, it might be helpful to read the following: Upload experiments run outside of LangSmith with the REST APISome users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should one consider before evaluating an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you bind evaluators to a dataset?"}, {"role": "assistant", "content": "Bind an evaluator to a dataset in the UI While you can specify evaluators to grade the results of your experiments programmatically (see this guide for more information), you can also bind evaluators to a dataset in the UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a useful way to run experiments aside from using the SDK?"}, {"role": "assistant", "content": "Run an evaluation from the prompt playground While you can kick off experiments easily using the sdk, as outlined here, it's often useful to run experiments directly in the prompt playground."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Why might it be important to evaluate the intermediate steps of a pipeline in certain scenarios?"}, {"role": "assistant", "content": "Evaluate on intermediate steps While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do before using LangChain off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of comparing results in different experiments when iterating on an LLM application?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming language is currently supported for evaluating an existing experiment?"}, {"role": "assistant", "content": "Evaluate an existing experiment Currently, evaluate_existing is only supported in the Python SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are LangSmith functional tests designed to identify in AI systems?"}, {"role": "assistant", "content": "Test LLM applications (Python only) LangSmith functional tests are assertions and expectations designed to quickly identify obvious bugs and regressions in your AI system. Relative to evaluations, tests typically are designed to be fast and cheap to run, focusing on specific functionality and edge cases."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should be read before diving into the content on pairwise evaluations?"}, {"role": "assistant", "content": "Run pairwise evaluations Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made to evaluator scores in LangSmith?"}, {"role": "assistant", "content": "Audit evaluator scores LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of using LLM-as-a-Judge evaluators when programmatic evaluation is not possible?"}, {"role": "assistant", "content": "Create few-shot evaluators Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What underlying data structure is used in the backend for tracing projects and experiments?"}, {"role": "assistant", "content": "Fetch performance metrics for an experiment Tracing projects and experiments use the same underlying data structure in our backend, which is called a \"session.\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do before diving into the content about running evals with the REST API?"}, {"role": "assistant", "content": "Run evals with the REST API Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can users upload experiments run outside of LangSmith to view results in the LangSmith UI?"}, {"role": "assistant", "content": "Upload experiments run outside of LangSmith with the REST API Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required for LangSmith to accurately calculate token-based costs for traces?"}, {"role": "assistant", "content": "Calculate token-based costs for traces Recommended ReadingBefore diving into this content, it might be helpful to read the following:Providing token counts for LLM runs (spans) LangSmith allows you to track costs for traces based on the number of tokens used for LLM invocations.\nThe costs are rolled up to the trace level and project level. For LangSmith to accurately calculate token-based costs, you need to provide the token counts for each LLM invocation in the trace, along with sending up ls_provider and ls_model_name in the run metadata. If you are using the LangSmith Python or TS/JS SDK, you should carefully read through the this guide.If you are using LangChain Python or TS/JS, ls_provider and ls_model_name along with token counts are automatically sent up to LangSmith. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts and calculating cost. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_nameinvocation_params.model_id (for costs only)invocation_params.model_path (for costs only)invocation_params.endpoint_name (for costs only) Once you are sending up the correct information to LangSmith, you must set up the model pricing map in LangSmith settings.\nIn order to do this, navigate to the model pricing map.\nHere, you can set the cost per token for each model and provider combination. This information is scoped to a workspace. Several default entries for OpenAI models are already present in the model pricing map, which you can clone and modify as needed. To create a new entry in the model pricing map, click on the Add new model button in the top right corner. Here, you can specify the following fields: Model Name: The name of the model, will also be used to name the entry in the model pricing map.Prompt Cost: The cost per input token for the model. This number is multiplied by the number of tokens in the prompt to calculate the prompt cost.Completion Cost: The cost per output token for the model. This number is multiplied by the number of tokens in the completion to calculate the completion cost.Model Activation Date: The date from which the pricing is applicable.Match Pattern: A regex pattern to match the model name and provider. This is used to match the value for ls_model_name in the run metadata.Provider: The provider of the model. This is used to match the value for ls_provider in the run metadata. Once you have set up the model pricing map, LangSmith will automatically calculate and aggregate the token-based costs for traces based on the token counts provided in the LLM invocations. To see the example above in action, you can execute the following code snippet: PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ],    \"usage_metadata\": {        \"input_tokens\": 27,        \"output_tokens\": 13,        \"total_tokens\": 40,    },}@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages }); In the above code snippet, we are sending up the ls_provider and ls_model_name in the run metadata, along with the token counts for the LLM invocation.\nThis information matches the model pricing map entry we set up earlier. The trace produced will contain the token-based costs based on the token counts provided in the LLM invocation and the model pricing map entry."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to adjust the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What can you access by clicking the expand icon in the hover state from any cell?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the benefit of using LangSmith-managed ClickHouse?"}, {"role": "assistant", "content": "LangSmith-managed ClickHouse (Beta) betaThis feature is currently in beta. Please reach out to our team at sales@langchain.dev if you are interested in leveraging this option. recommended readingPlease read the LangSmith architectural overview and guide on connecting to external Clickhouse before proceeding with this guide. As mentioned in previous guides, LangSmith uses Clickhouse as the primary storage engine for traces and feedback.\nFor easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external Clickhouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences in architecture when using LangSmith Managed Clickhouse compared to a fully self-hosted ClickHouse instance?"}, {"role": "assistant", "content": "Architecture Overview Using LangSmith Managed Clickhouse with your Self-Hosted LangSmith instance is fairly simple. The overall architecture is similar to using a fully self-hosted ClickHouse instance, with a few key differences: You will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.With this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of Clickhouse to ensure that sensitive information doesn't leave your VPC. More on sensitive informationThis reference doc explains the format we use to store runs (spans), which are the building blocks of traces.Our definition of sensitive information as it relates to application data are inputs, outputs, and errors of a run, since these fields can contain prompts and completions from LLMs.With LangSmith-managed ClickHouse, we store inputs, outputs, and errors in cloud object storage (S3 or GCS) within your cloud and store the rest of the run data in ClickHouse. This ensures that sensitive information doesn't leave your VPC. The LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance. The overall architecture looks like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What cloud platforms are supported for using the service mentioned in the requirements?"}, {"role": "assistant", "content": "Requirements You must be on AWS or GCP. We do not support Azure at this time as we require S3 or GCS for blob storage. Read the blob storage guide for more information.To use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported region. Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to whitelist your traffic.You must have a VPC that can connect to the LangSmith-managed Clickhouse service. You will need to work with our team to set up the necessary networking.You must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both Kubernetes and Docker installations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on specific splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to adjust the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of all experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What function should be used to run pairwise evaluations of existing experiments in LangSmith?"}, {"role": "assistant", "content": "Run pairwise evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function\nwith two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the required SDK versions for using the evaluate_comparative function and what arguments does it accept?"}, {"role": "assistant", "content": "Use theevaluate_comparativefunction notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the required inputs and outputs for configuring pairwise evaluators?"}, {"role": "assistant", "content": "Configure inputs and outputs for pairwise evaluators Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with\nruns[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent \"both equally bad\" or both to 1 for \"both equally good\". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for using LLM-based pairwise evaluators to compare the quality of responses from two AI assistants?"}, {"role": "assistant", "content": "Compare two experiments with LLM-based pairwise evaluators The following example uses a prompt\nwhich asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.\nThe prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=\"gpt-4\")        runnable = prompt | model    response = runnable.invoke({        \"question\": example.inputs[\"question\"],        \"answer_a\": runs[0].outputs[\"output\"] if runs[0].outputs is not None else \"N/A\",        \"answer_b\": runs[1].outputs[\"output\"] if runs[1].outputs is not None else \"N/A\",    })    score = response[\"Preference\"]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {\"key\": \"ranked_preference\", \"scores\": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [\"my-experiment-name-1\", \"my-experiment-name-2\"],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.\nimport type { Run, Example } from \"langsmith\";import { evaluateComparative } from \"langsmith/evaluation\";import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());import { z } from \"zod\";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(\"Expected at least two runs\");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? \"N/A\",    answer_b: runB?.outputs?.output ?? \"N/A\",  };    const output = await openai.chat.completions.create({    model: \"gpt-4-turbo\",    messages: [      {        role: \"system\",        content: [          \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.\",          \"You should choose the assistant that follows the user's instructions and answers the user's question better.\",          \"Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.\",          \"Begin your evaluation by comparing the two responses and provide a short explanation.\",          \"Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\",          \"Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\",        ].join(\" \"),      },      {        role: \"user\",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(\"\\n\\n\"),      },    ],    tool_choice: {      type: \"function\",      function: { name: \"Score\" },    },    tools: [      {        type: \"function\",        function: {          name: \"Score\",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output \"1\" if Assistant A answer is better based upon the factors above.`,            `Output \"2\" if Assistant B answer is better based upon the factors above.`,            `Output \"0\" if it is a tie.`,          ].join(\" \"),          parameters: {            type: \"object\",            properties: {              Preference: {                type: \"integer\",                description: \"Which assistant answer is preferred?\",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: \"ranked_preference\", scores };}await evaluateComparative([\"earnest-name-40\", \"reflecting-pump-91\"], {  evaluators: [evaluatePairwise],});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the Comparison View of pairwise experiments?"}, {"role": "assistant", "content": "View pairwise experiments Navigate to the \"Pairwise Experiments\" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What SDKs can be used to manage prompts programmatically according to the provided information?"}, {"role": "assistant", "content": "Manage prompts programmatically You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.\nAll functionality going forward will live in the langsmith package."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What commands are used to install the LangChain and Langsmith packages in Python and TypeScript?"}, {"role": "assistant", "content": "Install packages PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you do not have the LANGCHAIN_API_KEY set for your workspace?"}, {"role": "assistant", "content": "Configure environment variables If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=\"lsv2_...\" TerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create or update a prompt using the push prompt method in PythonLangChain or TypeScript?"}, {"role": "assistant", "content": "Push a prompt To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");url = prompts.push(\"joke-generator\", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.\nThis is useful for storing the model configuration you want to use with this prompt.\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"langchain-openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);prompts.push(\"joke-generator-with-model\", chain);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method do you use to pull a prompt from the LangChain Hub, and what information is required for pulling a public prompt?"}, {"role": "assistant", "content": "Pull a prompt To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { ChatOpenAI } from \"langchain-openai\";const prompt = prompts.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);chain.invoke({\"topic\": \"cats\"}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\nJust specify include_model when pulling the prompt.\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\nMake sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = prompts.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });chain.invoke({\"topic\": \"cats\"}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use prompts stored in LangSmith with a model provider's API?"}, {"role": "assistant", "content": "Use a prompt without LangChain If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.\nThese convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to list, delete, and like/unlike prompts in the LangSmith SDK?"}, {"role": "assistant", "content": "List, delete, and like prompts You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\nSee the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\"); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the workspace and what permissions do they have?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps to evaluate the performance of your LLM application?"}, {"role": "assistant", "content": "Evaluate your LLM application It can be hard to measure the performance of your application with respect to criteria important you or your users.\nHowever, doing so is crucial, especially as you iterate on your application.\nIn this guide we will go over how to test and evaluate your application.\nThis allows you to measure how well your application is performing over a fixed set of data.\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence. At a high level, in this tutorial we will go over how to: Create an initial golden dataset to measure performanceDefine metrics to use to measure performanceRun evaluations on a few different prompts or modelsCompare results manuallyTrack results over timeSet up automated testing to run in CI/CD For more information on the evaluation workflows LangSmith supports, check out the how-to guides. Lots to cover, let's dive in!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps to create a dataset for testing and evaluating an application?"}, {"role": "assistant", "content": "Create a dataset The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate.\nThere are a few aspects to consider here: What should the schema of each datapoint be?How many datapoints should I gather?How should I gather those datapoints? Schema: Each datapoint should consist of, at the very least, the inputs to the application.\nIf you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.\nOften times you cannot define the perfect output - that's okay! Evaluation is an iterative process.\nSometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent.\nLangSmith datasets are very flexible and allow you to define arbitrary schemas. How many: There's no hard and fast rule for how many you should gather.\nThe main thing is to make sure you have proper coverage of edge cases you may want to guard against.\nEven 10-50 examples can provide a lot of value!\nDon't worry about getting a large number to start - you can (and should) always add over time! How to get: This is maybe the trickiest part.\nOnce you know you want to gather a dataset... how do you actually go about it?\nFor most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand.\nAfter starting with these datapoints, these datasets are generally living constructs and grow over time.\nThey generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set.\nThere are also methods like synthetically generating data that can be used to augment your dataset.\nTo start, we recommend not worrying about those and just hand labeling ~10-20 examples. Once you've got your dataset, there are a few different ways to upload them to LangSmith.\nFor this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI). For this tutorial, we will create 5 datapoints to evaluate on.\nWe will be evaluating a question-answering application.\nThe input will be a question, and the output will be an answer.\nSince this is a question-answering application, we can define the expected answer.\nLet's show how to create and upload this dataset to LangSmith! from langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    inputs=[        {\"question\": \"What is LangChain?\"},        {\"question\": \"What is LangSmith?\"},        {\"question\": \"What is OpenAI?\"},        {\"question\": \"What is Google?\"},        {\"question\": \"What is Mistral?\"},    ],    outputs=[        {\"answer\": \"A framework for building LLM applications\"},        {\"answer\": \"A platform for observing and evaluating LLM applications\"},        {\"answer\": \"A company that creates Large Language Models\"},        {\"answer\": \"A technology company known for search\"},        {\"answer\": \"A company that creates Large Language Models\"},    ],    dataset_id=dataset.id,) Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page,\nwhen we click into it we should see that we have five new examples."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What metrics are defined for evaluating responses in the dataset?"}, {"role": "assistant", "content": "Define metrics After creating our dataset, we can now define some metrics to evaluate our responses on.\nSince we have an expected answer, we can compare to that as part of our evaluation.\nHowever, we do not expect our application to output those exact answers, but rather something that is similar.\nThis makes our evaluation a little trickier. In addition to evaluating correctness, let's also make sure our answers are short and concise.\nThis will be a little easier - we can define a simple Python function to measure the length of the response. Let's go ahead and define these two metrics. For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output).\nThis LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function.\nWe can define our own prompt and LLM to use for evaluation here: from langchain_anthropic import ChatAnthropicfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluator_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.You are grading the following question:{query}Here is the real answer:{answer}You are grading the following predicted answer:{result}Respond with CORRECT or INCORRECT:Grade:\"\"\"PROMPT = PromptTemplate(    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE)eval_llm = ChatAnthropic(temperature=0.0)qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT}) noteThis example assumes you have the ANTHROPIC_API_KEY environment variable set. You can just as easily run this example with OpenAI by replacing ChatAnthropic with ChatOpenAI from langchain_openai. For evaluating the length of the response, this is a lot easier!\nWe can just define a simple function that checks whether the actual output is less than 2x the length of the expected result. from langsmith.schemas import Run, Exampledef evaluate_length(run: Run, example: Example) -> dict:    prediction = run.outputs.get(\"output\") or \"\"    required = example.outputs.get(\"answer\") or \"\"    score = int(len(prediction) < 2 * len(required))    return {\"key\":\"length\", \"score\": score}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do we run evaluations using the OpenAI SDK?"}, {"role": "assistant", "content": "Run Evaluations Great! So now how do we run evaluations?\nNow that we have a dataset and evaluators, all that we need is our application!\nWe will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.\nWe will build this using the OpenAI SDK directly: import openaiopenai_client = openai.Client()def my_app(question):    return openai_client.chat.completions.create(        model=\"gpt-3.5-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.content Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call,\nand then also maps the output of the function to the output key we expect. def langsmith_app(inputs):    output = my_app(inputs[\"question\"])    return {\"output\": output} Great!\nNow we're ready to run evaluation.\nLet's do it! from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"openai-3.5\", # A prefix for your experiment names to easily identify them) This will output a URL. If we click on it, we should see results of our evaluation! If we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run! Let's now try it out with a different model! Let's try gpt-4-turbo import openaiopenai_client = openai.Client()def my_app_1(question):    return openai_client.chat.completions.create(        model=\"gpt-4-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence).\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentdef langsmith_app_1(inputs):    output = my_app_1(inputs[\"question\"])    return {\"output\": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_1, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"openai-4\", # A prefix for your experiment names to easily identify them) And now let's use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short. import openaiopenai_client = openai.Client()def my_app_2(question):    return openai_client.chat.completions.create(        model=\"gpt-4-turbo\",        temperature=0,        messages=[            {                \"role\": \"system\",                \"content\": \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"            },            {                \"role\": \"user\",                \"content\": question,            }        ],    ).choices[0].message.contentdef langsmith_app_2(inputs):    output = my_app_2(inputs[\"question\"])    return {\"output\": output}from langsmith.evaluation import evaluateexperiment_results = evaluate(    langsmith_app_2, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    experiment_prefix=\"strict-openai-4\", # A prefix for your experiment names to easily identify them) If we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can we compare the results of different runs in the Experiments tab?"}, {"role": "assistant", "content": "Comparing results Awesome, we've evaluated three different runs. But how can we compare results?\nThe first way we can do this is just by looking at the runs in the Experiments tab.\nIf we do that, we can see a high level view of the metrics for each run: Great! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length.\nBut what if we want to explore in more detail? In order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view: We immediately see all three tests side by side.\nSome of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline.\nWe automatically choose defaults for the baseline and metric, but you can change those yourself (outlined in blue below).\nYou can also choose which columns and which metrics you see by using the Display control (outlined in yellow below).\nYou can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top (outlined in red below). If we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can automated testing be set up to run in CI/CD using pytest?"}, {"role": "assistant", "content": "Set up automated testing to run in CI/CD Now that we've run this in a one-off manner, we can set it to run in an automated fashion.\nWe can do this pretty easily by just including it as a pytest file that we run in CI/CD.\nAs part of this, we can either just log the results OR set up some criteria to determine if it passes or not.\nFor example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check,\nwe could set that up with a test like: def test_length_score() -> None:    \"\"\"Test that the length score is at least 80%.\"\"\"    experiment_results = evaluate(        langsmith_app, # Your AI system        data=dataset_name, # The data to predict and grade over        evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results    )    # This will be cleaned up in the next release:    feedback = client.list_feedback(        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],        feedback_key=\"length\"    )    scores = [f.score for f in feedback]    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can we track the results of automated experiments over time?"}, {"role": "assistant", "content": "Track results over time Now that we've got these experiments running in an automated fashion, we want to track these results over time.\nWe can do this from the overall Experiments tab in the datasets page.\nBy default, we show evaluation metrics over time (highlighted in red).\nWe also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What key topics were covered in the tutorial regarding the evaluation process?"}, {"role": "assistant", "content": "Conclusion That's it for this tutorial! We've gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time.\nHopefully this can help you iterate with confidence. This is just the start. As mentioned earlier, evaluation is an ongoing process.\nFor example - the datapoints you will want to evaluate on will likely continue to change over time.\nThere are many types of evaluators you may wish to explore.\nFor information on this, check out the how-to guides. Additionally, there are other ways to evaluate data besides in this \"offline\" manner (e.g. you can evaluate production data).\nFor more information on online evaluation, check out this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of content do the how-to guides in LangSmith provide?"}, {"role": "assistant", "content": "How-to guides Step-by-step guides that cover key tasks and operations in LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to set up a LangSmith account and manage workspaces?"}, {"role": "assistant", "content": "Setup See the following guides to set up your LangSmith account. Create an account and API keySet up an organizationCreate an organizationManage and navigate workspacesManage usersSet up a workspaceCreate a workspaceManage usersConfigure workspace settingsSet up billingUpdate invoice email, tax id and, business informationSet up access control (enterprise only)Create a roleAssign a role to a user"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features does LangSmith offer for adding observability to LLM applications?"}, {"role": "assistant", "content": "Tracing Get started with LangSmith's tracing features to start adding observability to your LLM applications. Annotate code for tracingUse @traceable/traceableWrap the OpenAI API clientUse the RunTree APIUse the trace context manager (Python only)Toggle tracing on and offLog traces to specific projectSet the destination project staticallySet the destination project dynamicallySet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingDistributed tracing in PythonDistributed tracing in TypeScriptAccess the current span within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesChat-style modelsSpecify model nameStream outputsManually provide token countsInstruct-style modelsPrevent logging of sensitive data in tracesRule-based masking of inputs and outputsExport tracesUse filter argumentsUse filter query languageShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChainInstallationQuick startTrace selectivelyLog to specific projectAdd metadata and tags to tracesCustomize run nameAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTrace with LangGraphInteroperability between LangChain and LangGraphInteroperability between @traceable/traceable and LangGraphTrace with Instructor (Python only)Trace with the Vercel AI SDK (JS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for traces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the ways to create a new dataset in LangSmith?"}, {"role": "assistant", "content": "Datasets Manage datasets in LangSmith to evaluate and improve your LLM applications. Manage datasets in the applicationCreate a new dataset and add examples manuallyDataset schema validationAdd inputs and outputs from traces to datasetsUpload a CSV file to create a datasetGenerate synthetic examplesExport a datasetCreate and manage dataset splitsManage datasets programmaticallyCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from a pandas DataFrameFetch datasetsFetch examplesUpdate examplesBulk update examplesVersion datasetsCreate a new version of a datasetTag a versionShare or unshare a dataset publicly"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to evaluate the performance of LLM applications over time?"}, {"role": "assistant", "content": "Evaluation Evaluate your LLM applications to measure their performance over time. Evaluate an LLM applicationRun an evaluationUse custom evaluatorsEvaluate on a particular version of a datasetEvaluate on a subset of a datasetEvaluate on a dataset splitEvaluate on a dataset with repetitionsUse a summary evaluatorEvaluate a LangChain runnableReturn multiple scoresBind an evaluator to a dataset in the UIRun an evaluation from the prompt playgroundEvaluate on intermediate stepsUse LangChain off-the-shelf evaluators (Python only)Use question and answer (correctness) evaluatorsUse criteria evaluatorsUse labeled criteria evaluatorsUse string or embedding distance metricsUse a custom LLM in off-the-shelf evaluatorsHandle multiple input or output fieldsCompare experiment resultsOpen the comparison viewView regressions and improvementsFilter on regressions or improvementsUpdate baseline experimentSelect feedback keyOpen a traceExpand detailed viewUpdate display settingsEvaluate an existing experimentUnit test LLM applications (Python only)Run pairwise evaluationsUse the evaluate_comparative functionConfigure inputs and outputs for pairwise evaluatorsCompare two experiments with LLM-based pairwise evaluatorsView pairwise experimentsAudit evaluator scoresIn the comparison viewIn the runs tableIn the SDKCreate few-shot evaluatorsCreate your evaluatorMake correctionsView your corrections datasetFetch performance metrics for an experimentRun evals using the API onlyCreate a datasetRun a single experimentRun a pairwise experimentUpload experiments run outside of LangSmith with the REST APIRequest body schemaConsiderationsExample requestView the experiment in the UI"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps can be taken to collect and utilize human feedback for improving LLM applications?"}, {"role": "assistant", "content": "Human feedback Collect human feedback to improve your LLM applications. Capture user feedback from your application to tracesSet up a new feedback criteriaAnnotate traces inlineUse annotation queuesCreate an annotation queueAssign runs to an annotation queueReview runs in an annotation queue"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features does LangSmith offer for monitoring and automations of production data?"}, {"role": "assistant", "content": "Monitoring and automations Leverage LangSmith's powerful monitoring and automations features to make sense of your production data. Filter traces in the applicationCreate a filterFilter for intermediate runs (spans)Advanced: filter for intermediate runs (spans) on properties of the rootAdvanced: filter for runs (spans) whose child runs have some attributeFilter based on inputs and outputsFilter based on input / output key-value pairsCopy the filterManually specify a raw query in LangSmith query languageUse an AI Query to auto-generate a queryUse monitoring chartsChange the time periodSlice data by metadata or tagDrill down into specific subsetsSet up automation rulesCreate a ruleView logs for your automationsSet up online evaluationsConfigure online evaluationsSet API keysSet up webhook notifications for rulesWebhook payloadExample with ModalSet up threadsGroup traces into threadsView threads"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionalities does LangSmith offer for managing prompts in LLM development?"}, {"role": "assistant", "content": "Prompts Organize and manage prompts in LangSmith to streamline your LLM development workflow. Create a promptCompose your promptSave your promptView your promptsAdd metadataUpdate a promptUpdate metadataUpdate the prompt contentVersion a promptManage prompts programmaticallyConfigure environment variablesPush a promptPull a promptUse a prompt without LangChainList, delete, and like promptsLangChain Hub"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features are available in the LangSmith Playground for iterating on prompts and models?"}, {"role": "assistant", "content": "Playground Quickly iterate on prompts and models in the LangSmith Playground. Use custom TLS certificatesUse a custom modelSave settings configuration"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended way to enable or disable tracing programmatically without setting environment variables in Python?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT In some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. Recently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,\nwe changed the behavior of with trace to honor the LANGCHAIN_TRACING_V2 environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.\nThe recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below. PythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\nimport openaifrom langsmith import Client, tracing_context, traceablefrom langsmith.wrappers import wrap_openailangsmith_client = Client(    api_key=\"YOUR_LANGSMITH_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)client = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"@traceabledef chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}Context: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.content# Can set to False to disable tracing here without changing code structurewith tracing_context(enabled=True):    # Use langsmith_extra to pass in a custom client    chat_pipeline(\"Can you summarize this morning's meetings?\", langsmith_extra={\"client\": langsmith_client})In TypeScript, you can pass in both the client and the tracingEnabled flag to the traceable decorator.\nimport { Client } from \"langsmith\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { OpenAI } from \"openai\";const client = new Client({    apiKey: \"YOUR_API_KEY\",  // This can be retrieved from a secrets manager    apiUrl: \"https://api.smith.langchain.com\",  // Update appropriately for self-hosted installations or the EU region});const openai = wrapOpenAI(new OpenAI());const tool = traceable((question: string) => {    return \"During this morning's meeting, we solved all world conflict.\";}, { name: \"Retrieve Context\", runType: \"tool\" });const pipeline = traceable(    async (question: string) => {        const context = await tool(question);                const completion = await openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: [                { role: \"system\" as const, content: \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },                { role: \"user\" as const, content: `Question: ${question}\\nContext: ${context}`}            ]        });                return completion.choices[0].message.content;        },     { name: \"Chat\", client, tracingEnabled: true });await pipeline(\"Can you summarize this morning's meetings?\");"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the destination project of your traces?"}, {"role": "assistant", "content": "Log traces to specific project You can change the destination project of your traces both statically through environment variables and dynamically at runtime."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set a custom project name for an application run in LangSmith?"}, {"role": "assistant", "content": "Set the destination project statically As mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-custom-project If the project specified does not exist, it will be created automatically when the first trace is ingested."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name dynamically at runtime when logging traces in an application?"}, {"role": "assistant", "content": "Set the destination project dynamically You can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application. noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(    run_type=\"llm\",    name=\"OpenAI Call Decorator\",    project_name=\"My Project\")def call_openai(    messages: list[dict], model: str = \"gpt-3.5-turbo\") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(    messages,    langsmith_extra={\"project_name\": \"My Overriden Project\"},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=messages,    langsmith_extra={\"project_name\": \"My Project\"},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(    run_type=\"llm\",    name=\"OpenAI Call RunTree\",    inputs={\"messages\": messages},    project_name=\"My Project\")chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from \"langsmith\";const client = new OpenAI();const messages = [    {role: \"system\", content: \"You are a helpful assistant.\"},    {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: \"llm\",    name: \"OpenAI Call Traceable\",    project_name: \"My Project\"});// Call the traceable functionawait traceableCallOpenAI(messages, \"gpt-3.5-turbo\");// Create and use a RunTree objectconst rt = new RunTree({    runType: \"llm\",    name: \"OpenAI Call RunTree\",    inputs: { messages },    project_name: \"My Project\"});// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to generate LangSmith query statistics for troubleshooting?"}, {"role": "assistant", "content": "Generating Query Stats As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience. This command will generate a CSV that can be shared with the LangChain team."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools and credentials are required to connect to the Clickhouse database for running the get_query_stats script?"}, {"role": "assistant", "content": "Prerequisites Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/Clickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the Clickhouse database from the machine you will be running the get_query_stats script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should be run to generate the query statistics and save them to a CSV file?"}, {"role": "assistant", "content": "Running the query stats generation script Run the following command to run the stats generation script: sh get_query_stats.sh <clickhouse_url> --output path/to/file.csv For example, if you are using the bundled version with port-forwarding, the command would look like: sh get_query_stats.sh \"clickhouse://default:password@localhost:8123/default\" --output query_stats.csv and after running this command you should see a file, query_stats.csv, has been created with LangSmith query statistics."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to filter runs for analysis and export in the SDK and API?"}, {"role": "assistant", "content": "Trace query syntax Using the list_runs method in the SDK or /runs/query endpoint in the API, you can filter runs to analyze and export."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different filtering options available for fetching runs in a project?"}, {"role": "assistant", "content": "Filter arguments KeysDescriptionproject_id / project_nameThe project(s) to fetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a specific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name / dataset_idFetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.reference_example_idFetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.errorFetch runs that errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other filtering arguments.filterFetch runs that match a given structured filter statement. See the guide below for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to return in the response. By default, all fields are returned.query (experimental)Natural language query, which translates your query into a filter statement."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What filtering capabilities does LangSmith's filter query language support for fetching runs?"}, {"role": "assistant", "content": "Filter query language LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs. The filtering grammar is based on common comparators on fields in the run object. Supported comparators include: gte (greater than or equal to)gt (greater than)lte (less than or equal to)lt (less than)eq (equal to)neq (not equal to)has (check if run contains a tag or metadata json blob)search (search for a substring in a string field) Additionally, you can combine multiple comparisons through and and or operators. These can be applied on fields of the run object, such as its id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and any associated feedback through feedback_key and feedback_score."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What platform is LangSmith deployed on, and what is its design focus regarding scalability?"}, {"role": "assistant", "content": "Cloud architecture and scalability Cloud-managed solutionThis section is only relevant for the cloud-managed LangSmith services available at https://smith.langchain.com and https://eu.smith.langchain.com.For information on the self-hosted LangSmith solution, please refer to the self-hosted documentation. LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for both LLM application observability and evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "In which regions are the US-based and EU-based LangSmith services deployed in GCP?"}, {"role": "assistant", "content": "Architecture The US-based LangSmith service is deployed in the us-central1 (Iowa) region of GCP. NOTE: The EU-based LangSmith service is now available (as of mid-July 2024) and is deployed in the europe-west4 (Netherlands) region of GCP.\nIf you are interested in an enterprise plan in this region, please contact us at sales@langchain.dev."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where are the resources and services stored based on the sign-up location?"}, {"role": "assistant", "content": "Regional storage The resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU).\nCloud-managed LangSmith uses Supabase for authentication/authorization and ClickHouse Cloud for data warehouse. USEUURLhttps://smith.langchain.comhttps://eu.smith.langchain.comAPI URLhttps://api.smith.langchain.comhttps://eu.api.smith.langchain.comGCPus-central1 (Iowa)europe-west4 (Netherlands)SupabaseAWS us-east-1 (N. Virginia)AWS eu-central-1 (Germany)ClickHouse Cloudus-central1 (Iowa)europe-west4 (Netherlands)LangGraph Cloudus-central1 (Iowa)europe-west4 (Netherlands) See the Regions FAQ for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of data is stored exclusively in the US according to the information provided?"}, {"role": "assistant", "content": "Region-independent storage Data listed here is stored exclusively in the US: Payment and billing information with Stripe and Metronome"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What services does LangSmith use on Google Cloud Platform (GCP)?"}, {"role": "assistant", "content": "GCP services LangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE): LangSmith Frontend: serves the LangSmith UI.LangSmith Backend: serves the LangSmith API.LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.LangSmith Queue: handles processing of asynchronous tasks. (Internal service) LangSmith uses the following GCP storage services: Google Cloud Storage (GCS) for runs inputs and outputs.Google Cloud SQL PostgreSQL for transactional workloads.Google Cloud Memorystore for Redis for queuing and caching.Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint. Some additional GCP services we use include: Google Cloud Load Balancer for routing traffic to the LangSmith services.Google Cloud CDN for caching static assets.Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the anticipated processing capacity of LangSmith after recent optimizations?"}, {"role": "assistant", "content": "Scalability LangSmith is designed to be scalable and performant. As of load testing done in February 2024, LangSmith can comfortably process 500K+ runs (spans) per minute.\nWe anticipate that LangSmith can process 750K+ runs per minute with the optimizations we've made since then."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text does the dataset created for evaluating the LLM application contain?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments does the evaluate method require to run an evaluation in Python and TypeScript?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to fetch a subset of examples from a dataset for evaluation?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on multiple splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended approach for logging LLM traces from OpenAI models using the langsmith SDK?"}, {"role": "assistant", "content": "Log custom LLM traces noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.\nIn order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format must the inputs for chat-style models be in, and what are the acceptable output formats?"}, {"role": "assistant", "content": "Chat-style models For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.ls_model_name: The name of the model, eg \"gpt-3.5-turbo\", \"claude-3-opus-20240307\", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ]}# Can also use one of:# output = {#     \"message\": {#         \"role\": \"assistant\",#         \"content\": \"Sure, what time would you like to book the table for?\"#     }# }## output = {#     \"role\": \"assistant\",#     \"content\": \"Sure, what time would you like to book the table for?\"# }## output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" }];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\"      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: \"assistant\",//     content: \"Sure, what time would you like to book the table for?\"//   }// };//// const output = {//   role: \"assistant\",//   content: \"Sure, what time would you like to book the table for?\"// };//// const output = [\"assistant\", \"Sure, what time would you like to book the table for?\"];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the `_reduce_chunks` function in the context of streaming outputs?"}, {"role": "assistant", "content": "Stream outputs For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}@traceable(    run_type=\"llm\",    reduce_fn=_reduce_chunks,    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def my_streaming_chat_model(messages: list):    for chunk in [\"Hello, \" + messages[1][\"content\"]]:        yield {            \"choices\": [                {                    \"message\": {                        \"content\": chunk,                        \"role\": \"assistant\",                    }                }            ]        }list(    my_streaming_chat_model(        [            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},            {\"role\": \"user\", \"content\": \"polly the parrot\"},        ],    ))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can token counts be manually provided to LangSmith in a function's response?"}, {"role": "assistant", "content": "Manually provide token counts Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.\nMany models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.\nIf token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.\nIf using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ],    \"usage_metadata\": {        \"input_tokens\": 27,        \"output_tokens\": 13,        \"total_tokens\": 40,    },}@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for inputs and outputs in instruct-style models?"}, {"role": "assistant", "content": "Instruct-style models For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.\nThe same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def hello_llm(prompt: str):    return {        \"choices\": [            {\"text\": \"Hello, \" + prompt}        ],        \"usage_metadata\": {            \"input_tokens\": 4,            \"output_tokens\": 5,            \"total_tokens\": 9,        },    }hello_llm(\"polly the parrot\\n\")import { traceable } from \"langsmith/traceable\";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: \"Hello, \" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: \"llm\", name: \"hello_llm\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await helloLLM({ prompt: \"polly the parrot\\n\" }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features are available for setting up online evaluations in LangSmith?"}, {"role": "assistant", "content": "Set up online evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:Set up automation rules Online evaluations is a powerful LangSmith feature that allows you to run an LLM-as-a-judge evaluator on a set of your production traces. They are implemented as a possible action in an automation rule. Currently, we provide support for specifying a prompt template, a model, and a set of criteria to evaluate the runs on. After entering rules setup, you can select Online Evaluation from the list of possible actions:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What action must be selected to access the panel for configuring online evaluations?"}, {"role": "assistant", "content": "Configure online evaluations When selection Online Evaluation as an action in an automation, you are presented with a panel from which you can configure online evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What models are currently supported for selection in the dropdown?"}, {"role": "assistant", "content": "Model You can choose any model available in the dropdown. Currently, we support OpenAI, AzureOpenAI, and models hosted on Fireworks.\nIn order to set the API keys to use for these invocations, click on the Secrets & API Keys button and add the necessary keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for selecting or creating a prompt in the prompt template?"}, {"role": "assistant", "content": "Prompt template For the prompt template, you can select a suggested evaluator prompt, create a new prompt, or choose a prompt from the LangChain Hub. Suggested evaluator prompts We provide a list of pre-existing prompts that cater to common evaluator use cases. Create your own prompt We provide a base template from which you can form your own prompt. Pull a prompt from the LangChain Hub You can pull any structured prompt, private or public. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses.\nIf the prompt is your own, you can edit it in the playground and commit the version.\nIf the prompt is someone else's, you can fork the prompt, make your edits in the playground, and then pull in your new prompt to the online evaluator. When you choose a hub prompt for your online evaluator, the prompt will be locked to the commit version it was at rule creation. If you want to update the prompt, you can go to edit the online evaluator and re-select the prompt in the dropdown."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you map variables in your evaluator prompt based on recent runs or experiments?"}, {"role": "assistant", "content": "Mapping variables Prompts can be crafted with any variable name you choose. To map what is passed into your evaluator prompt from your runs or experiments, use the variable mapping inputs. There's a dropdown with suggestions provided based on the schema of your recent runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does the preview feature show when evaluating a prompt?"}, {"role": "assistant", "content": "Preview Previewing the prompt will show you an example of what the formatted prompt will look like. This preview pulls the input and output of the most recent run. In the case of a dataset evaluator, the preview will also pull reference output from an example in your dataset. noteYou can configure an evaluation prompt that doesn't match the schema of your recent runs, but the dropdown suggestions and preview function won't work as expected."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for naming a schema in the output schema?"}, {"role": "assistant", "content": "Output schema An evaluator will attach metadata tags to a run. These tags will have a name and a value. You can configure these in the Schema section.\nThe names and the descriptions of the fields will be passed into the prompt. Behind the scenes, we use tool calling to coerce the output of the LLM into the score you specify. noteThe name of the schema cannot have spaces since it is used as the name of a tool."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What feature in LangSmith can be used to track multi-turn conversations in LLM applications?"}, {"role": "assistant", "content": "Set up threads Recommended ReadingBefore diving into this content, it might be helpful to read the following:Add metadata and tags to traces Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the special metadata key used to associate traces in a thread?"}, {"role": "assistant", "content": "Group traces into threads A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread. To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread. The key value is the unique identifier for that conversation.\nThe key name should be one of: session_idthread_idconversation_id. The value should be a UUID, such as f47ac10b-58cc-4372-a567-0e02b2c3d479."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access and view threads in a project details page?"}, {"role": "assistant", "content": "View threads You can view threads by clicking on the Threads tad in any project details page. You will then see a list of all threads, sorted by the most recent activity. You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs. You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I edit a prompt in the application?"}, {"role": "assistant", "content": "Update a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt metadata?"}, {"role": "assistant", "content": "Update metadata To update the prompt metadata (description, use cases, etc.) click the \"Edit\" pencil icon. Your prompt metadata will be updated upon save."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt content in the prompt playground?"}, {"role": "assistant", "content": "Update the prompt content To update the prompt content itself, you need to enter the prompt playground. Click \"Edit in playground\".\nNow you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click \"Commit\" to save it. \n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view all historical versions of a prompt after adding a commit?"}, {"role": "assistant", "content": "Version a prompt When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the \"Commits\" tab in the prompt view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith's tracing functionality support streamed outputs in generator functions?"}, {"role": "assistant", "content": "Trace generator functions In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user. LangSmith's tracing functionality natively supports streamed outputs via generator functions. Below is an example. PythonTypeScriptfrom langsmith import traceable@traceabledef my_generator():    for chunk in [\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userfor output in my_generator():    print(output)# It also works with async functionsimport asyncio@traceableasync def my_async_generator():    hunk in [\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userasync def main():    async for output in my_async_generator():        print(output)asyncio.run(main())import { traceable } from \"langsmith/traceable\";const myGenerator = traceable(function* () {    for (const chunk of [\"Hello\", \"World\", \"!\"]) {        yield chunk;    }});for (const output of myGenerator()) {    console.log(output);}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you customize the aggregation of outputs in LangSmith when using traced functions?"}, {"role": "assistant", "content": "Aggregate Results By default, the outputs of the traced function are aggregated into a single array in LangSmith.\nIf you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the aggregate option (reduce_fn in python).\nThis is especially useful for aggregating streamed LLM outputs. noteAggregating outputs only impacts the traced representation of the outputs. It doesn not alter the values returned by your function. PythonTypeScriptfrom langsmith import traceabledef concatenate_strings(outputs: list):    return \"\".join(outputs)@traceable(reduce_fn=concatenate_strings)def my_generator():    for chunk in [\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userfor output in my_generator():    print(output)# It also works with async functionsimport asyncio@traceable(reduce_fn=concatenate_strings)async def my_async_generator():    for chunk in [\"Hello\", \"World\", \"!\"]:        yield chunk# Stream to the userasync def main():    async for output in my_async_generator():        print(output)asyncio.run(main())import { traceable } from \"langsmith/traceable\";const concatenateStrings = (outputs: string[]) => outputs.join(\"\");const myGenerator = traceable(function* () {    for (const chunk of [\"Hello\", \"World\", \"!\"]) {        yield chunk;    }}, { aggregator: concatenateStrings });for (const output of await myGenerator()) {    console.log(output);}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to filter runs for analysis and export in the SDK and API?"}, {"role": "assistant", "content": "Trace query syntax Using the list_runs method in the SDK or /runs/query endpoint in the API, you can filter runs to analyze and export."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different filtering options available for fetching runs in a project?"}, {"role": "assistant", "content": "Filter arguments KeysDescriptionproject_id / project_nameThe project(s) to fetch runs from - can be a single project or a list of projects.trace_idFetch runs that are part of a specific trace.run_typeThe type of run to get, such as llm, chain, tool, retriever, etc.dataset_name / dataset_idFetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.reference_example_idFetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.parent_run_idFetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.errorFetch runs that errored or did not error.run_idsFetch runs with a given list of run ids. Note: This will ignore all other filtering arguments.filterFetch runs that match a given structured filter statement. See the guide below for more information.trace_filterFilter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace.tree_filterFilter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace.is_rootOnly return root runs.selectSelect the fields to return in the response. By default, all fields are returned.query (experimental)Natural language query, which translates your query into a filter statement."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What filtering capabilities does LangSmith's filter query language support for fetching runs?"}, {"role": "assistant", "content": "Filter query language LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs. The filtering grammar is based on common comparators on fields in the run object. Supported comparators include: gte (greater than or equal to)gt (greater than)lte (less than or equal to)lt (less than)eq (equal to)neq (not equal to)has (check if run contains a tag or metadata json blob)search (search for a substring in a string field) Additionally, you can combine multiple comparisons through and and or operators. These can be applied on fields of the run object, such as its id, name, run_type, start_time / end_time, latency, total_tokens, error, execution_order, tags, and any associated feedback through feedback_key and feedback_score."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will API keys prefixed with ls__ be disabled in favor of lsv2 style keys?"}, {"role": "assistant", "content": "LangSmith Release Notes noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith v0.6 during the week of June 17, 2024?"}, {"role": "assistant", "content": "Week of June 17, 2024 - LangSmith v0.6 LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced in LangSmith?"}, {"role": "assistant", "content": "New Features Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the performance and reliability of run rules and querying the Hub via the SDK?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the default port and Helm configuration in the recent infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes [Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were added in the recent admin changes?"}, {"role": "assistant", "content": "Admin changes Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.5.x and earlier following the release of v0.6?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and features does LangSmith v0.5 introduce?"}, {"role": "assistant", "content": "Week of May 13, 2024 - LangSmith v0.5 LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after the LangSmith Helm release v0.7 in August 2024?"}, {"role": "assistant", "content": "Breaking changes We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced, including role-based access controls and improvements in regression testing and production monitoring?"}, {"role": "assistant", "content": "New Features Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to improve performance and reliability in resource management?"}, {"role": "assistant", "content": "Performance and Reliability Changes Split ingest, session deletion, and automation jobs to execute within separate resource pools."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default storage size for Clickhouse persistence in LangSmith v0.4, and how can it be adjusted?"}, {"role": "assistant", "content": "Infrastructure changes As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes regarding access controls and API keys?"}, {"role": "assistant", "content": "Admin changes Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.4.x and earlier following the release of v0.5?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.4 during the week of March 25, 2024?"}, {"role": "assistant", "content": "Week of March 25, 2024 - LangSmith v0.4 LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes should be made to ensure backwards compatibility when updating to the new release?"}, {"role": "assistant", "content": "Breaking changes This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to enhance performance and reduce delay in the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the image repositories and Clickhouse persistence storage in the infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to the API key management and OAuth flow in the recent admin updates?"}, {"role": "assistant", "content": "Admin changes Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.3.x and earlier versions following the release of 0.4?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.3 during the week of February 21, 2024?"}, {"role": "assistant", "content": "Week of Februrary 21, 2024 - LangSmith v0.3 LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do to retain run data when migrating from LangSmith v0.1?"}, {"role": "assistant", "content": "Breaking changes This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the LangSmith UI regarding trace ingestion and display delay?"}, {"role": "assistant", "content": "Performance and Reliability Changes Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is available regarding admin changes?"}, {"role": "assistant", "content": "Admin changes nan"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.2.x and earlier versions following the release of 0.3?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features does LangSmith 0.2 include?"}, {"role": "assistant", "content": "Week of January 29, 2024 - LangSmith v0.2 LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What versions of langsmith-sdk are required for this release to support changes in pagination of API results?"}, {"role": "assistant", "content": "Requirements This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to make custom metadata in traces searchable after the change in search syntax?"}, {"role": "assistant", "content": "Breaking changes The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,}) and then search using has(metadata, '{\"user_name\": ...}')"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to the performance and reliability of the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What database service has been added to improve performance and scalability for storing run results in LangSmith?"}, {"role": "assistant", "content": "Infrastructure Changes Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the new maximum number of users allowed per organization for new organizations?"}, {"role": "assistant", "content": "Admin changes Increased the maximum number of users per organization from 5 to 100 for new organizations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of LangSmith 0.1.x following the release of version 0.2?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What HTTP Status Code does LangSmith use to indicate that rate or usage limits have been exceeded?"}, {"role": "assistant", "content": "Rate Limits LangSmith has rate limits which are designed to ensure the stability of the service for all users. To ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the reason for receiving a 429 response from the application load balancer?"}, {"role": "assistant", "content": "Temporary throughput limit over a 1 minute period at our application load balancer This 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly  it is not guaranteed to start at the start of a clock minute  and may change depending on application deployment events. After the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats. This 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users. MethodEndpointLimitWindowDELETESessions301 minutePOST OR PATCHRuns50001 minutePOSTFeedback50001 minute**20001 minute noteThe LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the maximum number of hourly events that can be ingested for organizations on the Developer plan with payment on file?"}, {"role": "assistant", "content": "Plan-level hourly trace event limit This 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour. An event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit. This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use. PlanLimitWindowDeveloper (no payment on file)50,000 events1 hourDeveloper (with payment on file)250,000 events1 hourStartup/Plus500,000 events1 hourEnterpriseCustomCustom"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the maximum amount of data that can be ingested in an hour for the Developer plan with payment on file?"}, {"role": "assistant", "content": "Plan-level hourly trace data ingest limit This 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour. Typically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit. This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use. PlanLimitWindowDeveloper (no payment on file)500MB1 hourDeveloper (with payment on file)2.5GB1 hourStartup/Plus5.0GB1 hourEnterpriseCustomCustom"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the maximum number of unique traces that can be ingested in a month for the Developer Plan Tier without a payment method on file?"}, {"role": "assistant", "content": "Plan-level monthly unique traces limit This 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file. PlanLimitWindowDeveloper (no payment on file)5,000 traces1 month"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What triggers a 429 error related to monthly usage limits in an organization?"}, {"role": "assistant", "content": "Self-configured monthly usage limits This 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and varies by organization based on their configured settings."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you implement in your application when handling 429 responses from the LangSmith API?"}, {"role": "assistant", "content": "Handling 429s responses in your application Since some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter. For convenience, LangChain applications built with the LangSmith SDK has this capability built-in. noteIt is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.If that is the case, we would like to discuss your needs more specifically. Please reach out to LangSmith Support with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What function should be used to run pairwise evaluations of existing experiments in LangSmith?"}, {"role": "assistant", "content": "Run pairwise evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function\nwith two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the required SDK versions for using the evaluate_comparative function and what arguments does it accept?"}, {"role": "assistant", "content": "Use theevaluate_comparativefunction notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the inputs and outputs required for configuring pairwise evaluators, and how should the output dictionary be structured?"}, {"role": "assistant", "content": "Configure inputs and outputs for pairwise evaluators Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with\nruns[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent \"both equally bad\" or both to 1 for \"both equally good\". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for using LLM-based pairwise evaluators to compare the responses of two AI assistants?"}, {"role": "assistant", "content": "Compare two experiments with LLM-based pairwise evaluators The following example uses a prompt\nwhich asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.\nThe prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=\"gpt-4\")        runnable = prompt | model    response = runnable.invoke({        \"question\": example.inputs[\"question\"],        \"answer_a\": runs[0].outputs[\"output\"] if runs[0].outputs is not None else \"N/A\",        \"answer_b\": runs[1].outputs[\"output\"] if runs[1].outputs is not None else \"N/A\",    })    score = response[\"Preference\"]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {\"key\": \"ranked_preference\", \"scores\": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [\"my-experiment-name-1\", \"my-experiment-name-2\"],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.\nimport type { Run, Example } from \"langsmith\";import { evaluateComparative } from \"langsmith/evaluation\";import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());import { z } from \"zod\";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(\"Expected at least two runs\");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? \"N/A\",    answer_b: runB?.outputs?.output ?? \"N/A\",  };    const output = await openai.chat.completions.create({    model: \"gpt-4-turbo\",    messages: [      {        role: \"system\",        content: [          \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.\",          \"You should choose the assistant that follows the user's instructions and answers the user's question better.\",          \"Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.\",          \"Begin your evaluation by comparing the two responses and provide a short explanation.\",          \"Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\",          \"Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\",        ].join(\" \"),      },      {        role: \"user\",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(\"\\n\\n\"),      },    ],    tool_choice: {      type: \"function\",      function: { name: \"Score\" },    },    tools: [      {        type: \"function\",        function: {          name: \"Score\",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output \"1\" if Assistant A answer is better based upon the factors above.`,            `Output \"2\" if Assistant B answer is better based upon the factors above.`,            `Output \"0\" if it is a tie.`,          ].join(\" \"),          parameters: {            type: \"object\",            properties: {              Preference: {                type: \"integer\",                description: \"Which assistant answer is preferred?\",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: \"ranked_preference\", scores };}await evaluateComparative([\"earnest-name-40\", \"reflecting-pump-91\"], {  evaluators: [evaluatePairwise],});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the Comparison View of pairwise experiments?"}, {"role": "assistant", "content": "View pairwise experiments Navigate to the \"Pairwise Experiments\" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the core components of LangSmith evaluation?"}, {"role": "assistant", "content": "Evaluation The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of a single data point within a dataset used in the LangSmith evaluation workflow?"}, {"role": "assistant", "content": "Datasets Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some methods for creating datasets for evaluation?"}, {"role": "assistant", "content": "Creating datasets There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.\nFrom building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,\nand what \"good\" responses may be.\nYou probably want to cover a few different common edge cases or situations you can imagine.\nEven 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?\nThis information can be valuable to capture and store in datasets. This allows you to test against these\nuse cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?\nThere are a few heuristics you can follow.\nIf possible - try to collect end user feedback. You can then see which datapoints got negative feedback.\nThat is super valuable! These are spots where your application did not perform well.\nYou should add these to your dataset to test against in the future. You can also use other heuristics\nto identify \"interesting\" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.\nIt's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.\nThis can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three distinct types of datasets offered by LangSmith?"}, {"role": "assistant", "content": "Types of datasets LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:\"Inputs\" and \"outputs\" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating \"completion\" style language models.The \"inputs\" dictionary contains a single \"input\" key mapped to the prompt string.The \"outputs\" dictionary contains a single \"output\" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured \"chat\" messages as inputs and outputs.The \"inputs\" dictionary contains a single \"input\" key mapped to a list of serialized chat messagesThe \"outputs\" dictionary contains a single \"output\" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of partitioning a dataset into different splits during evaluation?"}, {"role": "assistant", "content": "Partitioning datasets When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of the EvaluationResult returned by evaluators in LangSmith?"}, {"role": "assistant", "content": "Evaluators Evaluators are functions in LangSmith that score how well your application performs on a particular example.\nEvaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate human evaluation of LLM application outputs?"}, {"role": "assistant", "content": "Human Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are heuristic evaluators and how do they function in evaluating tasks?"}, {"role": "assistant", "content": "Heuristic Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of evaluations that LLM-as-judge evaluators can perform on system output?"}, {"role": "assistant", "content": "LLM-as-judge LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When is pairwise evaluation particularly useful?"}, {"role": "assistant", "content": "Pairwise Pairwise evaluators pick the better of two task outputs based upon some criteria.\nThis can use either a heuristic (\"which response is longer\"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.\nThis can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different scoring functions that evaluators can use to assess application performance on datasets?"}, {"role": "assistant", "content": "Applying evaluations We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are unit tests used for in software development?"}, {"role": "assistant", "content": "Unit Tests Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.\nThis means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).\nThese types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)\nto cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of regression testing in application development?"}, {"role": "assistant", "content": "Regression Testing Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.\nThey are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of back-testing in evaluating new model versions?"}, {"role": "assistant", "content": "Back-testing Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.\nAnthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.\nThen compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Why might pairwise evaluations be considered a more stable scoring approach than assigning individual scores?"}, {"role": "assistant", "content": "Pairwise-testing It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the main difference between online evaluation and offline evaluation in the context of application testing?"}, {"role": "assistant", "content": "Online Evaluation Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using repetitions in LangSmith evaluations?"}, {"role": "assistant", "content": "Experiment Configurations LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will be discussed regarding specific, popular LLM applications?"}, {"role": "assistant", "content": "Evaluating Specific LLM Applications Below, we will discuss evaluation of a few specific, popular LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three components that LLM-powered autonomous agents combine to generate responses?"}, {"role": "assistant", "content": "Agents LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.\nNote that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the advantages and disadvantages of evaluating an agent's final response based on its overall performance on a task?"}, {"role": "assistant", "content": "Evaluating an agent's final response One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits and downsides of evaluating a single step of an agent?"}, {"role": "assistant", "content": "Evaluating a single step of an agent Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different methods for evaluating an agent's trajectory and their respective strengths and weaknesses?"}, {"role": "assistant", "content": "Evaluating an agent's trajectory Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an \"exact\" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the \"exact\" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of \"incorrect\" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some best practices for improving the reliability and cost-effectiveness of agents using LLMs?"}, {"role": "assistant", "content": "Best practices Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of Retrieval Augmented Generation (RAG) in AI applications?"}, {"role": "assistant", "content": "Retrieval Augmented Generation (RAG) Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the significance of reference answers in evaluating RAG applications?"}, {"role": "assistant", "content": "Dataset When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two main options for evaluating RAG applications?"}, {"role": "assistant", "content": "Evaluator LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different approaches to applying RAG evaluation?"}, {"role": "assistant", "content": "Applying RAG Evaluation When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluations of answer faithfulness, helpfulness, correctness, and chain comparison in the RAG evaluation summary?"}, {"role": "assistant", "content": "RAG evaluation summary Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluation methods used for summarization, and how do they differ in terms of reference summaries?"}, {"role": "assistant", "content": "Summarization Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components and considerations for evaluating Classification / Tagging when ground truth labels are available versus when they are not?"}, {"role": "assistant", "content": "Classification / Tagging Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does LangSmith offer to help improve the performance of applications through prompts?"}, {"role": "assistant", "content": "Prompts Writing good prompts is key to getting the best performance out of your applications. LangSmith provides ways to create, test, and manage prompts."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three types of prompt templates supported in LangSmith?"}, {"role": "assistant", "content": "Prompt types We support three types of prompt templates: StringPromptTemplateChatPromptTemplateStructuredPromptTemplate For detailed information about these templates, please refer to the LangChain documentation. In LangSmith, you can create prompts using the Playground. From the prompt view in the Playground, you can select either \"Chat-style prompt\" or \"Instruct-style prompt\" to get started. Chat-style prompt Chat prompts are used for chat-style models that accept a list of messages as an input and respond with an assistant message. A chat-style prompt is represented in LangSmith as a ChatPromptTemplate, which can contain multiple messages, each with prompt variables. You can also specify an output schema which is represented in LangSmith as a StructuredPromptTemplate. Instruct-style prompt An instruct-style prompt is represented as a StringPromptTemplate that gets formatted to a single string input for your model."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of template formats supported?"}, {"role": "assistant", "content": "Template formats We support two types of template formats: f-string and mustache. You can switch between these formats when editing prompts in the Playground."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is an f-string in Python and how is it used?"}, {"role": "assistant", "content": "F-string F-strings are a Python-specific string formatting method that allows you to embed expressions inside string literals, using curly braces {}.\nHere's an example of an f-string template: Hello, {name}!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some features of Mustache template syntax that differentiate it from f-strings?"}, {"role": "assistant", "content": "Mustache Mustache is a template syntax that allows you to embed variables inside double curly braces {{}}. NotePlease see the Mustache documentation for more detailed information on how to use Mustache templates. Here's an example of a mustache template: Hello, {{name}}! Mustache is more robust than f-strings since it supports more complex logic. You can use conditionals, loops, and access nested keys in mustache templates. Conditional Hello, {{#name}}{{name}}{{/name}}{{^name}}world{{/name}}! The template will output \"Hello, Bob!\" if \"Bob\" is provided as the name variable, and \"Hello, world!\" if the name variable is not provided. Loop {{#names}}{{name}}{{/names}} input: {  \"names\": [{ \"name\": \"Alice\" }, { \"name\": \"Bob\" }, { \"name\": \"Charlie\" }]} output: \"AliceBobCharlie\" Loop with nesting {{#people}}{{name}} is {{age}} years old. {{/people}} input: {  \"people\": [    { \"name\": \"Alice\", \"age\": 30 },    { \"name\": \"Bob\", \"age\": 25 },    { \"name\": \"Charlie\", \"age\": 35 }  ]} output: \"Alice is 30 years old. Bob is 25 years old. Charlie is 35 years old.\" Dot notation {{person.name}} is {{person.age}} years old. input: {  \"person\": {    \"name\": \"Alice\",    \"age\": 30  }} output: \"Alice is 30 years old.\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you create a new prompt in the application?"}, {"role": "assistant", "content": "Create a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage.\nClick the \"+ Prompt\" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for developing your prompt in the playground?"}, {"role": "assistant", "content": "Compose your prompt After choosing a prompt type, you're brought to the playground to develop your prompt.\nOn the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click \"Start\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you save a prompt and what options do you have for its visibility?"}, {"role": "assistant", "content": "Save your prompt To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\".\nPrivate prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.\nClick save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can I view a table of my prompts?"}, {"role": "assistant", "content": "View your prompts You've just created your first prompt! View a table of your prompts in the prompts tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add metadata to your prompt in the LangChain Hub?"}, {"role": "assistant", "content": "Add metadata To add metadata to your prompt, click the prompt and then click the \"Edit\" pencil icon next to the name.\nThis brings you to where you can add additional information about the prompt, including a description, a README, and use cases.\nFor public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you need specific upgrade instructions for certain versions?"}, {"role": "assistant", "content": "General Upgrade Instructions For general upgrade instructions, please follow the instructions below. Certain versions may have specific upgrade instructions, which will be detailed in more specific upgrade guides."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should you run to upgrade a Helm chart for the langchain repository?"}, {"role": "assistant", "content": "Kubernetes(Helm) If you don't have the repo added, run the following command to add it: helm repo add langchain https://langchain-ai.github.io/helm/ Update your local helm repo helm repo update Update your helm chart config file with any updates that are needed in the new version. These will be detailed in the release notes for the new version. Run the following command to upgrade the chart(replace version with the version you want to upgrade to): helm upgrade <release-name> langchain/langsmith --version <version> --values <path-to-values-file> Verify that the upgrade was successful: helm status <release-name> All pods should be in the Running state. Verify that clickhouse is running and that both migrations jobs have completed. kubectl get podsNAME                                     READY   STATUS      RESTARTS   AGElangsmith-backend-95b6d54f5-gz48b        1/1     Running     0          15hlangsmith-pg-migrations-d2z6k            0/1     Completed   0          5h48mlangsmith-ch-migrations-gasvk            0/1     Completed   0          5h48mlangsmith-clickhouse-0                   1/1     Running     0          26hlangsmith-frontend-84687d9d45-6cg4r      1/1     Running     0          15hlangsmith-hub-backend-66ffb75fb4-qg6kl   1/1     Running     0          15hlangsmith-playground-85b444d8f7-pl589    1/1     Running     0          15hlangsmith-queue-d58cb64f7-87d68          1/1     Running     0          15h"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should you run to validate your deployment and what should you check after curling the external IP of the langsmith-frontend service?"}, {"role": "assistant", "content": "Validate your deployment: Run kubectl get servicesOutput should look something like: NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGEkubernetes                   ClusterIP      172.20.0.1       <none>          443/TCP                      27dlangsmith-backend            ClusterIP      172.20.22.34     <none>          1984/TCP                     21dlangsmith-clickhouse         ClusterIP      172.20.117.62    <none>          8123/TCP,9000/TCP            21dlangsmith-frontend           LoadBalancer   172.20.218.30    <external ip>   80:30093/TCP,443:31130/TCP   21dlangsmith-platform-backend   ClusterIP      172.20.232.183   <none>          1986/TCP                     21dlangsmith-playground         ClusterIP      172.20.167.132   <none>          3001/TCP                     21dlangsmith-postgres           ClusterIP      172.20.59.63     <none>          5432/TCP                     21dlangsmith-redis              ClusterIP      172.20.229.98    <none>          6379/TCP                     20d Curl the external ip of the langsmith-frontend service:curl <external ip>/api/info{\"version\":\"0.5.7\",\"license_expiration_time\":\"2033-05-20T20:08:06\",\"batch_ingest_config\":{\"scale_up_qsize_trigger\":1000,\"scale_up_nthreads_limit\":16,\"scale_down_nempty_trigger\":4,\"size_limit\":100,\"size_limit_bytes\":20971520}} Check that the version matches the version you upgraded to. Visit the external ip for the langsmith-frontend service on your browserThe Langsmith UI should be visible/operational"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are required to upgrade the Docker version of LangSmith?"}, {"role": "assistant", "content": "Docker Upgrading the Docker version of LangSmith is a bit more involved than the Helm version and may require a small amount of downtime. Please follow the instructions below to upgrade your Docker version of LangSmith. Update your docker-compose.yml file to the file used in the latest release. You can find this in the LangSmith SDK GitHub repositoryUpdate your .env file with any new environment variables that are required in the new version. These will be detailed in the release notes for the new version.Run the following command to stop your current LangSmith instance: docker-compose down Run the following command to start your new LangSmith instance in the background: docker-compose up -d If everything ran successfully, you should see all the LangSmith containers running and healthy. CONTAINER ID   IMAGE                                  COMMAND                  CREATED        STATUS                        PORTS                                                      NAMESe1c8f01a4ffc   langchain/langsmith-frontend:0.5.7     \"/entrypoint.sh ngin\"   10 hours ago   Up 40 seconds                 0.0.0.0:80->80/tcp, 8080/tcp                               cli-langchain-frontend-139e1394846b9   langchain/langsmith-backend:0.5.7      \"/bin/sh -c 'exec uv\"   10 hours ago   Up 40 seconds                 0.0.0.0:1984->1984/tcp                                     cli-langchain-backend-1f8688dd58f2f   langchain/langsmith-go-backend:0.5.7   \"./smith-go\"             10 hours ago   Up 40 seconds                 0.0.0.0:1986->1986/tcp                                     cli-langchain-platform-backend-1006f1303b04d   langchain/langsmith-backend:0.5.7      \"saq app.workers.que\"   10 hours ago   Up 40 seconds                                                                            cli-langchain-queue-173a90242ed3a   redis:7                                \"docker-entrypoint.s\"   10 hours ago   Up About a minute (healthy)   0.0.0.0:63791->6379/tcp                                    cli-langchain-redis-1eecf75ca672b   postgres:14.7                          \"docker-entrypoint.s\"   10 hours ago   Up About a minute (healthy)   0.0.0.0:5433->5432/tcp                                     cli-langchain-db-13aa5652a864d   clickhouse/clickhouse-server:23.9      \"/entrypoint.sh\"         10 hours ago   Up About a minute (healthy)   9009/tcp, 0.0.0.0:8124->8123/tcp, 0.0.0.0:9001->9000/tcp   cli-langchain-clickhouse-184edc329a37f   langchain/langsmith-playground:0.5.7   \"docker-entrypoint.s\"   10 hours ago   Up About a minute             0.0.0.0:3001->3001/tcp                                     cli-langchain-playground-1"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you validate the deployment of the cli-langchain-frontend-1 container?"}, {"role": "assistant", "content": "Validate your deployment: Curl the exposed port of the cli-langchain-frontend-1 container:curl localhost:80/info{\"version\":\"0.5.7\",\"license_expiration_time\":\"2033-05-20T20:08:06\",\"batch_ingest_config\":{\"scale_up_qsize_trigger\":1000,\"scale_up_nthreads_limit\":16,\"scale_down_nempty_trigger\":4,\"size_limit\":100,\"size_limit_bytes\":20971520}} 1 . Visit the exposed port of the cli-langchain-frontend-1 container on your browser The Langsmith UI should be visible/operational"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangSmith assist in correcting evaluator scores?"}, {"role": "assistant", "content": "Audit evaluator scores LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you edit feedback details in the comparison view?"}, {"role": "assistant", "content": "In the comparison view In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the \"edit\" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under \"Make correction\".\nIf you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples\nin place of the few_shot_explanation prompt variable."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the feedback details and corrections view in the runs table?"}, {"role": "assistant", "content": "In the runs table In the runs table, find the \"Feedback\" column and click on the feedback tag to bring up the feedback details. Again, click the \"edit\" icon on the right to bring up the corrections view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made using the SDK's update_feedback function?"}, {"role": "assistant", "content": "In the SDK Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      \"score\": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I edit a prompt in the application?"}, {"role": "assistant", "content": "Update a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt metadata?"}, {"role": "assistant", "content": "Update metadata To update the prompt metadata (description, use cases, etc.) click the \"Edit\" pencil icon. Your prompt metadata will be updated upon save."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt content in the prompt playground?"}, {"role": "assistant", "content": "Update the prompt content To update the prompt content itself, you need to enter the prompt playground. Click \"Edit in playground\".\nNow you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click \"Commit\" to save it. \n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view all historical versions of a prompt after adding a commit?"}, {"role": "assistant", "content": "Version a prompt When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the \"Commits\" tab in the prompt view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in generating synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can you use to log in to LangSmith after creating an account?"}, {"role": "assistant", "content": "Create an account To get started with LangSmith, you need to create an account. You can sign up for a free account here.\nWe support logging in with Google, GitHub, Discord, and email."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of API keys supported by LangSmith?"}, {"role": "assistant", "content": "API keys LangSmith supports two types of API keys: Service Keys and Personal Access Tokens.\nBoth types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases. Read more about the differences between Service Keys and Personal Access Tokens under admin concepts"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps do you need to follow to create an API key for LangSmith?"}, {"role": "assistant", "content": "Create an API key To log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests.\nCurrently, an API key is scoped to a workspace, so you will need to create an API key for each workspace you want to use. To create either type of API key head to the Settings page, then scroll to the API Keys section. Then click Create API Key. noteThe API key will be shown only once, so make sure to copy it and store it in a safe place."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are available for monitoring and automations in the application?"}, {"role": "assistant", "content": "How-to guides: Monitoring and automations This section contains how-to guides related to monitoring and automations.  Filter traces in the applicationBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here: Use monitoring chartsLangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project. Set up automation rulesWhile you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users. Online EvaluationBefore diving into this content, it might be helpful to read the following: Set up threadsBefore diving into this content, it might be helpful to read the following: Set up webhook notifications for rulesWhen you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do before exploring the content on filtering traces in the application?"}, {"role": "assistant", "content": "Filter traces in the application Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can the monitoring charts for each tracing project be accessed in LangSmith?"}, {"role": "assistant", "content": "Use monitoring charts LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What challenges arise when manually processing production logs from the LLM application as it scales to more users?"}, {"role": "assistant", "content": "Set up automation rules While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should be done before engaging with the online evaluation content?"}, {"role": "assistant", "content": "Online Evaluation Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do before diving into the content about setting up threads?"}, {"role": "assistant", "content": "Set up threads Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens when you add a webhook URL on an automation action?"}, {"role": "assistant", "content": "Set up webhook notifications for rules When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What feature in LangSmith can be used to track multi-turn conversations in LLM applications?"}, {"role": "assistant", "content": "Set up threads Recommended ReadingBefore diving into this content, it might be helpful to read the following:Add metadata and tags to traces Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the special metadata key used to associate traces in a thread?"}, {"role": "assistant", "content": "Group traces into threads A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread. To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread. The key value is the unique identifier for that conversation.\nThe key name should be one of: session_idthread_idconversation_id. The value should be a UUID, such as f47ac10b-58cc-4372-a567-0e02b2c3d479."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view and interact with threads in a project details page?"}, {"role": "assistant", "content": "View threads You can view threads by clicking on the Threads tad in any project details page. You will then see a list of all threads, sorted by the most recent activity. You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs. You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I set up billing for my LangSmith organization?"}, {"role": "assistant", "content": "Set up billing for your LangSmith account noteIf you are interested in the Enterprise plan, please contact sales. This guide is\nonly for our self-serve billing plans. noteIf you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please skip to the final section. To set up billing for your LangSmith organization, head to the Usage and Billing page under Settings.\nDepending on your organization's settings, you will be given a different walkthrough to get started."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the limit on traces per month for personal organizations until a credit card is added?"}, {"role": "assistant", "content": "Developer Plan: set up billing on your personal organization Personal organizations are limited to 5000 traces per month until a credit card is added. You can\nadd a credit card on the Plans and Billing page as follows:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens after you add your credit card information?"}, {"role": "assistant", "content": "2. Add your credit card info After this step, you will no longer be rate limited to 5000 traces, and will be charged for any excess\ntraces at rates specified on our pricing page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do before a new organization becomes usable in the Plus Plan?"}, {"role": "assistant", "content": "Plus Plan: set up billing on a shared organization If you have not yet created an organization, please do so by following this guide. This walkthrough assumes you are\nalready in a new organization. noteNew organizations are not usable until a credit card is entered. After you complete the following steps, you will\ngain complete access to LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should a startup building with AI do instead of clicking Subscribe on the Plus page?"}, {"role": "assistant", "content": "1. ClickSubscribeon the Plus page noteIf you are a startup building with AI, please instead click Apply Now on our Startup Plan. You may be\neligible for discounted prices and a generous free, monthly trace allotment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What option does LangSmith provide before subscribing regarding added users?"}, {"role": "assistant", "content": "2. Review your existing members Before subscribing, LangSmith lets you remove any added users that you would not\nlike to be charged for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information do you need to enter if the organization belongs to a business?"}, {"role": "assistant", "content": "3. Enter your credit card info Enter business information, invoice email and tax id If this organization belongs to a business. Please check the \"This is a business\" checkbox and enter the information accordingly. For more information refer to this guide Once this step is complete, your org should now have access to the rest of LangSmith!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to accounts created before April 2, 2024, if billing is not set up by July 8, 2024?"}, {"role": "assistant", "content": "Set up billing for accounts created before pricing was introduced on April 2, 2024 If you joined LangSmith before pricing was introduced April 2, 2024, you have the option to upgrade your\nexisting account to setup billing. If you did not set up billing by July 8, 2024, then your account is now\nrate limited to a maximum of 5,000 traces per month."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen if I enter my credit card info while on a Personal Organization?"}, {"role": "assistant", "content": "3. Enter your credit card info If you are on a Personal Organization, this will add you to the Developer plan. If you are on a shared Organization, this will add you to the Plus plan. For more information,\nplease view the above walkthroughs for Developer or Plus respectively, starting at step 2."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the core components of LangSmith evaluation for AI applications?"}, {"role": "assistant", "content": "Evaluation The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of a single data point within a dataset used in the LangSmith evaluation workflow?"}, {"role": "assistant", "content": "Datasets Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some methods for creating datasets for evaluation?"}, {"role": "assistant", "content": "Creating datasets There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.\nFrom building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,\nand what \"good\" responses may be.\nYou probably want to cover a few different common edge cases or situations you can imagine.\nEven 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?\nThis information can be valuable to capture and store in datasets. This allows you to test against these\nuse cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?\nThere are a few heuristics you can follow.\nIf possible - try to collect end user feedback. You can then see which datapoints got negative feedback.\nThat is super valuable! These are spots where your application did not perform well.\nYou should add these to your dataset to test against in the future. You can also use other heuristics\nto identify \"interesting\" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.\nIt's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.\nThis can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three distinct dataset types offered by LangSmith and their specific purposes?"}, {"role": "assistant", "content": "Types of datasets LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:\"Inputs\" and \"outputs\" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating \"completion\" style language models.The \"inputs\" dictionary contains a single \"input\" key mapped to the prompt string.The \"outputs\" dictionary contains a single \"output\" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured \"chat\" messages as inputs and outputs.The \"inputs\" dictionary contains a single \"input\" key mapped to a list of serialized chat messagesThe \"outputs\" dictionary contains a single \"output\" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of partitioning a dataset into different splits during evaluation?"}, {"role": "assistant", "content": "Partitioning datasets When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of the EvaluationResult returned by evaluators in LangSmith?"}, {"role": "assistant", "content": "Evaluators Evaluators are functions in LangSmith that score how well your application performs on a particular example.\nEvaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate human evaluation of LLM application outputs?"}, {"role": "assistant", "content": "Human Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are heuristic evaluators and how do they function in evaluating tasks?"}, {"role": "assistant", "content": "Heuristic Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of evaluations that LLM-as-judge evaluators can perform on system output?"}, {"role": "assistant", "content": "LLM-as-judge LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When is pairwise evaluation particularly useful?"}, {"role": "assistant", "content": "Pairwise Pairwise evaluators pick the better of two task outputs based upon some criteria.\nThis can use either a heuristic (\"which response is longer\"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.\nThis can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different scoring functions that evaluators can use to assess application performance on datasets?"}, {"role": "assistant", "content": "Applying evaluations We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are unit tests used for in software development?"}, {"role": "assistant", "content": "Unit Tests Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.\nThis means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).\nThese types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)\nto cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of regression testing in application development?"}, {"role": "assistant", "content": "Regression Testing Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.\nThey are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of back-testing in evaluating new model versions?"}, {"role": "assistant", "content": "Back-testing Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.\nAnthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.\nThen compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Why might pairwise evaluations be considered a more stable scoring approach than assigning individual scores?"}, {"role": "assistant", "content": "Pairwise-testing It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the main difference between online evaluation and offline evaluation in the context of application testing?"}, {"role": "assistant", "content": "Online Evaluation Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using repetitions in the LangSmith evaluate function?"}, {"role": "assistant", "content": "Experiment Configurations LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will be discussed regarding specific, popular LLM applications?"}, {"role": "assistant", "content": "Evaluating Specific LLM Applications Below, we will discuss evaluation of a few specific, popular LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three components that LLM-powered autonomous agents combine to generate responses?"}, {"role": "assistant", "content": "Agents LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.\nNote that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the advantages and disadvantages of evaluating an agent's final response based on its overall performance on a task?"}, {"role": "assistant", "content": "Evaluating an agent's final response One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits and downsides of evaluating a single step of an agent?"}, {"role": "assistant", "content": "Evaluating a single step of an agent Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different methods for evaluating an agent's trajectory and their respective strengths and weaknesses?"}, {"role": "assistant", "content": "Evaluating an agent's trajectory Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an \"exact\" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the \"exact\" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of \"incorrect\" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some best practices for improving the reliability and cost-effectiveness of agents using LLMs?"}, {"role": "assistant", "content": "Best practices Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is Retrieval Augmented Generation (RAG) and how does it enhance AI applications?"}, {"role": "assistant", "content": "Retrieval Augmented Generation (RAG) Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the significance of reference answers in evaluating RAG applications?"}, {"role": "assistant", "content": "Dataset When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two main options for evaluating RAG applications?"}, {"role": "assistant", "content": "Evaluator LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different approaches to applying RAG evaluation?"}, {"role": "assistant", "content": "Applying RAG Evaluation When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluations of answer faithfulness, helpfulness, correctness, and chain comparison in the RAG evaluation summary?"}, {"role": "assistant", "content": "RAG evaluation summary Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluation criteria used for assessing summarization in writing?"}, {"role": "assistant", "content": "Summarization Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components and considerations for evaluating Classification / Tagging when ground truth labels are available versus when they are not?"}, {"role": "assistant", "content": "Classification / Tagging Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will infoWorkspaces be incrementally rolled out?"}, {"role": "assistant", "content": "Set up a workspace infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.\nWorkspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new workspace in your shared organization?"}, {"role": "assistant", "content": "Create a workspace To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.\nOnce your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.\nPlease see the pricing page for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What permissions do workspace admins have regarding user management in a workspace?"}, {"role": "assistant", "content": "Manage users infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.\nUsers may also be invited directly to one or more workspaces when they are invited to an organization."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options can be configured in the workspace settings page?"}, {"role": "assistant", "content": "Configure workspace settings Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you create a prompt in the application?"}, {"role": "assistant", "content": "Create a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage.\nClick the \"+ Prompt\" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for developing your prompt in the playground?"}, {"role": "assistant", "content": "Compose your prompt After choosing a prompt type, you're brought to the playground to develop your prompt.\nOn the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click \"Start\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you save a prompt and what options do you have for its visibility?"}, {"role": "assistant", "content": "Save your prompt To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\".\nPrivate prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.\nClick save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can I view a table of my prompts?"}, {"role": "assistant", "content": "View your prompts You've just created your first prompt! View a table of your prompts in the prompts tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add metadata to a prompt in the LangChain Hub?"}, {"role": "assistant", "content": "Add metadata To add metadata to your prompt, click the prompt and then click the \"Edit\" pencil icon next to the name.\nThis brings you to where you can add additional information about the prompt, including a description, a README, and use cases.\nFor public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What additional details can be found regarding the cloud architecture?"}, {"role": "assistant", "content": "Regions FAQ noteSee the cloud architecture reference for additional details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What privacy and data protection frameworks does LangSmith comply with?"}, {"role": "assistant", "content": "Legal and compliance What privacy and data protection frameworks does LangSmith, including its EU instance, comply with? LangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com.\nIf you would like to sign a Data Processing Addendum (DPA) with us, please reach out to support@langchain.dev. Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan. My company isnt based in the EU, can I still have my data hosted there? Yes, you can host your LangSmith data in the EU instance independent of your location. Do you have a legal entity in the EU that we can contract with? We do not have a legal entity in the EU for customer contracting today. Do different legal terms apply if I choose the EU region? The terms are the same for the EU and US regions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functional differences between US and EU cloud-managed LangSmith?"}, {"role": "assistant", "content": "Features Are there any functional differences between US and EU cloud-managed LangSmith? There may be a small delay between launches to each region depending on the feature.\nBesides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa. Can an organization have workspaces in different regions? LangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case. Can I connect an EU organization to a US organization and share billing? LangSmith does not support this at the moment, but if you are interested, please contact support@langchain.dev and share your use case. What data will be stored in my selected region? See the cloud architecture reference for details. How can I see my organization's region? Check your URL - any organizations on https://eu.smith.langchain.com are in the EU, and any on https://smith.langchain.com are in the US. Can I switch my organization from the US to EU or vice versa? We do not support migration between regions at this time, but if you are interested in this feature, please reach out to support@langchain.dev."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Is the pricing for the EU region different from the US region on LangSmith plans?"}, {"role": "assistant", "content": "Plans and pricing Is the EU region available on all LangSmith plans? Yes, you can sign up for the EU region on all plans including free plans. Is pricing different for the EU region compared to the US region? No, pricing is the same for the EU and US regions. What currency is used for payment if I use the EU region? All LangSmith plans are paid in USD."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What can beginners find in the LangSmith tutorials?"}, {"role": "assistant", "content": "Tutorials New to LangSmith? This is the place to start. Here, you'll find a hands-on introduction to key LangSmith workflows."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some key activities developers can undertake to enhance their LLM application?"}, {"role": "assistant", "content": "Developers Add observability to your LLM applicationEvaluate your LLM applicationOptimize a classifierRAG EvaluationsBacktestingAgent Evaluations"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can administrators optimize tracing spend on LangSmith?"}, {"role": "assistant", "content": "Administrators Optimize tracing spend on LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the destination project of your traces?"}, {"role": "assistant", "content": "Log traces to specific project You can change the destination project of your traces both statically through environment variables and dynamically at runtime."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set a custom project name for an application run in LangSmith?"}, {"role": "assistant", "content": "Set the destination project statically As mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-custom-project If the project specified does not exist, it will be created automatically when the first trace is ingested."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name dynamically at runtime when logging traces in an application?"}, {"role": "assistant", "content": "Set the destination project dynamically You can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application. noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(    run_type=\"llm\",    name=\"OpenAI Call Decorator\",    project_name=\"My Project\")def call_openai(    messages: list[dict], model: str = \"gpt-3.5-turbo\") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(    messages,    langsmith_extra={\"project_name\": \"My Overriden Project\"},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=messages,    langsmith_extra={\"project_name\": \"My Project\"},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(    run_type=\"llm\",    name=\"OpenAI Call RunTree\",    inputs={\"messages\": messages},    project_name=\"My Project\")chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from \"langsmith\";const client = new OpenAI();const messages = [    {role: \"system\", content: \"You are a helpful assistant.\"},    {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: \"llm\",    name: \"OpenAI Call Traceable\",    project_name: \"My Project\"});// Call the traceable functionawait traceableCallOpenAI(messages, \"gpt-3.5-turbo\");// Create and use a RunTree objectconst rt = new RunTree({    runType: \"llm\",    name: \"OpenAI Call RunTree\",    inputs: { messages },    project_name: \"My Project\"});// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of the dynamic few shot example selection feature?"}, {"role": "assistant", "content": "Dynamic few shot example selection noteThis feature is currently in closed beta. Please sign up here for access Configure your datasets so that you can search for few shot examples based on an incoming request."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for a dataset to be used in the LangSmith cloud?"}, {"role": "assistant", "content": "Pre-conditions Your dataset must have the KV store data type (we do not currently support chat model or LLM type datasets)You must have an input schema defined for your dataset. See our docs on setting up schema validation in our UI for details.You must be enabled for the closed betaYou must be on LangSmith cloud"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you index your dataset to make it searchable?"}, {"role": "assistant", "content": "Index your dataset to be searched On the datasets UI, click the Few-shot Index on the top right corner and hit Start Sync. This process will start to index your data to be searchable in the background. A note will appear on the\nmodal above that says if your index is up to date, and if not, what version of your dataset it last indexed. All new data added to your dataset will automatically be indexed. You do not need to re-index when adding new data."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search your dataset for similar examples using the API?"}, {"role": "assistant", "content": "Search your dataset for similar examples You can search your dataset via API for similar examples using the POST /datasets/<id>/search REST API. Its documentation\ncan be found here.\nYou can see detailed examples of how use this in your prompts both with and without LangChain in\nour cookbook on using indexed datasets with few shot prompts"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What additional features does LangSmith offer for user management?"}, {"role": "assistant", "content": "User management noteThis guide assumes you have read the admin guide and organization setup guide. LangSmith offers additional customization features for user management using feature flags."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required for a user to invite new users to an organization in LangSmith, and how can this responsibility be delegated to workspace Admins?"}, {"role": "assistant", "content": "Workspace level invites to an organization The default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization, as this operation can increase cost by adding seats.\nFor self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to\nthe organization as well as their specific workspace at the workspace level. Once this feature is enabled via the configuration option below, workspace Admins may add new users in the Workspace members tab under Settings > Workspaces.\nBoth of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before. Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspaceInvite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state). Admins may invite users for both cases at the same time. Configuration HelmDockerconfig:    workspaceScopeOrgInvitesEnabled: true# In your .env fileFF_WORKSPACE_SCOPE_ORG_INVITES_ENABLED=\"true\"    "}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to generate Clickhouse statistics for troubleshooting a self-hosted instance of LangSmith?"}, {"role": "assistant", "content": "Generating Clickhouse Stats As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency. This command will generate a CSV that can be shared with the LangChain team."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools and credentials are required to connect to the Clickhouse database for running the get_clickhouse_stats script?"}, {"role": "assistant", "content": "Prerequisites Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/Clickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the Clickhouse database from the machine you will be running the get_clickhouse_stats script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should be run to generate Clickhouse statistics and save them to a CSV file?"}, {"role": "assistant", "content": "Running the clickhouse stats generation script Run the following command to run the stats generation script: sh get_clickhouse_stats.sh <clickhouse_url> --output path/to/file.csv For example, if you are using the bundled version with port-forwarding, the command would look like: sh get_clickhouse_stats.sh \"clickhouse://default:password@localhost:8123/default\" --output clickhouse_stats.csv and after running this command you should see a file, clickhouse_stats.csv, has been created with Clickhouse statistics."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens when you add a webhook URL on an automation action?"}, {"role": "assistant", "content": "Set up webhook notifications for rules When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is included in the webhook payload sent to your endpoint?"}, {"role": "assistant", "content": "Webhook payload The payload we send to your webhook endpoint contains \"rule_id\" this is the ID of the automation that sent this payload\"start_time\" and \"end_time\" these are the time boundaries where we found matching runs\"runs\" this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.\"feedback_stats\" this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below. \"feedback_stats\": {    \"about_langchain\": {        \"n\": 1,        \"avg\": 0.0,        \"show_feedback_arrow\": true,        \"values\": {}    },    \"category\": {        \"n\": 0,        \"avg\": null,        \"show_feedback_arrow\": true,        \"values\": {            \"CONCEPTUAL\": 1        }    },    \"user_score\": {        \"n\": 2,        \"avg\": 0.0,        \"show_feedback_arrow\": false,        \"values\": {}    },    \"vagueness\": {        \"n\": 1,        \"avg\": 0.0,        \"show_feedback_arrow\": true,        \"values\": {}    }}, fetching from S3 URLsDepending on how recent your runs are, the inputs_s3_urls and outputs_s3_urls fields may contain S3 URLs to the actual data instead of the data itself.The inputs and outputs can be fetched by the ROOT.presigned_url provided in inputs_s3_urls and outputs_s3_urls respectively. This is an example of the entire payload we send to your webhook endpoint: {  \"rule_id\": \"d75d7417-0c57-4655-88fe-1db3cda3a47a\",  \"start_time\": \"2024-04-05T01:28:54.734491+00:00\",  \"end_time\": \"2024-04-05T01:28:56.492563+00:00\",  \"runs\": [    {      \"status\": \"success\",      \"is_root\": true,      \"trace_id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"dotted_order\": \"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"run_type\": \"tool\",      \"modified_at\": \"2024-04-05T01:28:54.145062\",      \"tenant_id\": \"2ebda79f-2946-4491-a9ad-d642f49e0815\",      \"end_time\": \"2024-04-05T01:28:54.085649\",      \"name\": \"Search\",      \"start_time\": \"2024-04-05T01:28:54.085646\",      \"id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"session_id\": \"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5\",      \"parent_run_ids\": [],      \"child_run_ids\": null,      \"direct_child_run_ids\": null,      \"total_tokens\": 0,      \"completion_tokens\": 0,      \"prompt_tokens\": 0,      \"total_cost\": null,      \"completion_cost\": null,      \"prompt_cost\": null,      \"first_token_time\": null,      \"app_path\": \"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809\",      \"in_dataset\": false,      \"last_queued_at\": null,      \"inputs\": null,      \"inputs_s3_urls\": null,      \"outputs\": null,      \"outputs_s3_urls\": null,      \"extra\": null,      \"events\": null,      \"feedback_stats\": null,      \"serialized\": null,      \"share_token\": null    }  ]}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a recommended method to enhance the security of a webhook URL?"}, {"role": "assistant", "content": "Webhook Security We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications. An example would be https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure specific HTTP headers for a webhook?"}, {"role": "assistant", "content": "Webhook custom HTTP headers If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the Headers option next to the URL field and add your headers. noteHeaders are stored in encrypted format."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens if a webhook endpoint returns a 4xx status code?"}, {"role": "assistant", "content": "Webhook Delivery When delivering events to your webhook endpoint we follow these guidelines If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.Anything your endpoint returns in the body will be ignored"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to set up a Modal account and install the Modal SDK?"}, {"role": "assistant", "content": "Setup For an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here. First, create a Modal account. Then, locally install the Modal SDK: pip install modal To finish setting up your account, run the command: modal setup and follow the instructions"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the name of the secret that needs to be created in Modal for LangSmith authentication?"}, {"role": "assistant", "content": "Secrets Next, you will need to set up some secrets in Modal. First, LangSmith will need to authenticate to Modal by passing in a secret.\nThe easiest way to do this is to pass in a secret in the query parameters.\nTo validate this secret, we will need to add a secret in Modal to validate it.\nWe will do that by creating a Modal secret.\nYou can see instructions for secrets here.\nFor this purpose, let's call our secret ls-webhook and have it set an environment variable with the name LS_WEBHOOK. We can also set up a LangSmith secret - luckily there is already an integration template for this!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the Python file created in the service described?"}, {"role": "assistant", "content": "Service After that, you can create a Python file that will serve as your endpoint.\nAn example is below, with comments explaining what is going on: from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub(\"auth-example\", image=Image.debian_slim().pip_install(\"langsmith\"))@stub.function(    secrets=[Secret.from_name(\"ls-webhook\"), Secret.from_name(\"my-langsmith-secret\")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method=\"POST\")# We set up a `secret` query parameterdef f(data: dict, secret: str = Query(...)):    # You can import dependencies you don't have locally inside Modal funxtions    from langsmith import Client    # First, we validate the secret key we pass    import os    if secret != os.environ[\"LS_WEBHOOK\"]:        raise HTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=\"Incorrect bearer token\",            headers={\"WWW-Authenticate\": \"Bearer\"},        )    # This is where we put the logic for what should happen inside this webhook    ls_client = Client()    runs = data[\"runs\"]    ids = [r[\"id\"] for r in runs]    feedback = list(ls_client.list_feedback(run_ids=ids))    for r, f in zip(runs, feedback):        try:            ls_client.create_example(                inputs=r[\"inputs\"],                outputs={\"output\": f.correction},                dataset_name=\"classifier-github-issues\",            )        except Exception:            raise ValueError(f\"{r} and {f}\")    # Function body    return \"success!\" We can now deploy this easily with modal deploy ... (see docs here). You should now get something like:  Created objects.  Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py  Created mount PythonPackage:langsmith  Created f => https://hwchase17--auth-example-f.modal.run App deployed! View Deployment: https://modal.com/apps/hwchase17/auth-example The important thing to remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.\nNOTE: this is NOT the final deployment URL, make sure not to accidentally use that."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How should the function URL be formatted when adding it as a webhook, including the secret key?"}, {"role": "assistant", "content": "Hooking it up We can now take the function URL we create above and add it as a webhook.\nWe have to remember to also pass in the secret key as a query parameter.\nPutting it all together, it should look something like: https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET} Replace {SECRET} with the secret key you created to access the Modal service."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can the monitoring charts for each tracing project be accessed in LangSmith?"}, {"role": "assistant", "content": "Use monitoring charts LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default time period set for viewing monitors?"}, {"role": "assistant", "content": "Change the time period You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view specific subsets of runs in the monitor tab?"}, {"role": "assistant", "content": "Slice data by metadata or tag By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\nThis can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you investigate the runs causing spikes in errors or latency when using a monitoring dashboard?"}, {"role": "assistant", "content": "Drill down into specific subsets Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does the technical reference for LangSmith cover?"}, {"role": "assistant", "content": "Reference Technical reference that covers components, APIs, and other aspects of LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the title of the API reference document?"}, {"role": "assistant", "content": "API reference LangSmith API Reference"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming language do the off-the-shelf evaluators in the LangChain SDK reference support?"}, {"role": "assistant", "content": "SDK reference LangChain off-the-shelf evaluators (Python only)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the relationship between cloud architecture and scalability in different regions?"}, {"role": "assistant", "content": "Architecture reference Cloud architecture and scalabilityRegions FAQ"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different data formats mentioned in the text?"}, {"role": "assistant", "content": "Data formats Run (span) data formatFeedback data formatTrace query syntaxFilter argumentsFilter query language"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different methods of authentication?"}, {"role": "assistant", "content": "Authentication and authorization Authentication methods"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to adjust the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of using basic authentication for login in LangSmith during the beta period?"}, {"role": "assistant", "content": "Email/password a.k.a. basic auth (beta) LangSmith supports login via username/password with a few limitations during the beta period: You cannot change an existing installation from basic auth mode to OIDC or vice versa - installations must be either one or the other. A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing None type installation (see below).Users must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the password requirements for the initial setup?"}, {"role": "assistant", "content": "Requirements and features There is a single Default organization that is provisioned during initial installation, and creating additional organizations is not supportedYour initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbolThere are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example: openssl rand -base64 32"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to the pre-existing workspace ID when migrating from None auth mode?"}, {"role": "assistant", "content": "Migrating from None auth Only supported in versions 0.7 and above. Migrating an installation from None auth mode replaces the single \"default\" user with a user with the configured credentials and keeps all existing resources.\nThe single pre-existing workspace ID post-migration remains 00000000-0000-0000-0000-000000000000, but everything else about the migrated installation is standard for a basic auth installation. To migrate, simply update your configuration as shown below and run helm upgrade (or docker-compose up) as usual."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to your users if you change the JWT secret in the configuration?"}, {"role": "assistant", "content": "Configuration noteChanging the JWT secret will log out your users HelmDockerconfig:  authType: mixed  basicAuth:    enabled: true    initialOrgAdminEmail: <YOUR EMAIL ADDRESS>    initialOrgAdminPassword: <PASSWORD>    jwtSecret: <SECRET># In your .env fileAUTH_TYPE=mixedBASIC_AUTH_ENABLED=trueINITIAL_ORG_ADMIN_EMAIL=<YOUR EMAIL ADDRESS>INITIAL_ORG_ADMIN_PASSWORD=<PASSWORD>BASIC_AUTH_JWT_SECRET=<SECRET> Once configured, you will see a login screen like the one below. You should be able to login with the initialOrgAdminEmail and initialOrgAdminPassword values, and your user will be auto-provisioned with role Organization Admin. See the admin guide for more details on organization roles."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps should you follow to share or unshare a trace publicly in LangSmith?"}, {"role": "assistant", "content": "Share or unshare a trace publicly cautionSharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.\nThis feature is only available in the cloud-hosted version of LangSmith. To share a trace publicly, simply click on the Share button in the upper right hand side of any trace view.\n This will open a dialog where you can copy the link to the trace. Shared traces will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the trace, but not edit it. To \"unshare\" a trace, either Click on Unshare by click on Public in the upper right hand corner of any publicly shared trace, then Unshare in the dialog.\nNavigate to your organization's list of publicly shared traces, either by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the trace you want to unshare.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What function should be used to run pairwise evaluations of existing experiments in LangSmith?"}, {"role": "assistant", "content": "Run pairwise evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function\nwith two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the required SDK versions for using the evaluate_comparative function and what arguments does it accept?"}, {"role": "assistant", "content": "Use theevaluate_comparativefunction notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the inputs and outputs required for configuring pairwise evaluators, and how should the output dictionary be structured?"}, {"role": "assistant", "content": "Configure inputs and outputs for pairwise evaluators Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with\nruns[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent \"both equally bad\" or both to 1 for \"both equally good\". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for using LLM-based pairwise evaluators to compare two AI assistant responses?"}, {"role": "assistant", "content": "Compare two experiments with LLM-based pairwise evaluators The following example uses a prompt\nwhich asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.\nThe prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=\"gpt-4\")        runnable = prompt | model    response = runnable.invoke({        \"question\": example.inputs[\"question\"],        \"answer_a\": runs[0].outputs[\"output\"] if runs[0].outputs is not None else \"N/A\",        \"answer_b\": runs[1].outputs[\"output\"] if runs[1].outputs is not None else \"N/A\",    })    score = response[\"Preference\"]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {\"key\": \"ranked_preference\", \"scores\": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [\"my-experiment-name-1\", \"my-experiment-name-2\"],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.\nimport type { Run, Example } from \"langsmith\";import { evaluateComparative } from \"langsmith/evaluation\";import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());import { z } from \"zod\";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(\"Expected at least two runs\");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? \"N/A\",    answer_b: runB?.outputs?.output ?? \"N/A\",  };    const output = await openai.chat.completions.create({    model: \"gpt-4-turbo\",    messages: [      {        role: \"system\",        content: [          \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.\",          \"You should choose the assistant that follows the user's instructions and answers the user's question better.\",          \"Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.\",          \"Begin your evaluation by comparing the two responses and provide a short explanation.\",          \"Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\",          \"Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\",        ].join(\" \"),      },      {        role: \"user\",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(\"\\n\\n\"),      },    ],    tool_choice: {      type: \"function\",      function: { name: \"Score\" },    },    tools: [      {        type: \"function\",        function: {          name: \"Score\",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output \"1\" if Assistant A answer is better based upon the factors above.`,            `Output \"2\" if Assistant B answer is better based upon the factors above.`,            `Output \"0\" if it is a tie.`,          ].join(\" \"),          parameters: {            type: \"object\",            properties: {              Preference: {                type: \"integer\",                description: \"Which assistant answer is preferred?\",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: \"ranked_preference\", scores };}await evaluateComparative([\"earnest-name-40\", \"reflecting-pump-91\"], {  evaluators: [evaluatePairwise],});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the Comparison View of pairwise experiments?"}, {"role": "assistant", "content": "View pairwise experiments Navigate to the \"Pairwise Experiments\" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What components make up the LangSmith application, and how can it be run in a controlled cloud environment?"}, {"role": "assistant", "content": "Architectural overview Enterprise License RequiredSelf-Hosted LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. LangSmith can be run via Kubernetes (recommended) or Docker in a Cloud environment that you control. The LangSmith application consists of several components including 5 LangSmith servers and 3 stateful services: LangSmith FrontendLangSmith BackendLangSmith Platform BackendLangSmith PlaygroundLangSmith QueueClickHousePostgresRedis To access the LangSmith UI and send API requests, you will need to expose the LangSmith Frontend service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended storage service configuration for LangSmith in a production setting?"}, {"role": "assistant", "content": "Storage Services noteLangSmith Self-Hosted will bundle all storage services by default. LangSmith can be configured to use external versions of all storage services.\nIn a production setting, we strongly recommend using external Storage Services."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database management system is ClickHouse and what is its primary use for LangSmith?"}, {"role": "assistant", "content": "ClickHouse ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database system is PostgreSQL and what is its primary use in LangSmith?"}, {"role": "assistant", "content": "PostgreSQL PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads LangSmith uses Postgres as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of database is Redis and how does it enhance performance for operations like caching?"}, {"role": "assistant", "content": "Redis Redis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching. LangSmith uses Redis to back queuing/caching operations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What role does Nginx play in the LangSmith frontend?"}, {"role": "assistant", "content": "LangSmith Frontend The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the primary function of the LangSmith backend in relation to API requests?"}, {"role": "assistant", "content": "LangSmith Backend The backend is the primary entrypoint for API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and sdk, preparing traces for ingestion, and supporting the hub API."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functions does the LangSmith Queue perform to manage incoming traces and feedback?"}, {"role": "assistant", "content": "LangSmith Queue The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the primary function of the LangSmith platform backend?"}, {"role": "assistant", "content": "LangSmith Platform Backend The platform backend is an internal service that primarily handles authentication and other high-volume tasks. The user should not need to interact with this service directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LangSmith Playground service?"}, {"role": "assistant", "content": "LangSmith Playground The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does LangSmith offer to help improve the performance of applications through prompts?"}, {"role": "assistant", "content": "Prompts Writing good prompts is key to getting the best performance out of your applications. LangSmith provides ways to create, test, and manage prompts."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three types of prompt templates supported in LangChain?"}, {"role": "assistant", "content": "Prompt types We support three types of prompt templates: StringPromptTemplateChatPromptTemplateStructuredPromptTemplate For detailed information about these templates, please refer to the LangChain documentation. In LangSmith, you can create prompts using the Playground. From the prompt view in the Playground, you can select either \"Chat-style prompt\" or \"Instruct-style prompt\" to get started. Chat-style prompt Chat prompts are used for chat-style models that accept a list of messages as an input and respond with an assistant message. A chat-style prompt is represented in LangSmith as a ChatPromptTemplate, which can contain multiple messages, each with prompt variables. You can also specify an output schema which is represented in LangSmith as a StructuredPromptTemplate. Instruct-style prompt An instruct-style prompt is represented as a StringPromptTemplate that gets formatted to a single string input for your model."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of template formats supported?"}, {"role": "assistant", "content": "Template formats We support two types of template formats: f-string and mustache. You can switch between these formats when editing prompts in the Playground."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is an f-string in Python and how is it used?"}, {"role": "assistant", "content": "F-string F-strings are a Python-specific string formatting method that allows you to embed expressions inside string literals, using curly braces {}.\nHere's an example of an f-string template: Hello, {name}!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some features and examples of Mustache template syntax?"}, {"role": "assistant", "content": "Mustache Mustache is a template syntax that allows you to embed variables inside double curly braces {{}}. NotePlease see the Mustache documentation for more detailed information on how to use Mustache templates. Here's an example of a mustache template: Hello, {{name}}! Mustache is more robust than f-strings since it supports more complex logic. You can use conditionals, loops, and access nested keys in mustache templates. Conditional Hello, {{#name}}{{name}}{{/name}}{{^name}}world{{/name}}! The template will output \"Hello, Bob!\" if \"Bob\" is provided as the name variable, and \"Hello, world!\" if the name variable is not provided. Loop {{#names}}{{name}}{{/names}} input: {  \"names\": [{ \"name\": \"Alice\" }, { \"name\": \"Bob\" }, { \"name\": \"Charlie\" }]} output: \"AliceBobCharlie\" Loop with nesting {{#people}}{{name}} is {{age}} years old. {{/people}} input: {  \"people\": [    { \"name\": \"Alice\", \"age\": 30 },    { \"name\": \"Bob\", \"age\": 25 },    { \"name\": \"Charlie\", \"age\": 35 }  ]} output: \"Alice is 30 years old. Bob is 25 years old. Charlie is 35 years old.\" Dot notation {{person.name}} is {{person.age}} years old. input: {  \"person\": {    \"name\": \"Alice\",    \"age\": 30  }} output: \"Alice is 30 years old.\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangGraph for tracing agentic workflows?"}, {"role": "assistant", "content": "Trace withLangGraph(Python and JS/TS) LangSmith smoothly integrates with LangGraph (Python and JS)\nto help you trace agentic workflows, whether you're using LangChain modules or other SDKs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What do you need to set in order to enable tracing when using LangChain modules within LangGraph?"}, {"role": "assistant", "content": "With LangChain If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing. This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangGraph library and the OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "0. Installation Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langgraphyarn add @langchain/openai @langchain/langgraphnpm install @langchain/openai @langchain/langgraphpnpm add @langchain/openai @langchain/langgraph"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the weather in San Francisco according to the LangChain runnable?"}, {"role": "assistant", "content": "2. Log a trace Once you've set up your environment, you can call LangChain runnables as normal.\nLangSmith will infer the proper tracing config: PythonTypeScriptfrom typing import Literalfrom langchain_core.messages import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain_core.tools import toolfrom langgraph.graph import StateGraph, MessagesStatefrom langgraph.prebuilt import ToolNode@tooldef search(query: str):    \"\"\"Call to surf the web.\"\"\"    if \"sf\" in query.lower() or \"san francisco\" in query.lower():        return \"It's 60 degrees and foggy.\"    return \"It's 90 degrees and sunny.\"tools = [search]tool_node = ToolNode(tools)model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:    messages = state['messages']    last_message = messages[-1]    if last_message.tool_calls:        return \"tools\"    return \"__end__\"def call_model(state: MessagesState):    messages = state['messages']    # Invoking `model` will automatically infer the correct tracing context    response = model.invoke(messages)    return {\"messages\": [response]}workflow = StateGraph(MessagesState)workflow.add_node(\"agent\", call_model)workflow.add_node(\"tools\", tool_node)workflow.add_edge(\"__start__\", \"agent\")workflow.add_conditional_edges(    \"agent\",    should_continue,)workflow.add_edge(\"tools\", 'agent')app = workflow.compile()final_state = app.invoke(    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},    config={\"configurable\": {\"thread_id\": 42}})final_state[\"messages\"][-1].contentimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";import { tool } from \"@langchain/core/tools\";import { z } from \"zod\";import { ChatOpenAI } from \"@langchain/openai\";import { StateGraph, StateGraphArgs } from \"@langchain/langgraph\";import { ToolNode } from \"@langchain/langgraph/prebuilt\";interface AgentState {  messages: HumanMessage[];}const graphState: StateGraphArgs<AgentState>[\"channels\"] = {  messages: {    reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),  },};const searchTool = tool(async ({ query }: { query: string }) => {  if (query.toLowerCase().includes(\"sf\") || query.toLowerCase().includes(\"san francisco\")) {    return \"It's 60 degrees and foggy.\"  }  return \"It's 90 degrees and sunny.\"}, {  name: \"search\",  description:    \"Call to surf the web.\",  schema: z.object({    query: z.string().describe(\"The query to use in your search.\"),  }),});const tools = [searchTool];const toolNode = new ToolNode<AgentState>(tools);const model = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).bindTools(tools);function shouldContinue(state: AgentState) {  const messages = state.messages;  const lastMessage = messages[messages.length - 1] as AIMessage;  if (lastMessage.tool_calls?.length) {    return \"tools\";  }  return \"__end__\";}async function callModel(state: AgentState) {  const messages = state.messages;  // Invoking `model` will automatically infer the correct tracing context  const response = await model.invoke(messages);  return { messages: [response] };}const workflow = new StateGraph<AgentState>({ channels: graphState })  .addNode(\"agent\", callModel)  .addNode(\"tools\", toolNode)  .addEdge(\"__start__\", \"agent\")  .addConditionalEdges(\"agent\", shouldContinue)  .addEdge(\"tools\", \"agent\");const app = workflow.compile();const finalState = await app.invoke(  { messages: [new HumanMessage(\"what is the weather in sf\")] },  { configurable: { thread_id: \"42\" } });finalState.messages[finalState.messages.length - 1].content; An example trace from running the above code looks like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to ensure that other SDKs or custom functions are properly integrated with LangGraph for tracing?"}, {"role": "assistant", "content": "Without LangChain If you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately\n(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).\nIf you do so, LangSmith will automatically nest traces from those wrapped methods. Here's an example. You can also see this page for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries need to be installed for using LangGraph with OpenAI in Python and JS?"}, {"role": "assistant", "content": "0. Installation Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below). pipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after setting up your environment to log a trace for custom functions or SDKs?"}, {"role": "assistant", "content": "2. Log a trace Once you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\nLangSmith will then infer the proper tracing config: PythonTypeScriptimport jsonimport openaiimport operatorfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaifrom typing import Annotated, Literal, TypedDictfrom langgraph.graph import StateGraphclass State(TypedDict):    messages: Annotated[list, operator.add]tool_schema = {    \"type\": \"function\",    \"function\": {        \"name\": \"search\",        \"description\": \"Call to surf the web.\",        \"parameters\": {            \"type\": \"object\",            \"properties\": {\"query\": {\"type\": \"string\"}},            \"required\": [\"query\"],        },    },}# Decorating the tool function will automatically trace it with the correct context@traceable(run_type=\"tool\", name=\"Search Tool\")def search(query: str):    \"\"\"Call to surf the web.\"\"\"    if \"sf\" in query.lower() or \"san francisco\" in query.lower():        return \"It's 60 degrees and foggy.\"    return \"It's 90 degrees and sunny.\"tools = [search]def call_tools(state):    function_name_to_function = {\"search\": search}    messages = state[\"messages\"]    tool_call = messages[-1][\"tool_calls\"][0]    function_name = tool_call[\"function\"][\"name\"]    function_arguments = tool_call[\"function\"][\"arguments\"]    arguments = json.loads(function_arguments)    function_response = function_name_to_function[function_name](**arguments)    tool_message = {        \"tool_call_id\": tool_call[\"id\"],        \"role\": \"tool\",        \"name\": function_name,        \"content\": function_response,    }    return {\"messages\": [tool_message]}wrapped_client = wrap_openai(openai.Client())def should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:    messages = state[\"messages\"]    last_message = messages[-1]    if last_message[\"tool_calls\"]:        return \"tools\"    return \"__end__\"def call_model(state: State):    messages = state[\"messages\"]    # Calling the wrapped client will automatically infer the correct tracing context    response = wrapped_client.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", tools=[tool_schema]    )    raw_tool_calls = response.choices[0].message.tool_calls    tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []    response_message = {        \"role\": \"assistant\",        \"content\": response.choices[0].message.content,        \"tool_calls\": tool_calls,    }    return {\"messages\": [response_message]}workflow = StateGraph(State)workflow.add_node(\"agent\", call_model)workflow.add_node(\"tools\", call_tools)workflow.add_edge(\"__start__\", \"agent\")workflow.add_conditional_edges(    \"agent\",    should_continue,)workflow.add_edge(\"tools\", 'agent')app = workflow.compile()final_state = app.invoke(    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})final_state[\"messages\"][-1][\"content\"]Note: The below example requires langsmith>=0.1.39 and @langchain/langgraph>=0.0.31\nimport OpenAI from \"openai\";import { StateGraph } from \"@langchain/langgraph\";import { wrapOpenAI } from \"langsmith/wrappers/openai\";import { traceable } from \"langsmith/traceable\";type GraphState = {  messages: OpenAI.ChatCompletionMessageParam[];};const wrappedClient = wrapOpenAI(new OpenAI({}));const toolSchema: OpenAI.ChatCompletionTool = {  type: \"function\",  function: {    name: \"search\",    description: \"Use this tool to query the web.\",    parameters: {      type: \"object\",      properties: {        query: {          type: \"string\",        },      },      required: [\"query\"],    }  }};// Wrapping the tool function will automatically trace it with the correct contextconst search = traceable(async ({ query }: { query: string }) => {  if (    query.toLowerCase().includes(\"sf\") ||    query.toLowerCase().includes(\"san francisco\")  ) {    return \"It's 60 degrees and foggy.\";  }  return \"It's 90 degrees and sunny.\"}, { run_type: \"tool\", name: \"Search Tool\" });const callTools = async ({ messages }: GraphState) => {  const mostRecentMessage = messages[messages.length - 1];  const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;  if (toolCalls === undefined || toolCalls.length === 0) {    throw new Error(\"No tool calls passed to node.\");  }  const toolNameMap = {    search,  };  const functionName = toolCalls[0].function.name;  const functionArguments = JSON.parse(toolCalls[0].function.arguments);  const response = await toolNameMap[functionName](functionArguments);  const toolMessage = {    tool_call_id: toolCalls[0].id,    role: \"tool\",    name: functionName,    content: response,  }  return { messages: [toolMessage] };}const callModel = async ({ messages }: GraphState) => {  // Calling the wrapped client will automatically infer the correct tracing context  const response = await wrappedClient.chat.completions.create({    messages,    model: \"gpt-4o-mini\",    tools: [toolSchema],  });  const responseMessage = {    role: \"assistant\",    content: response.choices[0].message.content,    tool_calls: response.choices[0].message.tool_calls ?? [],  };  return { messages: [responseMessage] };}const shouldContinue = ({ messages }: GraphState) => {  const lastMessage =    messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;  if (    lastMessage?.tool_calls !== undefined &&    lastMessage?.tool_calls.length > 0  ) {    return \"tools\";  }  return \"__end__\";}const workflow = new StateGraph<GraphState>({  channels: {    messages: {      reducer: (a: any, b: any) => a.concat(b),    }  }});const graph = workflow  .addNode(\"model\", callModel)  .addNode(\"tools\", callTools)  .addEdge(\"__start__\", \"model\")  .addConditionalEdges(\"model\", shouldContinue, {    tools: \"tools\",    __end__: \"__end__\",  })  .addEdge(\"tools\", \"model\")  .compile();await graph.invoke({  messages: [{ role: \"user\", content: \"what is the weather in sf\" }]}); An example trace from running the above code looks like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the maximum data retention period for traces introduced by LangSmith in May 2024?"}, {"role": "assistant", "content": "Data Retention In May 2024, LangSmith introduced a maximum data retention period on traces of 400 days. In June 2024, LangSmith introduced\na new data retention based pricing model where customers can configure a shorter data retention period on traces in exchange\nfor savings up to 10x. On this page, we'll go through how data retention works and is priced in LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What do data privacy regulations like GDPR and CCPA require organizations to do with personal data once it is no longer necessary?"}, {"role": "assistant", "content": "Privacy Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data\nonce it's no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with\nsuch regulations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith's pricing vary based on data retention for traces?"}, {"role": "assistant", "content": "Cost LangSmith charges less for traces that have low data retention. See our tutorial on how to optimize spend\nfor details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the characteristics of the two tiers of traces offered by LangSmith in terms of price and retention period?"}, {"role": "assistant", "content": "The basics LangSmith now has two tiers of traces based on Data Retention with the following characteristics: BaseExtendedPrice$.50 / 1k traces$5 / 1k tracesRetention Period14 days400 days"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to user data associated with a trace after the retention period ends?"}, {"role": "assistant", "content": "Data deletion after retention ends After the specified retention period, traces are no longer accessible via the runs table or API. All user data associated\nwith the trace (e.g. inputs and outputs) is deleted from our internal systems within a day thereafter. Some metadata\nassociated with each trace may be retained indefinitely for analytics and billing purposes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the conditions under which a trace will automatically upgrade to extended tier, and how does this affect the cost?"}, {"role": "assistant", "content": "Data retention auto-upgrades cautionAuto upgrades can have an impact on your bill. Please read this section carefully to fully understand your\nestimated LangSmith tracing costs. When you use certain features with base tier traces, their data retention will be automatically upgraded to\nextended tier. This will increase both the retention period, and the cost of the trace. The complete list of scenarios in which a trace will upgrade when: Feedback is added to any run on the traceAn Annotation Queue receives any run from the traceA Run Rule matches any run within a trace Why auto-upgrade traces? We have two reasons behind the auto-upgrade model for tracing: We think that traces that match any of these conditions are fundamentally more interesting than other traces, and\ntherefore it is good for users to be able to keep them around longer.We philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully.\nWe think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction\nare charged at a higher rate. If you have questions or concerns about our pricing model, please feel free to reach out to support@langchain.dev and let us know your thoughts!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does the data retention period of traces impact the functionality of Annotation Queues, Run Rules, and Feedback Traces?"}, {"role": "assistant", "content": "How does data retention affect downstream features? Annotation Queues, Run Rules, and Feedback Traces that use these features will be auto-upgraded. Monitoring The monitoring tab will continue to work even after a base tier trace's data retention period ends. It is powered by\ntrace metadata that exists for >30 days, meaning that your monitoring graphs will continue to stay accurate even on\nbase tier traces. Datasets Datasets have an indefinite data retention period. Restated differently, if you add a trace's inputs and outputs to a dataset,\nthey will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets\nfeature."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What two metrics are included in the LangSmith invoice for billing?"}, {"role": "assistant", "content": "Billable metrics On your LangSmith invoice, you will see two metrics that we charge for: LangSmith Traces (Base Charge)LangSmith Traces (Extended Data Retention Upgrades). The first metric includes all traces, regardless of tier. The second metric just counts the number of extended retention traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Why is it important to measure all traces and upgrades instead of just the base and extended traces for billing purposes?"}, {"role": "assistant", "content": "Why measure all traces + upgrades instead of base and extended traces? A natural question to ask when considering our pricing is why not just show the number of base tier and extended tier\ntraces directly on the invoice? While we understand this would be more straightforward, it doesn't fit trace upgrades properly. Consider a\nbase tier trace that was recorded on June 30, and upgraded to extended tier on July 3. The base tier\ntrace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore,\nwe need to be able to measure these two events independently to properly bill our customers. If your trace was recorded as an extended retention trace, then the base and extended metrics will both be recorded\nwith the same timestamp."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the cost of an extended retention trace compared to a base tier trace?"}, {"role": "assistant", "content": "Cost breakdown The Base Charge for a trace is .05 per trace. We priced the upgrade such that an extended retention trace\ncosts 10x the price of a base tier trace (.50 per trace) including both metrics. Thus, each upgrade costs .45."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the tutorial about?"}, {"role": "assistant", "content": "Related content Tutorial on how to optimize spend"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in generating synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you are using LangChain in relation to logging traces to LangSmith?"}, {"role": "assistant", "content": "Annotate code for tracing There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces using the @traceable decorator in Python and the traceable function in TypeScript?"}, {"role": "assistant", "content": "Use@traceable/traceable LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\nfrom langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful assistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What's a good name for a store that sells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(\"colorful socks\")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.\nimport { traceable } from \"langsmith/traceable\";import OpenAI from \"openai\";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: \"system\" as const,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: \"formatPrompt\" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: messages,            temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: \"parseOutput\" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(\"colorful socks\");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: \"runPipeline\" });await runPipeline();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces when using the wrap_openai or wrapOpenAI methods?"}, {"role": "assistant", "content": "Wrap the OpenAI client The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!\nThe wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"@traceable(name=\"Chat Pipeline\")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(\"Can you summarize this morning's meetings?\")import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return \"During this morning's meeting, we solved all world conflict.\";}, { name: \"Retrieve Context\", run_type: \"tool\" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: \"system\",            content:                \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",        },        { role: \"user\", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: \"Chat Pipeline\" });await chatPipeline(\"Can you summarize this morning's meetings?\");"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the RunTree API in LangSmith?"}, {"role": "assistant", "content": "Use theRunTreeAPI Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually\ncreate runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not\nnecessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \"Can you summarize this morning's meetings?\"# Create a top-level runpipeline = RunTree(    name=\"Chat Pipeline\",    run_type=\"chain\",    inputs={\"question\": question})# This can be retrieved in a retrieval stepcontext = \"During this morning's meeting, we solved all world conflict.\"messages = [    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}]# Create a child runchild_llm_run = pipeline.create_child(    name=\"OpenAI Call\",    run_type=\"llm\",    inputs={\"messages\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from \"openai\";import { RunTree } from \"langsmith\";// This can be a user input to your appconst question = \"Can you summarize this morning's meetings?\";const pipeline = new RunTree({    name: \"Chat Pipeline\",    run_type: \"chain\",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = \"During this morning's meeting, we solved all world conflict.\";const messages = [    { role: \"system\", content: \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { role: \"user\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: \"OpenAI Call\",    run_type: \"llm\",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using the trace context manager in Python?"}, {"role": "assistant", "content": "Use thetracecontext manager (Python only) In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {\"input\": \"Can you summarize this morning's meetings?\"}with trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:    output = chat_pipeline(\"Can you summarize this morning's meetings?\")    rt.end(outputs={\"output\": output})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of annotation queues in LangSmith?"}, {"role": "assistant", "content": "Use annotation queues Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.\nWhile you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in creating an annotation queue and what settings can be configured for multiple annotators?"}, {"role": "assistant", "content": "Create an annotation queue To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.\nThen click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.\nYou can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as \"Done\" for it to be removed from the queue. If you check \"All workspace members review each run,\" then a run will remain in the queue until all workspace members have marked it \"Done\".Enable reservations on runs: We recommend enabling reservations.\nThis will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified \"reservation length\". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it \"Done\", the reservation will expire after the specified \"reservation length\".\nThe run is then released back into the queue and can be reserved by another reviewer. noteClicking \"Requeue at end\" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you assign runs to an annotation queue?"}, {"role": "assistant", "content": "Assign runs to an annotation queue To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.\nSelect multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.\nSet up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.\nTo learn more about how to capture user feedback from your LLM application, follow this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you review runs in an annotation queue?"}, {"role": "assistant", "content": "Review runs in an annotation queue To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.\nThen click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.\nYou can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to \"View run\". The keyboard shortcuts shown can help streamline the review process."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a recommended reading resource before setting up an organization?"}, {"role": "assistant", "content": "Set up an organization Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must you take to create a separate organization for collaboration?"}, {"role": "assistant", "content": "Create an organization When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.\nShared organizations require a credit card before they can be used. You will need to set up billing to proceed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you navigate between workspaces to access resources in LangSmith?"}, {"role": "assistant", "content": "Manage and navigate workspaces Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.\nTo navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What actions can you perform to manage users in your shared organization?"}, {"role": "assistant", "content": "Manage users Manage membership in your shared organization in the Settings page Members and roles tab.\nHere you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in access and permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text does the dataset created for evaluating the LLM application contain?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on specific splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionality has been added to help users construct queries in natural language?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you stop tracing your runs in LangChain?"}, {"role": "assistant", "content": "Toggle tracing on and off noteThis section is only relevant for users who areUsing @traceable/traceableWrapping the OpenAI clientUsing the trace environment variableUsing LangChain If you've decided you no longer want to trace your runs, you can unset the LANGCHAIN_TRACING_V2 environment variable. Traces will no longer be logged to LangSmith.\nNote that this currently does not affect the RunTree objects or API users, as these are meant to be low-level and not affected by the tracing toggle."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Python client in Langsmith?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring in the provided code examples?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by metadata in a dataset?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the core components of LangSmith evaluation for AI applications?"}, {"role": "assistant", "content": "Evaluation The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of a single data point within a dataset used in the LangSmith evaluation workflow?"}, {"role": "assistant", "content": "Datasets Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some methods for creating datasets for evaluation?"}, {"role": "assistant", "content": "Creating datasets There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.\nFrom building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,\nand what \"good\" responses may be.\nYou probably want to cover a few different common edge cases or situations you can imagine.\nEven 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?\nThis information can be valuable to capture and store in datasets. This allows you to test against these\nuse cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?\nThere are a few heuristics you can follow.\nIf possible - try to collect end user feedback. You can then see which datapoints got negative feedback.\nThat is super valuable! These are spots where your application did not perform well.\nYou should add these to your dataset to test against in the future. You can also use other heuristics\nto identify \"interesting\" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.\nIt's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.\nThis can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three distinct dataset types offered by LangSmith and their specific purposes?"}, {"role": "assistant", "content": "Types of datasets LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:\"Inputs\" and \"outputs\" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating \"completion\" style language models.The \"inputs\" dictionary contains a single \"input\" key mapped to the prompt string.The \"outputs\" dictionary contains a single \"output\" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured \"chat\" messages as inputs and outputs.The \"inputs\" dictionary contains a single \"input\" key mapped to a list of serialized chat messagesThe \"outputs\" dictionary contains a single \"output\" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of partitioning a dataset into different splits during evaluation?"}, {"role": "assistant", "content": "Partitioning datasets When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of the EvaluationResult returned by evaluators in LangSmith?"}, {"role": "assistant", "content": "Evaluators Evaluators are functions in LangSmith that score how well your application performs on a particular example.\nEvaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate human evaluation of LLM application outputs?"}, {"role": "assistant", "content": "Human Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are heuristic evaluators and how do they function in evaluating outputs?"}, {"role": "assistant", "content": "Heuristic Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of evaluations that LLM-as-judge evaluators can perform on system output?"}, {"role": "assistant", "content": "LLM-as-judge LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When is pairwise evaluation particularly useful?"}, {"role": "assistant", "content": "Pairwise Pairwise evaluators pick the better of two task outputs based upon some criteria.\nThis can use either a heuristic (\"which response is longer\"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.\nThis can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different scoring functions that evaluators can use to assess application performance on datasets?"}, {"role": "assistant", "content": "Applying evaluations We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of unit tests in software development?"}, {"role": "assistant", "content": "Unit Tests Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.\nThis means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).\nThese types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)\nto cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of regression testing in application development?"}, {"role": "assistant", "content": "Regression Testing Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.\nThey are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of back-testing in evaluating new model versions?"}, {"role": "assistant", "content": "Back-testing Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.\nAnthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.\nThen compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Why might pairwise evaluations be considered a more stable scoring approach than assigning individual scores?"}, {"role": "assistant", "content": "Pairwise-testing It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the main difference between online evaluation and offline evaluation in the context of application testing?"}, {"role": "assistant", "content": "Online Evaluation Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using repetitions in LangSmith evaluations?"}, {"role": "assistant", "content": "Experiment Configurations LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will be discussed regarding specific, popular LLM applications?"}, {"role": "assistant", "content": "Evaluating Specific LLM Applications Below, we will discuss evaluation of a few specific, popular LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three components that LLM-powered autonomous agents combine to generate responses?"}, {"role": "assistant", "content": "Agents LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.\nNote that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the advantages and disadvantages of evaluating an agent's final response based on its overall performance on a task?"}, {"role": "assistant", "content": "Evaluating an agent's final response One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits and downsides of evaluating a single step of an agent?"}, {"role": "assistant", "content": "Evaluating a single step of an agent Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different methods for evaluating an agent's trajectory and their respective strengths and weaknesses?"}, {"role": "assistant", "content": "Evaluating an agent's trajectory Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an \"exact\" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the \"exact\" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of \"incorrect\" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some best practices for improving the reliability and cost-effectiveness of agents using LLMs?"}, {"role": "assistant", "content": "Best practices Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is Retrieval Augmented Generation (RAG) and how does it enhance AI applications?"}, {"role": "assistant", "content": "Retrieval Augmented Generation (RAG) Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the significance of reference answers in evaluating RAG applications?"}, {"role": "assistant", "content": "Dataset When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two main options for evaluating RAG applications?"}, {"role": "assistant", "content": "Evaluator LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different approaches to applying RAG evaluation?"}, {"role": "assistant", "content": "Applying RAG Evaluation When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluations of answer faithfulness, helpfulness, correctness, and chain comparison in the RAG evaluation summary?"}, {"role": "assistant", "content": "RAG evaluation summary Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluation criteria used for assessing summarization in writing?"}, {"role": "assistant", "content": "Summarization Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components and considerations for evaluating Classification / Tagging when ground truth labels are available versus when they are not?"}, {"role": "assistant", "content": "Classification / Tagging Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the benefit of running experiments in the prompt playground compared to using the SDK?"}, {"role": "assistant", "content": "Run an evaluation from the prompt playground While you can kick off experiments easily using the sdk, as outlined here, it's often useful to run experiments directly in the prompt playground. This allows you to test your prompt / model configuration over a series of inputs to see how well it generalizes across different contexts or scenarios, without having to write any code."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must be taken to create and run an experiment in the prompt playground?"}, {"role": "assistant", "content": "Create an experiment in the prompt playground Navigate to the prompt playground by clicking on \"Prompts\" in the sidebar, then selecting a prompt from the list of available prompts or creating a new one.Select the \"Switch to dataset\" button to switch to the dataset you want to use for the experiment. Please note that the dataset keys of the dataset inputs must match the input variables of the prompt. In the below sections, note that the selected dataset has inputs with keys \"text\", which correctly match the input variable of the prompt. Also note that there is a max capacity of 15 inputs for the prompt playground.\nClick on the \"Start\" button or CMD+Enter to start the experiment. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. Note that you need to commit the prompt to the prompt hub before you can start the experiment to ensure it can be referenced in the experiment. The result for each input will be streamed and displayed inline for each input in the dataset.\nView the results by clicking on the \"View Experiment\" button at the bottom of the page. This will take you to the experiment details page where you can see the results of the experiment.Navigate back to the commit page by clicking on the \"View Commit\" button. This will take you back to the prompt page where you can make changes to the prompt and run more experiments. The \"View Commit\" button is available to all experiments that were run from the prompt playground. The experiment is prefixed with the prompt repository name, a unique identifier, and the date and time the experiment was run.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add evaluation scores to experiments?"}, {"role": "assistant", "content": "Add evaluation scores to the experiment You can add evaluation scores to experiments by binding an evaluator to the dataset, again without writing any code. You can also programmatically evaluate an existing experiment using the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to fetch a subset of examples from a dataset for evaluation?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on multiple splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name under which the trace is logged in the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components and examples of automation rules that can be set up in LangSmith?"}, {"role": "assistant", "content": "Set up automation rules While you can manually sift through and process production logs from our LLM application, it often becomes difficult as your application scales to more users.\nLangSmith provides a powerful feature called automations that allow you to trigger certain actions on your trace data.\nAt a high level, automations are defined by a filter, sampling rate, and action. Automation rules can trigger actions such as online evaluation, adding inputs/outputs of traces to a dataset, adding to an annotation queue, and triggering a webhook. An example of an automation you can set up can be \"trigger an online evaluation that grades on vagueness for all of my downvoted traces.\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps for creating an automation rule in LangSmith?"}, {"role": "assistant", "content": "Create a rule We will outline the steps for creating an automation rule in LangSmith below."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of rules that can be created and what actions are allowed for each?"}, {"role": "assistant", "content": "Step 1: Navigate to rule creation To create a rule, head click on Rules in the top right corner of any project details page, then scroll to the bottom and click on + Add Rule. Alternatively, you can access rules in settings by navigating to this link, click on + Add Rule, then Project Rule. noteThere are currently two types of rules you can create: Project Rule and Dataset Rule.Project Rule: This rule will apply to traces in the specified project. Actions allowed are adding to a dataset, adding to an annotation queue, running online evaluation, and triggering a webhook.Dataset Rule: This rule will apply to traces that are part of an experiment in the specified dataset. Actions allowed are only running an evaluator on the experiment results. To see this in action, you can follow this guide. Give your rule a name, for example \"my_rule\":"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a filter to filter traces in the project?"}, {"role": "assistant", "content": "Step 2: Define the filter You can create a filter as you normally would to filter traces in the project. For more information on filters, you can refer to this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you apply a new rule to past runs?"}, {"role": "assistant", "content": "(Optional) Step 3: Apply Rule to Past Runs When creating a new rule, you can apply the rule to past runs as well. To do this, select Apply to Past Runs checkbox and enter Backfill From date as the\nstart date to apply the rule. This will start from the Backfill From date and apply the run rules until it is caught up with the latest runs. Note that you will have to expand the date range for logs if you wanted to look at the progress of the backfill, see View logs for your automations for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of setting a sampling rate in automations?"}, {"role": "assistant", "content": "Step 4: Define the sampling rate You can specify a sampling rate (between 0 and 1) for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the four actions that can be taken with an automation rule?"}, {"role": "assistant", "content": "Step 5: Define the action There are four actions you can take with an automation rule: Add to dataset: Add the inputs and outputs of the trace to a dataset.Add to annotation queue: Add the trace to an annotation queue.Run online evaluation: Run an online evaluation on the trace. For more information on online evaluations, you can refer to this guide.Trigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.Extend data retention: Extends the data retention period on matching traces that use base retention.\nNote that all other rules will also extend data retention on matching traces through the\nauto-upgrade mechanism,\nbut this rule takes no additional action."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view logs for your automations and what information do they provide?"}, {"role": "assistant", "content": "View logs for your automations You can view logs for your automations by going to Settings -> Rules and click on the Logs button in any row. You can also get to logs by clicking on Rules in the top right hand corner of any project details page, then clicking on See Logs for any rule. Logs allow you to gain confidence that your rules are working as expected. You can now view logs that list all runs processed by a given rule for the past day. For rules that apply online evaluation scores, you can easily see the output score and navigate to the run. For rules that add runs as examples to datasets, you can view the example produced.\nIf a particular rule execution has triggered an error, you can view the error message by hovering over the error icon. By default, rule logs only show results for runs that occurred in the last day. To see results for older runs, you can select Last 1 day and enter the desired date range.\nWhen applying a rule to past runs, the processing will start from the start date and go forward, so this would be needed to view logs while the backfill is proceeding."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made to evaluator scores in LangSmith?"}, {"role": "assistant", "content": "Audit evaluator scores LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you edit feedback details in the comparison view?"}, {"role": "assistant", "content": "In the comparison view In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the \"edit\" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under \"Make correction\".\nIf you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples\nin place of the few_shot_explanation prompt variable."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the feedback details and corrections view in the runs table?"}, {"role": "assistant", "content": "In the runs table In the runs table, find the \"Feedback\" column and click on the feedback tag to bring up the feedback details. Again, click the \"edit\" icon on the right to bring up the corrections view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made using the SDK's update_feedback function?"}, {"role": "assistant", "content": "In the SDK Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      \"score\": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Langsmith Python client?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangGraph for tracing agentic workflows?"}, {"role": "assistant", "content": "Trace withLangGraph(Python and JS/TS) LangSmith smoothly integrates with LangGraph (Python and JS)\nto help you trace agentic workflows, whether you're using LangChain modules or other SDKs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What do you need to set in order to enable tracing when using LangChain modules within LangGraph?"}, {"role": "assistant", "content": "With LangChain If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing. This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangGraph library and the OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "0. Installation Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langgraphyarn add @langchain/openai @langchain/langgraphnpm install @langchain/openai @langchain/langgraphpnpm add @langchain/openai @langchain/langgraph"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the weather in San Francisco according to the LangChain runnable?"}, {"role": "assistant", "content": "2. Log a trace Once you've set up your environment, you can call LangChain runnables as normal.\nLangSmith will infer the proper tracing config: PythonTypeScriptfrom typing import Literalfrom langchain_core.messages import HumanMessagefrom langchain_openai import ChatOpenAIfrom langchain_core.tools import toolfrom langgraph.graph import StateGraph, MessagesStatefrom langgraph.prebuilt import ToolNode@tooldef search(query: str):    \"\"\"Call to surf the web.\"\"\"    if \"sf\" in query.lower() or \"san francisco\" in query.lower():        return \"It's 60 degrees and foggy.\"    return \"It's 90 degrees and sunny.\"tools = [search]tool_node = ToolNode(tools)model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:    messages = state['messages']    last_message = messages[-1]    if last_message.tool_calls:        return \"tools\"    return \"__end__\"def call_model(state: MessagesState):    messages = state['messages']    # Invoking `model` will automatically infer the correct tracing context    response = model.invoke(messages)    return {\"messages\": [response]}workflow = StateGraph(MessagesState)workflow.add_node(\"agent\", call_model)workflow.add_node(\"tools\", tool_node)workflow.add_edge(\"__start__\", \"agent\")workflow.add_conditional_edges(    \"agent\",    should_continue,)workflow.add_edge(\"tools\", 'agent')app = workflow.compile()final_state = app.invoke(    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},    config={\"configurable\": {\"thread_id\": 42}})final_state[\"messages\"][-1].contentimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";import { tool } from \"@langchain/core/tools\";import { z } from \"zod\";import { ChatOpenAI } from \"@langchain/openai\";import { StateGraph, StateGraphArgs } from \"@langchain/langgraph\";import { ToolNode } from \"@langchain/langgraph/prebuilt\";interface AgentState {  messages: HumanMessage[];}const graphState: StateGraphArgs<AgentState>[\"channels\"] = {  messages: {    reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),  },};const searchTool = tool(async ({ query }: { query: string }) => {  if (query.toLowerCase().includes(\"sf\") || query.toLowerCase().includes(\"san francisco\")) {    return \"It's 60 degrees and foggy.\"  }  return \"It's 90 degrees and sunny.\"}, {  name: \"search\",  description:    \"Call to surf the web.\",  schema: z.object({    query: z.string().describe(\"The query to use in your search.\"),  }),});const tools = [searchTool];const toolNode = new ToolNode<AgentState>(tools);const model = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).bindTools(tools);function shouldContinue(state: AgentState) {  const messages = state.messages;  const lastMessage = messages[messages.length - 1] as AIMessage;  if (lastMessage.tool_calls?.length) {    return \"tools\";  }  return \"__end__\";}async function callModel(state: AgentState) {  const messages = state.messages;  // Invoking `model` will automatically infer the correct tracing context  const response = await model.invoke(messages);  return { messages: [response] };}const workflow = new StateGraph<AgentState>({ channels: graphState })  .addNode(\"agent\", callModel)  .addNode(\"tools\", toolNode)  .addEdge(\"__start__\", \"agent\")  .addConditionalEdges(\"agent\", shouldContinue)  .addEdge(\"tools\", \"agent\");const app = workflow.compile();const finalState = await app.invoke(  { messages: [new HumanMessage(\"what is the weather in sf\")] },  { configurable: { thread_id: \"42\" } });finalState.messages[finalState.messages.length - 1].content; An example trace from running the above code looks like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to ensure that other SDKs or custom functions are properly integrated with LangGraph for tracing?"}, {"role": "assistant", "content": "Without LangChain If you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately\n(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).\nIf you do so, LangSmith will automatically nest traces from those wrapped methods. Here's an example. You can also see this page for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries need to be installed to use the LangGraph library and the OpenAI SDK for Python and JS?"}, {"role": "assistant", "content": "0. Installation Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below). pipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after setting up your environment to log a trace for custom functions or SDKs?"}, {"role": "assistant", "content": "2. Log a trace Once you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\nLangSmith will then infer the proper tracing config: PythonTypeScriptimport jsonimport openaiimport operatorfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaifrom typing import Annotated, Literal, TypedDictfrom langgraph.graph import StateGraphclass State(TypedDict):    messages: Annotated[list, operator.add]tool_schema = {    \"type\": \"function\",    \"function\": {        \"name\": \"search\",        \"description\": \"Call to surf the web.\",        \"parameters\": {            \"type\": \"object\",            \"properties\": {\"query\": {\"type\": \"string\"}},            \"required\": [\"query\"],        },    },}# Decorating the tool function will automatically trace it with the correct context@traceable(run_type=\"tool\", name=\"Search Tool\")def search(query: str):    \"\"\"Call to surf the web.\"\"\"    if \"sf\" in query.lower() or \"san francisco\" in query.lower():        return \"It's 60 degrees and foggy.\"    return \"It's 90 degrees and sunny.\"tools = [search]def call_tools(state):    function_name_to_function = {\"search\": search}    messages = state[\"messages\"]    tool_call = messages[-1][\"tool_calls\"][0]    function_name = tool_call[\"function\"][\"name\"]    function_arguments = tool_call[\"function\"][\"arguments\"]    arguments = json.loads(function_arguments)    function_response = function_name_to_function[function_name](**arguments)    tool_message = {        \"tool_call_id\": tool_call[\"id\"],        \"role\": \"tool\",        \"name\": function_name,        \"content\": function_response,    }    return {\"messages\": [tool_message]}wrapped_client = wrap_openai(openai.Client())def should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:    messages = state[\"messages\"]    last_message = messages[-1]    if last_message[\"tool_calls\"]:        return \"tools\"    return \"__end__\"def call_model(state: State):    messages = state[\"messages\"]    # Calling the wrapped client will automatically infer the correct tracing context    response = wrapped_client.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", tools=[tool_schema]    )    raw_tool_calls = response.choices[0].message.tool_calls    tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []    response_message = {        \"role\": \"assistant\",        \"content\": response.choices[0].message.content,        \"tool_calls\": tool_calls,    }    return {\"messages\": [response_message]}workflow = StateGraph(State)workflow.add_node(\"agent\", call_model)workflow.add_node(\"tools\", call_tools)workflow.add_edge(\"__start__\", \"agent\")workflow.add_conditional_edges(    \"agent\",    should_continue,)workflow.add_edge(\"tools\", 'agent')app = workflow.compile()final_state = app.invoke(    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})final_state[\"messages\"][-1][\"content\"]Note: The below example requires langsmith>=0.1.39 and @langchain/langgraph>=0.0.31\nimport OpenAI from \"openai\";import { StateGraph } from \"@langchain/langgraph\";import { wrapOpenAI } from \"langsmith/wrappers/openai\";import { traceable } from \"langsmith/traceable\";type GraphState = {  messages: OpenAI.ChatCompletionMessageParam[];};const wrappedClient = wrapOpenAI(new OpenAI({}));const toolSchema: OpenAI.ChatCompletionTool = {  type: \"function\",  function: {    name: \"search\",    description: \"Use this tool to query the web.\",    parameters: {      type: \"object\",      properties: {        query: {          type: \"string\",        },      },      required: [\"query\"],    }  }};// Wrapping the tool function will automatically trace it with the correct contextconst search = traceable(async ({ query }: { query: string }) => {  if (    query.toLowerCase().includes(\"sf\") ||    query.toLowerCase().includes(\"san francisco\")  ) {    return \"It's 60 degrees and foggy.\";  }  return \"It's 90 degrees and sunny.\"}, { run_type: \"tool\", name: \"Search Tool\" });const callTools = async ({ messages }: GraphState) => {  const mostRecentMessage = messages[messages.length - 1];  const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;  if (toolCalls === undefined || toolCalls.length === 0) {    throw new Error(\"No tool calls passed to node.\");  }  const toolNameMap = {    search,  };  const functionName = toolCalls[0].function.name;  const functionArguments = JSON.parse(toolCalls[0].function.arguments);  const response = await toolNameMap[functionName](functionArguments);  const toolMessage = {    tool_call_id: toolCalls[0].id,    role: \"tool\",    name: functionName,    content: response,  }  return { messages: [toolMessage] };}const callModel = async ({ messages }: GraphState) => {  // Calling the wrapped client will automatically infer the correct tracing context  const response = await wrappedClient.chat.completions.create({    messages,    model: \"gpt-4o-mini\",    tools: [toolSchema],  });  const responseMessage = {    role: \"assistant\",    content: response.choices[0].message.content,    tool_calls: response.choices[0].message.tool_calls ?? [],  };  return { messages: [responseMessage] };}const shouldContinue = ({ messages }: GraphState) => {  const lastMessage =    messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;  if (    lastMessage?.tool_calls !== undefined &&    lastMessage?.tool_calls.length > 0  ) {    return \"tools\";  }  return \"__end__\";}const workflow = new StateGraph<GraphState>({  channels: {    messages: {      reducer: (a: any, b: any) => a.concat(b),    }  }});const graph = workflow  .addNode(\"model\", callModel)  .addNode(\"tools\", callTools)  .addEdge(\"__start__\", \"model\")  .addConditionalEdges(\"model\", shouldContinue, {    tools: \"tools\",    __end__: \"__end__\",  })  .addEdge(\"tools\", \"model\")  .compile();await graph.invoke({  messages: [{ role: \"user\", content: \"what is the weather in sf\" }]}); An example trace from running the above code looks like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing across multiple services?"}, {"role": "assistant", "content": "Implement distributed tracing Sometimes, you need to trace a request across multiple services. LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (langsmith-trace and optional baggage for metadata/tags). Example client-server setup: Trace starts on clientContinues on server"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can distributed tracing be implemented in a Python application using the provided example?"}, {"role": "assistant", "content": "Distributed tracing in Python # client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasync def my_client_function():    headers = {}    async with httpx.AsyncClient(base_url=\"...\") as client:        if run_tree := get_current_run_tree():            # add langsmith-id to headers            headers.update(run_tree.to_headers())        return await client.post(\"/my-route\", headers=headers) Then the server (or other service) can continue the trace by passing the headers in as langsmith_extra: # server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasync def my_application():    ...app = FastAPI()  # Or Flask, Django, or any other framework@app.post(\"/my-route\")async def fake_route(request: Request):    # request.headers:  {\"langsmith-trace\": \"...\"}    # as well as optional metadata/tags in `baggage`    with tracing_context(parent=request.headers):        return await my_application() The example above uses the tracing_context context manager. You can also directly specify the parent run context in the langsmith_extra parameter of a method wrapped with @traceable. from langsmith.run_helpers import traceable, trace# ... same as above@app.post(\"/my-route\")async def fake_route(request: Request):    # request.headers:  {\"langsmith-trace\": \"...\"}    my_application(langsmith_extra={\"parent\": request.headers})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What version of langsmith is required for distributed tracing in TypeScript?"}, {"role": "assistant", "content": "Distributed tracing in TypeScript noteDistributed tracing in TypeScript requires langsmith version >=0.1.31 First, we obtain the current run tree from the client and convert it to langsmith-trace and baggage header values, which we can pass to the server: // client.mtsimport { getCurrentRunTree, traceable } from \"langsmith/traceable\";const client = traceable(  async () => {    const runTree = getCurrentRunTree();    return await fetch(\"...\", {      method: \"POST\",      headers: runTree.toHeaders(),    }).then((a) => a.text());  },  { name: \"client\" });await client(); Then, the server converts the headers back to a run tree, which it uses to further continue the tracing. To pass the newly created run tree to a traceable function, we can use the withRunTree helper, which will ensure the run tree is propagated within traceable invocations. Express.JSHono// server.mtsimport { RunTree } from \"langsmith\";import { traceable, withRunTree } from \"langsmith/traceable\";import express from \"express\";import bodyParser from \"body-parser\";const server = traceable(  (text: string) => `Hello from the server! Received \"${text}\"`,  { name: \"server\" });const app = express();app.use(bodyParser.text());app.post(\"/\", async (req, res) => {  const runTree = RunTree.fromHeaders(req.headers);  const result = await withRunTree(runTree, () => server(req.body));  res.send(result);});// server.mtsimport { RunTree } from \"langsmith\";import { traceable, withRunTree } from \"langsmith/traceable\";import { Hono } from \"hono\";const server = traceable(  (text: string) => `Hello from the server! Received \"${text}\"`,  { name: \"server\" });const app = new Hono();app.post(\"/\", async (c) => {  const body = await c.req.text();  const runTree = RunTree.fromHeaders(c.req.raw.headers);  const result = await withRunTree(runTree, () => server(body));  return c.body(result);});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is LangSmith and what capabilities does it offer for building LLM applications?"}, {"role": "assistant", "content": "Get started with LangSmith LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the commands to install LangSmith for Python and TypeScript?"}, {"role": "assistant", "content": "1. Install LangSmith PythonTypeScriptpip install -U langsmithyarn add langsmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you create an API key?"}, {"role": "assistant", "content": "2. Create an API key To create an API key head to the Settings page. Then click Create API Key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables need to be set up for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "3. Set up your environment Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to log traces to LangSmith for LangChain users?"}, {"role": "assistant", "content": "4. Log your first trace Tracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here. We provide multiple ways to log traces to LangSmith. Below, we'll highlight\nhow to use traceable. See more on the Annotate code for tracing page. PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today? View a sample output trace.Learn more about tracing in the how-to guides."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the necessary components for running an evaluation in LangSmith?"}, {"role": "assistant", "content": "5. Run your first evaluation Evaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator. PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = \"Sample Dataset\"dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  }); Learn more about evaluation in the how-to guides."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What authentication methods does LangSmith Self-Hosted support for SSO?"}, {"role": "assistant", "content": "SSO with OAuth2.0 and OIDC LangSmith Self-Hosted provides SSO via OAuth2.0 and OIDC. Once configured, this will delegate authentication to your Identity Provider(IdP) to manage access to LangSmith. Our implementation supports almost anything that is OIDC compliant, with a few exceptions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for using OAuth SSO with LangSmith?"}, {"role": "assistant", "content": "Requirements There are a couple of requirements for using OAuth SSO with LangSmith: Your IdP must support the Authorization Code with PKCE flow(Google does not support this flow). This often displayed in your Oauth Provider as configuring a \"Single Page Application(SPA)Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.You must provide the OIDC, email, and profile scopes to LangSmith. We use these to fetch the necessary user information and email for your users.You will need to set the callback URL in your IdP to http://<host>/oauth-callback, where host is the domain or ip you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.You will need to provide the oauthClientId and oauthIssuerUrl in your values.yaml file. This is where you will configure your LangSmith instance. HelmDockerconfig:  oauth:    enabled: true    oauthClientId: <YOUR CLIENT ID>    oauthIssuerUrl: <YOUR DISCOVERY URL># In your .env fileAUTH_TYPE=oauthOAUTH_CLIENT_ID=your-client-idOAUTH_ISSUER_URL=https://your-issuer-url     Once configured, you will see a login screen like this:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What authentication methods does LangSmith support for sign-up and login?"}, {"role": "assistant", "content": "Authentication methods LangSmith supports multiple authentication methods for easy sign-up and login."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What credentials do users need to sign up and login to LangSmith?"}, {"role": "assistant", "content": "Email/Password Users can use an email address and password to sign up and login to LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What social providers can users use to log in?"}, {"role": "assistant", "content": "Social Providers Users can alternatively use their credentials from GitHub, Google, or Discord."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What advantages do self-hosted customers have regarding user login control in LangSmith?"}, {"role": "assistant", "content": "Self-Hosted Self-hosted customers have more control over how their users can login to LangSmith.\nFor more in-depth coverage of configuration options, see the self-hosting docs\nand Helm chart."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required for production installations to use an external identity provider for SSO?"}, {"role": "assistant", "content": "SSO with OAuth 2.0 and OIDC Production installations should configure SSO in order to use an external identity provider.\nThis enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider.\nLearn more about configuring SSO in the SSO configuration guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the primary use case for the Email/Password authentication method mentioned in the text?"}, {"role": "assistant", "content": "Email/Password a.k.a. basic auth (beta) This auth method requires very little configuration as it does not require an external identity provider.\nIt is most appropriate to use for self-hosted trials. Learn more in the basic auth configuration guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to the current authentication mode after the launch of Basic Auth?"}, {"role": "assistant", "content": "nan dangerThis authentication mode will be removed after the launch of Basic Auth. If zero authentication methods are enabled, a self-hosted installation does not require any login/sign-up.\nThis configuration should only be used for verifying installation at the infrastructure level, as the feature set\nsupported in this mode is restricted with only a single organization and workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing across multiple services?"}, {"role": "assistant", "content": "Implement distributed tracing Sometimes, you need to trace a request across multiple services. LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (langsmith-trace and optional baggage for metadata/tags). Example client-server setup: Trace starts on clientContinues on server"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can distributed tracing be implemented in a Python application using the provided example?"}, {"role": "assistant", "content": "Distributed tracing in Python # client.pyfrom langsmith.run_helpers import get_current_run_tree, traceableimport httpx@traceableasync def my_client_function():    headers = {}    async with httpx.AsyncClient(base_url=\"...\") as client:        if run_tree := get_current_run_tree():            # add langsmith-id to headers            headers.update(run_tree.to_headers())        return await client.post(\"/my-route\", headers=headers) Then the server (or other service) can continue the trace by passing the headers in as langsmith_extra: # server.pyfrom langsmith import traceablefrom langsmith.run_helpers import tracing_contextfrom fastapi import FastAPI, Request@traceableasync def my_application():    ...app = FastAPI()  # Or Flask, Django, or any other framework@app.post(\"/my-route\")async def fake_route(request: Request):    # request.headers:  {\"langsmith-trace\": \"...\"}    # as well as optional metadata/tags in `baggage`    with tracing_context(parent=request.headers):        return await my_application() The example above uses the tracing_context context manager. You can also directly specify the parent run context in the langsmith_extra parameter of a method wrapped with @traceable. from langsmith.run_helpers import traceable, trace# ... same as above@app.post(\"/my-route\")async def fake_route(request: Request):    # request.headers:  {\"langsmith-trace\": \"...\"}    my_application(langsmith_extra={\"parent\": request.headers})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What version of langsmith is required for distributed tracing in TypeScript?"}, {"role": "assistant", "content": "Distributed tracing in TypeScript noteDistributed tracing in TypeScript requires langsmith version >=0.1.31 First, we obtain the current run tree from the client and convert it to langsmith-trace and baggage header values, which we can pass to the server: // client.mtsimport { getCurrentRunTree, traceable } from \"langsmith/traceable\";const client = traceable(  async () => {    const runTree = getCurrentRunTree();    return await fetch(\"...\", {      method: \"POST\",      headers: runTree.toHeaders(),    }).then((a) => a.text());  },  { name: \"client\" });await client(); Then, the server converts the headers back to a run tree, which it uses to further continue the tracing. To pass the newly created run tree to a traceable function, we can use the withRunTree helper, which will ensure the run tree is propagated within traceable invocations. Express.JSHono// server.mtsimport { RunTree } from \"langsmith\";import { traceable, withRunTree } from \"langsmith/traceable\";import express from \"express\";import bodyParser from \"body-parser\";const server = traceable(  (text: string) => `Hello from the server! Received \"${text}\"`,  { name: \"server\" });const app = express();app.use(bodyParser.text());app.post(\"/\", async (req, res) => {  const runTree = RunTree.fromHeaders(req.headers);  const result = await withRunTree(runTree, () => server(req.body));  res.send(result);});// server.mtsimport { RunTree } from \"langsmith\";import { traceable, withRunTree } from \"langsmith/traceable\";import { Hono } from \"hono\";const server = traceable(  (text: string) => `Hello from the server! Received \"${text}\"`,  { name: \"server\" });const app = new Hono();app.post(\"/\", async (c) => {  const body = await c.req.text();  const runTree = RunTree.fromHeaders(c.req.raw.headers);  const result = await withRunTree(runTree, () => server(body));  return c.body(result);});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate the improvement of few-shot evaluators for LLM-based applications?"}, {"role": "assistant", "content": "Create few-shot evaluators Using LLM-as-a-Judge evaluators can be very helpful when you can't evaluate your system programmatically. However, improving/iterating on these prompts can add unnecessary\noverhead to the development process of an LLM-based application - you now need to maintain both your application and your evaluators. To make this process easier, LangSmith allows\nyou to automatically collect human corrections on evaluator prompts, which are then inserted into your prompt as few-shot examples. Recommended ReadingBefore learning how to create few-shot evaluators, it might be helpful to learn how to setup automations (both online and offline) and how to leave corrections on evaluator scores:Set up online evaluationsBind an evaluator to a dataset in the UI (offline evaluation)Audit evaluator scores"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default maximum number of few-shot examples that can be used in the prompt when creating an evaluator?"}, {"role": "assistant", "content": "Create your evaluator tipThe default maximum few-shot examples to use in the prompt is 5. Examples are pulled randomly from your dataset (if you have more than the maximum). When creating an online or offline evaluator - from a tracing project or a dataset, respectively - you will see the option to use corrections as few-shot examples. Note that these types of evaluators\nare only supported when using mustache prompts - you will not be able to click this option if your prompt uses f-string formatting. When you select this,\nwe will auto-create a few-shot prompt for you. Each individual few-shot example will be formatted according to this prompt, and inserted into your main prompt in place of the {{Few-shot examples}}\ntemplate variable which will be auto-added above. Your few-shot prompt should contain the same variables as your main prompt, plus a few_shot_explanation and a score variable which should have the same name\nas your output key. For example, if your main prompt has variables question and response, and your evaluator outputs a correctness score, then your few-shot prompt should have question, response,\nfew_shot_explanation, and correctness. You may also specify the number of few-shot examples to use. The default is 5. If your examples will tend to be very long, you may want to set this number lower to save tokens - whereas if your examples tend\nto be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you. Note that few-shot examples are not currently supported in evaluators that use Hub prompts. Once you create your evaluator, we will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do when you disagree with the scores given by your evaluator?"}, {"role": "assistant", "content": "Make corrections Main ArticleAudit evaluator scores As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will\nbegin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the few_shot_explanation variable. The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset.\nThe outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset: Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access your corrections dataset?"}, {"role": "assistant", "content": "View your corrections dataset In order to view your corrections dataset, go to your rule and click \"Edit Rule\" (or \"Edit Evaluator\" from a dataset): If this is an online evaluator (in a tracing project), you will need to click to edit your prompt: From this screen, you will see a button that says \"View few-shot dataset\". Clicking this will bring you to your dataset of corrections, where you can view and update your few-shot examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of logging to LangSmith?"}, {"role": "assistant", "content": "Tracing This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Run in the context of an LLM application?"}, {"role": "assistant", "content": "Runs A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of LangSmith, and how are runs related to it?"}, {"role": "assistant", "content": "Traces A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a project in the context of traces?"}, {"role": "assistant", "content": "Projects A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different ways to collect feedback on runs?"}, {"role": "assistant", "content": "Feedback Feedback allows you to score an individual run based on certain criteria.\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are tags used for in the LangSmith UI?"}, {"role": "assistant", "content": "Tags Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of metadata in relation to runs?"}, {"role": "assistant", "content": "Metadata Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How long does LangSmith retain trace data after it is inserted into the trace database?"}, {"role": "assistant", "content": "Data storage and retention For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you delete a trace from LangSmith before its expiration date?"}, {"role": "assistant", "content": "Deleting traces from LangSmith If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the \"Delete\" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a recommended reading resource before setting up an organization?"}, {"role": "assistant", "content": "Set up an organization Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must you take to create a separate organization for collaboration?"}, {"role": "assistant", "content": "Create an organization When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.\nShared organizations require a credit card before they can be used. You will need to set up billing to proceed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you navigate between workspaces to access resources in LangSmith?"}, {"role": "assistant", "content": "Manage and navigate workspaces Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.\nTo navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What actions can you perform to manage users in your shared organization?"}, {"role": "assistant", "content": "Manage users Manage membership in your shared organization in the Settings page Members and roles tab.\nHere you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in access and permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended approach for logging LLM traces from OpenAI models using the langsmith SDK?"}, {"role": "assistant", "content": "Log custom LLM traces noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.\nIn order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format should the input messages be in for chat-style models?"}, {"role": "assistant", "content": "Chat-style models For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.ls_model_name: The name of the model, eg \"gpt-3.5-turbo\", \"claude-3-opus-20240307\", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ]}# Can also use one of:# output = {#     \"message\": {#         \"role\": \"assistant\",#         \"content\": \"Sure, what time would you like to book the table for?\"#     }# }## output = {#     \"role\": \"assistant\",#     \"content\": \"Sure, what time would you like to book the table for?\"# }## output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" }];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\"      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: \"assistant\",//     content: \"Sure, what time would you like to book the table for?\"//   }// };//// const output = {//   role: \"assistant\",//   content: \"Sure, what time would you like to book the table for?\"// };//// const output = [\"assistant\", \"Sure, what time would you like to book the table for?\"];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the `_reduce_chunks` function in the context of streaming outputs?"}, {"role": "assistant", "content": "Stream outputs For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}@traceable(    run_type=\"llm\",    reduce_fn=_reduce_chunks,    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def my_streaming_chat_model(messages: list):    for chunk in [\"Hello, \" + messages[1][\"content\"]]:        yield {            \"choices\": [                {                    \"message\": {                        \"content\": chunk,                        \"role\": \"assistant\",                    }                }            ]        }list(    my_streaming_chat_model(        [            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},            {\"role\": \"user\", \"content\": \"polly the parrot\"},        ],    ))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can token counts be manually provided to LangSmith in a function's response?"}, {"role": "assistant", "content": "Manually provide token counts Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.\nMany models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.\nIf token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.\nIf using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ],    \"usage_metadata\": {        \"input_tokens\": 27,        \"output_tokens\": 13,        \"total_tokens\": 40,    },}@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for inputs and outputs in instruct-style models?"}, {"role": "assistant", "content": "Instruct-style models For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.\nThe same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def hello_llm(prompt: str):    return {        \"choices\": [            {\"text\": \"Hello, \" + prompt}        ],        \"usage_metadata\": {            \"input_tokens\": 4,            \"output_tokens\": 5,            \"total_tokens\": 9,        },    }hello_llm(\"polly the parrot\\n\")import { traceable } from \"langsmith/traceable\";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: \"Hello, \" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: \"llm\", name: \"hello_llm\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await helloLLM({ prompt: \"polly the parrot\\n\" }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features are available in the LangChain Hub for managing and utilizing prompts?"}, {"role": "assistant", "content": "LangChain Hub Navigate to the LangChain Hub section of the left-hand sidebar. Here you'll find all of the publicly listed prompts in the LangChain Hub.\nYou can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt's details, and run the prompt in the playground.\nYou can pull any public prompt into your code using the SDK. To view prompts tied to your workspace, visit the Prompts tab in the sidebar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionality has been added to help users construct queries in natural language?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What configuration options are available for your LangSmith instance?"}, {"role": "assistant", "content": "Configuration Your LangSmith instance supports configuring a variety of parameters to suit your needs. This section contains guides for configuring your LangSmith instance. SSO with OAuth2.0 and OIDCEmail/password a.k.a. basic authUser management featuresConnecting to an external ClickHouse databaseConnecting to an external Postgres databaseConnecting to an external Redis instance"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended method for exporting runs in LangSmith traces?"}, {"role": "assistant", "content": "Export traces Recommended ReadingBefore diving into this content, it might be helpful to read the following:Run (span) data formatLangSmith API ReferenceLangSmith trace query syntax The recommended way to export runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using filter arguments in simple queries?"}, {"role": "assistant", "content": "Use filter arguments For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client, Run } from \"langsmith\";const client = new Client(); Below are some examples of ways to list runs using keyword arguments:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you list all runs in a specific project using the provided code?"}, {"role": "assistant", "content": "List all runs in a project PythonTypeScriptproject_runs = client.list_runs(project_name=\"<your_project>\")// Download runs in a projectconst projectRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",})) {  projectRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What code is used to list LLM runs in the last 24 hours?"}, {"role": "assistant", "content": "List LLM and Chat runs in the last 24 hours PythonTypeScripttodays_llm_runs = client.list_runs(    project_name=\"<your_project>\",    start_time=datetime.now() - timedelta(days=1),    run_type=\"llm\",)const todaysLlmRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),  runType: \"llm\",})) {  todaysLlmRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for root runs in a project using the provided code?"}, {"role": "assistant", "content": "List root runs in a project Root runs are runs that have no parents. These are assigned a value of True for is_root. You can use this to filter for root runs. PythonTypeScriptroot_runs = client.list_runs(    project_name=\"<your_project>\",    is_root=True)const rootRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  isRoot: 1,})) {  rootRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you list runs without errors in a project using Python and TypeScript?"}, {"role": "assistant", "content": "List runs without errors PythonTypeScriptcorrect_runs = client.list_runs(project_name=\"<your_project>\", error=False)const correctRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  error: false,})) {  correctRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens when a list of run IDs is provided in the list_runs function?"}, {"role": "assistant", "content": "List runs by run ID Ignores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like project_name, run_type, etc. and directly return the runs matching the given IDs. If you have a list of run IDs, you can list them directly: PythonTypeScriptrun_ids = ['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)const runIds = [  \"a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836\",  \"9398e6be-964f-4aa4-8ae9-ad78cd4b7074\",];const selectedRuns: Run[] = [];for await (const run of client.listRuns({  id: runIds,})) {  selectedRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What can you use for more complex queries according to the filter query language reference?"}, {"role": "assistant", "content": "Use filter query language For more complex queries, you can use the query language described in the filter query language refernece."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the runs named \"extractor\" that have a \"user_score\" feedback score of 1?"}, {"role": "assistant", "content": "List all runs called \"extractor\" whose root of the trace was assigned feedback \"user_score\" score of 1 PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='eq(name, \"extractor\")',    trace_filter='and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))')client.listRuns({  projectName: \"<your_project>\",  filter: 'eq(name, \"extractor\")',  traceFilter: 'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list runs with a \"star_rating\" key where the score exceeds 4?"}, {"role": "assistant", "content": "List runs with \"star_rating\" key whose score is greater than 4 PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list runs that took longer than 5 seconds to complete in a specified project?"}, {"role": "assistant", "content": "List runs that took longer than 5 seconds to complete PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(latency, \"5s\")')client.listRuns({projectName: \"<your_project>\", filter: 'gt(latency, \"5s\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs where total_tokens is greater than 5000?"}, {"role": "assistant", "content": "List all runs wheretotal_tokensis greater than 5000 PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(total_tokens, 5000)')client.listRuns({projectName: \"<your_project>\", filter: 'gt(total_tokens, 5000)'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs with \"error\" not equal to null in a specified project?"}, {"role": "assistant", "content": "List all runs that have \"error\" not equal to null PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='neq(error, null)')client.listRuns({projectName: \"<your_project>\", filter: 'neq(error, null)'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list all runs with a start time greater than \"2023-07-15T12:34:56Z\" in a specific project?"}, {"role": "assistant", "content": "List all runs wherestart_timeis greater than a specific timestamp PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(start_time, \"2023-07-15T12:34:56Z\")')client.listRuns({projectName: \"<your_project>\", filter: 'gt(start_time, \"2023-07-15T12:34:56Z\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I list all runs that contain the string \"substring\" in a specific project using Python and TypeScript?"}, {"role": "assistant", "content": "List all runs that contain the string \"substring\" PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='search(\"substring\")')client.listRuns({projectName: \"<your_project>\", filter: 'search(\"substring\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs tagged with the git hash \"2aa1cf4\"?"}, {"role": "assistant", "content": "List all runs that are tagged with the git hash \"2aa1cf4\" PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='has(tags, \"2aa1cf4\")')client.listRuns({projectName: \"<your_project>\", filter: 'has(tags, \"2aa1cf4\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the \"chain\" type runs that took more than 10 seconds and had total_tokens greater than 5000?"}, {"role": "assistant", "content": "List all \"chain\" type runs that took more than 10 seconds and had total_tokens greater than 5000 PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(eq(run_type, \"chain\"), gt(latency, 10), gt(total_tokens, 5000))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(eq(run_type, \"chain\"), gt(latency, 10), gt(total_tokens, 5000))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the criteria for listing runs that started after the timestamp \"2023-07-15T12:34:56Z\"?"}, {"role": "assistant", "content": "List all runs that started after a specific timestamp and either have \"error\" not equal to null or a \"Correctness\" feedback score equal to 0 PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key, \"Correctness\"), eq(feedback_score, 0.0))))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key, \"Correctness\"), eq(feedback_score, 0.0))))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the query to list all runs with tags \"experimental\" or \"beta\" and a latency greater than 2 seconds?"}, {"role": "assistant", "content": "Complex query: List all runs wheretagsinclude \"experimental\" or \"beta\" and latency is greater than 2 seconds PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(or(has(tags, \"experimental\"), has(tags, \"beta\")), gt(latency, 2))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(or(has(tags, \"experimental\"), has(tags, \"beta\")), gt(latency, 2))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you perform a full text search for traces in a run using the search() function?"}, {"role": "assistant", "content": "Search trace trees by full text You can use thesearch()function without any specific field to do a full text search across all string fields in a run. This\nallows you to quickly find traces that match a search term. PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='search(\"image classification\")')client.listRuns({  projectName: \"<your_project>\",  filter: 'search(\"image classification\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you check for the presence of the \"user_id\" metadata key in your runs?"}, {"role": "assistant", "content": "Check for presence of metadata If you want to check for the presence of metadata, you can use the eq operator, optionally with an and statement to match by value. This is useful if you want to log more structured information\nabout your runs. PythonTypeScriptto_search = {    \"user_id\": \"\"}# Check for any run with the \"user_id\" metadata keyclient.list_runs(  project_name=\"default\",  filter=\"eq(metadata_key, 'user_id')\")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))\")// Check for any run with the \"user_id\" metadata keyclient.listRuns({  projectName: 'default',  filter: `eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that contain environment metadata in your traces?"}, {"role": "assistant", "content": "Check for environment details in metadata. A common pattern is to add environment\ninformation to your traces via metadata. If you want to filter for runs containing\nenvironment metadata, you can use the same pattern as above: PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on a conversation ID in the metadata?"}, {"role": "assistant", "content": "Check for conversation ID in metadata Another common way to associate traces\nin the same conversation is by using a shared conversation ID. If you want to filter\nruns based on a conversation ID in this way, you can search for that ID in the metadata. PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you combine multiple filters to search for runs named \"ChatOpenAI\" with a specific conversation_id in their metadata?"}, {"role": "assistant", "content": "Combine multiple filters If you want to combine multiple conditions to refine your search, you can use the and operator along with other\nfiltering functions. Here's how you can search for runs named \"ChatOpenAI\" that also\nhave a specific conversation_id in their metadata: PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the criteria for listing runs named \"RetrieveDocs\" in the provided query?"}, {"role": "assistant", "content": "Tree Filter List all runs named \"RetrieveDocs\" whose root run has a \"user_score\" feedback of 1 and any run in the full trace is named \"ExpandQuery\". This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace. PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='eq(name, \"RetrieveDocs\")',    trace_filter='and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))',    tree_filter='eq(name, \"ExpandQuery\")')client.listRuns({  projectName: \"<your_project>\",  filter: 'eq(name, \"RetrieveDocs\")',  traceFilter: 'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))',  treeFilter: 'eq(name, \"ExpandQuery\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a flattened view of traces that includes information on the tools used by the agent within each trace in Python?"}, {"role": "assistant", "content": "Advanced: export flattened trace view with child tool usage The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.\nThis can be used to analyze the behavior of your agents across multiple traces. This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information. To optimize the query, the example: Selects only the necessary fields when querying tool runs to reduce query time.Fetches root runs in batches while processing tool runs concurrently. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = \"my-project\"num_days = 30# List all tool runstool_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=\"tool\",    # We don't need to fetch inputs, outputs, and other values that # may increase the query time    select=[\"trace_id\", \"name\", \"run_type\"],)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group tool runs by parent run ID    for run in tqdm(tool_runs):        # Collect all tools invoked within a given trace        tool_runs_by_parent[run.trace_id][\"tools_involved\"].add(run.name)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the tool runs        if len(tool_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(tool_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],                    )                )    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = tool_runs_by_parent[root_run.id]        data.append(            {                \"run_id\": root_run.id,                \"run_name\": root_run.name,                \"run_type\": root_run.run_type,                \"inputs\": root_run.inputs,                \"outputs\": root_run.outputs,                \"tools_involved\": list(root_data[\"tools_involved\"]),            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of exporting retriever inputs and outputs within traces that have a specific feedback score?"}, {"role": "assistant", "content": "Advanced: export retriever IO for traces with feedback This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.\nThe following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = \"your-project-name\"num_days = 1# List all tool runsretriever_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=\"retriever\",    # This time we do want to fetch the inputs and outputs, since they    # may be adjusted by query expansion steps.    select=[\"trace_id\", \"name\", \"run_type\", \"inputs\", \"outputs\"],    trace_filter='eq(feedback_key, \"user_score\")',)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group retriever runs by parent run ID    for run in tqdm(retriever_runs):        # Collect all retriever calls invoked within a given trace        for k, v in run.inputs.items():            retriever_runs_by_parent[run.trace_id][f\"retriever.inputs.{k}\"].append(v)        for k, v in (run.outputs or {}).items():            # Extend the docs            retriever_runs_by_parent[run.trace_id][f\"retriever.outputs.{k}\"].extend(v)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the retriever runs        if len(retriever_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(retriever_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[                            \"name\",                            \"inputs\",                            \"outputs\",                            \"run_type\",                            \"feedback_stats\",                        ],                    )                )    if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = retriever_runs_by_parent[root_run.id]        feedback = {            f\"feedback.{k}\": v.get(\"avg\")            for k, v in (root_run.feedback_stats or {}).items()        }        inputs = {f\"inputs.{k}\": v for k, v in root_run.inputs.items()}        outputs = {f\"outputs.{k}\": v for k, v in (root_run.outputs or {}).items()}        data.append(            {                \"run_id\": root_run.id,                \"run_name\": root_run.name,                **inputs,                **outputs,                **feedback,                **root_data,            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from the runs of your traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Langsmith Python client?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you create a new prompt in the application?"}, {"role": "assistant", "content": "Create a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage.\nClick the \"+ Prompt\" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for developing your prompt in the playground?"}, {"role": "assistant", "content": "Compose your prompt After choosing a prompt type, you're brought to the playground to develop your prompt.\nOn the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click \"Start\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you save a prompt and what options do you have for its visibility?"}, {"role": "assistant", "content": "Save your prompt To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\".\nPrivate prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.\nClick save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can I view a table of my prompts?"}, {"role": "assistant", "content": "View your prompts You've just created your first prompt! View a table of your prompts in the prompts tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add metadata to your prompt in the LangChain Hub?"}, {"role": "assistant", "content": "Add metadata To add metadata to your prompt, click the prompt and then click the \"Edit\" pencil icon next to the name.\nThis brings you to where you can add additional information about the prompt, including a description, a README, and use cases.\nFor public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended method for exporting runs in LangSmith traces?"}, {"role": "assistant", "content": "Export traces Recommended ReadingBefore diving into this content, it might be helpful to read the following:Run (span) data formatLangSmith API ReferenceLangSmith trace query syntax The recommended way to export runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using filter arguments in simple queries?"}, {"role": "assistant", "content": "Use filter arguments For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client, Run } from \"langsmith\";const client = new Client(); Below are some examples of ways to list runs using keyword arguments:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you list all runs in a specific project using the provided code?"}, {"role": "assistant", "content": "List all runs in a project PythonTypeScriptproject_runs = client.list_runs(project_name=\"<your_project>\")// Download runs in a projectconst projectRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",})) {  projectRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What code is used to list LLM runs in the last 24 hours?"}, {"role": "assistant", "content": "List LLM and Chat runs in the last 24 hours PythonTypeScripttodays_llm_runs = client.list_runs(    project_name=\"<your_project>\",    start_time=datetime.now() - timedelta(days=1),    run_type=\"llm\",)const todaysLlmRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),  runType: \"llm\",})) {  todaysLlmRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for root runs in a project using the provided code?"}, {"role": "assistant", "content": "List root runs in a project Root runs are runs that have no parents. These are assigned a value of True for is_root. You can use this to filter for root runs. PythonTypeScriptroot_runs = client.list_runs(    project_name=\"<your_project>\",    is_root=True)const rootRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  isRoot: 1,})) {  rootRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you list runs without errors in Python and TypeScript?"}, {"role": "assistant", "content": "List runs without errors PythonTypeScriptcorrect_runs = client.list_runs(project_name=\"<your_project>\", error=False)const correctRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  error: false,})) {  correctRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens when a list of run IDs is provided in the list_runs function?"}, {"role": "assistant", "content": "List runs by run ID Ignores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like project_name, run_type, etc. and directly return the runs matching the given IDs. If you have a list of run IDs, you can list them directly: PythonTypeScriptrun_ids = ['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)const runIds = [  \"a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836\",  \"9398e6be-964f-4aa4-8ae9-ad78cd4b7074\",];const selectedRuns: Run[] = [];for await (const run of client.listRuns({  id: runIds,})) {  selectedRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What can you use for more complex queries according to the filter query language reference?"}, {"role": "assistant", "content": "Use filter query language For more complex queries, you can use the query language described in the filter query language refernece."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What runs named \"extractor\" have a \"user_score\" feedback score of 1?"}, {"role": "assistant", "content": "List all runs called \"extractor\" whose root of the trace was assigned feedback \"user_score\" score of 1 PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='eq(name, \"extractor\")',    trace_filter='and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))')client.listRuns({  projectName: \"<your_project>\",  filter: 'eq(name, \"extractor\")',  traceFilter: 'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list runs with a \"star_rating\" key where the score exceeds 4?"}, {"role": "assistant", "content": "List runs with \"star_rating\" key whose score is greater than 4 PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list runs that took longer than 5 seconds to complete in a specified project?"}, {"role": "assistant", "content": "List runs that took longer than 5 seconds to complete PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(latency, \"5s\")')client.listRuns({projectName: \"<your_project>\", filter: 'gt(latency, \"5s\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list all runs where total_tokens is greater than 5000?"}, {"role": "assistant", "content": "List all runs wheretotal_tokensis greater than 5000 PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(total_tokens, 5000)')client.listRuns({projectName: \"<your_project>\", filter: 'gt(total_tokens, 5000)'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs where the \"error\" field is not null in a specified project?"}, {"role": "assistant", "content": "List all runs that have \"error\" not equal to null PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='neq(error, null)')client.listRuns({projectName: \"<your_project>\", filter: 'neq(error, null)'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list all runs with a start time greater than \"2023-07-15T12:34:56Z\" in a specific project?"}, {"role": "assistant", "content": "List all runs wherestart_timeis greater than a specific timestamp PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(start_time, \"2023-07-15T12:34:56Z\")')client.listRuns({projectName: \"<your_project>\", filter: 'gt(start_time, \"2023-07-15T12:34:56Z\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list all runs that contain the string \"substring\" in a specified project?"}, {"role": "assistant", "content": "List all runs that contain the string \"substring\" PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='search(\"substring\")')client.listRuns({projectName: \"<your_project>\", filter: 'search(\"substring\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs tagged with the git hash \"2aa1cf4\"?"}, {"role": "assistant", "content": "List all runs that are tagged with the git hash \"2aa1cf4\" PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='has(tags, \"2aa1cf4\")')client.listRuns({projectName: \"<your_project>\", filter: 'has(tags, \"2aa1cf4\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the \"chain\" type runs that took more than 10 seconds and had total_tokens greater than 5000?"}, {"role": "assistant", "content": "List all \"chain\" type runs that took more than 10 seconds and had total_tokens greater than 5000 PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(eq(run_type, \"chain\"), gt(latency, 10), gt(total_tokens, 5000))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(eq(run_type, \"chain\"), gt(latency, 10), gt(total_tokens, 5000))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the criteria for listing runs that started after the timestamp \"2023-07-15T12:34:56Z\"?"}, {"role": "assistant", "content": "List all runs that started after a specific timestamp and either have \"error\" not equal to null or a \"Correctness\" feedback score equal to 0 PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key, \"Correctness\"), eq(feedback_score, 0.0))))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key, \"Correctness\"), eq(feedback_score, 0.0))))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the query to list all runs with tags \"experimental\" or \"beta\" and a latency greater than 2 seconds?"}, {"role": "assistant", "content": "Complex query: List all runs wheretagsinclude \"experimental\" or \"beta\" and latency is greater than 2 seconds PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(or(has(tags, \"experimental\"), has(tags, \"beta\")), gt(latency, 2))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(or(has(tags, \"experimental\"), has(tags, \"beta\")), gt(latency, 2))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you perform a full text search for traces in a run using the search() function?"}, {"role": "assistant", "content": "Search trace trees by full text You can use thesearch()function without any specific field to do a full text search across all string fields in a run. This\nallows you to quickly find traces that match a search term. PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='search(\"image classification\")')client.listRuns({  projectName: \"<your_project>\",  filter: 'search(\"image classification\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you check for the presence of the \"user_id\" metadata key in your runs?"}, {"role": "assistant", "content": "Check for presence of metadata If you want to check for the presence of metadata, you can use the eq operator, optionally with an and statement to match by value. This is useful if you want to log more structured information\nabout your runs. PythonTypeScriptto_search = {    \"user_id\": \"\"}# Check for any run with the \"user_id\" metadata keyclient.list_runs(  project_name=\"default\",  filter=\"eq(metadata_key, 'user_id')\")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))\")// Check for any run with the \"user_id\" metadata keyclient.listRuns({  projectName: 'default',  filter: `eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that contain environment metadata in your traces?"}, {"role": "assistant", "content": "Check for environment details in metadata. A common pattern is to add environment\ninformation to your traces via metadata. If you want to filter for runs containing\nenvironment metadata, you can use the same pattern as above: PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on a conversation ID in the metadata?"}, {"role": "assistant", "content": "Check for conversation ID in metadata Another common way to associate traces\nin the same conversation is by using a shared conversation ID. If you want to filter\nruns based on a conversation ID in this way, you can search for that ID in the metadata. PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you combine multiple filters to search for runs named \"ChatOpenAI\" with a specific conversation_id in their metadata?"}, {"role": "assistant", "content": "Combine multiple filters If you want to combine multiple conditions to refine your search, you can use the and operator along with other\nfiltering functions. Here's how you can search for runs named \"ChatOpenAI\" that also\nhave a specific conversation_id in their metadata: PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the criteria for listing runs named \"RetrieveDocs\" in the provided query?"}, {"role": "assistant", "content": "Tree Filter List all runs named \"RetrieveDocs\" whose root run has a \"user_score\" feedback of 1 and any run in the full trace is named \"ExpandQuery\". This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace. PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='eq(name, \"RetrieveDocs\")',    trace_filter='and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))',    tree_filter='eq(name, \"ExpandQuery\")')client.listRuns({  projectName: \"<your_project>\",  filter: 'eq(name, \"RetrieveDocs\")',  traceFilter: 'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))',  treeFilter: 'eq(name, \"ExpandQuery\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a flattened view of traces that includes information on the tools used by the agent within each trace in Python?"}, {"role": "assistant", "content": "Advanced: export flattened trace view with child tool usage The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.\nThis can be used to analyze the behavior of your agents across multiple traces. This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information. To optimize the query, the example: Selects only the necessary fields when querying tool runs to reduce query time.Fetches root runs in batches while processing tool runs concurrently. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = \"my-project\"num_days = 30# List all tool runstool_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=\"tool\",    # We don't need to fetch inputs, outputs, and other values that # may increase the query time    select=[\"trace_id\", \"name\", \"run_type\"],)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group tool runs by parent run ID    for run in tqdm(tool_runs):        # Collect all tools invoked within a given trace        tool_runs_by_parent[run.trace_id][\"tools_involved\"].add(run.name)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the tool runs        if len(tool_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(tool_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],                    )                )    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = tool_runs_by_parent[root_run.id]        data.append(            {                \"run_id\": root_run.id,                \"run_name\": root_run.name,                \"run_type\": root_run.run_type,                \"inputs\": root_run.inputs,                \"outputs\": root_run.outputs,                \"tools_involved\": list(root_data[\"tools_involved\"]),            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of exporting retriever inputs and outputs within traces that have a specific feedback score?"}, {"role": "assistant", "content": "Advanced: export retriever IO for traces with feedback This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.\nThe following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = \"your-project-name\"num_days = 1# List all tool runsretriever_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=\"retriever\",    # This time we do want to fetch the inputs and outputs, since they    # may be adjusted by query expansion steps.    select=[\"trace_id\", \"name\", \"run_type\", \"inputs\", \"outputs\"],    trace_filter='eq(feedback_key, \"user_score\")',)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group retriever runs by parent run ID    for run in tqdm(retriever_runs):        # Collect all retriever calls invoked within a given trace        for k, v in run.inputs.items():            retriever_runs_by_parent[run.trace_id][f\"retriever.inputs.{k}\"].append(v)        for k, v in (run.outputs or {}).items():            # Extend the docs            retriever_runs_by_parent[run.trace_id][f\"retriever.outputs.{k}\"].extend(v)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the retriever runs        if len(retriever_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(retriever_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[                            \"name\",                            \"inputs\",                            \"outputs\",                            \"run_type\",                            \"feedback_stats\",                        ],                    )                )    if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = retriever_runs_by_parent[root_run.id]        feedback = {            f\"feedback.{k}\": v.get(\"avg\")            for k, v in (root_run.feedback_stats or {}).items()        }        inputs = {f\"inputs.{k}\": v for k, v in root_run.inputs.items()}        outputs = {f\"outputs.{k}\": v for k, v in (root_run.outputs or {}).items()}        data.append(            {                \"run_id\": root_run.id,                \"run_name\": root_run.name,                **inputs,                **outputs,                **feedback,                **root_data,            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key fields included in the feedback data format used by LangSmith?"}, {"role": "assistant", "content": "Feedback data format Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and feedback Feedback is LangSmith's way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span).\nFeedback can be produced from a variety of ways, such as: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator Feedback is stored in a simple format with the following fields: Field NameTypeDescriptionidUUIDUnique identifier for the record itselfcreated_atdatetimeTimestamp when the record was createdmodified_atdatetimeTimestamp when the record was last modifiedsession_idUUIDUnique identifier for the experiment or tracing project the run was a part ofrun_idUUIDUnique identifier for a specific run within a sessionkeystringA key describing the criteria of the feedback, eg \"correctness\"scorenumberNumerical score associated with the feedback keyvaluestringReserved for storing a value associated with the score. Useful for categorical feedback.commentstringAny comment or annotation associated with the record. This can be a justification for the score given.correctionobjectReserved for storing correction details, if anyfeedback_sourceobjectObject containing information about the feedback sourcefeedback_source.typestringThe type of source where the feedback originated, eg \"api\", \"app\", \"evaluator\"feedback_source.metadataobjectReserved for additional metadata, currentlyfeedback_source.user_idUUIDUnique identifier for the user providing feedback Here is an example JSON representation of a feedback record in the above format: {  \"created_at\": \"2024-05-05T23:23:11.077838\",  \"modified_at\": \"2024-05-05T23:23:11.232962\",  \"session_id\": \"c919298b-0af2-4517-97a2-0f98ed4a48f8\",  \"run_id\": \"e26174e5-2190-4566-b970-7c3d9a621baa\",  \"key\": \"correctness\",  \"score\": 1.0,  \"value\": null,  \"comment\": \"I gave this score because the answer was correct.\",  \"correction\": null,  \"id\": \"62104630-c7f5-41dc-8ee2-0acee5c14224\",  \"feedback_source\": {    \"type\": \"app\",    \"metadata\": null,    \"user_id\": \"ad52b092-1346-42f4-a934-6e5521562fab\"  }}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the current run (span) within a traced function in Python or TypeScript?"}, {"role": "assistant", "content": "Access the current run (span) within a traced function In some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run. You can access the current run by calling the get_current_run_tree/getCurrentRunTree function in the Python or TypeScript SDK, respectively. For a full list of available properties on the RunTree object, see this reference. PythonTypeScriptfrom langsmith import traceablefrom langsmith.run_helpers import get_current_run_treefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    run = get_current_run_tree()    print(f\"format_prompt Run Id: {run.id}\")    print(f\"format_prompt Trace Id: {run.trace_id}\")    print(f\"format_prompt Parent Run Id: {run.parent_run.id}\")    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful assistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What's a good name for a store that sells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    run = get_current_run_tree()    print(f\"invoke_llm Run Id: {run.id}\")    print(f\"invoke_llm Trace Id: {run.trace_id}\")    print(f\"invoke_llm Parent Run Id: {run.parent_run.id}\")    return openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )@traceabledef parse_output(response):    run = get_current_run_tree()    print(f\"parse_output Run Id: {run.id}\")    print(f\"parse_output Trace Id: {run.trace_id}\")    print(f\"parse_output Parent Run Id: {run.parent_run.id}\")    return response.choices[0].message.content@traceabledef run_pipeline():    run = get_current_run_tree()    print(f\"run_pipeline Run Id: {run.id}\")    print(f\"run_pipeline Trace Id: {run.trace_id}\")    messages = format_prompt(\"colorful socks\")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()import { traceable, getCurrentRunTree } from \"langsmith/traceable\";import OpenAI from \"openai\";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    const run = getCurrentRunTree();    console.log(\"formatPrompt Run ID\", run.id)    console.log(\"formatPrompt Trace ID\", run.trace_id)    console.log(\"formatPrompt Parent Run ID\", run.parent_run.id)    return [      {        role: \"system\" as const,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content: `What's a good name for a store that sells ${subject}?`,      },    ];  },  { name: \"formatPrompt\" });const invokeLLM = traceable(    async (messages: { role: string; content: string }[]) => {        const run = getCurrentRunTree();        console.log(\"invokeLLM Run ID\", run.id)        console.log(\"invokeLLM Trace ID\", run.trace_id)        console.log(\"invokeLLM Parent Run ID\", run.parent_run.id)        return openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: messages,            temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const parseOutput = traceable(    (response: any) => {        const run = getCurrentRunTree();        console.log(\"parseOutput Run ID\", run.id)        console.log(\"parseOutput Trace ID\", run.trace_id)        console.log(\"parseOutput Parent Run ID\", run.parent_run.id)        return response.choices[0].message.content;    },    { name: \"parseOutput\" });const runPipeline = traceable(    async () => {        const run = getCurrentRunTree();        console.log(\"runPipline Run ID\", run.id)        console.log(\"runPipline Trace ID\", run.trace_id)        console.log(\"runPipline Parent Run ID\", run.parent_run?.id)        const messages = await formatPrompt(\"colorful socks\");        const response = await invokeLLM(messages);        return parseOutput(response);    },    { name: \"runPipeline\" });await runPipeline();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the recommended methods for running evals, and what should you do if you cannot use the SDKs?"}, {"role": "assistant", "content": "Run evals with the REST API Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.\nHowever, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the provided Python code in the context of creating a dataset?"}, {"role": "assistant", "content": "Create a dataset Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries - API Example\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in running a single experiment using the provided API?"}, {"role": "assistant", "content": "Run a single experiment First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { \"dataset\": dataset_id }resp = requests.get(    \"https://api.smith.langchain.com/api/v1/examples\",    params=params,    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"def run_completion_on_example(example, model_name, experiment_id):    \"\"\"Run completions on a list of examples.\"\"\"    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        \"\"\"Function to post a new run to the API.\"\"\"        data = {            \"id\": run_id.hex,            \"name\": name,            \"run_type\": run_type,            \"inputs\": inputs,            \"start_time\": datetime.utcnow().isoformat(),            \"reference_example_id\": example[\"id\"],            \"session_id\": experiment_id,        }        if parent_id:            data[\"parent_run_id\"] = parent_id.hex        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/runs\", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        \"\"\"Function to patch a run with outputs.\"\"\"        resp = requests.patch(            f\"https://api.smith.langchain.com/api/v1/runs/{run_id}\",            json={                \"outputs\": outputs,                \"end_time\": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    text = example[\"inputs\"][\"text\"]    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, \"LLM Pipeline\", \"chain\", {\"text\": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {\"label\": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (\"gpt-3.5-turbo\", \"gpt-4o-mini\")experiment_ids = []for model_name in model_names:    resp = requests.post(        \"https://api.smith.langchain.com/api/v1/sessions\",        json={            \"start_time\": datetime.utcnow().isoformat(),            \"reference_dataset_id\": str(dataset_id),            \"description\": \"An optional description for the experiment\",            \"name\": f\"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}\",  # A name for the experiment            \"extra\": {                \"metadata\": {\"foo\": \"bar\"},  # Optional metadata            },        },        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )    experiment = resp.json()    experiment_ids.append(experiment[\"id\"])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[\"id\"])    # Issue a patch request to \"end\" the experiment by updating the end_time    requests.patch(        f\"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}\",        json={\"end_time\": datetime.utcnow().isoformat()},        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for running a pairwise experiment and ranking the outputs?"}, {"role": "assistant", "content": "Run a pairwise experiment Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.\nFor more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/comparative\",    json={        \"experiment_ids\": experiment_ids,        \"name\": \"Toxicity detection - API Example - Comparative - \" + str(uuid4())[0:8],        \"description\": \"An optional description for the comparative experiment\",        \"extra\": {            \"metadata\": {\"foo\": \"bar\"},  # Optional metadata        },        \"reference_dataset_id\": str(dataset_id),    },    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[\"id\"]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f\"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative\",    params={\"id\": comparative_experiment_id},    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()[0]experiment_ids = [info[\"id\"] for info in comparative_experiment[\"experiments_info\"]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f\"https://api.smith.langchain.com/api/v1/runs/query\",    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]},    json={        \"session\": experiment_ids,        \"is_root\": True, # Only fetch root runs (spans) which contain the end outputs        \"select\": [\"id\", \"reference_example_id\", \"outputs\"],    }).json()runs = runs[\"runs\"]for run in runs:    example_id = run[\"reference_example_id\"]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f\"Example ID: {example_id}\")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f\"Run ID: {run['id']}\")        feedback = {            \"score\": 1 if i == 0 else 0,            \"run_id\": str(run[\"id\"]),            \"key\": \"ranked_preference\",            \"feedback_group_id\": str(feedback_group_id),            \"comparative_experiment_id\": comparative_experiment_id,        }        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/feedback\",            json=feedback,            headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}        )        resp.raise_for_status()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using LangChain's off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.\nThese evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello Harrison\"),]dataset_name = \"Hello Set\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three QA evaluators mentioned, and which one is recommended based on meta-evals?"}, {"role": "assistant", "content": "Use question and answer (correctness) evaluators Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we recommend using \"cot_qa\", or Chain of Thought QA. Here is a trivial example that uses a \"cot_qa\" evaluator to evaluate a simple pipeline that prefixes the input with \"Hello\": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using \"criteria\" evaluators when ground truth reference labels are not available?"}, {"role": "assistant", "content": "Use criteria evaluators If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The \"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",    config={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent \"direction\" (i.e., higher is not necessarily better).\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for \"helpfulness\" means that the prediction was deemed to be helpful by the model.\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content, which, of course, is \"bad\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functions of the \"labeled_criteria\" and \"labeled_score_string\" evaluators in evaluating predictions against reference labels?"}, {"role": "assistant", "content": "Use labeled criteria evaluators If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators. The \"labeled_criteria\" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={        \"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of distance metrics used to measure similarity between a predicted string and a reference?"}, {"role": "assistant", "content": "Use string or embedding distance metrics To measure the similarity between a predicted string and a reference, you can use string distance metrics: The \"string_distance\" evaluator computes a normalized string edit distance between the prediction and referenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    \"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\": True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # \"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",      })evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries are required to customize the model used for LLM-based evaluators?"}, {"role": "assistant", "content": "Use a custom LLM in off-the-shelf evaluators You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a model that outputs multiple fields using LangChain's evaluators?"}, {"role": "assistant", "content": "Handle multiple input or output fields LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\". We want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    },    prepare_data=lambda run, example: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],        \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" + input[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a recommended reading resource before setting up an organization?"}, {"role": "assistant", "content": "Set up an organization Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must you take to create a separate organization for collaboration?"}, {"role": "assistant", "content": "Create an organization When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.\nShared organizations require a credit card before they can be used. You will need to set up billing to proceed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you navigate between workspaces to access resources in LangSmith?"}, {"role": "assistant", "content": "Manage and navigate workspaces Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.\nTo navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What actions can you perform to manage users in your shared organization?"}, {"role": "assistant", "content": "Manage users Manage membership in your shared organization in the Settings page Members and roles tab.\nHere you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in access and permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method must be used to trace runs from the Vercel AI SDK using LangSmith?"}, {"role": "assistant", "content": "Trace with the Vercel AI SDK (JS/TS only) betaThis feature is currently in beta while Vercel rolls out official telemetry support. You can use LangSmith to trace runs from the Vercel AI SDK using\nthe special wrapAISDKModel method. The important detail is that you must wrap the Vercel model wrapper\nrather than of the top-level generateText or generateStream calls. This guide will walk through an example. noteThe wrapAISDKModel method is only available in langsmith JS SDK version >=0.1.40."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can you use to install the Vercel AI SDK with npm?"}, {"role": "assistant", "content": "0. Installation Install the Vercel AI SDK. We use their OpenAI integration for the code snippets below, but you can use any of their\nother options as well. yarnnpmpnpmyarn add ai @ai-sdk/openainpm install ai @ai-sdk/openaipnpm add ai @ai-sdk/openai"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables need to be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment Shellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you log a trace when using a Vercel model with LangSmith?"}, {"role": "assistant", "content": "2. Log a trace Once you've set up your environment, wrap an initialized Vercel model as shown below: import { wrapAISDKModel } from \"langsmith/wrappers/vercel\";import { openai } from \"@ai-sdk/openai\";import { generateText } from \"ai\";const vercelModel = openai(\"gpt-4o-mini\");const modelWithTracing = wrapAISDKModel(vercelModel);const { text } = await generateText({  model: modelWithTracing,  prompt: \"Write a vegetarian lasagna recipe for 4 people.\",});console.log(text); An example trace from running the above code looks like this: If you are using a streaming method, LangSmith will trace chunks as a single aggregated response: import { wrapAISDKModel } from \"langsmith/wrappers/vercel\";import { openai } from \"@ai-sdk/openai\";import { streamText } from \"ai\";const vercelModel = openai(\"gpt-4o-mini\");const modelWithTracing = wrapAISDKModel(vercelModel);const { textStream } = await streamText({  model: modelWithTracing,  prompt: \"Write a vegetarian lasagna recipe for 4 people.\",});for await (const chunk of textStream) {  ...} An example trace from running the above code will look the same as the above one from generateText."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functionalities of custom evaluators in the LangSmith UI?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on multiple splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What SDKs can be used to manage prompts programmatically according to the provided information?"}, {"role": "assistant", "content": "Manage prompts programmatically You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.\nAll functionality going forward will live in the langsmith package."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What commands are used to install the LangChain and Langsmith packages in Python and TypeScript?"}, {"role": "assistant", "content": "Install packages PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you do not have the LANGCHAIN_API_KEY set for your workspace?"}, {"role": "assistant", "content": "Configure environment variables If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=\"lsv2_...\" TerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create or update a prompt using the push prompt method in PythonLangChain or TypeScript?"}, {"role": "assistant", "content": "Push a prompt To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");url = prompts.push(\"joke-generator\", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.\nThis is useful for storing the model configuration you want to use with this prompt.\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"langchain-openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);prompts.push(\"joke-generator-with-model\", chain);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method do you use to pull a prompt from the LangChain Hub, and what information is required for pulling a public prompt?"}, {"role": "assistant", "content": "Pull a prompt To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { ChatOpenAI } from \"langchain-openai\";const prompt = prompts.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);chain.invoke({\"topic\": \"cats\"}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\nJust specify include_model when pulling the prompt.\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\nMake sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = prompts.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });chain.invoke({\"topic\": \"cats\"}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use prompts stored in LangSmith with a model provider's API?"}, {"role": "assistant", "content": "Use a prompt without LangChain If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.\nThese convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to list, delete, and like/unlike prompts in the LangSmith SDK?"}, {"role": "assistant", "content": "List, delete, and like prompts You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\nSee the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\"); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key features of LangGraph Cloud for deploying LangGraph applications?"}, {"role": "assistant", "content": "LangGraph Cloud LangGraph Cloud is a managed service for deploying and hosting LangGraph applications. Deploying applications with LangGraph Cloud shortens the time-to-market for developers. With one click, deploy a production-ready API with built-in persistence for your LangGraph application. LangGraph Cloud APIs are horizontally scalable and deployed with durable storage. LangGraph Cloud is seamlessly integrated with LangSmith and is accessible from the Deployments section in the left-hand navigation bar. See the official LangGraph Cloud documentation for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What function should be used to run pairwise evaluations of existing experiments in LangSmith?"}, {"role": "assistant", "content": "Run pairwise evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function\nwith two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the required SDK versions for using the evaluate_comparative function and what arguments does it accept?"}, {"role": "assistant", "content": "Use theevaluate_comparativefunction notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the inputs and outputs required for configuring pairwise evaluators, and how should the output dictionary be structured?"}, {"role": "assistant", "content": "Configure inputs and outputs for pairwise evaluators Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with\nruns[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent \"both equally bad\" or both to 1 for \"both equally good\". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for using LLM-based pairwise evaluators to compare the quality of responses from two AI assistants?"}, {"role": "assistant", "content": "Compare two experiments with LLM-based pairwise evaluators The following example uses a prompt\nwhich asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.\nThe prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=\"gpt-4\")        runnable = prompt | model    response = runnable.invoke({        \"question\": example.inputs[\"question\"],        \"answer_a\": runs[0].outputs[\"output\"] if runs[0].outputs is not None else \"N/A\",        \"answer_b\": runs[1].outputs[\"output\"] if runs[1].outputs is not None else \"N/A\",    })    score = response[\"Preference\"]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {\"key\": \"ranked_preference\", \"scores\": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [\"my-experiment-name-1\", \"my-experiment-name-2\"],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.\nimport type { Run, Example } from \"langsmith\";import { evaluateComparative } from \"langsmith/evaluation\";import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());import { z } from \"zod\";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(\"Expected at least two runs\");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? \"N/A\",    answer_b: runB?.outputs?.output ?? \"N/A\",  };    const output = await openai.chat.completions.create({    model: \"gpt-4-turbo\",    messages: [      {        role: \"system\",        content: [          \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.\",          \"You should choose the assistant that follows the user's instructions and answers the user's question better.\",          \"Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.\",          \"Begin your evaluation by comparing the two responses and provide a short explanation.\",          \"Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\",          \"Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\",        ].join(\" \"),      },      {        role: \"user\",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(\"\\n\\n\"),      },    ],    tool_choice: {      type: \"function\",      function: { name: \"Score\" },    },    tools: [      {        type: \"function\",        function: {          name: \"Score\",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output \"1\" if Assistant A answer is better based upon the factors above.`,            `Output \"2\" if Assistant B answer is better based upon the factors above.`,            `Output \"0\" if it is a tie.`,          ].join(\" \"),          parameters: {            type: \"object\",            properties: {              Preference: {                type: \"integer\",                description: \"Which assistant answer is preferred?\",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: \"ranked_preference\", scores };}await evaluateComparative([\"earnest-name-40\", \"reflecting-pump-91\"], {  evaluators: [evaluatePairwise],});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the Comparison View of pairwise experiments?"}, {"role": "assistant", "content": "View pairwise experiments Navigate to the \"Pairwise Experiments\" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of annotation queues in LangSmith?"}, {"role": "assistant", "content": "Use annotation queues Annotation queues are a powerful LangSmith feature that provide a streamlined, directed view for human annotators to attach feedback to specific runs.\nWhile you can always annotate runs inline, annotation queues provide another option to group runs together, then have annotators review and provide feedback on them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in creating an annotation queue and what settings can be configured for multiple annotators?"}, {"role": "assistant", "content": "Create an annotation queue To create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar.\nThen click + New annotation queue in the top right corner. Fill in the form with the name and description of the queue.\nYou can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace. There are a few settings related to multiple annotators: Number of reviewers per run: This determines the number of reviewers that must mark a run as \"Done\" for it to be removed from the queue. If you check \"All workspace members review each run,\" then a run will remain in the queue until all workspace members have marked it \"Done\".Enable reservations on runs: We recommend enabling reservations.\nThis will prevent multiple annotators from reviewing the same run at the same time. How do reservations work? When a reviewer views a run, the run is reserved for that reviewer for the specified \"reservation length\". If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time. What happens if time runs out? If a reviewer has viewed a run and then leaves the run without marking it \"Done\", the reservation will expire after the specified \"reservation length\".\nThe run is then released back into the queue and can be reserved by another reviewer. noteClicking \"Requeue at end\" will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run. Because of these settings, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue as well as anyone else's queue size. You can update these settings at any time by clicking on the pencil icon in the Annotation Queues section."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you assign runs to an annotation queue?"}, {"role": "assistant", "content": "Assign runs to an annotation queue To assign runs to an annotation queue, either: Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.\nSelect multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.\nSet up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue. tipIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction.\nTo learn more about how to capture user feedback from your LLM application, follow this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you review runs in an annotation queue?"}, {"role": "assistant", "content": "Review runs in an annotation queue To review runs in an annotation queue, navigate to the Annotation Queues section through the homepage or left-hand navigation bar.\nThen click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review. You can attach a comment, attach a score for a particular feedback criteria, add the run a dataset and/or mark the run as reviewed.\nYou can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the Trash icon next to \"View run\". The keyboard shortcuts shown can help streamline the review process."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended approach for logging LLM traces from OpenAI models using the langsmith SDK?"}, {"role": "assistant", "content": "Log custom LLM traces noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.\nIn order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format must the inputs for chat-style models be in, and what are the acceptable output formats?"}, {"role": "assistant", "content": "Chat-style models For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.ls_model_name: The name of the model, eg \"gpt-3.5-turbo\", \"claude-3-opus-20240307\", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ]}# Can also use one of:# output = {#     \"message\": {#         \"role\": \"assistant\",#         \"content\": \"Sure, what time would you like to book the table for?\"#     }# }## output = {#     \"role\": \"assistant\",#     \"content\": \"Sure, what time would you like to book the table for?\"# }## output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" }];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\"      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: \"assistant\",//     content: \"Sure, what time would you like to book the table for?\"//   }// };//// const output = {//   role: \"assistant\",//   content: \"Sure, what time would you like to book the table for?\"// };//// const output = [\"assistant\", \"Sure, what time would you like to book the table for?\"];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the `_reduce_chunks` function in the context of streaming outputs?"}, {"role": "assistant", "content": "Stream outputs For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}@traceable(    run_type=\"llm\",    reduce_fn=_reduce_chunks,    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def my_streaming_chat_model(messages: list):    for chunk in [\"Hello, \" + messages[1][\"content\"]]:        yield {            \"choices\": [                {                    \"message\": {                        \"content\": chunk,                        \"role\": \"assistant\",                    }                }            ]        }list(    my_streaming_chat_model(        [            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},            {\"role\": \"user\", \"content\": \"polly the parrot\"},        ],    ))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can token counts be manually provided to LangSmith in a function's response?"}, {"role": "assistant", "content": "Manually provide token counts Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.\nMany models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.\nIf token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.\nIf using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ],    \"usage_metadata\": {        \"input_tokens\": 27,        \"output_tokens\": 13,        \"total_tokens\": 40,    },}@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for inputs and outputs in instruct-style models?"}, {"role": "assistant", "content": "Instruct-style models For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.\nThe same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def hello_llm(prompt: str):    return {        \"choices\": [            {\"text\": \"Hello, \" + prompt}        ],        \"usage_metadata\": {            \"input_tokens\": 4,            \"output_tokens\": 5,            \"total_tokens\": 9,        },    }hello_llm(\"polly the parrot\\n\")import { traceable } from \"langsmith/traceable\";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: \"Hello, \" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: \"llm\", name: \"hello_llm\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await helloLLM({ prompt: \"polly the parrot\\n\" }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can users upload experiments run outside of LangSmith to view results in the LangSmith UI?"}, {"role": "assistant", "content": "Upload experiments run outside of LangSmith with the REST API Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required to upload an experiment using the /datasets/upload-experiment endpoint?"}, {"role": "assistant", "content": "Request body schema Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within\nthe experiment. Each object in the results represents a \"row\" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name\nrefer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset\nin LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  \"experiment_name\": \"string (required)\",  \"experiment_description\": \"string (optional)\",  \"experiment_start_time\": \"datetime (required)\",  \"experiment_end_time\": \"datetime (required)\",  \"dataset_id\": \"uuid (optional - an external dataset id, used to group experiments together)\",  \"dataset_name\": \"string (optional - must provide either dataset_id or dataset_name)\",  \"dataset_description\": \"string (optional)\",  \"experiment_metadata\": { // Object (any shape - optional)    \"key\": \"value\"  },  \"summary_experiment_scores\": [ // List of summary feedback objects (optional)    {      \"key\": \"string (required)\",      \"score\": \"number (optional)\",      \"value\": \"string (optional)\",      \"comment\": \"string (optional)\",      \"feedback_source\": { // Object (optional)        \"type\": \"string (required)\"      },      \"feedback_config\": { // Object (optional)        \"type\": \"string enum: continuous, categorical, or freeform\",        \"min\": \"number (optional)\",        \"max\": \"number (optional)\",        \"categories\": [ // List of feedback category objects (optional)            \"value\": \"number (required)\",            \"label\": \"string (optional)\"        ]      },      \"created_at\": \"datetime (optional - defaults to now)\",      \"modified_at\": \"datetime (optional - defaults to now)\",      \"correction\": \"Object or string (optional)\"    }  ],  \"results\": [ // List of experiment row objects (required)    {      \"row_id\": \"uuid (required)\",      \"inputs\": {     // Object (required - any shape). This will        \"key\": \"val\"  // be the input to both the run and the dataset example.      },      \"expected_outputs\": { // Object (optional - any shape).        \"key\": \"val\"        // These will be the outputs of the dataset examples.      },      \"actual_outputs\": { // Object (optional - any shape).        \"key\": \"val       // These will be the outputs of the runs.      },      \"evaluation_scores\": [ // List of feedback objects for the run (optional)        {            \"key\": \"string (required)\",            \"score\": \"number (optional)\",            \"value\": \"string (optional)\",            \"comment\": \"string (optional)\",            \"feedback_source\": { // Object (optional)                \"type\": \"string (required)\"            },            \"feedback_config\": { // Object (optional)                \"type\": \"string enum: continuous, categorical, or freeform\",                \"min\": \"number (optional)\",                \"max\": \"number (optional)\",                \"categories\": [ // List of feedback category objects (optional)                    \"value\": \"number (required)\",                    \"label\": \"string (optional)\"                ]            },            \"created_at\": \"datetime (optional - defaults to now)\",            \"modified_at\": \"datetime (optional - defaults to now)\",            \"correction\": \"Object or string (optional)\"        }      ],      \"start_time\": \"datetime (required)\", // The start/end times for the runs will be used to      \"end_time\": \"datetime (required)\",   // calculate latency. They must all fall between the      \"run_name\": \"string (optional)\",     // start and end times for the experiment.      \"error\": \"string (optional)\",      \"run_metadata\": { // Object (any shape - optional)        \"key\": \"value\"      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for uploading multiple experiments to the same dataset?"}, {"role": "assistant", "content": "Considerations You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together\nunder a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if\nyou only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key details of the experiment uploaded to LangSmith, including its name, description, and start and end times?"}, {"role": "assistant", "content": "Example request Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  \"experiment_name\": \"My external experiment\",  \"experiment_description\": \"An experiment uploaded to LangSmith\",  \"dataset_name\": \"my-external-dataset\",  \"summary_experiment_scores\": [    {      \"key\": \"summary_accuracy\",      \"score\": 0.9,      \"comment\": \"Great job!\"    }  ],  \"results\": [    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the weather in San Francisco today?\"      },      \"expected_outputs\": {        \"output\": \"Sorry, I am unable to provide information about the current weather.\"      },      \"actual_outputs\": {        \"output\": \"The weather is partly cloudy with a high of 65.\"      },      \"evaluation_scores\": [        {          \"key\": \"hallucination\",          \"score\": 1,          \"comment\": \"The chatbot made up the weather instead of identifying that \"                     \"they don't have enough info to answer the question. This is \"                     \"a hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:39\",      \"end_time\": \"2024-08-03T00:12:41\",      \"run_name\": \"Chatbot\"    },    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the square root of 49?\"      },      \"expected_outputs\": {        \"output\": \"The square root of 49 is 7.\"      },      \"actual_outputs\": {        \"output\": \"7.\"      },      \"evaluation_scores\": [       {          \"key\": \"hallucination\",          \"score\": 0,          \"comment\": \"The chatbot correctly identified the answer. This is not a \"                     \"hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:40\",      \"end_time\": \"2024-08-03T00:12:42\",      \"run_name\": \"Chatbot\"    }  ],  \"experiment_start_time\": \"2024-08-03T00:12:38\",  \"experiment_end_time\": \"2024-08-03T00:12:43\"}resp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\",    json=body,    headers={\"x-api-key\": os.environ[\"LANGCHAIN_API_KEY\"]})print(resp.json()) Below is the response received: {  \"dataset\": {    \"name\": \"my-external-dataset\",    \"description\": null,    \"created_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"data_type\": \"kv\",    \"inputs_schema_definition\": null,    \"outputs_schema_definition\": null,    \"externally_managed\": true,    \"id\": \"<<uuid>>\",    \"tenant_id\": \"<<uuid>>\",    \"example_count\": 0,    \"session_count\": 0,    \"modified_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"last_session_start_time\": null  },  \"experiment\": {    \"start_time\": \"2024-08-03T00:12:38\",    \"end_time\": \"2024-08-03T00:12:43+00:00\",    \"extra\": null,    \"name\": \"My external experiment\",    \"description\": \"An experiment uploaded to LangSmith\",    \"default_dataset_id\": null,    \"reference_dataset_id\": \"<<uuid>>\",    \"trace_tier\": \"longlived\",    \"id\": \"<<uuid>>\",    \"run_count\": null,    \"latency_p50\": null,    \"latency_p99\": null,    \"first_token_p50\": null,    \"first_token_p99\": null,    \"total_tokens\": null,    \"prompt_tokens\": null,    \"completion_tokens\": null,    \"total_cost\": null,    \"prompt_cost\": null,    \"completion_cost\": null,    \"tenant_id\": \"<<uuid>>\",    \"last_run_start_time\": null,    \"last_run_start_time_live\": null,    \"feedback_stats\": null,    \"session_feedback_stats\": null,    \"run_facets\": null,    \"error_rate\": null,    \"streaming_rate\": null,    \"test_run_number\": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.\nIf you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this\ninformation in the request body)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will you see after clicking on your newly-created dataset in the UI?"}, {"role": "assistant", "content": "View the experiment in the UI Now, login to the UI and click on your newly-created dataset! You should see a single experiment:\n Your examples will have been uploaded:\n Clicking on your experiment will bring you to the comparison view:\n As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will API keys prefixed with ls__ be disabled in favor of lsv2 style keys?"}, {"role": "assistant", "content": "LangSmith Release Notes noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith v0.6 during the week of June 17, 2024?"}, {"role": "assistant", "content": "Week of June 17, 2024 - LangSmith v0.6 LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced in LangSmith?"}, {"role": "assistant", "content": "New Features Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the performance and reliability of run rules in the Hub SDK version 0.1.20 or greater?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the default port and Helm configuration in the recent infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes [Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes?"}, {"role": "assistant", "content": "Admin changes Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.5.x and earlier following the release of v0.6?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and features does LangSmith v0.5 introduce?"}, {"role": "assistant", "content": "Week of May 13, 2024 - LangSmith v0.5 LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after the LangSmith Helm release v0.7 in August 2024?"}, {"role": "assistant", "content": "Breaking changes We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced, including role-based access controls and improvements in regression testing and production monitoring?"}, {"role": "assistant", "content": "New Features Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to improve performance and reliability in resource management?"}, {"role": "assistant", "content": "Performance and Reliability Changes Split ingest, session deletion, and automation jobs to execute within separate resource pools."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default storage size for Clickhouse persistence in LangSmith v0.4, and how can it be adjusted?"}, {"role": "assistant", "content": "Infrastructure changes As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes regarding access controls and API keys?"}, {"role": "assistant", "content": "Admin changes Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.4.x and earlier following the release of v0.5?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.4 during the week of March 25, 2024?"}, {"role": "assistant", "content": "Week of March 25, 2024 - LangSmith v0.4 LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes should be made to ensure backwards compatibility when updating to the new release?"}, {"role": "assistant", "content": "Breaking changes This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to enhance performance and reliability in the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the image repositories and Clickhouse persistence storage in the infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to the API key management and OAuth flow in the recent admin updates?"}, {"role": "assistant", "content": "Admin changes Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.3.x and earlier versions following the release of 0.4?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.3 during the week of February 21, 2024?"}, {"role": "assistant", "content": "Week of Februrary 21, 2024 - LangSmith v0.3 LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do to retain run data when migrating from LangSmith v0.1?"}, {"role": "assistant", "content": "Breaking changes This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the LangSmith UI regarding trace ingestion and display delay?"}, {"role": "assistant", "content": "Performance and Reliability Changes Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is available regarding admin changes?"}, {"role": "assistant", "content": "Admin changes nan"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.2.x and earlier versions following the release of 0.3?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features does LangSmith 0.2 include?"}, {"role": "assistant", "content": "Week of January 29, 2024 - LangSmith v0.2 LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What versions of langsmith-sdk are required for this release to support changes in pagination of API results?"}, {"role": "assistant", "content": "Requirements This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to make custom metadata in traces searchable after the change in search syntax?"}, {"role": "assistant", "content": "Breaking changes The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,}) and then search using has(metadata, '{\"user_name\": ...}')"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to the performance and reliability of the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What database service has been added to improve performance and scalability in LangSmith?"}, {"role": "assistant", "content": "Infrastructure Changes Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the new maximum number of users allowed per organization for new organizations?"}, {"role": "assistant", "content": "Admin changes Increased the maximum number of users per organization from 5 to 100 for new organizations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of LangSmith 0.1.x following the release of version 0.2?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can an individual workspace be deleted from an organization in LangSmith?"}, {"role": "assistant", "content": "Deleting Workspaces The LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table. This command using the Workspace ID as an argument."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools and database connection details are required to prepare for running the migration script?"}, {"role": "assistant", "content": "Prerequisites Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/PostgreSQL clienthttps://www.postgresql.org/download/PostgreSQL database connection:HostPortUsernameIf using the bundled version, this is postgresPasswordIf using the bundled version, this is postgresDatabase nameIf using the bundled version, this is postgresClickhouse database credentialsHostPortUsernameIf using the bundled version, this is defaultPasswordIf using the bundled version, this is passwordDatabase nameIf using the bundled version, this is defaultConnectivity to the PostgreSQL database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the postgresql service to your local machine.Run kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine.Connectivity to the Clickhouse database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.Run kubectl port-forward svc/langsmith-clickhouse 8123:8123 to port forward the clickhouse service to your local machine.If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port 8443"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should be run to delete a specific workspace using the deletion script?"}, {"role": "assistant", "content": "Running the deletion script for a single workspace Run the following command to run the workspace removal script: sh delete_workspace.sh <postgres_url> <clickhouse_url> --workspace_id <workspace_id> For example, if you are using the bundled version with port-forwarding, the command would look like: sh delete_workspace.sh \"postgres://postgres:postgres@localhost:5432/postgres\" \"clickhouse://default:password@localhost:8123/default\" --workspace_id 4ec70ec7-0808-416a-b836-7100aeec934b If you visit the Langsmith UI, you should now see workspace is deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you customize the run name in LangSmith when creating a run?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What functionality has been added to help users construct queries in natural language?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you are using LangChain in relation to logging traces to LangSmith?"}, {"role": "assistant", "content": "Annotate code for tracing There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces using the @traceable decorator in Python and the traceable function in TypeScript?"}, {"role": "assistant", "content": "Use@traceable/traceable LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\nfrom langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful assistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What's a good name for a store that sells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(\"colorful socks\")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.\nimport { traceable } from \"langsmith/traceable\";import OpenAI from \"openai\";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: \"system\" as const,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: \"formatPrompt\" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: messages,            temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: \"parseOutput\" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(\"colorful socks\");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: \"runPipeline\" });await runPipeline();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces when using the wrap_openai or wrapOpenAI methods?"}, {"role": "assistant", "content": "Wrap the OpenAI client The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!\nThe wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"@traceable(name=\"Chat Pipeline\")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(\"Can you summarize this morning's meetings?\")import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return \"During this morning's meeting, we solved all world conflict.\";}, { name: \"Retrieve Context\", run_type: \"tool\" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: \"system\",            content:                \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",        },        { role: \"user\", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: \"Chat Pipeline\" });await chatPipeline(\"Can you summarize this morning's meetings?\");"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the RunTree API in LangSmith?"}, {"role": "assistant", "content": "Use theRunTreeAPI Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually\ncreate runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not\nnecessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \"Can you summarize this morning's meetings?\"# Create a top-level runpipeline = RunTree(    name=\"Chat Pipeline\",    run_type=\"chain\",    inputs={\"question\": question})# This can be retrieved in a retrieval stepcontext = \"During this morning's meeting, we solved all world conflict.\"messages = [    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}]# Create a child runchild_llm_run = pipeline.create_child(    name=\"OpenAI Call\",    run_type=\"llm\",    inputs={\"messages\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from \"openai\";import { RunTree } from \"langsmith\";// This can be a user input to your appconst question = \"Can you summarize this morning's meetings?\";const pipeline = new RunTree({    name: \"Chat Pipeline\",    run_type: \"chain\",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = \"During this morning's meeting, we solved all world conflict.\";const messages = [    { role: \"system\", content: \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { role: \"user\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: \"OpenAI Call\",    run_type: \"llm\",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using the trace context manager in Python?"}, {"role": "assistant", "content": "Use thetracecontext manager (Python only) In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {\"input\": \"Can you summarize this morning's meetings?\"}with trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:    output = chat_pipeline(\"Can you summarize this morning's meetings?\")    rt.end(outputs={\"output\": output})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the basic steps for backtesting a new system against production data in LangSmith?"}, {"role": "assistant", "content": "Backtesting Deploying your app into production is just one step in a longer journey continuous improvement. You'll likely want to develop other candidate systems that improve on your production model using improved prompts, llms, indexing strategies, and other techniques. While you may have a set of offline datasets already created by this point, it's often useful to compare system performance on more recent production data. This notebook shows how to do this in LangSmith. The basic steps are: Sample runs to test against from your production tracing project.Convert runs to dataset + initial experiment.Run new system against the dataset to compare. You will then have a new dataset of representative inputs you can you can version and backtest your models against. Note: In most cases, you won't have \"ground truth\" answers in this case, but you can manually compare and label or use reference-free evaluators to score the outputs.(If your application DOES permit capturing ground-truth labels, then we obviously recommend you use those."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using the beta utilities in the langsmith library?"}, {"role": "assistant", "content": "Prerequisites Install + set environment variables. This requires langsmith>=0.1.29 to use the beta utilities. %%capture --no-stderr%pip install -U --quiet langsmith langchain_anthropic langchainhub langchain import os# Set the project name to whichever project you'd like to be testing againstproject_name = \"Tweet Critic\"os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR ANTHROPIC API KEY\"os.environ[\"LANGCHAIN_PROJECT\"] = project_name (Preliminary) Production Deployment You likely have a project already and can skip this step. We'll simulate one here so no one reading this notebook gets left out.\nOur example app is a \"tweet critic\" that revises tweets we put out. from langchain import hubfrom langchain_anthropic import ChatAnthropicfrom langchain_core.messages import HumanMessagefrom langchain_core.output_parsers import StrOutputParserprompt = hub.pull(\"wfh/tweet-critic:7e4f539e\")llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")system = prompt | llm | StrOutputParser()inputs = [    \"\"\"RAG From Scratch: Our RAG From Scratch video series covers some important RAG concepts in short, focused videos with code. This is the 10th video and it discusses query routing. Problem: We sometimes have multiple datastores (e.g., different vector DBs, SQL DBs, etc) and prompts to choose from based on a user query. Idea: Logical routing can use an LLM to decide which datastore is more appropriate. Semantic routing embeds the query and prompts, then chooses the best prompt based on similarity. Video: https://youtu.be/pfpIndq7Fi8 Code: https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb\"\"\",    \"\"\"@Voyage_AI_ Embedding Integration Package Use the same custom embeddings that power Chat LangChain via the new langchain-voyageai package! Voyage AI builds custom embedding models that can improve retrieval quality. ChatLangChain: https://chat.langchain.com Python Docs: https://python.langchain.com/docs/integrations/providers/voyageai\"\"\",    \"\"\"Implementing RAG: How to Write a Graph Retrieval Query in LangChain Our friends at @neo4j have a nice guide on combining LLMs and graph databases. Blog:\"\"\",    \"\"\"Text-to-PowerPoint with LangGraph.js You can now generate PowerPoint presentations from text! @TheGreatBonnie wrote a guide showing how to use LangGraph.js, @tavilyai, and @CopilotKit to build a Next.js app for this. Tutorial: https://dev.to/copilotkit/how-to-build-an-ai-powered-powerpoint-generator-langchain-copilotkit-openai-nextjs-4c76 Repo: https://github.com/TheGreatBonnie/aipoweredpowerpointapp\"\"\",    \"\"\"Build an Answer Engine Using Groq, Mixtral, Langchain, Brave & OpenAI in 10 Min Our friends at @Dev__Digest have a tutorial on building an answer engine over the internet. Code: https://github.com/developersdigest/llm-answer-engine YouTube: https://youtube.com/watch?v=43ZCeBTcsS8&t=96s\"\"\",    \"\"\"Building a RAG Pipeline with LangChain and Amazon Bedrock Amazon Bedrock has great models for building LLM apps. This guide covers how to get started with them to build a RAG pipeline. https://gettingstarted.ai/langchain-bedrock/\"\"\",    \"\"\"SF Meetup on March 27! Join our meetup to hear from LangChain and Pulumi experts and learn about building AI-enabled capabilities. Sign up: https://meetup.com/san-francisco-pulumi-user-group/events/299491923/?utm_campaign=FY2024Q3_Meetup_PUG%20SF&utm_content=286236214&utm_medium=social&utm_source=twitter&hss_channel=tw-837770064870817792\"\"\",    \"\"\"Chat model response metadata @LangChainAI chat model invocations now include metadata like logprobs directly in the output. Upgrade your version of `langchain-core` to try it. PY: https://python.langchain.com/docs/modules/model_io/chat/logprobs JS: https://js.langchain.com/docs/integrations/chat/openai#generation-metadata\"\"\",    \"\"\"Benchmarking Query Analysis in High Cardinality Situations Handling high-cardinality categorical values can be challenging. This blog explores 6 different approaches you can take in these situations. Blog: https://blog.langchain.dev/high-cardinality\"\"\",    \"\"\"Building Google's Dramatron with LangGraph.js & Claude 3 We just released a long YouTube video (1.5 hours!) on building Dramatron using LangGraphJS and @AnthropicAI's Claude 3 \"Haiku\" model. It's a perfect fit for LangGraph.js and Haiku's speed. Check out the tutorial: https://youtube.com/watch?v=alHnQjyn7hg\"\"\",    \"\"\"Document Loading Webinar with @AirbyteHQ Join a webinar on document loading with PyAirbyte and LangChain on 3/14 at 10am PDT. Features our founding engineer @eyfriis and the @aaronsteers and Bindi Pankhudi team. Register: https://airbyte.com/session/airbyte-monthly-ai-demo\"\"\",]_ = system.batch(    [{\"messages\": [HumanMessage(content=content)]} for content in inputs],    {\"max_concurrency\": 3},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for converting production runs into an experiment dataset using the convert_runs_to_test function?"}, {"role": "assistant", "content": "Convert Prod Runs to Experiment The first step is to generate a dataset based on the production inputs.\nThen copy over all the traces to serve as a baseline run. convert_runs_to_test is a function which takes some runs and does the following: The inputs, and optionally the outputs, are saved to a dataset as Examples.The inputs and outputs are stored as an experiment, as if you had run the evaluate\nfunction and received those outputs. from datetime import datetime, timedelta, timezonefrom langsmith import Clientfrom langsmith.beta import convert_runs_to_test# How we are sampling runs to include in our datasetend_time = datetime.now(tz=timezone.utc)start_time = end_time - timedelta(days=1)run_filter = f'and(gt(start_time, \"{start_time.isoformat()}\"), lt(end_time, \"{end_time.isoformat()}\"))'# Fetch the runs we want to convert to a dataset/experimentclient = Client()prod_runs = list(    client.list_runs(        project_name=project_name,        execution_order=1,        filter=run_filter,    ))# Name of the dataset we want to createdataset_name = f'{project_name}-backtesting {start_time.strftime(\"%Y-%m-%d\")}-{end_time.strftime(\"%Y-%m-%d\")}'# This converts the runs to a dataset + experiment# It does not actually invoke your modelconvert_runs_to_test(    prod_runs,    # Name of the resulting dataset    dataset_name=dataset_name,    # Whether to include the run outputs as reference/ground truth    include_outputs=False,    # Whether to include the full traces in the resulting experiment    # (default is to just include the root run)    load_child_runs=True,)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of running inference on the new system as described in the text?"}, {"role": "assistant", "content": "Benchmark new system Now we have the dataset and prod runs saved as an experiment. Let's run inference on our new system to compare. from langsmith.evaluation import evaluatedef predict(example_input: dict):    # The dataset includes serialized messages that we    # must convert to a format accepted by our system.    messages = {        \"messages\": [            (message[\"type\"], message[\"content\"])            for message in example_input[\"messages\"]        ]    }    return system.invoke(messages)# Use an updated version of the promptprompt = hub.pull(\"wfh/tweet-critic:34c57e4f\")llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")system = prompt | llm | StrOutputParser()test_results = evaluate(    predict, data=dataset_name, experiment_prefix=\"HaikuBenchmark\", max_concurrency=3)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new feature allows you to compare outputs in the UI?"}, {"role": "assistant", "content": "Review runs You can now compare the outputs in the UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps can be taken to review the results after sampling production runs and benchmarking other systems?"}, {"role": "assistant", "content": "Conclusion Congrats! You've sampled production runs and started benchmarking other systems against them.\nIn this exercise, we chose not to apply any evaluators to simplify things (since we lack ground-truth answers for this task).\nYou can manually review the results in LangSmith and/or apply a reference-free evaluator to the results to generate metrics instead."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you are unable to use the Python or TypeScript SDKs for running evals?"}, {"role": "assistant", "content": "Run evals with the REST API Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.\nHowever, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the code provided in the text?"}, {"role": "assistant", "content": "Create a dataset Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries - API Example\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in running a single experiment using the provided API?"}, {"role": "assistant", "content": "Run a single experiment First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { \"dataset\": dataset_id }resp = requests.get(    \"https://api.smith.langchain.com/api/v1/examples\",    params=params,    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"def run_completion_on_example(example, model_name, experiment_id):    \"\"\"Run completions on a list of examples.\"\"\"    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        \"\"\"Function to post a new run to the API.\"\"\"        data = {            \"id\": run_id.hex,            \"name\": name,            \"run_type\": run_type,            \"inputs\": inputs,            \"start_time\": datetime.utcnow().isoformat(),            \"reference_example_id\": example[\"id\"],            \"session_id\": experiment_id,        }        if parent_id:            data[\"parent_run_id\"] = parent_id.hex        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/runs\", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        \"\"\"Function to patch a run with outputs.\"\"\"        resp = requests.patch(            f\"https://api.smith.langchain.com/api/v1/runs/{run_id}\",            json={                \"outputs\": outputs,                \"end_time\": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    text = example[\"inputs\"][\"text\"]    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, \"LLM Pipeline\", \"chain\", {\"text\": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {\"label\": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (\"gpt-3.5-turbo\", \"gpt-4o-mini\")experiment_ids = []for model_name in model_names:    resp = requests.post(        \"https://api.smith.langchain.com/api/v1/sessions\",        json={            \"start_time\": datetime.utcnow().isoformat(),            \"reference_dataset_id\": str(dataset_id),            \"description\": \"An optional description for the experiment\",            \"name\": f\"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}\",  # A name for the experiment            \"extra\": {                \"metadata\": {\"foo\": \"bar\"},  # Optional metadata            },        },        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )    experiment = resp.json()    experiment_ids.append(experiment[\"id\"])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[\"id\"])    # Issue a patch request to \"end\" the experiment by updating the end_time    requests.patch(        f\"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}\",        json={\"end_time\": datetime.utcnow().isoformat()},        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for running a pairwise experiment and ranking the outputs?"}, {"role": "assistant", "content": "Run a pairwise experiment Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.\nFor more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/comparative\",    json={        \"experiment_ids\": experiment_ids,        \"name\": \"Toxicity detection - API Example - Comparative - \" + str(uuid4())[0:8],        \"description\": \"An optional description for the comparative experiment\",        \"extra\": {            \"metadata\": {\"foo\": \"bar\"},  # Optional metadata        },        \"reference_dataset_id\": str(dataset_id),    },    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[\"id\"]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f\"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative\",    params={\"id\": comparative_experiment_id},    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()[0]experiment_ids = [info[\"id\"] for info in comparative_experiment[\"experiments_info\"]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f\"https://api.smith.langchain.com/api/v1/runs/query\",    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]},    json={        \"session\": experiment_ids,        \"is_root\": True, # Only fetch root runs (spans) which contain the end outputs        \"select\": [\"id\", \"reference_example_id\", \"outputs\"],    }).json()runs = runs[\"runs\"]for run in runs:    example_id = run[\"reference_example_id\"]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f\"Example ID: {example_id}\")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f\"Run ID: {run['id']}\")        feedback = {            \"score\": 1 if i == 0 else 0,            \"run_id\": str(run[\"id\"]),            \"key\": \"ranked_preference\",            \"feedback_group_id\": str(feedback_group_id),            \"comparative_experiment_id\": comparative_experiment_id,        }        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/feedback\",            json=feedback,            headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}        )        resp.raise_for_status()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in generating synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended method for exporting runs in LangSmith traces?"}, {"role": "assistant", "content": "Export traces Recommended ReadingBefore diving into this content, it might be helpful to read the following:Run (span) data formatLangSmith API ReferenceLangSmith trace query syntax The recommended way to export runs (the span data in LangSmith traces) is to use the list_runs method in the SDK or /runs/query endpoint in the API. LangSmith stores traces in a simple format that is specified in the Run (span) data format."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using filter arguments in simple queries?"}, {"role": "assistant", "content": "Use filter arguments For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client, Run } from \"langsmith\";const client = new Client(); Below are some examples of ways to list runs using keyword arguments:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you list all runs in a specific project using the provided code?"}, {"role": "assistant", "content": "List all runs in a project PythonTypeScriptproject_runs = client.list_runs(project_name=\"<your_project>\")// Download runs in a projectconst projectRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",})) {  projectRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What code is used to list LLM runs in the last 24 hours?"}, {"role": "assistant", "content": "List LLM and Chat runs in the last 24 hours PythonTypeScripttodays_llm_runs = client.list_runs(    project_name=\"<your_project>\",    start_time=datetime.now() - timedelta(days=1),    run_type=\"llm\",)const todaysLlmRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  startTime: new Date(Date.now() - 1000 * 60 * 60 * 24),  runType: \"llm\",})) {  todaysLlmRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for root runs in a project using the provided code?"}, {"role": "assistant", "content": "List root runs in a project Root runs are runs that have no parents. These are assigned a value of True for is_root. You can use this to filter for root runs. PythonTypeScriptroot_runs = client.list_runs(    project_name=\"<your_project>\",    is_root=True)const rootRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  isRoot: 1,})) {  rootRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you list runs without errors in a project using Python and TypeScript?"}, {"role": "assistant", "content": "List runs without errors PythonTypeScriptcorrect_runs = client.list_runs(project_name=\"<your_project>\", error=False)const correctRuns: Run[] = [];for await (const run of client.listRuns({  projectName: \"<your_project>\",  error: false,})) {  correctRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens when a list of run IDs is provided in the list_runs function?"}, {"role": "assistant", "content": "List runs by run ID Ignores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like project_name, run_type, etc. and directly return the runs matching the given IDs. If you have a list of run IDs, you can list them directly: PythonTypeScriptrun_ids = ['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']selected_runs = client.list_runs(id=run_ids)const runIds = [  \"a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836\",  \"9398e6be-964f-4aa4-8ae9-ad78cd4b7074\",];const selectedRuns: Run[] = [];for await (const run of client.listRuns({  id: runIds,})) {  selectedRuns.push(run);};"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What can you use for more complex queries according to the filter query language reference?"}, {"role": "assistant", "content": "Use filter query language For more complex queries, you can use the query language described in the filter query language refernece."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the runs named \"extractor\" that have a \"user_score\" feedback score of 1?"}, {"role": "assistant", "content": "List all runs called \"extractor\" whose root of the trace was assigned feedback \"user_score\" score of 1 PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='eq(name, \"extractor\")',    trace_filter='and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))')client.listRuns({  projectName: \"<your_project>\",  filter: 'eq(name, \"extractor\")',  traceFilter: 'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I list runs with a \"star_rating\" key that have a score greater than 4 using the provided code?"}, {"role": "assistant", "content": "List runs with \"star_rating\" key whose score is greater than 4 PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(eq(feedback_key, \"star_rating\"), gt(feedback_score, 4))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list runs that took longer than 5 seconds to complete in a specified project?"}, {"role": "assistant", "content": "List runs that took longer than 5 seconds to complete PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(latency, \"5s\")')client.listRuns({projectName: \"<your_project>\", filter: 'gt(latency, \"5s\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs where total_tokens is greater than 5000?"}, {"role": "assistant", "content": "List all runs wheretotal_tokensis greater than 5000 PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(total_tokens, 5000)')client.listRuns({projectName: \"<your_project>\", filter: 'gt(total_tokens, 5000)'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs where the \"error\" field is not null in a specified project?"}, {"role": "assistant", "content": "List all runs that have \"error\" not equal to null PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='neq(error, null)')client.listRuns({projectName: \"<your_project>\", filter: 'neq(error, null)'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list all runs with a start time greater than \"2023-07-15T12:34:56Z\" in a specific project?"}, {"role": "assistant", "content": "List all runs wherestart_timeis greater than a specific timestamp PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='gt(start_time, \"2023-07-15T12:34:56Z\")')client.listRuns({projectName: \"<your_project>\", filter: 'gt(start_time, \"2023-07-15T12:34:56Z\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I list all runs that contain the string \"substring\" in a specified project using Python or TypeScript?"}, {"role": "assistant", "content": "List all runs that contain the string \"substring\" PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='search(\"substring\")')client.listRuns({projectName: \"<your_project>\", filter: 'search(\"substring\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to list all runs tagged with the git hash \"2aa1cf4\" in a specified project?"}, {"role": "assistant", "content": "List all runs that are tagged with the git hash \"2aa1cf4\" PythonTypeScriptclient.list_runs(project_name=\"<your_project>\", filter='has(tags, \"2aa1cf4\")')client.listRuns({projectName: \"<your_project>\", filter: 'has(tags, \"2aa1cf4\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the \"chain\" type runs that took more than 10 seconds and had total_tokens greater than 5000?"}, {"role": "assistant", "content": "List all \"chain\" type runs that took more than 10 seconds and had total_tokens greater than 5000 PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(eq(run_type, \"chain\"), gt(latency, 10), gt(total_tokens, 5000))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(eq(run_type, \"chain\"), gt(latency, 10), gt(total_tokens, 5000))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the criteria for listing runs that started after the timestamp \"2023-07-15T12:34:56Z\"?"}, {"role": "assistant", "content": "List all runs that started after a specific timestamp and either have \"error\" not equal to null or a \"Correctness\" feedback score equal to 0 PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key, \"Correctness\"), eq(feedback_score, 0.0))))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(gt(start_time, \"2023-07-15T12:34:56Z\"), or(neq(error, null), and(eq(feedback_key, \"Correctness\"), eq(feedback_score, 0.0))))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the query to list all runs with tags \"experimental\" or \"beta\" and a latency greater than 2 seconds?"}, {"role": "assistant", "content": "Complex query: List all runs wheretagsinclude \"experimental\" or \"beta\" and latency is greater than 2 seconds PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='and(or(has(tags, \"experimental\"), has(tags, \"beta\")), gt(latency, 2))')client.listRuns({  projectName: \"<your_project>\",  filter: 'and(or(has(tags, \"experimental\"), has(tags, \"beta\")), gt(latency, 2))'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you perform a full text search for traces in a run using the search() function?"}, {"role": "assistant", "content": "Search trace trees by full text You can use thesearch()function without any specific field to do a full text search across all string fields in a run. This\nallows you to quickly find traces that match a search term. PythonTypeScriptclient.list_runs(  project_name=\"<your_project>\",  filter='search(\"image classification\")')client.listRuns({  projectName: \"<your_project>\",  filter: 'search(\"image classification\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you check for the presence of the \"user_id\" metadata key in your runs?"}, {"role": "assistant", "content": "Check for presence of metadata If you want to check for the presence of metadata, you can use the eq operator, optionally with an and statement to match by value. This is useful if you want to log more structured information\nabout your runs. PythonTypeScriptto_search = {    \"user_id\": \"\"}# Check for any run with the \"user_id\" metadata keyclient.list_runs(  project_name=\"default\",  filter=\"eq(metadata_key, 'user_id')\")# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))\")// Check for any run with the \"user_id\" metadata keyclient.listRuns({  projectName: 'default',  filter: `eq(metadata_key, 'user_id')`});// Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs to include only those with environment metadata set to 'production'?"}, {"role": "assistant", "content": "Check for environment details in metadata. A common pattern is to add environment\ninformation to your traces via metadata. If you want to filter for runs containing\nenvironment metadata, you can use the same pattern as above: PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on a conversation ID in the metadata?"}, {"role": "assistant", "content": "Check for conversation ID in metadata Another common way to associate traces\nin the same conversation is by using a shared conversation ID. If you want to filter\nruns based on a conversation ID in this way, you can search for that ID in the metadata. PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you combine multiple filters to search for runs named \"ChatOpenAI\" with a specific conversation_id in their metadata?"}, {"role": "assistant", "content": "Combine multiple filters If you want to combine multiple conditions to refine your search, you can use the and operator along with other\nfiltering functions. Here's how you can search for runs named \"ChatOpenAI\" that also\nhave a specific conversation_id in their metadata: PythonTypeScriptclient.list_runs(  project_name=\"default\",  filter=\"and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))\")client.listRuns({  projectName: 'default',  filter: `and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))`});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the criteria for listing runs named \"RetrieveDocs\" in the provided query?"}, {"role": "assistant", "content": "Tree Filter List all runs named \"RetrieveDocs\" whose root run has a \"user_score\" feedback of 1 and any run in the full trace is named \"ExpandQuery\". This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace. PythonTypeScriptclient.list_runs(    project_name=\"<your_project>\",    filter='eq(name, \"RetrieveDocs\")',    trace_filter='and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))',    tree_filter='eq(name, \"ExpandQuery\")')client.listRuns({  projectName: \"<your_project>\",  filter: 'eq(name, \"RetrieveDocs\")',  traceFilter: 'and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))',  treeFilter: 'eq(name, \"ExpandQuery\")'})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a flattened view of traces that includes information on the tools used by the agent within each trace in Python?"}, {"role": "assistant", "content": "Advanced: export flattened trace view with child tool usage The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.\nThis can be used to analyze the behavior of your agents across multiple traces. This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information. To optimize the query, the example: Selects only the necessary fields when querying tool runs to reduce query time.Fetches root runs in batches while processing tool runs concurrently. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltafrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = \"my-project\"num_days = 30# List all tool runstool_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=\"tool\",    # We don't need to fetch inputs, outputs, and other values that # may increase the query time    select=[\"trace_id\", \"name\", \"run_type\"],)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50tool_runs_by_parent = defaultdict(lambda: defaultdict(set))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group tool runs by parent run ID    for run in tqdm(tool_runs):        # Collect all tools invoked within a given trace        tool_runs_by_parent[run.trace_id][\"tools_involved\"].add(run.name)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the tool runs        if len(tool_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(tool_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],                    )                )    if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = tool_runs_by_parent[root_run.id]        data.append(            {                \"run_id\": root_run.id,                \"run_name\": root_run.name,                \"run_type\": root_run.run_type,                \"inputs\": root_run.inputs,                \"outputs\": root_run.outputs,                \"tools_involved\": list(root_data[\"tools_involved\"]),            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of exporting retriever inputs and outputs within traces that have a specific feedback score?"}, {"role": "assistant", "content": "Advanced: export retriever IO for traces with feedback This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.\nThe following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score. Pythonfrom collections import defaultdictfrom concurrent.futures import Future, ThreadPoolExecutorfrom datetime import datetime, timedeltaimport pandas as pdfrom langsmith import Clientfrom tqdm.auto import tqdmclient = Client()project_name = \"your-project-name\"num_days = 1# List all tool runsretriever_runs = client.list_runs(    project_name=project_name,    start_time=datetime.now() - timedelta(days=num_days),    run_type=\"retriever\",    # This time we do want to fetch the inputs and outputs, since they    # may be adjusted by query expansion steps.    select=[\"trace_id\", \"name\", \"run_type\", \"inputs\", \"outputs\"],    trace_filter='eq(feedback_key, \"user_score\")',)data = []futures: list[Future] = []trace_cursor = 0trace_batch_size = 50retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))# Do not exceed rate limitwith ThreadPoolExecutor(max_workers=2) as executor:    # Group retriever runs by parent run ID    for run in tqdm(retriever_runs):        # Collect all retriever calls invoked within a given trace        for k, v in run.inputs.items():            retriever_runs_by_parent[run.trace_id][f\"retriever.inputs.{k}\"].append(v)        for k, v in (run.outputs or {}).items():            # Extend the docs            retriever_runs_by_parent[run.trace_id][f\"retriever.outputs.{k}\"].extend(v)        # maybe send a batch of parent run IDs to the server        # this lets us query for the root runs in batches        # while still processing the retriever runs        if len(retriever_runs_by_parent) % trace_batch_size == 0:            if this_batch := list(retriever_runs_by_parent.keys())[                trace_cursor : trace_cursor + trace_batch_size            ]:                trace_cursor += trace_batch_size                futures.append(                    executor.submit(                        client.list_runs,                        project_name=project_name,                        run_ids=this_batch,                        select=[                            \"name\",                            \"inputs\",                            \"outputs\",                            \"run_type\",                            \"feedback_stats\",                        ],                    )                )    if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:        futures.append(            executor.submit(                client.list_runs,                project_name=project_name,                run_ids=this_batch,                select=[\"name\", \"inputs\", \"outputs\", \"run_type\"],            )        )for future in tqdm(futures):    root_runs = future.result()    for root_run in root_runs:        root_data = retriever_runs_by_parent[root_run.id]        feedback = {            f\"feedback.{k}\": v.get(\"avg\")            for k, v in (root_run.feedback_stats or {}).items()        }        inputs = {f\"inputs.{k}\": v for k, v in root_run.inputs.items()}        outputs = {f\"outputs.{k}\": v for k, v in (root_run.outputs or {}).items()}        data.append(            {                \"run_id\": root_run.id,                \"run_name\": root_run.name,                **inputs,                **outputs,                **feedback,                **root_data,            }        )# (Optional): Convert to a pandas DataFrameimport pandas as pddf = pd.DataFrame(data)df.head()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features are available for setting up online evaluations in LangSmith?"}, {"role": "assistant", "content": "Set up online evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:Set up automation rules Online evaluations is a powerful LangSmith feature that allows you to run an LLM-as-a-judge evaluator on a set of your production traces. They are implemented as a possible action in an automation rule. Currently, we provide support for specifying a prompt template, a model, and a set of criteria to evaluate the runs on. After entering rules setup, you can select Online Evaluation from the list of possible actions:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What action must be selected to access the panel for configuring online evaluations?"}, {"role": "assistant", "content": "Configure online evaluations When selection Online Evaluation as an action in an automation, you are presented with a panel from which you can configure online evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What models are currently supported for selection in the dropdown?"}, {"role": "assistant", "content": "Model You can choose any model available in the dropdown. Currently, we support OpenAI, AzureOpenAI, and models hosted on Fireworks.\nIn order to set the API keys to use for these invocations, click on the Secrets & API Keys button and add the necessary keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for selecting or creating a prompt in the prompt template?"}, {"role": "assistant", "content": "Prompt template For the prompt template, you can select a suggested evaluator prompt, create a new prompt, or choose a prompt from the LangChain Hub. Suggested evaluator prompts We provide a list of pre-existing prompts that cater to common evaluator use cases. Create your own prompt We provide a base template from which you can form your own prompt. Pull a prompt from the LangChain Hub You can pull any structured prompt, private or public. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses.\nIf the prompt is your own, you can edit it in the playground and commit the version.\nIf the prompt is someone else's, you can fork the prompt, make your edits in the playground, and then pull in your new prompt to the online evaluator. When you choose a hub prompt for your online evaluator, the prompt will be locked to the commit version it was at rule creation. If you want to update the prompt, you can go to edit the online evaluator and re-select the prompt in the dropdown."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you map variables in your evaluator prompt based on recent runs?"}, {"role": "assistant", "content": "Mapping variables Prompts can be crafted with any variable name you choose. To map what is passed into your evaluator prompt from your runs or experiments, use the variable mapping inputs. There's a dropdown with suggestions provided based on the schema of your recent runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does the preview feature show when evaluating a prompt?"}, {"role": "assistant", "content": "Preview Previewing the prompt will show you an example of what the formatted prompt will look like. This preview pulls the input and output of the most recent run. In the case of a dataset evaluator, the preview will also pull reference output from an example in your dataset. noteYou can configure an evaluation prompt that doesn't match the schema of your recent runs, but the dropdown suggestions and preview function won't work as expected."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for naming a schema in the output schema?"}, {"role": "assistant", "content": "Output schema An evaluator will attach metadata tags to a run. These tags will have a name and a value. You can configure these in the Schema section.\nThe names and the descriptions of the fields will be passed into the prompt. Behind the scenes, we use tool calling to coerce the output of the LLM into the score you specify. noteThe name of the schema cannot have spaces since it is used as the name of a tool."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What prerequisites are needed before using your self-hosted instance of LangSmith?"}, {"role": "assistant", "content": "Using your self-hosted instance of LangSmith This guide will walk you through the process of using your self-hosted instance of LangSmith. Self-Hosted LangSmith Instance RequiredThis guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the kubernetes deployment guide or the docker deployment guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables need to be set to configure the LangSmith application?"}, {"role": "assistant", "content": "Configuring the application you want to use with LangSmith LangSmith has two APIs: One for interacting with the LangChain Hub/prompts and one for interacting with the backend of the LangSmith application. Each exists at its own URL and in a self-hosted environment are set via the LANGCHAIN_HUB_API_URL and LANGCHAIN_ENDPOINT environment variables, respectively, and have their own separate SDKs. Once you have deployed your instance, you can access the LangSmith UI at http://<host>.The backend API will be available at http://<host>/apiThe hub/prompts API will be available at http://<host>/api-hub. To use the LangSmith API, you will need to set the following environment variables in your application: LANGCHAIN_ENDPOINT=http://<host>/apiLANGCHAIN_HUB_API_URL=http://<host>/api-hubLANGCHAIN_API_KEY=foo # Set to a legitimate API key if using OAuth You can also configure these variables directly in the LangSmith SDK client: import langsmithlangsmith_client = langsmith.Client(    api_key='<api_key>',    api_url='http://<host>/api',)import langchainhublangchainhub.Client(    api_key='<api_key>',    api_url='http://<host>/api-hub') After setting the above, you should be able to run your code and see the results in your self-hosted instance.\nWe recommend running through the quickstart guide to get a feel for how to use LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a recommended reading resource before setting up an organization?"}, {"role": "assistant", "content": "Set up an organization Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must you take to create a separate organization for collaboration?"}, {"role": "assistant", "content": "Create an organization When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, head to the Settings page and click Create Organization.\nShared organizations require a credit card before they can be used. You will need to set up billing to proceed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you navigate between workspaces to access resources in LangSmith?"}, {"role": "assistant", "content": "Manage and navigate workspaces Once you've subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users.\nTo navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What actions can you perform to manage users in your shared organization?"}, {"role": "assistant", "content": "Manage users Manage membership in your shared organization in the Settings page Members and roles tab.\nHere you can Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace roleEdit a user's organization roleRemove users from your organization Organizations on the Enterprise plan may set up custom workspace roles in the Roles tab here. See the access control setup guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in access and permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles These are organization-scoped roles that are used to determine access to organization settings. The role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. Any Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See this conceptual guide for a full list of permissions associated with each role."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some key features and functionalities related to tracing in LangSmith?"}, {"role": "assistant", "content": "How-to guides: Tracing This section contains how-to guides related to tracing.  Annotate code for tracingThere are several ways to log traces to LangSmith. Toggle tracing on and offThis section is only relevant for users who are Log traces to specific projectYou can change the destination project of your traces both statically through environment variables and dynamically at runtime. Set a sampling rate for tracesThis section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API. Add metadata and tags to tracesLangSmith supports sending arbitrary metadata and tags along with traces. Implement distributed tracingSometimes, you need to trace a request across multiple services. Access the current run (span) within a traced functionIn some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run. Log multimodal tracesLangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs. Log retriever tracesNothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps. Log custom LLM tracesNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. Prevent logging of sensitive data in tracesIn some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend. Export tracesBefore diving into this content, it might be helpful to read the following: Share or unshare a trace publiclySharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information. Compare tracesTo compare traces, click on the Compare button in the upper right hand side of any trace view. Trace generator functionsIn most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user. Trace with LangChain (Python and JS/TS)LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications. Trace with LangGraph (Python and JS/TS)LangSmith smoothly integrates with LangGraph (Python and JS) Trace with Instructor (Python only)We provide a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs. Trace with the Vercel AI SDK (JS/TS only)This feature is currently in beta while Vercel rolls out official telemetry support. Trace without setting environment variablesAs mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: Trace using the LangSmith REST APIIt is HIGHLY recommended to use our Python or TypeScript SDKs to send traces to LangSmith. Calculate token-based costs for tracesBefore diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods for logging traces to LangSmith?"}, {"role": "assistant", "content": "Annotate code for tracing There are several ways to log traces to LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Who is the section about toggling tracing on and off relevant for?"}, {"role": "assistant", "content": "Toggle tracing on and off This section is only relevant for users who are"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the destination project of your traces?"}, {"role": "assistant", "content": "Log traces to specific project You can change the destination project of your traces both statically through environment variables and dynamically at runtime."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Who is this section relevant for when setting a sampling rate for traces?"}, {"role": "assistant", "content": "Set a sampling rate for traces This section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does LangSmith support in relation to traces?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of implementing distributed tracing?"}, {"role": "assistant", "content": "Implement distributed tracing Sometimes, you need to trace a request across multiple services."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the current run (span) within a traced function?"}, {"role": "assistant", "content": "Access the current run (span) within a traced function In some cases you will want to access the current run (span) within a traced function. This can be useful for extracting UUIDs, tags, or other information from the current run."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of runs does LangSmith support for logging and rendering images as part of traces?"}, {"role": "assistant", "content": "Log multimodal traces LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens if log retriever traces are not recorded in the correct format?"}, {"role": "assistant", "content": "Log retriever traces Nothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens if LLM traces are not logged in the correct format?"}, {"role": "assistant", "content": "Log custom LLM traces Nothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you prevent sensitive data from being logged in traces when using LangSmith?"}, {"role": "assistant", "content": "Prevent logging of sensitive data in traces In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should be read before diving into the content about export traces?"}, {"role": "assistant", "content": "Export traces Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you ensure before sharing a trace publicly?"}, {"role": "assistant", "content": "Share or unshare a trace publicly Sharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you compare traces in a trace view?"}, {"role": "assistant", "content": "Compare traces To compare traces, click on the Compare button in the upper right hand side of any trace view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of streaming outputs in most LLM applications?"}, {"role": "assistant", "content": "Trace generator functions In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace with LangChain (Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages does LangSmith integrate with LangGraph?"}, {"role": "assistant", "content": "Trace with LangGraph (Python and JS/TS) LangSmith smoothly integrates with LangGraph (Python and JS)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the integration with Instructor in the context of Python?"}, {"role": "assistant", "content": "Trace with Instructor (Python only) We provide a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of the telemetry support feature in the Vercel AI SDK?"}, {"role": "assistant", "content": "Trace with the Vercel AI SDK (JS/TS only) This feature is currently in beta while Vercel rolls out official telemetry support."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables are mentioned for configuring tracing?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended method for sending traces to LangSmith?"}, {"role": "assistant", "content": "Trace using the LangSmith REST API It is HIGHLY recommended to use our Python or TypeScript SDKs to send traces to LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do before calculating token-based costs for traces?"}, {"role": "assistant", "content": "Calculate token-based costs for traces Before diving into this content, it might be helpful to read the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you down-sample the number of traces logged to LangSmith using the LangSmith SDK or LangChain?"}, {"role": "assistant", "content": "Set a sampling rate for traces noteThis section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API. By default, all traces are logged to LangSmith.\nTo down-sample the number of traces logged to LangSmith, set the LANGCHAIN_TRACING_SAMPLING_RATE environment variable to\nany float between 0 (no traces) and 1 (all traces).\nFor instance, setting the following environment variable will log 75% of the traces. export LANGCHAIN_TRACING_SAMPLING_RATE=0.75 This works for the traceable decorator and RunTree objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the core components of LangSmith evaluation?"}, {"role": "assistant", "content": "Evaluation The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence. LangSmith allows you to build high-quality evaluations for your AI application. This conceptual guide will give you the foundations to get started. First, let's introduce the core components of LangSmith evaluation: Dataset: These are the inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of a single data point within a dataset used in the LangSmith evaluation workflow?"}, {"role": "assistant", "content": "Datasets Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some methods for creating datasets for evaluation?"}, {"role": "assistant", "content": "Creating datasets There are various ways to build datasets for evaluation, including: Manually curated examples This is how we typically recommend people get started creating datasets.\nFrom building your application, you probably have some idea of what types of inputs you expect your application to be able to handle,\nand what \"good\" responses may be.\nYou probably want to cover a few different common edge cases or situations you can imagine.\nEven 10-20 high-quality, manually-curated examples can go a long way. Historical logs Once you have an application in production, you start getting valuable information: how are users actually using it?\nThis information can be valuable to capture and store in datasets. This allows you to test against these\nuse cases as you iterate on your application. If your application is going well, you will likely get a lot of usage! How can you determine which datapoints are valuable to add?\nThere are a few heuristics you can follow.\nIf possible - try to collect end user feedback. You can then see which datapoints got negative feedback.\nThat is super valuable! These are spots where your application did not perform well.\nYou should add these to your dataset to test against in the future. You can also use other heuristics\nto identify \"interesting\" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset. Synthetic data Once you have a few examples, you can try to artificially generate examples.\nIt's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.\nThis can be a useful way to get a lot of datapoints, quickly. tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from traces"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three distinct dataset types offered by LangSmith and their specific purposes?"}, {"role": "assistant", "content": "Types of datasets LangSmith offers three distinct dataset types: kv (Key-Value) Dataset:\"Inputs\" and \"outputs\" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating \"completion\" style language models.The \"inputs\" dictionary contains a single \"input\" key mapped to the prompt string.The \"outputs\" dictionary contains a single \"output\" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs and outputs.chat Dataset:The chat dataset is designed for evaluating LLM structured \"chat\" messages as inputs and outputs.The \"inputs\" dictionary contains a single \"input\" key mapped to a list of serialized chat messagesThe \"outputs\" dictionary contains a single \"output\" key mapped to a list of serialized chat messages.This dataset type is useful for evaluating conversational AI systems or chatbots."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of partitioning a dataset into different splits during evaluation?"}, {"role": "assistant", "content": "Partitioning datasets When setting up your evaluation, you may want to partition your dataset into different splits. This can help save cost. For example, you might use a smaller split for many rapid iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately. tipTo learn more about creating dataset splits in LangSmith:See our video on dataset splits in the LangSmith Evaluation series.See our documentation here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the components of the EvaluationResult returned by evaluators in LangSmith?"}, {"role": "assistant", "content": "Evaluators Evaluators are functions in LangSmith that score how well your application performs on a particular example.\nEvaluators receive these inputs: Example: The example from your Dataset.Root_run: The output and intermediate steps from running the inputs through the application. The evaluator returns an EvaluationResult (or a similarly structured dictionary), which consists of: Key: The name of the metric being evaluated.Score: The value of the metric for this example.Comment: The reasoning or additional string information justifying the score. There are a few approaches and types of scoring functions that can be used in LangSmith evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith facilitate human evaluation of LLM application outputs?"}, {"role": "assistant", "content": "Human Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps). tipSee our video using LangSmith to capture human feedback for prompt engineering."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are heuristic evaluators and how do they function in evaluating tasks?"}, {"role": "assistant", "content": "Heuristic Heuristic evaluators are hard-coded functions that perform computations to determine a score. To use them, you typically will need a set of rules that can be easily encoded into a function. They can be reference-free (e.g., check the output for empty string or valid JSON). Or they can compare task output to a reference (e.g., check if the output matches the reference exactly). tipFor some tasks, like code generation, custom heuristic evaluation (e.g., import and code execution-evaluation) are often extremely useful and superior to other evaluations (e.g., LLM-as-judge, discussed below).Watch the Custom evaluator video in our LangSmith Evaluation series for a comprehensive overview.Read our documentation on custom evaluators.See our blog using custom evaluators for code generation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of evaluations that LLM-as-judge evaluators can perform on system output?"}, {"role": "assistant", "content": "LLM-as-judge LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference). tipCheck out our video on LLM-as-judge evaluators in our LangSmith Evaluation series. With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often a process of trial-and-error is required to get LLM-as-judge evaluator prompts to produce reliable scores. tipSee documentation on our workflow to audit and manually correct evaluator scores here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When is pairwise evaluation particularly useful?"}, {"role": "assistant", "content": "Pairwise Pairwise evaluators pick the better of two task outputs based upon some criteria.\nThis can use either a heuristic (\"which response is longer\"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples). When should you use pairwise evaluation? Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs.\nThis can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different scoring functions that evaluators can use to assess application performance on datasets?"}, {"role": "assistant", "content": "Applying evaluations We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference. Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset and track your application's performance over time. Additionally, you can compare multiple experiments in a comparison view. In the Dataset section above, we discussed a few ways to build datasets (e.g., from historical logs or manual curation). One common way to use these datasets is offline evaluation, which is usually conducted prior to deployment of your LLM application. Below we'll discuss a few common paradigms for offline evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are unit tests used for in software development?"}, {"role": "assistant", "content": "Unit Tests Unit tests are often used in software development to verify the correctness of individual system components. Unit tests are often lightweight assertions on LLM inputs or outputs (e.g., type or schema checks). Often these are triggered by any change to your application as quick assertions of basic functionality.\nThis means they often use heuristics to evaluate. You generally expect unit tests to always pass (this is not strictly true, but more so than other types of evaluation flows).\nThese types of tests are nice to run as part of CI, but when doing so it is useful to set up a cache (or something similar)\nto cache LLM calls (because those can quickly rack up!). tipTo learn more about unit tests with LangSmith, check out our unit testing video."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of regression testing in application development?"}, {"role": "assistant", "content": "Regression Testing Regression testing is often used to measure performance across versions of your application over time. They are used to ensure new app versions do not regress on examples that your current version is passing. In practice, they help you assess how much better or worse your new version is relative to the baseline. Often these are triggered when you are making app updates that are expected to influence the user experience.\nThey are also commonly done when evaluating new or different models. tipTo learn more about regression testing with LangSmith, see our regression testing videoSee our video focusing on regression testing applied to GPT4-o vs GPT4-turbo video. LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline (with regressions on specific examples shown in red and improvements in green):"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of back-testing in evaluating new model versions?"}, {"role": "assistant", "content": "Back-testing Back-testing is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs. This is commonly used to evaluate new model versions.\nAnthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model.\nThen compare those results to what actually happened in production. tipSee our video on Back-testing to learn about this workflow."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Why might pairwise evaluations be considered a more stable scoring approach than assigning individual scores?"}, {"role": "assistant", "content": "Pairwise-testing It can be easier for a human (or an LLM grader) to determine A is better than B than to assign an individual score to either A or B. This helps to explain why some have observed that pairwise evaluations can be a more stable scoring approach than assigning individual scores to each experiment, particularly when working with LLM-as-judge evaluators. tipWatch the Pairwise evaluation video in our LangSmith Evaluation series.See our blog post on pairwise evaluation."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the main difference between online evaluation and offline evaluation in the context of application testing?"}, {"role": "assistant", "content": "Online Evaluation Whereas offline evaluation focuses on pre-deployment testing, online evaluation allow you to evaluate an application in production. This can be useful for applying guardrails to LLM inputs or outputs, such as correctness and toxicity. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can be later used to curate a dataset for offline evaluation. tipExplore our videos on online evaluation:Online evaluation in our LangSmith Evaluation seriesOnline evaluation with focus on guardrails in our LangSmith Evaluation seriesOnline evaluation with focus on RAG in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using repetitions in the LangSmith evaluate function?"}, {"role": "assistant", "content": "Experiment Configurations LangSmith evaluations are kicked off using a single function, evaluate, which takes in a dataset, evaluator, and various optional configurations, some of which we discuss below. tipSee documentation on using evaluate here. Repetitions One of the most common questions when evaluating AI applications is: how can I build confidence in the result of an experiment? This is particularly relevant for LLM applications (e.g., agents), which can exhibit considerable run-to-run variability. Repetitions involve running the same evaluation multiple times and aggregating the results to smooth out run-to-run variability and examine the reproducibility of the AI application's performance. LangSmith evaluate function allows you to easily set the number of repetitions and aggregates (the mean) of replicate experiments for you in the UI. tipSee the video on Repetitions in our LangSmith Evaluation seriesSee our documentation on Repetitions"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will be discussed regarding specific popular LLM applications?"}, {"role": "assistant", "content": "Evaluating Specific LLM Applications Below, we will discuss evaluation of a few specific, popular LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three components that LLM-powered autonomous agents combine to generate responses?"}, {"role": "assistant", "content": "Agents LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. The assistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node. The tool node executes the tool and returns the output as a tool message to the assistant node. This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response: Evaluate the agent's final response.Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this.\nNote that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the advantages and disadvantages of evaluating an agent's final response based on its overall performance on a task?"}, {"role": "assistant", "content": "Evaluating an agent's final response One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent's final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics. tipSee our tutorial on evaluating agent response."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits and downsides of evaluating a single step of an agent?"}, {"role": "assistant", "content": "Evaluating a single step of an agent Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristc evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses). tipSee our tutorial on evaluating a single step of an agent."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different methods for evaluating an agent's trajectory and their respective strengths and weaknesses?"}, {"role": "assistant", "content": "Evaluating an agent's trajectory Evaluating an agent's trajectory involves looking at all the steps an agent took and evaluating that sequence of steps. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an \"exact\" trajectory (e.g., an expected sequence of tool calls) or simply a list of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the \"exact\" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of \"incorrect\" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with. tipSee our tutorial on evaluating agent trajectory."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some best practices for improving the reliability and cost-effectiveness of agents using LLMs?"}, {"role": "assistant", "content": "Best practices Agents can be both costly (in terms of LLM invocations) and unreliable (due to variability in tool calling). Some approaches to help address these effects: tipTest multiple tool calling LLMs with your agent.It's possible that faster and / or lower cost LLMs show acceptable performance for your application.Trying evaluating the agent at multiple levels - both end-to-end, as well as at particular stepsUse repetitions to smooth out noise, as tool selection and agent behavior can show run-to-run variability.See the video on Repetitions in our LangSmith Evaluation series"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of Retrieval Augmented Generation (RAG) in AI applications?"}, {"role": "assistant", "content": "Retrieval Augmented Generation (RAG) Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge. tipFor a comprehensive review of RAG concepts, see our RAG From Scratch series."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the significance of reference answers in evaluating RAG applications?"}, {"role": "assistant", "content": "Dataset When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two main options for evaluating RAG applications?"}, {"role": "assistant", "content": "Evaluator LLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you have two main options: Reference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.Reference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure). tipDive deeper into RAG evaluation concepts with our LangSmith video series:RAG answer correctness evaluationRAG answer hallucinationRAG document relevanceRAG intermediate steps evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different approaches to applying RAG evaluation?"}, {"role": "assistant", "content": "Applying RAG Evaluation When applying RAG evaluation, consider the following approaches: Offline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.Online evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.Pairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. tipExplore our LangSmith video series for more insights on RAG evaluation:RAG with online evaluationRAG pairwise evaluation"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluations of answer faithfulness, helpfulness, correctness, and chain comparison in the RAG evaluation summary?"}, {"role": "assistant", "content": "RAG evaluation summary Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantDocument relevanceAre documents relevant to the question?YesYes - promptNoAnswer faithfulnessIs the answer grounded in the documents?YesYes - promptNoAnswer helpfulnessDoes the answer help address the question?YesYes - promptNoAnswer correctnessIs the answer consistent with a reference answer?NoYes - promptNoChain comparisonHow do multiple answer versions compare?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the evaluation methods used for summarization, and how do they differ in terms of reference summaries?"}, {"role": "assistant", "content": "Summarization Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria. Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): tipSee our LangSmith video series to go deeper on these concepts:Video on pairwise evaluation: https://youtu.be/yskkOAfTwcQ?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantFactual accuracyIs the summary accurate relative to the source documents?YesYes - promptYesFaithfulnessIs the summary grounded in the source documents (e.g., no hallucinations)?YesYes - promptYesHelpfulnessIs summary helpful relative to user need?YesYes - promptYes"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components and considerations for evaluating Classification / Tagging when ground truth labels are available versus when they are not?"}, {"role": "assistant", "content": "Classification / Tagging Classification / Tagging applies a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification / Tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for Classification / Tagging evaluation is whether you have a dataset with reference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a Classification / Tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it's common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the Classification / Tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). tipSee our LangSmith video series to go deeper on these concepts:Online evaluation video: https://youtu.be/O0x6AcImDpM?feature=shared Use CaseDetailReference-free?LLM-as-judge?Pairwise relevantCriteriaTag if specific criteria is metYesYes - promptNoAccuracyStandard definitionNo (ground truth class)NoNoPrecisionStandard definitionNo (ground truth class)NoNoRecallStandard definitionNo (ground truth class)NoNo"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the easiest way to interact with datasets in the LangSmith app?"}, {"role": "assistant", "content": "Manage datasets in the application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Concepts guide on evaluation and datasets The easiest way to interact with datasets is directly in the LangSmith app. Here, you can create and edit datasets and example."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new dataset and add examples manually in the application?"}, {"role": "assistant", "content": "Create a new dataset and add examples manually To get started, you can create a new datasets by heading to the \"Datasets and Testing\" section of the application and clicking on \"+ New Dataset\". Then, enter the relevant dataset details, including a name, optional description, and dataset type. Please see the concepts for more information on dataset types. For most flexibility, the key-value dataset type is recommended. You can then add examples to the dataset by clicking on \"Add Example\". Here, you can enter the input and output as JSON objects."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format is used to define dataset schemas for key-value datasets?"}, {"role": "assistant", "content": "Dataset schema validation If you are creating a key-value dataset, you may optionally define a schema for your dataset. All examples you create will be validated against this schema. Dataset schemas are defined with standard JSON schemas. If you would rather manually enter raw JSON, click \"Editor\" at the bottom of the schema editor and then select \"JSON\". If you have defined a schema for your dataset, you will get easy validation when creating new examples:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add inputs and outputs from traces to a dataset?"}, {"role": "assistant", "content": "Add inputs and outputs from traces to datasets We typically construct datasets over time by collecting representative examples from debugging or other runs. To do this, we first filter the traces to find the ones we want to add to the dataset. Then we add the inputs and outputs from these traces to the dataset. You can do this from any 'run' details page by clicking the 'Add to Dataset' button in the top right-hand corner. tipAn extremely powerful technique to build datasets is to drill-down into the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset.\nFor tips on how to filter traces, see the filtering traces guide. automationsYou can use automations to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that have a certain tag to a dataset. From there, we select the dataset to organize it in and update the ground truth output values if necessary."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from your own data?"}, {"role": "assistant", "content": "Upload a CSV file to create a dataset The easiest way to create a dataset from your own data is by clicking the 'upload a CSV dataset' button on the home page or in the top right-hand corner of the 'Datasets & Testing' page. Select a name and description for the dataset, and then confirm that the inferred input and output columns are correct."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps must be taken to generate synthetic examples for a dataset with a specified schema?"}, {"role": "assistant", "content": "Generate synthetic examples For a dataset with a specified schema, you can generate synthetic examples to enhance your dataset. Select few-shot examples: Choose a set of examples to guide the LLM's generation. You can manually select these examples from your dataset or use the automatic selection option.Specify the number of examples: Enter the number of synthetic examples you want to generate.Configure API Key: Ensure your OpenAI API key is entered at the \"API Key\" link.\n After clicking \"Generate,\" the examples will appear on the page. You can choose which examples to add to your dataset, with the option to edit them before finalizing.\nEach example will be validated against your specified dataset schema and tagged as \"synthetic\" in the source metadata.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you export a LangSmith dataset and what formats are available?"}, {"role": "assistant", "content": "Export a dataset You can export your LangSmith dataset to CSV or OpenAI evals format directly from the web application. To do so, click \"Export Dataset\" from the homepage.\nTo do so, select a dataset, click on \"Examples\", and then click the \"Export Dataset\" button at the top of the examples table. This will open a modal where you can select the format you want to export to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of creating dataset splits in machine learning workflows?"}, {"role": "assistant", "content": "Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common\nin machine learning workflows to split datasets into training, validation, and test sets. This can be useful\nto prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation\nworkflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate\nseparately; or if you are testing a new use case that you may want to include in your dataset in the future, but want\nto keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits\nto be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas\nmetadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split).\nHowever, we allow you to select multiple splits for the same example in LangSmith because it can make sense for\nsome evaluation workflows - for example, if an example falls into multiple categories on which you may want to\nevaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \"Add to Split\". From the resulting popup menu,\nyou can select and unselect splits for the selected examples, or create a new split."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add or update metadata for your examples?"}, {"role": "assistant", "content": "Edit example metadata You can add metadata to your examples by clicking on an example and then clicking on the \"Metadata\" tab in the side pane.\nFrom this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about\nyour examples, such as tags or version info, which you can then filter by when you call list_examples in the SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples in the table?"}, {"role": "assistant", "content": "Filter examples You can filter examples by metadata key/value or full-text search. To filter examples, click \"Filter\" in the top left of the table: Next, click \"Add filter\" and select \"Full Text\" or \"Metadata\" from the resulting dropdown. You may add multiple filters, and only examples that satisfy all of the\nfilters will be displayed in the table."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you create a new prompt in the application?"}, {"role": "assistant", "content": "Create a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage.\nClick the \"+ Prompt\" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for developing your prompt in the playground?"}, {"role": "assistant", "content": "Compose your prompt After choosing a prompt type, you're brought to the playground to develop your prompt.\nOn the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click \"Start\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you save a prompt and what options do you have for its visibility?"}, {"role": "assistant", "content": "Save your prompt To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\".\nPrivate prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.\nClick save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can I view a table of my prompts?"}, {"role": "assistant", "content": "View your prompts You've just created your first prompt! View a table of your prompts in the prompts tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add metadata to your prompt in the LangChain Hub?"}, {"role": "assistant", "content": "Add metadata To add metadata to your prompt, click the prompt and then click the \"Edit\" pencil icon next to the name.\nThis brings you to where you can add additional information about the prompt, including a description, a README, and use cases.\nFor public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the benefits of enabling blob storage for run inputs, outputs, errors, and manifests in LangSmith?"}, {"role": "assistant", "content": "Enable blob storage By default, LangSmith stores run inputs, outputs, and errors in ClickHouse. In addition, LangSmith will store run manifests in Postgres. If you so choose, you can instead store this information in blob storage, which has a couple of notable benefits: In high trace environments, inputs, outputs, errors, and manifests may balloon the size of your databases.If using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment.\nTo alleviate this, LangSmith supports storing run manifests, inputs, outputs, and errors in an external blob storage system."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for using LangSmith with a blob storage service?"}, {"role": "assistant", "content": "Requirements Access to a valid blob storage serviceAmazon S3Google Cloud StorageCurrently, Azure Blob Storage is not supported (coming soon)A bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data.If you are using TTLs, you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs here. These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss.Credentials to permit LangSmith Services to access the bucket/directoryYou will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication section below for more information.An API url for your blob storage serviceThis will be the URL that LangSmith uses to access your blob storage systemIn the case of Amazon S3, this will be the URL of the S3 endpoint. Something like: https://s3.amazonaws.com or https://s3.us-west-1.amazonaws.com if using a regional endpoint.In the case of Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: https://storage.googleapis.com"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two methods to authenticate with Amazon S3 after creating an IAM policy?"}, {"role": "assistant", "content": "Amazon S3 To authenticate to Amazon S3, you will need to create an IAM policy granting admin permissions on your bucket.\nThis will look something like the following: {  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Effect\": \"Allow\",      \"Action\": \"s3:*\",      \"Resource\": [        \"arn:aws:s3:::your-bucket-name\",        \"arn:aws:s3:::your-bucket-name/*\"      ]    }  ]} Once you have the correct policy, there are two ways to authenticate with Amazon S3: IAM Role for Service Account: You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.You will need to create an IAM role with the policy attached.You will need to allow LangSmith service accounts to assume the role. The langsmith-queue and langsmith-backend service accounts will need to be able to assume the role.Service Account NamesThe service account names will be different if you are using a custom release name. You can find the service account names by running kubectl get serviceaccounts in your cluster.You will need to provide the role ARN to LangSmith. You can do this by adding the eks.amazonaws.com/role-arn: \"<role_arn>\" annotation to the langsmith-queue and langsmith-backend services in your Helm Chart installation.Access Key and Secret Key: You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are required to authenticate with Google Cloud Storage?"}, {"role": "assistant", "content": "Google Cloud Storage To authenticate with Google Cloud Storage, you will need to create a service account with the necessary permissions to access your bucket. Your service account will need the Storage Admin role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using. Once you have a provisioned service account, you will need to generate a HMAC key for that service account. This key and secret will be used to authenticate with Google Cloud Storage."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the necessary configurations to set up LangSmith with a blob storage system?"}, {"role": "assistant", "content": "Configuration After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system. Helm  config:    blobStorage:      enabled: true      bucketName: \"your-bucket-name\"      apiURL: \"Your connection url\"      accessKey: \"Your access key\" # Optional. Only required if using access key and secret key      accessKeySecret: \"Your access key secret\" # Optional. Only required if using access key and secret key    backend: # Optional, only required if using IAM role for service account     serviceAccount:       annotations:         eks.amazonaws.com/role-arn: \"<role_arn>\"    queue: # Optional, only required if using IAM role for service account      serviceAccount:        annotations:          eks.amazonaws.com/role-arn: \"<role_arn>\"       Using an existing secretIf using an access key and secret, you can also provide an existing kubernetes secret that contains the access key and secret key.\nThis is recommended over providing the access key and secret key directly in your config."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can users upload experiments run outside of LangSmith to view results in the LangSmith UI?"}, {"role": "assistant", "content": "Upload experiments run outside of LangSmith with the REST API Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our /datasets/upload-experiment endpoint. This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is required to upload an experiment using the /datasets/upload-experiment endpoint?"}, {"role": "assistant", "content": "Request body schema Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within\nthe experiment. Each object in the results represents a \"row\" in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name\nrefer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset\nin LangSmith (unless that dataset was created via this endpoint). You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: {  \"experiment_name\": \"string (required)\",  \"experiment_description\": \"string (optional)\",  \"experiment_start_time\": \"datetime (required)\",  \"experiment_end_time\": \"datetime (required)\",  \"dataset_id\": \"uuid (optional - an external dataset id, used to group experiments together)\",  \"dataset_name\": \"string (optional - must provide either dataset_id or dataset_name)\",  \"dataset_description\": \"string (optional)\",  \"experiment_metadata\": { // Object (any shape - optional)    \"key\": \"value\"  },  \"summary_experiment_scores\": [ // List of summary feedback objects (optional)    {      \"key\": \"string (required)\",      \"score\": \"number (optional)\",      \"value\": \"string (optional)\",      \"comment\": \"string (optional)\",      \"feedback_source\": { // Object (optional)        \"type\": \"string (required)\"      },      \"feedback_config\": { // Object (optional)        \"type\": \"string enum: continuous, categorical, or freeform\",        \"min\": \"number (optional)\",        \"max\": \"number (optional)\",        \"categories\": [ // List of feedback category objects (optional)            \"value\": \"number (required)\",            \"label\": \"string (optional)\"        ]      },      \"created_at\": \"datetime (optional - defaults to now)\",      \"modified_at\": \"datetime (optional - defaults to now)\",      \"correction\": \"Object or string (optional)\"    }  ],  \"results\": [ // List of experiment row objects (required)    {      \"row_id\": \"uuid (required)\",      \"inputs\": {     // Object (required - any shape). This will        \"key\": \"val\"  // be the input to both the run and the dataset example.      },      \"expected_outputs\": { // Object (optional - any shape).        \"key\": \"val\"        // These will be the outputs of the dataset examples.      },      \"actual_outputs\": { // Object (optional - any shape).        \"key\": \"val       // These will be the outputs of the runs.      },      \"evaluation_scores\": [ // List of feedback objects for the run (optional)        {            \"key\": \"string (required)\",            \"score\": \"number (optional)\",            \"value\": \"string (optional)\",            \"comment\": \"string (optional)\",            \"feedback_source\": { // Object (optional)                \"type\": \"string (required)\"            },            \"feedback_config\": { // Object (optional)                \"type\": \"string enum: continuous, categorical, or freeform\",                \"min\": \"number (optional)\",                \"max\": \"number (optional)\",                \"categories\": [ // List of feedback category objects (optional)                    \"value\": \"number (required)\",                    \"label\": \"string (optional)\"                ]            },            \"created_at\": \"datetime (optional - defaults to now)\",            \"modified_at\": \"datetime (optional - defaults to now)\",            \"correction\": \"Object or string (optional)\"        }      ],      \"start_time\": \"datetime (required)\", // The start/end times for the runs will be used to      \"end_time\": \"datetime (required)\",   // calculate latency. They must all fall between the      \"run_name\": \"string (optional)\",     // start and end times for the experiment.      \"error\": \"string (optional)\",      \"run_metadata\": { // Object (any shape - optional)        \"key\": \"value\"      }    }  ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for uploading multiple experiments to the same dataset?"}, {"role": "assistant", "content": "Considerations You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together\nunder a single dataset, and you will be able to use the comparison view to compare results between experiments. Ensure that the start and end times of your individual rows are all between the start and end time of your experiment. You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if\nyou only provide a name. You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key details of the experiment uploaded to LangSmith, including its name, description, and start and end times?"}, {"role": "assistant", "content": "Example request Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. import osimport requestsbody = {  \"experiment_name\": \"My external experiment\",  \"experiment_description\": \"An experiment uploaded to LangSmith\",  \"dataset_name\": \"my-external-dataset\",  \"summary_experiment_scores\": [    {      \"key\": \"summary_accuracy\",      \"score\": 0.9,      \"comment\": \"Great job!\"    }  ],  \"results\": [    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the weather in San Francisco today?\"      },      \"expected_outputs\": {        \"output\": \"Sorry, I am unable to provide information about the current weather.\"      },      \"actual_outputs\": {        \"output\": \"The weather is partly cloudy with a high of 65.\"      },      \"evaluation_scores\": [        {          \"key\": \"hallucination\",          \"score\": 1,          \"comment\": \"The chatbot made up the weather instead of identifying that \"                     \"they don't have enough info to answer the question. This is \"                     \"a hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:39\",      \"end_time\": \"2024-08-03T00:12:41\",      \"run_name\": \"Chatbot\"    },    {      \"row_id\": \"<<uuid>>\",      \"inputs\": {        \"input\": \"Hello, what is the square root of 49?\"      },      \"expected_outputs\": {        \"output\": \"The square root of 49 is 7.\"      },      \"actual_outputs\": {        \"output\": \"7.\"      },      \"evaluation_scores\": [       {          \"key\": \"hallucination\",          \"score\": 0,          \"comment\": \"The chatbot correctly identified the answer. This is not a \"                     \"hallucination.\"        }      ],      \"start_time\": \"2024-08-03T00:12:40\",      \"end_time\": \"2024-08-03T00:12:42\",      \"run_name\": \"Chatbot\"    }  ],  \"experiment_start_time\": \"2024-08-03T00:12:38\",  \"experiment_end_time\": \"2024-08-03T00:12:43\"}resp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\",    json=body,    headers={\"x-api-key\": os.environ[\"LANGCHAIN_API_KEY\"]})print(resp.json()) Below is the response received: {  \"dataset\": {    \"name\": \"my-external-dataset\",    \"description\": null,    \"created_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"data_type\": \"kv\",    \"inputs_schema_definition\": null,    \"outputs_schema_definition\": null,    \"externally_managed\": true,    \"id\": \"<<uuid>>\",    \"tenant_id\": \"<<uuid>>\",    \"example_count\": 0,    \"session_count\": 0,    \"modified_at\": \"2024-08-03T00:36:23.289730+00:00\",    \"last_session_start_time\": null  },  \"experiment\": {    \"start_time\": \"2024-08-03T00:12:38\",    \"end_time\": \"2024-08-03T00:12:43+00:00\",    \"extra\": null,    \"name\": \"My external experiment\",    \"description\": \"An experiment uploaded to LangSmith\",    \"default_dataset_id\": null,    \"reference_dataset_id\": \"<<uuid>>\",    \"trace_tier\": \"longlived\",    \"id\": \"<<uuid>>\",    \"run_count\": null,    \"latency_p50\": null,    \"latency_p99\": null,    \"first_token_p50\": null,    \"first_token_p99\": null,    \"total_tokens\": null,    \"prompt_tokens\": null,    \"completion_tokens\": null,    \"total_cost\": null,    \"prompt_cost\": null,    \"completion_cost\": null,    \"tenant_id\": \"<<uuid>>\",    \"last_run_start_time\": null,    \"last_run_start_time_live\": null,    \"feedback_stats\": null,    \"session_feedback_stats\": null,    \"run_facets\": null,    \"error_rate\": null,    \"streaming_rate\": null,    \"test_run_number\": 1  }} Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds.\nIf you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this\ninformation in the request body)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will you see after clicking on your newly-created dataset in the UI?"}, {"role": "assistant", "content": "View the experiment in the UI Now, login to the UI and click on your newly-created dataset! You should see a single experiment:\n Your examples will have been uploaded:\n Clicking on your experiment will bring you to the comparison view:\n As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the AI Query functionality mentioned in the text?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you create a prompt in the application?"}, {"role": "assistant", "content": "Create a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage.\nClick the \"+ Prompt\" button to enter the Playground. The dropdown next to the button gives you a choice between a chat style prompt and an instructional prompt - chat is the default.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options do you have for developing your prompt in the playground?"}, {"role": "assistant", "content": "Compose your prompt After choosing a prompt type, you're brought to the playground to develop your prompt.\nOn the left is an editable view of the prompt. You can add more messages, change the template format (f-string or mustache), and add an output schema (which makes your prompt a StructuredPrompt type). To the right, we can enter sample inputs for our prompt variables and then run our prompt against a model. (If you haven't yet, you'll need to enter an API key for whichever model you want to run your prompt with.) To see the response from the model, click \"Start\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you save a prompt and what options do you have for its visibility?"}, {"role": "assistant", "content": "Save your prompt To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\".\nPrivate prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub.\nClick save to create your prompt. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. Public PromptsThe first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can I view a table of my prompts?"}, {"role": "assistant", "content": "View your prompts You've just created your first prompt! View a table of your prompts in the prompts tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add metadata to your prompt in the LangChain Hub?"}, {"role": "assistant", "content": "Add metadata To add metadata to your prompt, click the prompt and then click the \"Edit\" pencil icon next to the name.\nThis brings you to where you can add additional information about the prompt, including a description, a README, and use cases.\nFor public prompts this information will be visible to anyone who views your prompt in the LangChain Hub."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What role does LangSmith play in enhancing observability for LLM applications during different stages of development?"}, {"role": "assistant", "content": "Add observability to your LLM application Observability is important for any software application, but especially so for LLM applications.\nLLMs are non-deterministic by nature, meaning they can produce unexpected results.\nThis makes them trickier than normal to debug. Luckily, this is where LangSmith can help!\nLangSmith has LLM-native observability, allowing you to get meaningful insights into your application. Note that observability is important throughout all stages of application development - from prototyping, to beta testing, to production.\nThere are different considerations at all stages, but they are all intricately tied together.\nIn this tutorial we walk through the natural progression. Let's assume that we're building a simple RAG application using the OpenAI SDK.\nThe simple application we're adding observability to looks like: PythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-3.5-turbo\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\n\\n\" +    docs.join(\"\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-3.5-turbo\",  });}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does setting up observability from the start benefit the prototyping process?"}, {"role": "assistant", "content": "Prototyping Having observability set up from the start can you help iterate much more quickly than you would otherwise be able to.\nIt allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using.\nIn this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are required to set up your environment for using the LangSmith SDK?"}, {"role": "assistant", "content": "Set up your environment First, create an API key by navigating to the settings page. Next, install the LangSmith SDK: Python SDKTypeScript SDKpip install langsmithnpm install langsmith Finally, set up the appropriate environment variables. This will log traces to the default project (though you can easily change that). export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key>export LANGCHAIN_PROJECT=default"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace your OpenAI calls using LangSmith?"}, {"role": "assistant", "content": "Trace your LLM calls The first thing you might want to trace is all your OpenAI calls.\nAfter all, this is where the LLM is actually being called, so it is the most important part!\nWe've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper.\nAll you have to do is modify your code to look something like: PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-3.5-turbo\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\n\\n\" +    docs.join(\"\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-3.5-turbo\",  });} Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client (openai_client = wrap_openai(OpenAI())). What happens if you call it in the following way? rag(\"where did harrison work\") This will produce a trace of just the OpenAI call - it should look something like this"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does LangSmith allow you to trace in the LLM pipeline?"}, {"role": "assistant", "content": "Trace the whole chain Great - we've traced the LLM call. But it's often very informative to trace more than that.\nLangSmith is built for tracing the entire LLM pipeline - so let's do that!\nWe can do this by modifying the code to now look something like this: PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-3.5-turbo\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\n\\n\" +    docs.join(\"\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-3.5-turbo\",  });}); Notice how we import from langsmith import traceable and use it decorate the overall function (@traceable). What happens if you call it in the following way? rag(\"where did harrison work\") This will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the retrieval step in LLM applications as described in the text?"}, {"role": "assistant", "content": "Trace the retrieval step There's one last part of the application we haven't traced - the retrieval step!\nRetrieval is a key part of LLM applications, and we've made it easy to log retrieval steps as well.\nAll we have to do is modify our code to look like: PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-3.5-turbo\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());const retriever = traceable(  async function retriever(query: string) {    return [\"This is a document\"];  },  { run_type: \"retriever\" })const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\n\\n\" +    docs.join(\"\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-3.5-turbo\",  });}); Notice how we import from langsmith import traceable and use it decorate the overall function (@traceable(run_type=\"retriever\")). What happens if you call it in the following way? rag(\"where did harrison work\") This will produce a trace of the whole chain including the retrieval step - it should look something like this"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of observability during the beta testing stage of LLM application development?"}, {"role": "assistant", "content": "Beta Testing The next stage of LLM application development is beta testing your application.\nThis is when you release it to a few initial users.\nHaving good observability set up here is crucial as often you don't know exactly how users will actually use your application, so this allows you get insights into how they do so.\nThis also means that you probably want to make some changes to your tracing set up to better allow for that.\nThis extends the observability you set up in the previous section"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you associate feedback with a specific run during beta testing using LangSmith?"}, {"role": "assistant", "content": "Collecting Feedback A huge part of having good observability during beta testing is collecting feedback.\nWhat feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start.\nAfter logging that feedback, you need to be able to easily associate it with the run that caused that.\nLuckily LangSmith makes it easy to do that. First, you need to log the feedback from your app.\nAn easy way to do this is to keep track of a run ID for each run, and then use that to log feedback.\nKeeping track of the run ID would look something like: import uuidrun_id = str(uuid.uuid4())rag(    \"where did harrison work\",    langsmith_extra={\"run_id\": run_id}) Associating feedback with that run would look something like: from langsmith import Clientls_client = Client()ls_client.create_feedback(    run_id,    key=\"user-score\",    score=1.0,) Once the feedback is logged, you can then see it associated with each run by clicking into the Metadata tab when inspecting the run.\nIt should look something like this You can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table.\nYou can do this by creating a filter like the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of logging metadata in an application?"}, {"role": "assistant", "content": "Logging Metadata It is also a good idea to start logging metadata.\nThis allows you to start keep track of different attributes of your app.\nThis is important in allowing you to know what version or variant of your app was used to produce a given result. For this example, we will log the LLM used.\nOftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering.\nIn order to do that, we can add it as such: from openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceable(metadata={\"llm\": \"gpt-3.5-turbo\"})def rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:    {docs}\"\"\".format(docs='\\n'.join(docs))    return openai_client.chat.completions.create(messages = [        {\"role\": \"system\", \"content\": system_message},        {\"role\": \"user\", \"content\": question},    ], model=\"gpt-3.5-turbo\") Notice we added @traceable(metadata={\"llm\": \"gpt-3.5-turbo\"}) to the rag function. Keeping track of metadata in this way assumes that it is known ahead of time.\nThis is fine for LLM types, but less desirable for other types of information - like a User ID.\nIn order to log information that, we can pass it in at run time with the run ID. import uuidrun_id = str(uuid.uuid4())rag(    \"where did harrison work\",    langsmith_extra={\"run_id\": run_id, \"metadata\": {\"user_id\": \"harrison\"}}) Now that we've logged these two pieces of metadata, we should be able to see them both show up in the UI here. We can filter for these pieces of information by constructing a filter like the following:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools does LangSmith offer to help with observability in production?"}, {"role": "assistant", "content": "Production Great - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well.\nTime to ship it to production!\nWhat new observability do you need to add? First of all, let's note that the same observability you've already added will keep on providing value in production.\nYou will continue to be able to drill down into particular runs. In production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time.\nLuckily, LangSmith has a set of tools to help with observability in production."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What statistics can be tracked in the monitoring charts under the Monitor tab in a project?"}, {"role": "assistant", "content": "Monitoring If you click on the Monitor tab in a project, you will see a series of monitoring charts.\nHere we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc.\nYou can view these over time across a few different time bins."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required for the noteGroup-by functionality in A/B testing?"}, {"role": "assistant", "content": "A/B Testing noteGroup-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key. You can also use this tab to perform a version of A/B Testing.\nIn the previous tutorial we starting tracking a few different metadata attributes - one of which was llm.\nWe can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time.\nThis allows us to experiment with different LLMs (or prompts, or other) and track their performance over time. In order to do this, we just need to click on the Metadata button at the top.\nThis will give us a drop down of options to choose from to group by: Once we select this, we will start to see charts grouped by this attribute:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you drill down into problematic datapoints in LangSmith's monitoring charts?"}, {"role": "assistant", "content": "Drilldown One of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify\nas problematic while looking at monitoring charts.\nIn order to do this, you can simply hover over a datapoint in the monitoring chart.\nWhen you do this, you will be able to click the datapoint.\nThis will lead you back to the runs table with a filtered view:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are some additional areas, besides observability, that LangSmith can assist with?"}, {"role": "assistant", "content": "Conclusion In this tutorial you saw how to set up your LLM application with best-in-class observability.\nNo matter what stage your application is in, you will still benefit from observability. If you have more in-depth questions about observability, check out the how-to section for guides on topics like testing, prompt management, and more. Observability is not the only thing LangSmith can help with!\nIt can also help with evaluation, optimization, and more!\nCheck out the other tutorials to see how to get started with those."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith integrate with LangChain?"}, {"role": "assistant", "content": "Trace withLangChain(Python and JS/TS) LangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the installation commands for the LangChain core library and OpenAI integration in Python and JavaScript?"}, {"role": "assistant", "content": "Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs. pipyarnnpmpnpmpip install langchain_openai langchain_coreyarn add @langchain/openai @langchain/corenpm install @langchain/openai @langchain/corepnpm add @langchain/openai @langchain/core"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables should be configured for using LangChain with the OpenAI API?"}, {"role": "assistant", "content": "1. Configure your environment PythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain with LangSmith and are not in a serverless environment, we also suggest setting the following to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to log a trace to LangSmith when using LangChain?"}, {"role": "assistant", "content": "2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"chain.invoke({\"question\": question, \"context\": context})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke({ question: question, context: context });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default name of the trace logged to the project?"}, {"role": "assistant", "content": "3. View your trace By default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you trace specific invocations of LangChain runnables in Python and JS/TS?"}, {"role": "assistant", "content": "Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. PythonTypeScript# You can configure a LangChainTracer instance to trace a specific invocation.from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer()chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager for tracing a specific block of code.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled():    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})# This will NOT be traced (assuming LANGCHAIN_TRACING_V2 is not set)chain.invoke({\"question\": \"Am I being traced?\", \"context\": \"I'm not being traced\"})// You can configure a LangChainTracer instance to trace a specific invocation.import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer();await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure a custom project name for a LangSmith application run?"}, {"role": "assistant", "content": "Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. export LANGCHAIN_PROJECT=my-project"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you set the project name for a LangChainTracer instance in Python?"}, {"role": "assistant", "content": "Dynamically This largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python. PythonTypeScript# You can set the project name for a specific tracer instance:from langchain.callbacks.tracers import LangChainTracertracer = LangChainTracer(project_name=\"My Project\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# You can set the project name using the project_name parameter.from langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})// You can set the project name for a specific tracer instance:import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";const tracer = new LangChainTracer({ projectName: \"My Project\" });await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\"  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces.\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\nFor information on how to query traces and runs by metadata and tags, see this guide noteWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"You are a helpful AI.\"),  (\"user\", \"{input}\")])# The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlychat_model = ChatOpenAI().with_config({\"tags\": [\"model-tag\"], \"metadata\": {\"model-key\": \"model-value\"}})output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"config-value\"}})# Tags and metadata can also be passed at runtimechain.invoke({\"input\": \"What is the meaning of life?\"}, {\"tags\": [\"invoke-tag\"], \"metadata\": {\"invoke-key\": \"invoke-value\"}})import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful AI.\"],  [\"user\", \"{input}\"]])// The tag \"model-tag\" and metadata {\"model-key\": \"model-value\"} will be attached to the ChatOpenAI run onlyconst model = new ChatOpenAI().withConfig({ tags: [\"model-tag\"], metadata: { \"model-key\": \"model-value\" } });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\"tags\": [\"config-tag\"], \"metadata\": {\"config-key\": \"top-level-value\"}});// Tags and metadata can also be passed at runtimeawait chain.invoke({input: \"What is the meaning of life?\"}, {tags: [\"invoke-tag\"], metadata: {\"invoke-key\": \"invoke-value\"}})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you specify a custom name for a run in LangSmith?"}, {"role": "assistant", "content": "Customize run name When you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. noteThis feature is not currently supported directly for LLM objects. PythonTypeScript# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').configured_chain = chain.with_config({\"run_name\": \"MyCustomChain\"})configured_chain.invoke({\"input\": \"What is the meaning of life?\"})# You can also configure the run name at invocation time, like belowchain.invoke({\"input\": \"What is the meaning of life?\"}, {\"run_name\": \"MyCustomChain\"})// When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').const configuredChain = chain.withConfig({ runName: \"MyCustomChain\" });await configuredChain.invoke({ input: \"What is the meaning of life?\" });// You can also configure the run name at invocation time, like belowawait chain.invoke({ input: \"What is the meaning of life?\" }, {runName: \"MyCustomChain\"})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the run ID for a LangChain invocation in Python and JavaScript/TypeScript?"}, {"role": "assistant", "content": "Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith. In Python, you can use the collect_runs context manager to access the run ID. In JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID. PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.tracers.context import collect_runsprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\n\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parserquestion = \"Can you summarize this morning's meetings?\"context = \"During this morning's meeting, we solved all world conflict.\"with collect_runs() as cb:  result = chain.invoke({\"question\": question, \"context\": context})  # Get the root run id  run_id = cb.traced_runs[0].idprint(run_id)import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { RunCollectorCallbackHandler } from \"@langchain/core/tracers/run_collector\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"],  [\"user\", \"Question: {question\\n\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const runCollector = new RunCollectorCallbackHandler();const question = \"Can you summarize this morning's meetings?\"const context = \"During this morning's meeting, we solved all world conflict.\"await chain.invoke(    { question: question, context: context },    { callbacks: [runCollector] });const runId = runCollector.tracedRuns[0].id;console.log(runId);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods does LangChain provide to ensure all traces are submitted before exiting an application?"}, {"role": "assistant", "content": "Ensure all traces are submitted before exiting In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. In LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\". For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\nBelow is an example: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.tracers.langchain import wait_for_all_tracersllm = ChatOpenAI()try:    llm.invoke(\"Hello, World!\")finally:    wait_for_all_tracers()import { ChatOpenAI } from \"@langchain/openai\";import { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";try {  const llm = new ChatOpenAI();  const response = await llm.invoke(\"Hello, World!\");} catch (e) {  // handle error} finally {  await awaitAllCallbacks();}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure tracing programmatically without setting environment variables?"}, {"role": "assistant", "content": "Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project: LANGCHAIN_TRACING_V2LANGCHAIN_API_KEYLANGCHAIN_ENDPOINTLANGCHAIN_PROJECT However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically. This largely builds off of the previous section. PythonTypeScriptfrom langchain.callbacks.tracers import LangChainTracerfrom langsmith import Client# You can create a client instance with an api key and api urlclient = Client(    api_key=\"YOUR_API_KEY\",  # This can be retrieved from a secrets manager    api_url=\"https://api.smith.langchain.com\",  # Update appropriately for self-hosted installations or the EU region)# You can pass the client and project_name to the LangChainTracer instancetracer = LangChainTracer(client=client, project_name=\"test-no-env\")chain.invoke({\"question\": \"Am I using a callback?\", \"context\": \"I'm using a callback\"}, config={\"callbacks\": [tracer]})# LangChain Python also supports a context manager which allows passing the client and project_namefrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(client=client, project_name=\"test-no-env\"):    chain.invoke({\"question\": \"Am I using a context manager?\", \"context\": \"I'm using a context manager\"})import { LangChainTracer } from \"@langchain/core/tracers/tracer_langchain\";import { Client } from \"langsmith\";// You can create a client instance with an api key and api urlconst client = new Client(    {        apiKey: \"YOUR_API_KEY\",        apiUrl: \"https://api.smith.langchain.com\",    });// You can pass the client and project_name to the LangChainTracer instanceconst tracer = new LangChainTracer({client, projectName: \"test-no-env\"});await chain.invoke(  {    question: \"Am I using a callback?\",    context: \"I'm using a callback\",  },  { callbacks: [tracer] });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith support distributed tracing with LangChain in Python?"}, {"role": "assistant", "content": "Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications.\nThe principles are similar to the distributed tracing guide for the LangSmith SDK. import langsmithfrom langchain_core.runnables import chainfrom langsmith.run_helpers import get_current_run_tree# -- This code should be in a separate file or service --@chaindef child_chain(inputs):    return inputs[\"test\"] + 1def child_wrapper(x, headers):    with langsmith.tracing_context(parent=headers):        child_chain.invoke({\"test\": x})# -- This code should be in a separate file or service --@chaindef parent_chain(inputs):    rt = get_current_run_tree()    headers = rt.to_headers()    # ... make a request to another service with the headers    # The headers should be passed to the other service, eventually to the child_wrapper functionparent_chain.invoke({\"test\": 1})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can LangChain objects be traced when used in conjunction with the LangSmith SDK?"}, {"role": "assistant", "content": "Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within a traceable function and be bound as a child run of the traceable function. from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant. Please respond to the user's request only based on the given context.\"),    (\"user\", \"Question: {question}\\nContext: {context}\")])model = ChatOpenAI(model=\"gpt-3.5-turbo\")output_parser = StrOutputParser()chain = prompt | model | output_parser# The above chain will be traced as a child run of the traceable function@traceable(    tags=[\"openai\", \"chat\"],    metadata={\"foo\": \"bar\"})def invoke_runnnable(question, context):    result = chain.invoke({\"question\": question, \"context\": context})    return \"The response is: \" + resultinvoke_runnnable(\"Can you summarize this morning's meetings?\", \"During this morning's meeting, we solved all world conflict.\") This will produce the following trace tree:\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are LangChain objects traced in versions 0.2.x and above when used inside @traceable functions?"}, {"role": "assistant", "content": "Tracing LangChain objects insidetraceable(JS only) Starting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable. import { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";import { getLangchainCallbacks } from \"langsmith/langchain\";const prompt = ChatPromptTemplate.fromMessages([  [    \"system\",    \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",  ],  [\"user\", \"Question: {question}\\nContext: {context}\"],]);const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });const outputParser = new StringOutputParser();const chain = prompt.pipe(model).pipe(outputParser);const main = traceable(  async (input: { question: string; context: string }) => {    const callbacks = await getLangchainCallbacks();    const response = await chain.invoke(input, { callbacks });    return response;  },  { name: \"main\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the limitations of combining LangChain with traceable when using the RunTree API?"}, {"role": "assistant", "content": "Tracing LangChain child runs viatraceable/ RunTree API (JS only) noteWe're working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable:Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op.It's discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes.Different child runs may have the same execution_order and child_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on the start_time. In some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable-wrapped functions within RunnableLambda. import { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    return await tracedChild(input.text);  },}); Alternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function. TraceableRun Treeimport { traceable } from \"langsmith/traceable\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const tracedChild = traceable((input: string) => `Child Run: ${input}`, {  name: \"Child Run\",});const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // Pass the config to existing traceable function    await tracedChild(config, input.text);    return input.text;  },});import { RunTree } from \"langsmith/run_trees\";import { RunnableLambda } from \"@langchain/core/runnables\";import { RunnableConfig } from \"@langchain/core/runnables\";const parrot = new RunnableLambda({  func: async (input: { text: string }, config?: RunnableConfig) => {    // create the RunTree from the RunnableConfig of the RunnableLambda    const childRunTree = RunTree.fromRunnableConfig(config, {      name: \"Child Run\",    });        childRunTree.inputs = { input: input.text };    await childRunTree.postRun();        childRunTree.outputs = { output: `Child Run: ${input.text}` };    await childRunTree.patchRun();        return input.text;  },});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Langsmith Python client?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring in the provided code examples?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on specific splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps need to be taken to ensure that retrieval steps are rendered correctly in retriever traces?"}, {"role": "assistant", "content": "Log retriever traces noteNothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps. Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever.\nLangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken. Annotate the retriever step with run_type=\"retriever\".Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:page_content: The text of the document.type: This should always be \"Document\".metadata: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace. The following code snippets show how to log a retrieval steps in Python and TypeScript. PythonTypeScriptfrom langsmith import traceabledef _convert_docs(results):    return [        {            \"page_content\": r,            \"type\": \"Document\",            \"metadata\": {\"foo\": \"bar\"}        }        for r in results    ]@traceable(run_type=\"retriever\")def retrieve_docs(query):    # Foo retriever returning hardcoded dummy documents.    # In production, this could be a real vector datatabase or other document index.    contents = [\"Document contents 1\", \"Document contents 2\", \"Document contents 3\"]    return _convert_docs(contents)retrieve_docs(\"User query\")import { traceable } from \"langsmith/traceable\";interface Document {  page_content: string;  type: string;  metadata: { foo: string };}function convertDocs(results: string[]): Document[] {  return results.map((r) => ({    page_content: r,    type: \"Document\",    metadata: { foo: \"bar\" }  }));}const retrieveDocs = traceable(  (query: string): Document[] => {    // Foo retriever returning hardcoded dummy documents.    // In production, this could be a real vector database or other document index.    const contents = [\"Document contents 1\", \"Document contents 2\", \"Document contents 3\"];    return convertDocs(contents);  },  { name: \"retrieveDocs\", run_type: \"retriever\" } // Configuration for traceable);await retrieveDocs(\"User query\"); The following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How many saved filters are displayed in the filter bar if you have more than 3 filters saved?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language during a future session?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the AI Query functionality mentioned in the text?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the Helm repository mentioned in the context of running support queries against Postgres?"}, {"role": "assistant", "content": "Running Support Queries against Postgres This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining trace counts for multiple organizations in a single query). This command takes a postgres connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the pg_get_trace_counts_daily.sql input file in the support_queries directory."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What tools and items do you need to prepare before running the migration script?"}, {"role": "assistant", "content": "Prerequisites Ensure you have the following tools/items ready. kubectlhttps://kubernetes.io/docs/tasks/tools/PostgreSQL clienthttps://www.postgresql.org/download/PostgreSQL database connection:HostPortUsernameIf using the bundled version, this is postgresPasswordIf using the bundled version, this is postgresDatabase nameIf using the bundled version, this is postgresConnectivity to the PostgreSQL database from the machine you will be running the migration script on.If you are using the bundled version, you may need to port forward the postgresql service to your local machine.Run kubectl port-forward svc/langsmith-postgres 5432:5432 to port forward the postgresql service to your local machine."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should be used to run a query script and output the results to a CSV file?"}, {"role": "assistant", "content": "Running the query script Run the following command to run the desired query: sh run_support_query_pg.sh <postgres_url> --input path/to/query.sql For example, if you are using the bundled version with port-forwarding, the command might look like: sh run_support_query_pg.sh \"postgres://postgres:postgres@localhost:5432/postgres\" --input support_queries/pg_get_trace_counts_daily.sql which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag --output path/to/file.csv"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What feature in LangSmith can be used to track multi-turn conversations in LLM applications?"}, {"role": "assistant", "content": "Set up threads Recommended ReadingBefore diving into this content, it might be helpful to read the following:Add metadata and tags to traces Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the special metadata key used to associate traces in a thread?"}, {"role": "assistant", "content": "Group traces into threads A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread. To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread. The key value is the unique identifier for that conversation.\nThe key name should be one of: session_idthread_idconversation_id. The value should be a UUID, such as f47ac10b-58cc-4372-a567-0e02b2c3d479."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access and view threads in a project details page?"}, {"role": "assistant", "content": "View threads You can view threads by clicking on the Threads tad in any project details page. You will then see a list of all threads, sorted by the most recent activity. You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs. You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What feature is available only to Enterprise customers for managing user permissions in a LangSmith workspace?"}, {"role": "assistant", "content": "Set up access control noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Read more about roles under admin concepts Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces LangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it.\nOnly users with the workspace:manage permission can manage access control settings for a workspace."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can Organization Admins create custom roles in LangSmith?"}, {"role": "assistant", "content": "Create a role By default, LangSmith comes with a set of system roles: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) If these do not fit your access model, Organization Admins can create custom roles to suit your needs. To create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization. Click on the Create Role button to create a new role. You should see a form like the one below: Assign permissions for the different LangSmith resources that you want to control access to."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you assign a role to a user in the Organization settings?"}, {"role": "assistant", "content": "Assign a role to a user Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the Workspace members tab in the Workspaces section of the Organization settings page Each user will have a Role dropdown that you can use to assign a role to them. You can also invite new users with a given role."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the recommended methods for running evals, and what should you do if you cannot use the SDKs?"}, {"role": "assistant", "content": "Run evals with the REST API Recommended ReadingBefore diving into this content, it might be helpful to read the following:Evaluate LLM applicationsLangSmith API Reference It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.\nHowever, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly. This guide will show you how to run evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the code provided in the text?"}, {"role": "assistant", "content": "Create a dataset Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information. import openaiimport osimport requestsfrom datetime import datetimefrom langsmith import Clientfrom uuid import uuid4client = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries - API Example\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are involved in running a single experiment using the provided API?"}, {"role": "assistant", "content": "Run a single experiment First, pull all of the examples you'd want to use in your experiment. # Pick a dataset id. In this case, we are using the dataset we created above.# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__deletedataset_id = dataset.idparams = { \"dataset\": dataset_id }resp = requests.get(    \"https://api.smith.langchain.com/api/v1/examples\",    params=params,    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})examples = resp.json() Next, we'll define a method that will create a run for a single example. os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"def run_completion_on_example(example, model_name, experiment_id):    \"\"\"Run completions on a list of examples.\"\"\"    # We are using the OpenAI API here, but you can use any model you like    def _post_run(run_id, name, run_type, inputs, parent_id=None):        \"\"\"Function to post a new run to the API.\"\"\"        data = {            \"id\": run_id.hex,            \"name\": name,            \"run_type\": run_type,            \"inputs\": inputs,            \"start_time\": datetime.utcnow().isoformat(),            \"reference_example_id\": example[\"id\"],            \"session_id\": experiment_id,        }        if parent_id:            data[\"parent_run_id\"] = parent_id.hex        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/runs\", # Update appropriately for self-hosted installations or the EU region            json=data,            headers=headers        )        resp.raise_for_status()    def _patch_run(run_id, outputs):        \"\"\"Function to patch a run with outputs.\"\"\"        resp = requests.patch(            f\"https://api.smith.langchain.com/api/v1/runs/{run_id}\",            json={                \"outputs\": outputs,                \"end_time\": datetime.utcnow().isoformat(),            },            headers=headers,        )        resp.raise_for_status()    # Send your API Key in the request headers    headers = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    text = example[\"inputs\"][\"text\"]    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    # Create parent run    parent_run_id = uuid4()    _post_run(parent_run_id, \"LLM Pipeline\", \"chain\", {\"text\": text})    # Create child run    child_run_id = uuid4()    _post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)    # Generate a completion    client = openai.Client()    chat_completion = client.chat.completions.create(model=model_name, messages=messages)    # End runs    _patch_run(child_run_id, chat_completion.dict())    _patch_run(parent_run_id, {\"label\": chat_completion.choices[0].message.content}) We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini. # Create a new experiment using the /sessions endpoint# An experiment is a collection of runs with a reference to the dataset used# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_postmodel_names = (\"gpt-3.5-turbo\", \"gpt-4o-mini\")experiment_ids = []for model_name in model_names:    resp = requests.post(        \"https://api.smith.langchain.com/api/v1/sessions\",        json={            \"start_time\": datetime.utcnow().isoformat(),            \"reference_dataset_id\": str(dataset_id),            \"description\": \"An optional description for the experiment\",            \"name\": f\"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}\",  # A name for the experiment            \"extra\": {                \"metadata\": {\"foo\": \"bar\"},  # Optional metadata            },        },        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )    experiment = resp.json()    experiment_ids.append(experiment[\"id\"])    # Run completions on all examples    for example in examples:        run_completion_on_example(example, model_name, experiment[\"id\"])    # Issue a patch request to \"end\" the experiment by updating the end_time    requests.patch(        f\"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}\",        json={\"end_time\": datetime.utcnow().isoformat()},        headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}    )"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for running a pairwise experiment and ranking the outputs of two or more experiments?"}, {"role": "assistant", "content": "Run a pairwise experiment Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.\nFor more information, check out this guide. # A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_postresp = requests.post(    \"https://api.smith.langchain.com/api/v1/datasets/comparative\",    json={        \"experiment_ids\": experiment_ids,        \"name\": \"Toxicity detection - API Example - Comparative - \" + str(uuid4())[0:8],        \"description\": \"An optional description for the comparative experiment\",        \"extra\": {            \"metadata\": {\"foo\": \"bar\"},  # Optional metadata        },        \"reference_dataset_id\": str(dataset_id),    },    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()comparative_experiment_id = comparative_experiment[\"id\"]# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs# Fetch the comparative experimentresp = requests.get(    f\"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative\",    params={\"id\": comparative_experiment_id},    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})comparative_experiment = resp.json()[0]experiment_ids = [info[\"id\"] for info in comparative_experiment[\"experiments_info\"]]from collections import defaultdictexample_id_to_runs_map = defaultdict(list)# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_postruns = requests.post(    f\"https://api.smith.langchain.com/api/v1/runs/query\",    headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]},    json={        \"session\": experiment_ids,        \"is_root\": True, # Only fetch root runs (spans) which contain the end outputs        \"select\": [\"id\", \"reference_example_id\", \"outputs\"],    }).json()runs = runs[\"runs\"]for run in runs:    example_id = run[\"reference_example_id\"]    example_id_to_runs_map[example_id].append(run)for example_id, runs in example_id_to_runs_map.items():    print(f\"Example ID: {example_id}\")    # Preferentially rank the outputs, in this case we will always prefer the first output    # In reality, you can use an LLM to rank the outputs    feedback_group_id = uuid4()    # Post a feedback score for each run, with the first run being the preferred one    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post    # We'll use the feedback group ID to associate the feedback scores with the same group    for i, run in enumerate(runs):        print(f\"Run ID: {run['id']}\")        feedback = {            \"score\": 1 if i == 0 else 0,            \"run_id\": str(run[\"id\"]),            \"key\": \"ranked_preference\",            \"feedback_group_id\": str(feedback_group_id),            \"comparative_experiment_id\": comparative_experiment_id,        }        resp = requests.post(            \"https://api.smith.langchain.com/api/v1/feedback\",            json=feedback,            headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}        )        resp.raise_for_status()"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you read before proceeding to the Usage Limits page?"}, {"role": "assistant", "content": "Usage Limits noteThis page assumes that you have already read our guide on data retention. Please read that page before proceeding.  "}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of usage limits can you configure on LangSmith for tracing?"}, {"role": "assistant", "content": "How usage limits work LangSmith lets you configure usage limits on tracing. Note that these are usage limits, not spend limits, which\nmean they let you limit the quantity of occurrances of some event rather than the total amount you will spend. LangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide: All traces limitExtended data retention traces limit These let you limit the number of total traces, and extended data retention traces respectively."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What does the term \"usage limiting\" imply regarding the accuracy of the limit threshold?"}, {"role": "assistant", "content": "Properties of usage limiting Usage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there\nmay be a small period of time where additional traces are processed above the limit threshold before usage limiting\nbegins to apply."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What features become inaccessible when the extended data retention traces limit is reached?"}, {"role": "assistant", "content": "Side effects of extended data retention traces limit The extended data retention traces limit has side effects. If the limit is already reached, any feature that could\ncause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause\nanother extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can\nno longer: match run rulesadd feedback to tracesadd runs to annotation queues Each of these features may cause an auto upgrade, so we shut them off when the limit is reached."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can usage limits be updated?"}, {"role": "assistant", "content": "Updating usage limits Usage limits can be updated from the Settings page under Usage and Billing. Limit values are cached, so it\nmay take a minute or two before the new limits apply."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the tutorial about?"}, {"role": "assistant", "content": "Related content Tutorial on how to optimize spend"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using LangChain's off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.\nThese evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello Harrison\"),]dataset_name = \"Hello Set\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three QA evaluators mentioned, and which one is recommended based on meta-evals?"}, {"role": "assistant", "content": "Use question and answer (correctness) evaluators Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we recommend using \"cot_qa\", or Chain of Thought QA. Here is a trivial example that uses a \"cot_qa\" evaluator to evaluate a simple pipeline that prefixes the input with \"Hello\": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using \"criteria\" evaluators when ground truth reference labels are not available?"}, {"role": "assistant", "content": "Use criteria evaluators If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The \"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",    config={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent \"direction\" (i.e., higher is not necessarily better).\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for \"helpfulness\" means that the prediction was deemed to be helpful by the model.\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content, which, of course, is \"bad\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functions of the \"labeled_criteria\" and \"labeled_score_string\" evaluators in evaluating predictions against reference labels?"}, {"role": "assistant", "content": "Use labeled criteria evaluators If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators. The \"labeled_criteria\" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={        \"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of distance metrics used to measure similarity between a predicted string and a reference?"}, {"role": "assistant", "content": "Use string or embedding distance metrics To measure the similarity between a predicted string and a reference, you can use string distance metrics: The \"string_distance\" evaluator computes a normalized string edit distance between the prediction and referenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    \"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\": True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # \"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",      })evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries are required to customize the model used for LLM-based evaluators?"}, {"role": "assistant", "content": "Use a custom LLM in off-the-shelf evaluators You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a model that outputs multiple fields using LangChain's evaluators?"}, {"role": "assistant", "content": "Handle multiple input or output fields LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\". We want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    },    prepare_data=lambda run, example: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],        \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" + input[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended approach for logging LLM traces from OpenAI models using the langsmith SDK?"}, {"role": "assistant", "content": "Log custom LLM traces noteNothing will break if you don't log LLM traces in the correct format and data will still be logged. However, the data will not be processed or rendered in a way that is specific to LLMs. The best way to logs traces from OpenAI models is to use the wrapper available in the langsmith SDK for Python and TypeScript. However, you can also log traces from custom models by following the guidelines below. LangSmith provides special rendering and processing for LLM traces, including token counting (assuming token counts are not available from the model provider) and token-based cost calculation.\nIn order to make the most of this feature, you must log your LLM traces in a specific format. noteThe examples below uses the traceable decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What format must the inputs for chat-style models be in, and what are the acceptable output formats?"}, {"role": "assistant", "content": "Chat-style models For chat-style models, inputs must be a list of messages in OpenAI-compatible format, represented as Python dictionaries or TypeScript object. Each message must contain the key role and content. The output is accepted in any of the following formats: A dictionary/object that contains the key choices with a value that is a list of dictionaries/objects. Each dictionary/object must contain the key message, which maps to a message object with the keys role and content.A dictionary/object that contains the key message with a value that is a message object with the keys role and content.A tuple/array of two elements, where the first element is the role and the second element is the content.A dictionary/object that contains the key role and content. The input to your function should be named messages. You can also provide the following metadata fields to help LangSmith identify the model and calculate costs. If using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. To learn more about how to use the metadata fields, see this guide. ls_provider: The provider of the model, eg \"openai\", \"anthropic\", etc.ls_model_name: The name of the model, eg \"gpt-3.5-turbo\", \"claude-3-opus-20240307\", etc. PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ]}# Can also use one of:# output = {#     \"message\": {#         \"role\": \"assistant\",#         \"content\": \"Sure, what time would you like to book the table for?\"#     }# }## output = {#     \"role\": \"assistant\",#     \"content\": \"Sure, what time would you like to book the table for?\"# }## output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" }];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\"      }    }  ]};// Can also use one of:// const output = {//   message: {//     role: \"assistant\",//     content: \"Sure, what time would you like to book the table for?\"//   }// };//// const output = {//   role: \"assistant\",//   content: \"Sure, what time would you like to book the table for?\"// };//// const output = [\"assistant\", \"Sure, what time would you like to book the table for?\"];const chatModel = traceable(  async ({ messages }: { messages: { role: string; content: string }[] }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the `_reduce_chunks` function in the context of streaming outputs?"}, {"role": "assistant", "content": "Stream outputs For streaming, you can \"reduce\" the outputs into the same format as the non-streaming version. This is currently only supported in Python. def _reduce_chunks(chunks: list):    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}@traceable(    run_type=\"llm\",    reduce_fn=_reduce_chunks,    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def my_streaming_chat_model(messages: list):    for chunk in [\"Hello, \" + messages[1][\"content\"]]:        yield {            \"choices\": [                {                    \"message\": {                        \"content\": chunk,                        \"role\": \"assistant\",                    }                }            ]        }list(    my_streaming_chat_model(        [            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},            {\"role\": \"user\", \"content\": \"polly the parrot\"},        ],    ))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can token counts be manually provided to LangSmith in a function's response?"}, {"role": "assistant", "content": "Manually provide token counts Token-based cost trackingTo learn how to set up token-based cost tracking based on the token usage information, see this guide. By default, LangSmith uses TikToken to count tokens, utilizing a best guess at the model's tokenizer based on the ls_model_name provided.\nMany models already include token counts as part of the response. You can send these token counts to LangSmith by providing the usage_metadata field in the response.\nIf token information is passed to LangSmith, the system will use this information instead of using TikToken. You can add a usage_metadata key to the function's response, containing a dictionary with the keys input_tokens, output_tokens and total_tokens.\nIf using LangChain or OpenAI wrapper, these fields will be automatically populated correctly. noteIf ls_model_name is not present in extra.metadata, other fields might be used from the extra.invocation_metadata for estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_nameinvocation_params.modelinvocation_params.model_name PythonTypeScriptfrom langsmith import traceableinputs = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},]output = {    \"choices\": [        {            \"message\": {                \"role\": \"assistant\",                \"content\": \"Sure, what time would you like to book the table for?\"            }        }    ],    \"usage_metadata\": {        \"input_tokens\": 27,        \"output_tokens\": 13,        \"total_tokens\": 40,    },}@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def chat_model(messages: list):    return outputchat_model(inputs)import { traceable } from \"langsmith/traceable\";const messages = [  { role: \"system\", content: \"You are a helpful assistant.\" },  { role: \"user\", content: \"I'd like to book a table for two.\" },];const output = {  choices: [    {      message: {        role: \"assistant\",        content: \"Sure, what time would you like to book the table for?\",      },    },  ],  usage_metadata: {    input_tokens: 27,    output_tokens: 13,    total_tokens: 40,  },};const chatModel = traceable(  async ({    messages,  }: {    messages: { role: string; content: string }[];    model: string;  }) => {    return output;  },  { run_type: \"llm\", name: \"chat_model\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await chatModel({ messages });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for inputs and outputs in instruct-style models?"}, {"role": "assistant", "content": "Instruct-style models For instruct-style models (string in, string out), your inputs must contain a key prompt with a string value. Other inputs are also permitted. The output must return an object that, when serialized, contains the key choices with a list of dictionaries/objects. Each must contain the key text with a string value.\nThe same rules for metadata and usage_metadata apply as for chat-style models. PythonTypeScript@traceable(    run_type=\"llm\",    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"})def hello_llm(prompt: str):    return {        \"choices\": [            {\"text\": \"Hello, \" + prompt}        ],        \"usage_metadata\": {            \"input_tokens\": 4,            \"output_tokens\": 5,            \"total_tokens\": 9,        },    }hello_llm(\"polly the parrot\\n\")import { traceable } from \"langsmith/traceable\";const helloLLM = traceable(  ({ prompt }: { prompt: string }) => {    return {      choices: [        { text: \"Hello, \" + prompt }      ],        usage_metadata: {            input_tokens: 4,            output_tokens: 5,            total_tokens: 9,        },    };  },  { run_type: \"llm\", name: \"hello_llm\", metadata: { ls_provider: \"my_provider\", ls_model_name: \"my_model\" } });await helloLLM({ prompt: \"polly the parrot\\n\" }); The above code will log the following trace:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two methods mentioned for installing LangSmith on your own infrastructure?"}, {"role": "assistant", "content": "Installation This section contains guides for installing LangSmith on your own infrastructure. Kubernetes: Deploy LangSmith on Kubernetes.Docker: Deploy LangSmith using Docker."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Langsmith Python client?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith in Python and TypeScript?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the example IDs used to list multiple examples in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps should you follow to share or unshare a dataset publicly in LangSmith?"}, {"role": "assistant", "content": "Share or unshare a dataset publicly cautionSharing a dataset publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.\nThis link gives viewers access to all example rows, experiments, and associated runs and feedback on this dataset.\nThis feature is only available in the cloud-hosted version of LangSmith. To share a dataset publicly, simply click on the Share button in the upper right hand side of any dataset details page.\n This will open a dialog where you can copy the link to the dataset. Shared datasets will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the dataset, experiments and examples, but not edit any of this information. To \"unshare\" a dataset, either Click on Unshare by click on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog.\nNavigate to your organization's list of publicly shared dataset, either by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of logging to LangSmith?"}, {"role": "assistant", "content": "Tracing This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Run in the context of an LLM application?"}, {"role": "assistant", "content": "Runs A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of LangSmith, and how are runs related to it?"}, {"role": "assistant", "content": "Traces A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a project in the context of traces?"}, {"role": "assistant", "content": "Projects A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different ways to collect feedback on runs?"}, {"role": "assistant", "content": "Feedback Feedback allows you to score an individual run based on certain criteria.\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are tags used for in the LangSmith UI?"}, {"role": "assistant", "content": "Tags Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of metadata in relation to runs?"}, {"role": "assistant", "content": "Metadata Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How long does LangSmith retain trace data after it is inserted into the trace database?"}, {"role": "assistant", "content": "Data storage and retention For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you delete a trace from LangSmith before its expiration date?"}, {"role": "assistant", "content": "Deleting traces from LangSmith If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the \"Delete\" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to manage users and create API keys in LangSmith?"}, {"role": "assistant", "content": "I can't create API keys or manage users in the UI, what's wrong? You have likely deployed LangSmith without setting up SSO. LangSmith requires SSO to manage users and API keys. You can find more information on setting up SSO in the configuration section. How does load balancing/ingress work? You will need to expose the frontend container/service to your applications/users. This will handle routing to all downstream services.You will need to terminate SSL at the ingress level. We recommend using a managed service like AWS ALB, GCP Load Balancer, or Nginx. How can we authenticate to the application? Currently, our self-hosted solution supports SSO with OAuth2.0 and OIDC as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production. You can find more information on setting up SSO in the configuration section. Can I use external storage services? You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services. Check out the configuration section for more information. Does my application need egress to function properly? Our deployment only needs egress for a few things: Fetching images (If mirroring your images, this may not be needed)Talking to any LLMsTalking to any external storage services you may have configuredFetching OAuth information Your VPC can set up rules to limit any other access.\nNote: We require the X-Organization-Id and X-Tenant-Id headers to be allowed to be passed through to the backend service. These are used to determine which organization and workspace (previously called \"tenant\") the request is for. Resource requirements for the application? In kubernetes, we recommend a minimum helm configuration which can be found in here. For docker, we recommend a minimum of 16GB of RAM and 4 CPUs.For Postgres, we recommend a minimum of 8GB of RAM and 2 CPUs.For Redis, we recommend 4GB of RAM and 2 CPUs.For Clickhouse, we recommend 32GB of RAM and 8 CPUs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the Kubernetes distributions on which LangSmith has been successfully tested for self-hosting?"}, {"role": "assistant", "content": "Self-hosting LangSmith on Kubernetes Enterprise License RequiredSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. This guide will walk you through the process of deploying LangSmith to a Kubernetes cluster. We will use Helm to install LangSmith and its dependencies. We've successfully tested LangSmith on the following Kubernetes distributions: Google Kubernetes Engine (GKE)Amazon Elastic Kubernetes Service (EKS)Azure Kubernetes Service (AKS)OpenShiftMinikube and Kind (for development purposes) To review all configuration options, look at the values.yaml for the LangSmith helm chart."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the minimum requirements for a Kubernetes cluster to be ready for use according to the prerequisites?"}, {"role": "assistant", "content": "Prerequisites Ensure you have the following tools/items ready. Some items are marked optional: A working Kubernetes cluster that you can access via kubectl. Your cluster should have the following minimum requirements:Recommended: At least 4 vCPUs, 16GB Memory availableYou may need to tune resource requests/limits for all of our different services based off of organization size/usageValid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running:kubectl get storageclassThe output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:  NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE  gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   true                   161dnoteWe highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.Refer to the Kubernetes documentation for more information on storage classes.Helmbrew install helmLangSmith License KeyYou can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.Api Key SaltThis is a secret key that you can generate. It should be a random string of characters.You can generate this using the following command:openssl rand -base64 32ConfigurationThere are several configuration options that you can set in the langsmith_config.yaml file. You can find more information on the available configuration options in the Configuration section."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to override values in the LangSmith Helm Chart?"}, {"role": "assistant", "content": "Configure your Helm Charts: Create a new file called langsmith_config.yaml. This should have a similar structure to the values.yaml file in the LangSmith Helm Chart repository. Only include the values you want to override to avoid having to update the file every time the chart is updated.Set the appropriate values in the langsmith_config.yaml file. You can find the available configuration options in the configuration section. You can also see some example configurations in the examples directory of the Helm Chart repository here: LangSmith helm chart examples."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps should you follow to deploy Langsmith to a Kubernetes cluster and verify its status?"}, {"role": "assistant", "content": "Deploying to Kubernetes: Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)Run kubectl get podsOutput should look something like:kubectl get pods                                                                                                                                                                      langsmith-eks-2vauP7wf 21:07:46No resources found in default namespace.Ensure you have the Langchain Helm repo added. (skip this step if you are using local charts)helm repo add langchain https://langchain-ai.github.io/helm/\"langchain\" has been added to your repositoriesRun helm install langsmith langchain/langsmith --values langsmith_config.yaml --namespace <your-namespace> --version <version>Output should look something like:NAME: langsmithLAST DEPLOYED: Fri Sep 17 21:08:47 2021NAMESPACE: langsmithSTATUS: deployedREVISION: 1TEST SUITE: NoneRun kubectl get pods\nOutput should now look something like:langsmith-backend-6ff46c99c4-wz22d       1/1     Running   0          3h2mlangsmith-frontend-6bbb94c5df-8xrlr      1/1     Running   0          3h2mlangsmith-hub-backend-5cc68c888c-vppjj   1/1     Running   0          3h2mlangsmith-playground-6d95fd8dc6-x2d9b    1/1     Running   0          3h2mlangsmith-postgres-0                     1/1     Running   0          9hlangsmith-queue-5898b9d566-tv6q8         1/1     Running   0          3h2mlangsmith-redis-0                        1/1     Running   0          9h"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should you run to validate your deployment and check the services?"}, {"role": "assistant", "content": "Validate your deployment: Run kubectl get servicesOutput should look something like:NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGElangsmith-backend       ClusterIP      172.20.140.77    <none>                                                                    1984/TCP       35hlangsmith-frontend      LoadBalancer   172.20.253.251   <external ip>                                                             80:31591/TCP   35hlangsmith-hub-backend   ClusterIP      172.20.112.234   <none>                                                                    1985/TCP       35hlangsmith-playground    ClusterIP      172.20.153.194   <none>                                                                    3001/TCP       9hlangsmith-postgres      ClusterIP      172.20.244.82    <none>                                                                    5432/TCP       35hlangsmith-redis         ClusterIP      172.20.81.217    <none>                                                                    6379/TCP       35hCurl the external ip of the langsmith-frontend service:curl <external ip>/api/tenants[{\"id\":\"00000000-0000-0000-0000-000000000000\",\"has_waitlist_access\":true,\"created_at\":\"2023-09-13T18:25:10.488407\",\"display_name\":\"Personal\",\"config\":{\"is_personal\":true,\"max_identities\":1},\"tenant_handle\":\"default\"}]%Visit the external ip for the langsmith-frontend service on your browserThe LangSmith UI should be visible/operational"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you find more information on using self-hosted LangSmith?"}, {"role": "assistant", "content": "Using LangSmith Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the self-hosted usage guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What administrative tasks can be performed using scripts for LangSmith that are not available in the UI?"}, {"role": "assistant", "content": "Scripts for administering LangSmith This section contains guides for performing common administrative tasks that are not currently available via the LangSmith UI. You can find these scripts in the Helm chart repository. Delete an organization: Delete an organization in LangSmith.Delete a workspace: Delete a workspace in LangSmith.Delete a trace: Delete a trace in LangSmith.Generate Clickhouse statistics: Generate Clickhouse statistics.Generate LangSmith Query statistics: Generate LangSmith Query statistics from Clickhouse.Running Support Queries: Running other support queries supplied by the LangChain support team."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will API keys prefixed with ls__ be disabled in favor of lsv2 style keys?"}, {"role": "assistant", "content": "LangSmith Release Notes noteReminder: API keys prefixed with ls__ will be disabled in favor of lsv2... style keys as of LangSmith Helm release v0.7 to be released in August 2024. For more information see the Admin concepts guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith v0.6 during the week of June 17, 2024?"}, {"role": "assistant", "content": "Week of June 17, 2024 - LangSmith v0.6 LangSmith v0.6 improves run rules performance and reliability, adds support for multiple Workspaces within an Organization, custom models in Playground, and significant enhancements to Evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced in LangSmith?"}, {"role": "assistant", "content": "New Features Dataset splits for evaluation and filtering/editing dataset examples. Learn More...You can now run multiple repetitions of your experiment in LangSmith. Learn More...Off-the-shelf online evaluator prompts to catch bad retrieval and hallucinations for RAG. Learn More...Manage private prompts without a handle. Learn More...Workspaces in LangSmith for improved collaboration & organization. Learn More...Enter the playground from scratch instead of from a trace or a prompt. Learn More...Variable mapping for online evaluator prompts. Learn More...Custom Model support in Playground. Learn More..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the performance and reliability of run rules and querying the Hub via the SDK?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance of run rules especially in cases where rule execution may exceed the interval of rule execution.Reduced run rule interval from 5 minutes to 1 minute resulting in more frequent application of rulesImproved performance when querying Hub via the SDK. NOTE: Accessing these improvements requires v0.1.20 or greater of the Hub SDK"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the default port and Helm configuration in the recent infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes [Docker Compose only] The default port has changed from 80 to 1980.[Helm] The playground image start command has changed. If you are using a custom Helm chart, you may need to review the configuration for Playground and adjust your Helm config accordingly.[Helm] Added the ability to configure your probes in the values.yaml file. This allows you to adjust the readiness and liveness probes for the LangSmith services. You may need to adjust these if you had changed container ports.[Helm] Added ArgoCD PostSync annotations to hook jobs to ensure that the jobs are run properly in ArgoCD. You may need to remove this annotation if you were previously setting it manually.Updated Clickhouse from v23.9 to v24.2 NOTE: Applies only to environments using the LangSmith-provided Clickhouse."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were added in the recent admin changes?"}, {"role": "assistant", "content": "Admin changes Added support for Workspaces. See the Admin concepts guide for more details.Added global setting orgCreationDisabled to values.yaml to disable creation of new Organizations.Added support for custom TLS certificates for the for the Azure OpenAI model provider. See the how-to guide for more details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.5.x and earlier following the release of v0.6?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.6: LangSmith v0.5.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and features does LangSmith v0.5 introduce?"}, {"role": "assistant", "content": "Week of May 13, 2024 - LangSmith v0.5 LangSmith v0.5 improves performance and reliability, adds features to improve regression testing, production monitoring and automation, and implements Role-Based Access Controls (RBAC)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after the LangSmith Helm release v0.7 in August 2024?"}, {"role": "assistant", "content": "Breaking changes We will be dropping support for API keys in favor of personal access tokens (PATs) and Service Keys. We recommend using PATs and Service Keys for all new integrations. API keys prefixed with ls__ will NO LONGER work as of LangSmith Helm release v0.7 to be released in August 2024."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features have been introduced, including role-based access controls and improvements in regression testing and production monitoring?"}, {"role": "assistant", "content": "New Features Role-Based Access Controls. See: https://blog.langchain.dev/access-control-updates-for-langsmith/Improved regression testing experience. See: https://blog.langchain.dev/regression-testing/Improved production monitoring and automation: See: https://blog.langchain.dev/langsmith-production-logging-automations/"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to improve performance and reliability in resource management?"}, {"role": "assistant", "content": "Performance and Reliability Changes Split ingest, session deletion, and automation jobs to execute within separate resource pools."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default storage size for Clickhouse persistence in LangSmith v0.4, and how can it be adjusted?"}, {"role": "assistant", "content": "Infrastructure changes As of LangSmith v0.4, Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing theclickhouse.statefulSet.persistence.sizevalue in yourvalues.yamlfile.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or setclickhouse.statefulSet.persistence.sizeto the previous default value of8Gi.It is strongly recommend that you monitor the consumption of storage on your Clickhouse volume to ensure the volume does not near full capacity, which may cause run ingest to behave erratically.New Platform-Backend service used internally. This service also uses its own image. You may need to adjust your helm values files accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What new features were introduced in the admin changes regarding access controls and API keys?"}, {"role": "assistant", "content": "Admin changes Added new Role-Based Access Controls. For more details see the Admin and Set Up Access Control sections of the docs.Introduction of PATs and Service Keys. Old API keys have been migrated to service keys."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith v0.4.x and earlier following the release of v0.5?"}, {"role": "assistant", "content": "Deprecation notices With the release of v0.5: LangSmith v0.4.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.4 during the week of March 25, 2024?"}, {"role": "assistant", "content": "Week of March 25, 2024 - LangSmith v0.4 LangSmith 0.4 improves performance and reliability, implements a new asynchronous queue worker to optimize run ingests, and an API key salt parameter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes should be made to ensure backwards compatibility when updating to the new release?"}, {"role": "assistant", "content": "Breaking changes This release adds an API key salt parameter. This previously defaulted to your LangSmith License Key. For updates from earlier versions you should set this parameter to your license key to ensure backwards compatibility. Using a new api key salt will invalidate all existing api keys.This release makes Clickhouse persistence use 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration does not configure persistence already, you will need to resize your existing pvc or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to enhance performance and reliability in the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Implemented a new asynchronous queue worker and cached token encodings to improve performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes have been made to the image repositories and Clickhouse persistence storage in the infrastructure updates?"}, {"role": "assistant", "content": "Infrastructure changes Some our image repositories have been updated. You can see the root repositories in our values.yaml file and may need to update mirrors to pick up the new images.Clickhouse persistence now uses 50Gi of storage by default. You can adjust this by changing the clickhouse.statefulSet.persistence.size value in your values.yaml file.If your existing configuration cannot support 50Gi, you may need to resize your existing storage class or set clickhouse.statefulSet.persistence.size to the previous default value of 8Gi.Consolidation of hubBackend and backend services. We now use one service to serve both of these endpoints. This should not impact your application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What changes were made to the API key management and OAuth flow in the recent admin updates?"}, {"role": "assistant", "content": "Admin changes Added an API key salt parameter in values.yml. This can be set to a custom value and changing it will invalidate all existing api keys.Changed the OAuth flow to leverage Access Tokens instead of OIDC ID tokens. This change should not impact the end user experience.Added scripts to enable feature flags in self-hosted environments for use in previewing pre-release features. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/ADD-FEATURE-FLAG.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.3.x and earlier versions following the release of 0.4?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.4: LangSmith 0.3.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features were introduced in LangSmith version 0.3 during the week of February 21, 2024?"}, {"role": "assistant", "content": "Week of Februrary 21, 2024 - LangSmith v0.3 LangSmith 0.3 improves performance and reliability, adds improved monitoring charts group by metadata and tag, and adds cost tracking."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What must you do to retain run data when migrating from LangSmith v0.1?"}, {"role": "assistant", "content": "Breaking changes This release will drop the postgres run tables - if you are making a migration from LangSmith v0.1 and wish to retain run data, you must first update to v0.2 and perform a data migration. See https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md for additional details"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements have been made to the LangSmith UI regarding trace ingestion and display delay?"}, {"role": "assistant", "content": "Performance and Reliability Changes Continued performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is available regarding admin changes?"}, {"role": "assistant", "content": "Admin changes nan"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the status of LangSmith 0.2.x and earlier versions following the release of 0.3?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.3: LangSmith 0.2.x and earlier are now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements and new features does LangSmith 0.2 include?"}, {"role": "assistant", "content": "Week of January 29, 2024 - LangSmith v0.2 LangSmith 0.2 improves performance and reliability, adds a updated interface for reviewing trace data, and adds support for batch processing of traces."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What versions of langsmith-sdk are required for this release to support changes in pagination of API results?"}, {"role": "assistant", "content": "Requirements This release requires langsmith-sdk version  0.0.71 (Python) and  0.0.56 (JS/TS) to support changes in pagination of API results. Older versions will only return the first 100 results when querying an endpoint."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to make custom metadata in traces searchable after the change in search syntax?"}, {"role": "assistant", "content": "Breaking changes The search syntax for metadata in runs has changed and limits support for nested JSON to a single level. If you are supplying custom metadata in traces, you should flatten your metadata structure in order to allow it to be searchable, (e.g. {\"user_id\": ..., \"user_name\":...,}) and then search using has(metadata, '{\"user_name\": ...}')"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What improvements were made to the performance and reliability of the LangSmith UI?"}, {"role": "assistant", "content": "Performance and Reliability Changes Improved performance when ingesting traces, reducing the delay between ingest and display in the LangSmith UI.Improved performance for updates and deletes on annotation labels.Added pagination of API responses.Fixed an issue impacting natural language searches."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What database service has been added to improve performance and scalability in LangSmith?"}, {"role": "assistant", "content": "Infrastructure Changes Added the clickhouse database service. Run results will now be stored in ClickHouse instead of Postgres to improve performance and scalability and reduce delays in the time it takes for runs to appear in LangSmith.Note that if you wish to retain access to run data in the Langsmith UI after updating, a data migration will need to be performed. Details are available at https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docs/UPGRADE-0.2.x.md"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the new maximum number of users allowed per organization for new organizations?"}, {"role": "assistant", "content": "Admin changes Increased the maximum number of users per organization from 5 to 100 for new organizations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the current status of LangSmith 0.1.x following the release of version 0.2?"}, {"role": "assistant", "content": "Deprecation notices With the release of 0.2: LangSmith 0.1.x is now in maintenance mode and may only receive critical security fixes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "When will infoWorkspaces be incrementally rolled out?"}, {"role": "assistant", "content": "Set up a workspace infoWorkspaces will be incrementally rolled out being week of June 10, 2024. Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on organizations and workspaces When you log in for the first time, a default workspace will be created for you automatically in your personal organization.\nWorkspaces are often used to separate resources between different teams, business units, or deployment environments. Most LangSmith activity happens in the context of a workspace, each of which has its own settings."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a new workspace in your shared organization?"}, {"role": "assistant", "content": "Create a workspace To create a new workspace, head to the Settings page Workspaces tab in your shared organization and click Add Workspace.\nOnce your workspace has been created, you can manage its members and other configuration by selecting it on this page. noteDifferent plans have different limits placed on the number of workspaces that can be used in an organization.\nPlease see the pricing page for more information."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What permissions do workspace admins have regarding user management in a workspace?"}, {"role": "assistant", "content": "Manage users infoOnly workspace Admins may manage workspace membership and, if RBAC is enabled, change a user's workspace role. For users that are already members of an organization, a workspace admin may add them to a workspace in the Workspace members tab under workspace settings page.\nUsers may also be invited directly to one or more workspaces when they are invited to an organization."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options can be configured in the workspace settings page?"}, {"role": "assistant", "content": "Configure workspace settings Workspace configuration exists in the workspace settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The example below shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on specific splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in the specified format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of logging to LangSmith?"}, {"role": "assistant", "content": "Tracing This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Run in the context of an LLM application?"}, {"role": "assistant", "content": "Runs A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace and how is it related to runs in the context of a single operation?"}, {"role": "assistant", "content": "Traces A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a project in the context of traces?"}, {"role": "assistant", "content": "Projects A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different ways feedback can be collected on runs?"}, {"role": "assistant", "content": "Feedback Feedback allows you to score an individual run based on certain criteria.\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are tags used for in the LangSmith UI?"}, {"role": "assistant", "content": "Tags Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of metadata in relation to runs?"}, {"role": "assistant", "content": "Metadata Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How long does LangSmith retain trace data after it is inserted into the trace database?"}, {"role": "assistant", "content": "Data storage and retention For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you delete a trace from LangSmith before its expiration date?"}, {"role": "assistant", "content": "Deleting traces from LangSmith If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the \"Delete\" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the focus of the evaluation workflow discussed in the RAG evaluations?"}, {"role": "assistant", "content": "RAG Evaluations We will walk through the evaluation workflow for RAG (retrieval augmented generation)."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will be discussed in the overview?"}, {"role": "assistant", "content": "Overview We will discuss each piece of the workflow below."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the name of the dataset created for LCEL related questions?"}, {"role": "assistant", "content": "Dataset Here is a dataset of LCEL (LangChain Expression Language) related questions that we will use. This dataset was created using csv upload in the LangSmith UI: https://smith.langchain.com/public/730d833b-74da-43e2-a614-4e2ca2502606/d Here, we ensure that API keys for OpenAI as well as LangSmith are set. import getpassimport osdef _set_env(var: str):    if not os.environ.get(var):        os.environ[var] = getpass.getpass(f\"{var}: \")_set_env(\"OPENAI_API_KEY\")os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Update appropriately for self-hosted installations or the EU region_set_env(\"LANGCHAIN_API_KEY\") ### Dataset namedataset_name = \"LCEL-QA\""}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the RagBot class in the context of RAG on LCEL documents?"}, {"role": "assistant", "content": "Task Here is a chain that will perform RAG on LCEL (LangChain Expression Language) docs. We will be using LangChain strictly for creating the retriever and retrieving the relevant documents. The overall pipeline does not use LangChain; LangSmith works regardless of whether or not your pipeline is built with LangChain. Here, we return the retrieved documents as part of the final answer. However, below we will show that this is not required (using evaluation of intermediate steps). See our RAG-From-Scratch repo and tutorial video series for more on this. ### INDEXfrom bs4 import BeautifulSoup as Soupfrom langchain_community.vectorstores import Chromafrom langchain_openai import OpenAIEmbeddingsfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoaderfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load docsurl = \"https://python.langchain.com/v0.1/docs/expression_language/\"loader = RecursiveUrlLoader(    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text)docs = loader.load()# Split into chunkstext_splitter = RecursiveCharacterTextSplitter(chunk_size=4500, chunk_overlap=200)splits = text_splitter.split_documents(docs)# Embed and store in Chromavectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())# Indexretriever = vectorstore.as_retriever() ### RAG botimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclass RagBot:    def __init__(self, retriever, model: str = \"gpt-4-0125-preview\"):        self._retriever = retriever        # Wrapping the client instruments the LLM        self._client = wrap_openai(openai.Client())        self._model = model    @traceable()    def retrieve_docs(self, question):        return self._retriever.invoke(question)    @traceable()    def invoke_llm(self, question, docs):        response = self._client.chat.completions.create(            model=self._model,            messages=[                {                    \"role\": \"system\",                    \"content\": \"You are a helpful AI code assistant with expertise in LCEL.\"                    \" Use the following docs to produce a concise code solution to the user question.\\n\\n\"                    f\"## Docs\\n\\n{docs}\",                },                {\"role\": \"user\", \"content\": question},            ],        )        # Evaluators will expect \"answer\" and \"contexts\"        return {            \"answer\": response.choices[0].message.content,            \"contexts\": [str(doc) for doc in docs],        }    @traceable()    def get_answer(self, question: str):        docs = self.retrieve_docs(question)        return self.invoke_llm(question, docs)rag_bot = RagBot(retriever) response = rag_bot.get_answer(\"How to build a RAG chain in LCEL?\")response[\"answer\"][:150] Define a function that will: Take a dataset exampleExtract the relevant key (e.g., question) from the examplePass it to the RAG chainReturn the relevant output values from the RAG chain def predict_rag_answer(example: dict):    \"\"\"Use this for answer evaluation\"\"\"    response = rag_bot.get_answer(example[\"input_question\"])    return {\"answer\": response[\"answer\"]}def predict_rag_answer_with_context(example: dict):    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"    response = rag_bot.get_answer(example[\"input_question\"])    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the four types of RAG evaluations that users are typically interested in?"}, {"role": "assistant", "content": "Evaluator There are at least 4 types of RAG eval that users are typically interested in. Response vs reference answer Goal: Measure \"how similar/correct is the RAG chain answer, relative to a ground-truth answer\"Mode: Uses ground truth (reference) answer supplied through a datasetJudge: Use LLM-as-judge to assess answer correctness. Response vs input Goal: Measure \"how well does the generated response address the initial user input\"Mode: Reference-free, because it will compare the answer to the input questionJudge: Use LLM-as-judge to assess answer relevance, helpfulness, etc. Response vs retrieved docs Goal: Measure \"to what extent does the generated response agree with the retrieved context\"Mode: Reference-free, because it will compare the answer to the retrieved contextJudge: Use LLM-as-judge to assess faithfulness, hallucinations, etc. Retrieved docs vs input Goal: Measure \"how good are my retrieved results for this query\"Mode: Reference-free, because it will compare the question to the retrieved contextJudge: Use LLM-as-judge to assess relevance"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the answer_evaluator function in the context of evaluating RAG answer accuracy?"}, {"role": "assistant", "content": "Response vs reference answer Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/lTfhw_9cJqc?feature=shared Here is our evaluator function: run is the invocation of predict_rag_answer, which has key answerexample is from our eval set, which has keys input_question and output_answerWe extract these values and pass them into our grader from langchain import hubfrom langchain_openai import ChatOpenAI# Grade promptgrade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")def answer_evaluator(run, example) -> dict:    \"\"\"    A simple evaluator for RAG answer accuracy    \"\"\"    # Get question, ground truth answer, RAG chain answer    input_question = example.inputs[\"input_question\"]    reference = example.outputs[\"output_answer\"]    prediction = run.outputs[\"answer\"]    # LLM grader    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)    # Structured prompt    answer_grader = grade_prompt_answer_accuracy | llm    # Run evaluator    score = answer_grader.invoke({\"question\": input_question,                                  \"correct_answer\": reference,                                  \"student_answer\": prediction})    score = score[\"Score\"]    return {\"key\": \"answer_v_reference_score\", \"score\": score} Now, we kick off evaluation: predict_rag_answer: Takes an example from our eval set, extracts the question, passes to our RAG chainanswer_evaluator: Passes RAG chain answer, question, and ground truth answer to an evaluator from langsmith.evaluation import evaluateexperiment_results = evaluate(    predict_rag_answer,    data=dataset_name,    evaluators=[answer_evaluator],    experiment_prefix=\"rag-answer-v-reference\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the answer_helpfulness_evaluator function in the context of evaluating RAG answer helpfulness?"}, {"role": "assistant", "content": "Response vs input Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-answer-helpfulness The information flow is similar to above, but we simply look at the run answer versus the example question. # Grade promptgrade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")def answer_helpfulness_evaluator(run, example) -> dict:    \"\"\"    A simple evaluator for RAG answer helpfulness    \"\"\"    # Get question, ground truth answer, RAG chain answer    input_question = example.inputs[\"input_question\"]    prediction = run.outputs[\"answer\"]    # LLM grader    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)    # Structured prompt    answer_grader = grade_prompt_answer_helpfulness | llm    # Run evaluator    score = answer_grader.invoke({\"question\": input_question,                                  \"student_answer\": prediction})    score = score[\"Score\"]    return {\"key\": \"answer_helpfulness_score\", \"score\": score} experiment_results = evaluate(    predict_rag_answer,    data=dataset_name,    evaluators=[answer_helpfulness_evaluator],    experiment_prefix=\"rag-answer-helpfulness\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the answer_hallucination_evaluator in the context of RAG (Retrieval-Augmented Generation) models?"}, {"role": "assistant", "content": "Response vs retrieved docs Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/IlNglM9bKLw?feature=shared # Promptgrade_prompt_hallucinations = prompt = hub.pull(\"langchain-ai/rag-answer-hallucination\")def answer_hallucination_evaluator(run, example) -> dict:    \"\"\"    A simple evaluator for generation hallucination    \"\"\"    # RAG inputs    input_question = example.inputs[\"input_question\"]    contexts = run.outputs[\"contexts\"]    # RAG answer    prediction = run.outputs[\"answer\"]    # LLM grader    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)    # Structured prompt    answer_grader = grade_prompt_hallucinations | llm    # Get score    score = answer_grader.invoke({\"documents\": contexts,                                  \"student_answer\": prediction})    score = score[\"Score\"]    return {\"key\": \"answer_hallucination\", \"score\": score} experiment_results = evaluate(    predict_rag_answer_with_context,    data=dataset_name,    evaluators=[answer_hallucination_evaluator],    experiment_prefix=\"rag-answer-hallucination\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the `docs_relevance_evaluator` function in the context of document relevance evaluation?"}, {"role": "assistant", "content": "Retrieved docs vs input Here is an example prompt that we can use: https://smith.langchain.com/hub/langchain-ai/rag-document-relevance Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/Fr_7HtHjcf0?feature=shared # Grade promptgrade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")def docs_relevance_evaluator(run, example) -> dict:    \"\"\"    A simple evaluator for document relevance    \"\"\"    # RAG inputs    input_question = example.inputs[\"input_question\"]    contexts = run.outputs[\"contexts\"]    # LLM grader    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)    # Structured prompt    answer_grader = grade_prompt_doc_relevance | llm    # Get score    score = answer_grader.invoke({\"question\":input_question,                                  \"documents\":contexts})    score = score[\"Score\"]    return {\"key\": \"document_relevance\", \"score\": score} experiment_results = evaluate(    predict_rag_answer_with_context,    data=dataset_name,    evaluators=[docs_relevance_evaluator],    experiment_prefix=\"rag-doc-relevance\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of isolating intermediate chain steps in the evaluation process?"}, {"role": "assistant", "content": "Evaluating intermediate steps Above, we returned the retrieved documents as part of the final answer. However, we will show that this is not required. We can isolate them as intermediate chain steps. See detail on isolating intermediate chain steps here. Here is the a video from our LangSmith evaluation series for reference: https://youtu.be/yx3JMAaNggQ?feature=shared from langsmith.schemas import Example, Runfrom langsmith.evaluation import evaluatedef document_relevance_grader(root_run: Run, example: Example) -> dict:    \"\"\"    A simple evaluator that checks to see if retrieved documents are relevant to the question    \"\"\"    # Get specific steps in our RAG pipeline, which are noted with @traceable decorator    rag_pipeline_run = next(        run for run in root_run.child_runs if run.name == \"get_answer\"    )    retrieve_run = next(        run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\"    )    contexts = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])    input_question = example.inputs[\"input_question\"]    # LLM grader    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)    # Structured prompt    answer_grader = grade_prompt_doc_relevance | llm    # Get score    score = answer_grader.invoke({\"question\":input_question,                                  \"documents\":contexts})    score = score[\"Score\"]    return {\"key\": \"document_relevance\", \"score\": score}def answer_hallucination_grader(root_run: Run, example: Example) -> dict:    \"\"\"    A simple evaluator that checks to see the answer is grounded in the documents    \"\"\"    # RAG input    rag_pipeline_run = next(        run for run in root_run.child_runs if run.name == \"get_answer\"    )    retrieve_run = next(        run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\"    )    contexts = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])    # RAG output    prediction = rag_pipeline_run.outputs[\"answer\"]    # LLM grader    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)    # Structured prompt    answer_grader = grade_prompt_hallucinations | llm    # Get score    score = answer_grader.invoke({\"documents\": contexts,                                  \"student_answer\": prediction})    score = score[\"Score\"]    return {\"key\": \"answer_hallucination\", \"score\": score}experiment_results = evaluate(    predict_rag_answer,    data=dataset_name,    evaluators=[document_relevance_grader, answer_hallucination_grader],    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What SDKs can be used to manage prompts programmatically according to the provided information?"}, {"role": "assistant", "content": "Manage prompts programmatically You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.\nAll functionality going forward will live in the langsmith package."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What commands are used to install the LangChain and Langsmith packages in Python and TypeScript?"}, {"role": "assistant", "content": "Install packages PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you do not have the LANGCHAIN_API_KEY set for your workspace?"}, {"role": "assistant", "content": "Configure environment variables If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=\"lsv2_...\" TerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create or update a prompt using the push prompt method in PythonLangChain or TypeScript?"}, {"role": "assistant", "content": "Push a prompt To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");url = prompts.push(\"joke-generator\", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.\nThis is useful for storing the model configuration you want to use with this prompt.\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"langchain-openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);prompts.push(\"joke-generator-with-model\", chain);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method do you use to pull a prompt from the LangChain Hub, and what information is required for pulling a public prompt?"}, {"role": "assistant", "content": "Pull a prompt To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { ChatOpenAI } from \"langchain-openai\";const prompt = prompts.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);chain.invoke({\"topic\": \"cats\"}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\nJust specify include_model when pulling the prompt.\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\nMake sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = prompts.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });chain.invoke({\"topic\": \"cats\"}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use prompts stored in LangSmith with a model provider's API?"}, {"role": "assistant", "content": "Use a prompt without LangChain If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.\nThese convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to list, delete, and like/unlike prompts in the LangSmith SDK?"}, {"role": "assistant", "content": "List, delete, and like prompts You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\nSee the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\"); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is required to self-host LangSmith using Docker?"}, {"role": "assistant", "content": "Self-hosting LangSmith with Docker Enterprise License RequiredSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact us at sales@langchain.dev if you want to get a license key to trial LangSmith in your environment. This guide provides instructions for installing and setting up your environment to run LangSmith locally using Docker. You can do this either by using the LangSmith SDK or by using Docker Compose directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for installing and running LangSmith?"}, {"role": "assistant", "content": "Prerequisites Ensure Docker is installed and running on your system. You can verify this by running:docker infoIf you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.Recommended: At least 4 vCPUs, 16GB Memory available on your machine.You may need to tune resource requests/limits for all of our different services based off of organization size/usageDisk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.LangSmith License KeyYou can get this from your Langchain representative. Contact us at sales@langchain.dev for more information.Api Key SaltThis is a secret key that you can generate. It should be a random string of characters.You can generate this using the following command:openssl rand -base64 32ConfigurationThere are several configuration options that you can set in the .env file. You can find more information on the available configuration options in the Configuration section."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the recommended way to run LangSmith in production?"}, {"role": "assistant", "content": "Running via Docker Compose The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. In production, we highly recommend using Kubernetes."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can you find the docker-compose.yml file and related files for LangSmith?"}, {"role": "assistant", "content": "1. Fetch the LangSmithdocker-compose.ymlfile You can find the docker-compose.yml file and related files in the LangSmith SDK repository here: LangSmith Docker Compose File Copy the docker-compose.yml file and all files in that directory from the LangSmith SDK to your project directory. Ensure that you copy the users.xml file as well."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps should be taken to configure environment variables for the LangSmith SDK?"}, {"role": "assistant", "content": "2. Configure environment variables Copy the .env.example file from the LangSmith SDK to your project directory and rename it to .env. Then, set the following environment variables in the .env file:Configure the appropriate values in the .env file. You can find the available configuration options in the Configuration section. You can also set these environment variables in the docker-compose.yml file directly or export them in your terminal. We recommend setting them in the .env file."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command should you execute in your terminal to start the LangSmith application?"}, {"role": "assistant", "content": "2. Start server Start the LangSmith application by executing the following command in your terminal: docker-compose up You can also run the server in the background by running: docker-compose up -d"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command can be used to validate the deployment of the cli-langchain-frontend-1 container?"}, {"role": "assistant", "content": "Validate your deployment: Curl the exposed port of the cli-langchain-frontend-1 container:curl localhost:1980/info{\"version\":\"0.5.7\",\"license_expiration_time\":\"2033-05-20T20:08:06\",\"batch_ingest_config\":{\"scale_up_qsize_trigger\":1000,\"scale_up_nthreads_limit\":16,\"scale_down_nempty_trigger\":4,\"size_limit\":100,\"size_limit_bytes\":20971520}}Visit the exposed port of the cli-langchain-frontend-1 container on your browserThe Langsmith UI should be visible/operational at http://localhost:1980"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you check if the server is running and view the logs?"}, {"role": "assistant", "content": "Checking the logs If, at any point, you want to check if the server is running and see the logs, run docker-compose logs"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What command is used to stop the server in Docker Compose?"}, {"role": "assistant", "content": "Stopping the server docker-compose down"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of information can be associated with traces using metadata and tags in LangSmith?"}, {"role": "assistant", "content": "Add metadata and tags to traces LangSmith supports sending arbitrary metadata and tags along with traces. Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace. Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID.\nFor more information on tags and metadata, see the Concepts page. For information on how to query traces and runs by metadata and tags, see the Filter traces in the application page. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},    {\"role\": \"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with tags and metadata# Ensure that the LANGCHAIN_TRACING_V2 environment variables are set for @traceable to work@traceable(    run_type=\"llm\",    name=\"OpenAI Call Decorator\",    tags=[\"my-tag\"],    metadata={\"my-key\": \"my-value\"})def call_openai(    messages: list[dict], model: str = \"gpt-3.5-turbo\") -> str:    return client.chat.completions.create(        model=model,        messages=messages,    ).choices[0].message.contentcall_openai(    messages,    # You can also provide tags and metadata at invocation time    # via the langsmith_extra parameter    langsmith_extra={\"tags\": [\"my-other-tag\"], \"metadata\": {\"my-other-key\": \"my-value\"}})# Alternatively, you can create a RunTree object with tags and metadatart = RunTree(    run_type=\"llm\",    name=\"OpenAI Call RunTree\",    inputs={\"messages\": messages},    tags=[\"my-tag\"],    extra={\"metadata\": {\"my-key\": \"my-value\"}})chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from \"langsmith\";const client = new OpenAI();const messages = [    {role: \"system\", content: \"You are a helpful assistant.\"},    {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[]) => {    const completion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return completion.choices[0].message.content;},{    run_type: \"llm\",    name: \"OpenAI Call Traceable\",    tags: [\"my-tag\"],    metadata: { \"my-key\": \"my-value\" }});// Call the traceable functionawait traceableCallOpenAI(messages, \"gpt-3.5-turbo\");// Create a RunTree objectconst rt = new RunTree({  run_type: \"llm\",  name: \"OpenAI Call RunTree\",  inputs: { messages },  tags: [\"my-tag\"],  extra: { metadata: { \"my-key\": \"my-value\" } },});const chatCompletion = await client.chat.completions.create({  model: \"gpt-3.5-turbo\",  messages: messages,});// End and submit the runrt.end(chatCompletion);await rt.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to set up new feedback criteria in the application?"}, {"role": "assistant", "content": "Set up feedback criteria Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on tracing and feedbackReference guide on feedback data format Feedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback. To set up a new feedback criteria, follow this link to view all existing tags for your workspace, then click New Tag."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What values can be accepted as feedback scores in the continuous feedback system?"}, {"role": "assistant", "content": "Continuous feedback For continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is logged when providing categorical feedback using a feedback tag name and categories?"}, {"role": "assistant", "content": "Categorical feedback For categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score.\nBoth the category label and the score will be logged as feedback in value and score fields, respectively."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you save your model configuration in the LangSmith playground for later use?"}, {"role": "assistant", "content": "Save settings configuration Within the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time. To save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you are using LangChain in relation to logging traces to LangSmith?"}, {"role": "assistant", "content": "Annotate code for tracing There are several ways to log traces to LangSmith. tipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces using the @traceable decorator in Python and the traceable function in TypeScript?"}, {"role": "assistant", "content": "Use@traceable/traceable LangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript. noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\nfrom langsmith import traceablefrom openai import Clientopenai = Client()@traceabledef format_prompt(subject):    return [        {            \"role\": \"system\",            \"content\": \"You are a helpful assistant.\",        },        {            \"role\": \"user\",            \"content\": f\"What's a good name for a store that sells {subject}?\"        }    ]@traceable(run_type=\"llm\")def invoke_llm(messages):    return openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )@traceabledef parse_output(response):    return response.choices[0].message.content@traceabledef run_pipeline():    messages = format_prompt(\"colorful socks\")    response = invoke_llm(messages)    return parse_output(response)run_pipeline()The traceable function is a simple way to log traces from the LangSmith TypeScript SDK. Simply wrap any function with traceable.\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to ensure the trace is logged correctly.\nimport { traceable } from \"langsmith/traceable\";import OpenAI from \"openai\";const openai = new OpenAI();const formatPrompt = traceable(  (subject: string) => {    return [      {        role: \"system\" as const,        content: \"You are a helpful assistant.\",      },      {        role: \"user\" as const,        content: `What's a good name for a store that sells ${subject}?`,    },];},{ name: \"formatPrompt\" });const invokeLLM = traceable(    async ({ messages }: { messages: { role: string; content: string }[] }) => {        return openai.chat.completions.create({            model: \"gpt-3.5-turbo\",            messages: messages,            temperature: 0,        });    },    { run_type: \"llm\", name: \"invokeLLM\" });const parseOutput = traceable(    (response: any) => {        return response.choices[0].message.content;    },    { name: \"parseOutput\" });const runPipeline = traceable(    async () => {        const messages = await formatPrompt(\"colorful socks\");        const response = await invokeLLM({ messages });        return parseOutput(response);    },    { name: \"runPipeline\" });await runPipeline();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What environment variables must be set to log traces when using the wrap_openai or wrapOpenAI methods?"}, {"role": "assistant", "content": "Wrap the OpenAI client The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required!\nThe wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. Tool calls are automatically rendered noteThe LANGCHAIN_TRACING_V2 environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_openai or wrapOpenAI. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGCHAIN_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\nTo log traces to a different project, see this section. PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"@traceable(name=\"Chat Pipeline\")def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentchat_pipeline(\"Can you summarize this morning's meetings?\")import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const client = wrapOpenAI(new OpenAI());const myTool = traceable(async (question: string) => {    return \"During this morning's meeting, we solved all world conflict.\";}, { name: \"Retrieve Context\", run_type: \"tool\" });const chatPipeline = traceable(async (question: string) => {    const context = await myTool(question);    const messages = [        {            role: \"system\",            content:                \"You are a helpful assistant. Please respond to the user's request only based on the given context.\",        },        { role: \"user\", content: `Question: ${question} Context: ${context}` },    ];    const chatCompletion = await client.chat.completions.create({        model: \"gpt-3.5-turbo\",        messages: messages,    });    return chatCompletion.choices[0].message.content;}, { name: \"Chat Pipeline\" });await chatPipeline(\"Can you summarize this morning's meetings?\");"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the RunTree API in LangSmith?"}, {"role": "assistant", "content": "Use theRunTreeAPI Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually\ncreate runs and children runs to assemble your trace. You still need to set your LANGCHAIN_API_KEY, but LANGCHAIN_TRACING_V2 is not\nnecessary for this method. PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \"Can you summarize this morning's meetings?\"# Create a top-level runpipeline = RunTree(    name=\"Chat Pipeline\",    run_type=\"chain\",    inputs={\"question\": question})# This can be retrieved in a retrieval stepcontext = \"During this morning's meeting, we solved all world conflict.\"messages = [    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}]# Create a child runchild_llm_run = pipeline.create_child(    name=\"OpenAI Call\",    run_type=\"llm\",    inputs={\"messages\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(    model=\"gpt-3.5-turbo\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.post()pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})pipeline.post()import OpenAI from \"openai\";import { RunTree } from \"langsmith\";// This can be a user input to your appconst question = \"Can you summarize this morning's meetings?\";const pipeline = new RunTree({    name: \"Chat Pipeline\",    run_type: \"chain\",    inputs: { question }});// This can be retrieved in a retrieval stepconst context = \"During this morning's meeting, we solved all world conflict.\";const messages = [    { role: \"system\", content: \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },    { role: \"user\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({    name: \"OpenAI Call\",    run_type: \"llm\",    inputs: { messages },});// Generate a completionconst client = new OpenAI();const chatCompletion = await client.chat.completions.create({    model: \"gpt-3.5-turbo\",    messages: messages,});// End the runs and log themchildRun.end(chatCompletion);await childRun.postRun();pipeline.end({ outputs: { answer: chatCompletion.choices[0].message.content } });await pipeline.postRun();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using the trace context manager in Python?"}, {"role": "assistant", "content": "Use thetracecontext manager (Python only) In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where: You want to log traces for a specific block of code.You want control over the inputs, outputs, and other attributes of the trace.It is not feasible to use a decorator or wrapper.Any or all of the above. The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application. import openaifrom langsmith import tracefrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())@traceable(run_type=\"tool\", name=\"Retrieve Context\")def my_tool(question: str) -> str:    return \"During this morning's meeting, we solved all world conflict.\"def chat_pipeline(question: str):    context = my_tool(question)    messages = [        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond to the user's request only based on the given context.\" },        { \"role\": \"user\", \"content\": f\"Question: {question}\\nContext: {context}\"}    ]    chat_completion = client.chat.completions.create(        model=\"gpt-3.5-turbo\", messages=messages    )    return chat_completion.choices[0].message.contentapp_inputs = {\"input\": \"Can you summarize this morning's meetings?\"}with trace(\"Chat Pipeline\", \"chain\", project_name=\"my_test\", inputs=app_inputs) as rt:    output = chat_pipeline(\"Can you summarize this morning's meetings?\")    rt.end(outputs={\"output\": output})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you annotate traces in LangSmith, and what options are available for adding feedback?"}, {"role": "assistant", "content": "Annotate traces and runs inline LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue.\nYou can annotate a trace either inline or by sending the trace to an annotation queue, which allows you closely inspect and log feedbacks to runs one at a time.\nFeedback tags are associated with your workspace. noteYou can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.\nThis is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline. To annotate a trace inline, click on the Annotate in the upper right corner of trace view for any particular run that is part of the trace. This will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow this guide to set up feedback tags for your workspace.\nYou can also set up new feedback criteria from within the pane itself. You can use the labeled keyboard shortcuts to streamline the annotation process."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens in LangSmith when you add, update, or delete examples in a dataset?"}, {"role": "assistant", "content": "Version datasets In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to a dataset when examples are added, updated, or deleted?"}, {"role": "assistant", "content": "Create a new version of a dataset Any time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and to understand how your dataset has evolved. By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the \"Examples\" tab, you can see the state of the dataset at that point in time. Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the \"latest\" version of the dataset. Also, by default the latest version of the dataset is shown in the \"Examples\" tab and experiments from all versions are shown in the \"Tests\" tab. In the \"Tests\" tab, you can see the results of tests run on the dataset at different versions."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you tag a version of your dataset to mark important milestones?"}, {"role": "assistant", "content": "Tag a version You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history. For example, you might tag a version of your dataset as \"prod\" and use it to run tests against your LLM pipeline. Tagging can be done in the UI by clicking on \"+ Tag this version\" in the \"Examples\" tab. You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the python SDK: from langsmith import Clientfromt datetime import datetimeclient = Client()initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag# You can tag a specific dataset version with a semantic name, like \"prod\"client.update_dataset_tag(    dataset_name=toxic_dataset_name, as_of=initial_time, tag=\"prod\") To run an evaluation on a particular tagged version of a dataset, you can follow this guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I edit a prompt in the application?"}, {"role": "assistant", "content": "Update a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt metadata?"}, {"role": "assistant", "content": "Update metadata To update the prompt metadata (description, use cases, etc.) click the \"Edit\" pencil icon. Your prompt metadata will be updated upon save."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt content in the prompt playground?"}, {"role": "assistant", "content": "Update the prompt content To update the prompt content itself, you need to enter the prompt playground. Click \"Edit in playground\".\nNow you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click \"Commit\" to save it. \n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view all historical versions of a prompt after adding a commit?"}, {"role": "assistant", "content": "Version a prompt When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the \"Commits\" tab in the prompt view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you add additional evaluation metrics to an already run experiment using the Python SDK?"}, {"role": "assistant", "content": "Evaluate an existing experiment noteCurrently, evaluate_existing is only supported in the Python SDK. If you have already run an experiment and want to add additional evaluation metrics, you\ncan apply any evaluators to the experiment using the evaluate_existing method. from langsmith.evaluation import evaluate_existingdef always_half(run, example):    return {\"score\": 0.5}experiment_name = \"my-experiment:abcd123\" # Replace with an actual experiment name or IDevaluate_existing(experiment_name, evaluators=[always_half])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What metrics can be added to an existing experiment using the evaluate_existing method in LangSmith?"}, {"role": "assistant", "content": "Example Suppose you are evaluating a semantic router. You may first run an experiment: from langsmith.evaluation import evaluatedef semantic_router(inputs: dict):    return {\"class\": 1}def accuracy(run, example):    prediction = run.outputs[\"class\"]    expected = example.outputs[\"label\"]    return {\"score\": prediction == expected}results = evaluate(semantic_router, data=\"Router Classification Dataset\", evaluators=[accuracy])experiment_name = results.experiment_name Later, you realize you want to add precision and recall summary metrics. The evaluate_existing method accepts the same arguments as the evaluate method, replacing the target system with the experiment you wish to add metrics to, meaning\nyou can add both instance-level evaluator's and aggregate summary_evaluator's. from langsmith.evaluation import evaluate_existingdef precision(runs: list, examples: list):    true_positives = sum([1 for run, example in zip(runs, examples) if run.outputs[\"class\"] == example.outputs[\"label\"]])    false_positives = sum([1 for run, example in zip(runs, examples) if run.outputs[\"class\"] != example.outputs[\"label\"]])    return {\"score\": true_positives / (true_positives + false_positives)}def recall(runs: list, examples: list):    true_positives = sum([1 for run, example in zip(runs, examples) if run.outputs[\"class\"] == example.outputs[\"label\"]])    false_negatives = sum([1 for run, example in zip(runs, examples) if run.outputs[\"class\"] != example.outputs[\"label\"]])    return {\"score\": true_positives / (true_positives + false_negatives)}evaluate_existing(experiment_name, summary_evaluators=[precision, recall]) The precision and recall metrics will now be available in the LangSmith UI for the experiment_name experiment. As is the case with the evaluate function, there is an identical, asynchronous aevaluate_existing function that can be used to evaluate experiments asynchronously."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What SDKs can be used to manage prompts programmatically according to the provided information?"}, {"role": "assistant", "content": "Manage prompts programmatically You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically. notePreviously this functionality lived in the langchainhub package which is now deprecated.\nAll functionality going forward will live in the langsmith package."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What commands are used to install the LangChain and Langsmith packages in Python and TypeScript?"}, {"role": "assistant", "content": "Install packages PythonLangChain (Python)TypeScriptpip install -U langsmithpip install -U langchain langsmithyarn add langsmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do if you do not have the LANGCHAIN_API_KEY set for your current workspace?"}, {"role": "assistant", "content": "Configure environment variables If you already have LANGCHAIN_API_KEY set to your current workspace's api key from LangSmith, you can skip this step. Otherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith. Set your environment variable. export LANGCHAIN_API_KEY=\"lsv2_...\" TerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create or update a prompt using the push prompt method in PythonLangChain or TypeScript?"}, {"role": "assistant", "content": "Push a prompt To create a new prompt or update an existing prompt, you can use the push prompt method. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");url = prompts.push(\"joke-generator\", chain);// url is a link to the prompt in the UIconsole.log(url); You can also push a prompt as a RunnableSequence of a prompt and a model.\nThis is useful for storing the model configuration you want to use with this prompt.\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers) PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as prompts from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"langchain-openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);prompts.push(\"joke-generator-with-model\", chain);"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method do you use to pull a prompt from the LangChain Hub, and what information is required for pulling a public prompt?"}, {"role": "assistant", "content": "Pull a prompt To pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate. To pull a private prompt you do not need to specify the owner handle (though you can, if you have one set). To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptfrom langsmith import clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { ChatOpenAI } from \"langchain-openai\";const prompt = prompts.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);chain.invoke({\"topic\": \"cats\"}); Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\nJust specify include_model when pulling the prompt.\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\nMake sure you have the proper environment variables set for the model you are using. PythonLangChain (Python)TypeScriptfrom langsmith import clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as prompts from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = prompts.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });chain.invoke({\"topic\": \"cats\"}); When pulling a prompt, you can also specify a specific commit hash to pull a specific version of the prompt. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\") To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author. PythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you use prompts stored in LangSmith directly with a model provider's API?"}, {"role": "assistant", "content": "Use a prompt without LangChain If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods.\nThese convert your prompt into the payload required for the OpenAI or Anthropic API. PythonTypeScriptfrom langsmith import Client, convert_prompt_to_openaifrom openai import OpenAI# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)// Coming soon..."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to list, delete, and like/unlike prompts in the LangSmith SDK?"}, {"role": "assistant", "content": "List, delete, and like prompts You can also list, delete, and like/unline prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\nSee the LangSmith SDK client for extensive documentation on these methods. PythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {  console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\"); Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method for selecting the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of all experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What programming languages can be used to manage datasets programmatically?"}, {"role": "assistant", "content": "Manage datasets programmatically You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods can be used to create a dataset from a list of values using the client?"}, {"role": "assistant", "content": "Create a dataset from list of values The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary. Bulk example creationIf you have many examples to create, consider using the create_examples/createExamples method to create multiple examples in a single request.\nIf creating a single example, you can use the create_example/createExample method. PythonTypeScriptfrom langsmith import Clientexample_inputs = [  (\"What is the largest mammal?\", \"The blue whale\"),  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),  (\"What are reptiles known for?\", \"Having scales\"),  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),]client = Client()dataset_name = \"Elementary Animal Questions\"# Storing inputs in a dataset lets us# run chains and LLMs over a shared set of examples.dataset = client.create_dataset(    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",)# Prepare inputs, outputs, and metadata for bulk creationinputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]outputs = [{\"answer\": output_answer} for _, output_answer in example_inputs]metadata = [{\"source\": \"Wikipedia\"} for _ in example_inputs]client.create_examples(    inputs=inputs,    outputs=outputs,    metadata=metadata,    dataset_id=dataset.id,)import { Client } from \"langsmith\";const client = new Client();const exampleInputs: [string, string][] = [  [\"What is the largest mammal?\", \"The blue whale\"],  [\"What do mammals and birds have in common?\", \"They are both warm-blooded\"],  [\"What are reptiles known for?\", \"Having scales\"],  [    \"What's the main characteristic of amphibians?\",    \"They live both in water and on land\",  ],];const datasetName = \"Elementary Animal Questions\";// Storing inputs in a dataset lets us// run chains and LLMs over a shared set of examples.const dataset = await client.createDataset(datasetName, {  description: \"Questions and answers about animal phylogenetics\",});// Prepare inputs, outputs, and metadata for bulk creationconst inputs = exampleInputs.map(([inputPrompt]) => ({ question: inputPrompt }));const outputs = exampleInputs.map(([, outputAnswer]) => ({ answer: outputAnswer }));const metadata = exampleInputs.map(() => ({ source: \"Wikipedia\" }));// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  metadata,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from traces using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from traces To create datasets from the runs (spans) of your traces, you can use the same approach.\nFor many more examples of how to fetch and filter runs, see the export traces guide.\nBelow is an example: PythonTypeScriptfrom langsmith import Clientclient = Client()dataset_name = \"Example Dataset\"# Filter runs to add to the datasetruns = client.list_runs(    project_name=\"my_project\",    is_root=True,    error=False,)dataset = client.create_dataset(dataset_name, description=\"An example dataset\")# Prepare inputs and outputs for bulk creationinputs = [run.inputs for run in runs]outputs = [run.outputs for run in runs]# Use the bulk create_examples methodclient.create_examples(    inputs=inputs,    outputs=outputs,    dataset_id=dataset.id,)import { Client, Run } from \"langsmith\";const client = new Client();const datasetName = \"Example Dataset\";// Filter runs to add to the datasetconst runs: Run[] = [];for await (const run of client.listRuns({  projectName: \"my_project\",  isRoot: 1,  error: false,})) {  runs.push(run);}const dataset = await client.createDataset(datasetName, {  description: \"An example dataset\",  dataType: \"kv\",});// Prepare inputs and outputs for bulk creationconst inputs = runs.map(run => run.inputs);const outputs = runs.map(run => run.outputs ?? {});// Use the bulk createExamples methodawait client.createExamples({  inputs,  outputs,  datasetId: dataset.id,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you create a dataset from a CSV file using the provided code examples?"}, {"role": "assistant", "content": "Create a dataset from a CSV file In this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided. PythonTypeScriptfrom langsmith import Clientimport osclient = Client()csv_file = 'path/to/your/csvfile.csv'input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_csv(    csv_file=csv_file,    input_keys=input_keys,    output_keys=output_keys,    name=\"My CSV Dataset\",    description=\"Dataset created from a CSV file\"    data_type=\"kv\")import { Client } from \"langsmith\";const client = new Client();const csvFile = 'path/to/your/csvfile.csv';const inputKeys = ['column1', 'column2']; // replace with your input column namesconst outputKeys = ['output1', 'output2']; // replace with your output column namesconst dataset = await client.uploadCsv({    csvFile: csvFile,    fileName: \"My CSV Dataset\",    inputKeys: inputKeys,    outputKeys: outputKeys,    description: \"Dataset created from a CSV file\",    dataType: \"kv\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you upload a dataset from a pandas DataFrame using the Python client in Langsmith?"}, {"role": "assistant", "content": "Create a dataset from pandas DataFrame (Python only) The python client offers an additional convenience method to upload a dataset from a pandas dataframe. from langsmith import Clientimport osimport pandas as pdclient = Client()df = pd.read_parquet('path/to/your/myfile.parquet')input_keys = ['column1', 'column2'] # replace with your input column namesoutput_keys = ['output1', 'output2'] # replace with your output column namesdataset = client.upload_dataframe(    df=df,    input_keys=input_keys,    output_keys=output_keys,    name=\"My Parquet Dataset\",    description=\"Dataset created from a parquet file\",    data_type=\"kv\" # The default)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically fetch datasets from LangSmith using the SDKs?"}, {"role": "assistant", "content": "Fetch datasets You can programmatically fetch datasets from LangSmith using the list_datasets/listDatasets method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What methods are used to query all datasets in Python and TypeScript?"}, {"role": "assistant", "content": "Query all datasets PythonTypeScriptdatasets = client.list_datasets()const datasets = await client.listDatasets();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you search for datasets by their exact name or by a case-invariant substring in Python and TypeScript?"}, {"role": "assistant", "content": "List datasets by name If you want to search by the exact name, you can do the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name=\"My Test Dataset 1\")const datasets = await client.listDatasets({datasetName: \"My Test Dataset 1\"}); If you want to do a case-invariant substring search, try the following: PythonTypeScriptdatasets = client.list_datasets(dataset_name_contains=\"some substring\")const datasets = await client.listDatasets({datasetNameContains: \"some substring\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter datasets by type in the provided example?"}, {"role": "assistant", "content": "List datasets by type You can filter datasets by type. Below is an example querying for chat datasets. PythonTypeScriptdatasets = client.list_datasets(data_type=\"chat\")const datasets = await client.listDatasets({dataType: \"chat\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically fetch examples from LangSmith using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Fetch examples You can programmatically fetch examples from LangSmith using the list_examples/listExamples method in the Python and TypeScript SDKs. Below are some common calls. PrerequisitesInitialize the client before running the below code snippets. PythonTypeScriptfrom langsmith import Clientclient = Client()import { Client } from \"langsmith\";const client = new Client();"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the methods to filter examples from a dataset using dataset ID or dataset name?"}, {"role": "assistant", "content": "List all examples for a dataset You can filter by dataset ID: PythonTypeScriptexamples = client.list_examples(dataset_id=\"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\")const examples = await client.listExamples({datasetId: \"c9ace0d8-a82c-4b6c-13d2-83401d68e9ab\"}); Or you can filter by dataset name (this must exactly match the dataset name you want to query) PythonTypeScriptexamples = client.list_examples(dataset_name=\"My Test Dataset\")const examples = await client.listExamples({datasetName: \"My test Dataset\"});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the method to list multiple examples by their IDs in Python and TypeScript?"}, {"role": "assistant", "content": "List examples by id You can also list multiple examples all by ID. PythonTypeScriptexample_ids = [ '734fc6a0-c187-4266-9721-90b7a025751a', 'd6b4c1b9-6160-4d63-9b61-b034c585074f', '4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [  \"734fc6a0-c187-4266-9721-90b7a025751a\",  \"d6b4c1b9-6160-4d63-9b61-b034c585074f\",  \"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter examples by specific metadata key-value pairs?"}, {"role": "assistant", "content": "List examples by metadata You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}. PythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What operators can be used in the structured filter query language to fetch examples with specific metadata fields?"}, {"role": "assistant", "content": "List examples by structured filter Similar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples. noteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields. You can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator. PythonTypeScriptexamples = client.list_examples(                dataset_name=dataset_name,                filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'            )const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you programmatically update examples in LangSmith using the SDKs?"}, {"role": "assistant", "content": "Update examples You can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_example(    example_id=example.id,    inputs={\"input\": \"updated input\"},    outputs={\"output\": \"updated output\"},    metadata={\"foo\": \"bar\"},    split=\"train\")await client.updateExample(example.id, {  inputs: { input: \"updated input\" },  outputs: { output: \"updated output\" },  metadata: { \"foo\": \"bar\" },  split: \"train\",})"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What method can be used to programmatically update multiple examples in a single request using the Python and TypeScript SDKs?"}, {"role": "assistant", "content": "Bulk update examples You can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example. PythonTypeScriptclient.update_examples(    example_ids=[example.id, example_2.id],    inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],    outputs=[        {\"output\": \"updated output 1\"},        {\"output\": \"updated output 2\"},    ],    metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],    splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([  {    id: example.id,    inputs: { input: \"updated input 1\" },    outputs: { output: \"updated output 1\" },    metadata: { foo: \"baz\" },    split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings  },  {    id: example2.id,    inputs: { input: \"updated input 2\" },    outputs: { output: \"updated output 2\" },    metadata: { foo: \"qux\" },    split: \"training\"  },])"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you bind an evaluator to a dataset in the UI to automatically grade experiment results?"}, {"role": "assistant", "content": "Bind an evaluator to a dataset in the UI While you can specify evaluators to grade the results of your experiments programmatically (see this guide for more information), you can also bind evaluators to a dataset in the UI.\nThis allows you to configure automatic evaluators that grade your experiment results without having to write any code. Currently, only LLM-based evaluators are supported. The process for configuring this is very similar to the process for configuring an online evaluator for traces. Only affects subsequent experiment runsWhen you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured. Navigate to the dataset details page by clicking Datasets and Testing in the sidebar and selecting the dataset you want to configure the evaluator for.Click on the Add Evaluator button to add an evaluator to the dataset. This will open a modal you can use to configure the evaluator. Give your evaluator a name and set an inline prompt or load a prompt from the prompt hub that will be used to evaluate the results of the runs in the experiment. Importantly, evaluator prompts can only contain the following input variables: input (required): the input to the target you are evaluatingoutput (required): the output of the target you are evaluatingreference: the reference output, taken from the dataset noteAutomatic evaluators you configure in the application will only work if the inputs to your evaluation target, outputs from your evaluation target, and examples in your dataset are all single-key dictionaries.\nLangSmith will automatically extract the values from the dictionaries and pass them to the evaluator.LangSmith currently doesn't support setting up evaluators in the application that act on multiple keys in the inputs or outputs or examples dictionaries. You can specify the scoring criteria in the \"schema\" field. In this example, we are asking the LLM to grade on \"correctness\" of the output with respect to the reference, with a boolean output of 0 or 1. The name of the field in the schema will be interpreted as the feedback key and the type will be the type of the score. Save the evaluator and navigate back to the dataset details page. Each subsequent experiment run from the dataset will now be evaluated by the evaluator you configured. Note that in the below image, each run in the experiment has a \"correctness\" score."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using LangChain's off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.\nThese evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello Harrison\"),]dataset_name = \"Hello Set\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three QA evaluators mentioned, and which one is recommended based on meta-evals?"}, {"role": "assistant", "content": "Use question and answer (correctness) evaluators Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we recommend using \"cot_qa\", or Chain of Thought QA. Here is a trivial example that uses a \"cot_qa\" evaluator to evaluate a simple pipeline that prefixes the input with \"Hello\": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using \"criteria\" evaluators when ground truth reference labels are not available?"}, {"role": "assistant", "content": "Use criteria evaluators If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The \"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",    config={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent \"direction\" (i.e., higher is not necessarily better).\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for \"helpfulness\" means that the prediction was deemed to be helpful by the model.\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content, which, of course, is \"bad\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functions of the \"labeled_criteria\" and \"labeled_score_string\" evaluators in evaluating predictions against reference labels?"}, {"role": "assistant", "content": "Use labeled criteria evaluators If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators. The \"labeled_criteria\" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={        \"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of distance metrics that can be used to measure the similarity between a predicted string and a reference?"}, {"role": "assistant", "content": "Use string or embedding distance metrics To measure the similarity between a predicted string and a reference, you can use string distance metrics: The \"string_distance\" evaluator computes a normalized string edit distance between the prediction and referenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    \"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\": True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # \"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",      })evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries are required to customize the model used for LLM-based evaluators?"}, {"role": "assistant", "content": "Use a custom LLM in off-the-shelf evaluators You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a model that outputs multiple fields using LangChain's evaluators?"}, {"role": "assistant", "content": "Handle multiple input or output fields LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\". We want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    },    prepare_data=lambda run, example: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],        \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" + input[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "Where can the monitoring charts for each tracing project be accessed in LangSmith?"}, {"role": "assistant", "content": "Use monitoring charts LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the default time period set for viewing monitors?"}, {"role": "assistant", "content": "Change the time period You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view specific subsets of runs in the monitor tab?"}, {"role": "assistant", "content": "Slice data by metadata or tag By default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\nThis can be useful to compare how two different prompts or models are performing. In order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you investigate the runs causing spikes in errors or latency observed in monitoring charts?"}, {"role": "assistant", "content": "Drill down into specific subsets Monitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard. From there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using LangChain's off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.\nThese evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello Harrison\"),]dataset_name = \"Hello Set\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three QA evaluators mentioned, and which one is recommended based on meta-evals?"}, {"role": "assistant", "content": "Use question and answer (correctness) evaluators Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we recommend using \"cot_qa\", or Chain of Thought QA. Here is a trivial example that uses a \"cot_qa\" evaluator to evaluate a simple pipeline that prefixes the input with \"Hello\": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using \"criteria\" evaluators when ground truth reference labels are not available?"}, {"role": "assistant", "content": "Use criteria evaluators If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The \"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",    config={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent \"direction\" (i.e., higher is not necessarily better).\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for \"helpfulness\" means that the prediction was deemed to be helpful by the model.\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content, which, of course, is \"bad\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functions of the \"labeled_criteria\" and \"labeled_score_string\" evaluators in evaluating predictions against reference labels?"}, {"role": "assistant", "content": "Use labeled criteria evaluators If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators. The \"labeled_criteria\" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={        \"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of distance metrics used to measure similarity between a predicted string and a reference?"}, {"role": "assistant", "content": "Use string or embedding distance metrics To measure the similarity between a predicted string and a reference, you can use string distance metrics: The \"string_distance\" evaluator computes a normalized string edit distance between the prediction and referenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    \"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\": True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # \"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",      })evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries are required to customize the model used for LLM-based evaluators?"}, {"role": "assistant", "content": "Use a custom LLM in off-the-shelf evaluators You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a model that outputs multiple fields using LangChain's evaluators?"}, {"role": "assistant", "content": "Handle multiple input or output fields LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\". We want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    },    prepare_data=lambda run, example: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],        \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" + input[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What performance metrics can be extracted from an experiment using the read_project/readProject methods?"}, {"role": "assistant", "content": "Fetch performance metrics for an experiment Experiments, Projects, and SessionsTracing projects and experiments use the same underlying data structure in our backend, which is called a \"session.\"You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.We are working on unifying the terminology across our documentation and APIs. When you run an experiment using evaluate with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the read_project/readProject methods. The payload for experiment details includes the following values: {  \"start_time\": \"2024-06-06T01:02:51.299960\",  \"end_time\": \"2024-06-06T01:03:04.557530+00:00\",  \"extra\": {    \"metadata\": {      \"git\": {        \"tags\": null,        \"dirty\": true,        \"branch\": \"ankush/agent-eval\",        \"commit\": \"...\",        \"repo_name\": \"...\",        \"remote_url\": \"...\",        \"author_name\": \"Ankush Gola\",        \"commit_time\": \"...\",        \"author_email\": \"...\"      },      \"revision_id\": null,      \"dataset_splits\": [\"base\"],      \"dataset_version\": \"2024-06-05T04:57:01.535578+00:00\",      \"num_repetitions\": 3    }  },  \"name\": \"SQL Database Agent-ae9ad229\",  \"description\": null,  \"default_dataset_id\": null,  \"reference_dataset_id\": \"...\",  \"id\": \"...\",  \"run_count\": 9,  \"latency_p50\": 7.896,  \"latency_p99\": 13.09332,  \"first_token_p50\": null,  \"first_token_p99\": null,  \"total_tokens\": 35573,  \"prompt_tokens\": 32711,  \"completion_tokens\": 2862,  \"total_cost\": 0.206485,  \"prompt_cost\": 0.163555,  \"completion_cost\": 0.04293,  \"tenant_id\": \"...\",  \"last_run_start_time\": \"2024-06-06T01:02:51.366397\",  \"last_run_start_time_live\": null,  \"feedback_stats\": {    \"cot contextual accuracy\": {      \"n\": 9,      \"avg\": 0.6666666666666666,      \"values\": {        \"CORRECT\": 6,        \"INCORRECT\": 3      }    }  },  \"session_feedback_stats\": {},  \"run_facets\": [],  \"error_rate\": 0,  \"streaming_rate\": 0,  \"test_run_number\": 11} From here, you can extract performance metrics such as: latency_p50: The 50th percentile latency in seconds.latency_p99: The 99th percentile latency in seconds.total_tokens: The total number of tokens used.prompt_tokens: The number of prompt tokens used.completion_tokens: The number of completion tokens used.total_cost: The total cost of the experiment.prompt_cost: The cost of the prompt tokens.completion_cost: The cost of the completion tokens.feedback_stats: The feedback statistics for the experiment.error_rate: The error rate for the experiment.first_token_p50: The 50th percentile latency for the time to generate the first token (if using streaming).first_token_p99: The 99th percentile latency for the time to generate the first token (if using streaming). Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs. First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript.\nPlease view the how-to guide on evaluation for more details. from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Harrison\", \"Hello Harrison\"),    (\"Ankush\", \"Hello Ankush\"),]dataset_name = \"HelloDataset\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": text}, {\"expected\": result}) for text, result in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id) Next, we will create an experiment, retrieve the experiment name from the result of evaluate, then fetch the performance metrics for the experiment. PythonTypeScriptfrom langsmith.schemas import Example, Rundataset_name = \"HelloDataset\"def foo_label(root_run: Run, example: Example) -> dict:    return {\"score\": 1, \"key\": \"foo\"}from langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: \"Hello \" + inputs[\"input\"],    data=dataset_name,    evaluators=[foo_label],    experiment_prefix=\"Hello\",)resp = client.read_project(project_name=results.experiment_name, include_stats=True)print(resp.json(indent=2))import { Client } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction fooLabel(rootRun: Run, example: Example): EvaluationResult {  return {score: 1, key: \"foo\"};}const client = new Client();const results = await evaluate((inputs) => {  return { output: \"Hello \" + inputs.input };}, {  data: \"HelloDataset\",  experimentPrefix: \"Hello\",  evaluators: [fooLabel],});const resp = await client.readProject({ projectName: results.experimentName, includeStats: true })console.log(JSON.stringify(resp, null, 2))"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the prerequisites for using LangChain's off-the-shelf evaluators in Python?"}, {"role": "assistant", "content": "Use LangChain off-the-shelf evaluators (Python only) Recommended ReadingBefore diving into this content, it might be helpful to read the following:LangChain evaluator reference LangChain provides a suite of off-the-shelf evaluators you can use right away to evaluate your application performance without writing any custom code.\nThese evaluators are meant to be used more as a starting point for evaluation. PrerequisitesCreate a dataset and set up the LangSmith client in Python to follow along from langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Ankush\", \"Hello Ankush\"),    (\"Harrison\", \"Hello Harrison\"),]dataset_name = \"Hello Set\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"input\": input}, {\"expected\": expected}) for input, expected in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the three QA evaluators mentioned, and which one is recommended based on meta-evals?"}, {"role": "assistant", "content": "Use question and answer (correctness) evaluators Question and answer (QA) evaluators help to measure the correctness of a response to a user query or question. If you have a dataset with reference labels or reference context docs, these are the evaluators for you!\nThree QA evaluators you can load are: \"qa\", \"context_qa\", \"cot_qa\". Based on our meta-evals, we recommend using \"cot_qa\", or Chain of Thought QA. Here is a trivial example that uses a \"cot_qa\" evaluator to evaluate a simple pipeline that prefixes the input with \"Hello\": from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\")client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of using \"criteria\" evaluators when ground truth reference labels are not available?"}, {"role": "assistant", "content": "Use criteria evaluators If you don't have ground truth reference labels, you can evaluate your run against a custom set of criteria using the \"criteria\" evaluators. These are helpful when there are high level semantic aspects of your model's output you'd like to monitor that aren't captured by other explicit checks or rules. The \"criteria\" evaluator instructs an LLM to assess if a prediction satisfies the given criteria, outputting a binary score (0 or 1) for each criterion from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatecriteria_evaluator = LangChainStringEvaluator(    \"criteria\",    config={        \"criteria\": {            \"says_hello\": \"Does the submission say hello?\",        }    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        criteria_evaluator,    ],) Supported CriteriaDefault criteria are implemented for the following aspects: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.\nTo specify custom criteria, write a mapping of a criterion name to its description, such as:criterion = {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={\"criteria\": criterion}) Interpreting the ScoreEvaluation scores don't have an inherent \"direction\" (i.e., higher is not necessarily better).\nThe direction of the score depends on the criteria being evaluated. For example, a score of 1 for \"helpfulness\" means that the prediction was deemed to be helpful by the model.\nHowever, a score of 1 for \"maliciousness\" means that the prediction contains malicious content, which, of course, is \"bad\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the functions of the \"labeled_criteria\" and \"labeled_score_string\" evaluators in evaluating predictions against reference labels?"}, {"role": "assistant", "content": "Use labeled criteria evaluators If you have ground truth reference labels, you can evaluate your run against custom criteria while also providing that reference information to the LLM using the \"labeled_criteria\" or \"labeled_score_string\" evaluators. The \"labeled_criteria\" evaluator instructs an LLM to assess if a prediction satisfies the criteria, taking into account the reference labelThe \"labeled_score_string\" evaluator instructs an LLM to assess the prediction against a reference label on a specified scale from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    })labeled_score_evaluator = LangChainStringEvaluator(    \"labeled_score_string\",    config={        \"criteria\": {            \"accuracy\": \"How accurate is this prediction compared to the reference on a scale of 1-10?\"        },        \"normalize_by\": 10,    })client = Client()evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        labeled_criteria_evaluator,        labeled_score_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two types of distance metrics used to measure similarity between a predicted string and a reference?"}, {"role": "assistant", "content": "Use string or embedding distance metrics To measure the similarity between a predicted string and a reference, you can use string distance metrics: The \"string_distance\" evaluator computes a normalized string edit distance between the prediction and referenceThe \"embedding_distance\" evaluator computes the distance between the text embeddings of the prediction and reference # !pip install rapidfuzzfrom langsmith.evaluation import LangChainStringEvaluator, evaluatestring_distance_evaluator = LangChainStringEvaluator(    \"string_distance\",    config={\"distance\": \"levenshtein\", \"normalize_score\": True})embedding_distance_evaluator = LangChainStringEvaluator(    \"embedding_distance\",    config={      # Defaults to OpenAI, but you can customize which embedding provider to use:      # \"embeddings\": HuggingFaceEmbeddings(model=\"distilbert-base-uncased\"),      # Can also choose \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"        \"distance_metric\": \"cosine\",      })evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[        string_distance_evaluator,        embedding_distance_evaluator,    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What libraries are required to customize the model used for LLM-based evaluators?"}, {"role": "assistant", "content": "Use a custom LLM in off-the-shelf evaluators You can customize the model used for any LLM-based evaluator (criteria or QA). Note that this currently requires using LangChain libraries. from langchain_openai import ChatOpenAIfrom langchain_core.prompts.prompt import PromptTemplatefrom langsmith.evaluation import LangChainStringEvaluatoreval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})evaluate(    lambda input: \"Hello \" + input[\"input\"],    data=dataset_name,    evaluators=[cot_qa_evaluator],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a model that outputs multiple fields using LangChain's evaluators?"}, {"role": "assistant", "content": "Handle multiple input or output fields LangChain off-the-shelf evaluators work seamlessly if your input dictionary, output dictionary, or example dictionary each have single fields. If you have multiple fields, you can use the prepare_data function to extract the relevant fields for evaluation.\nThese map the keys \"prediction\", \"reference\", and \"input\" to the correct fields in the input and output dictionaries. For the below example, we have a model that outputs two fields: \"greeting\" and \"foo\". We want to evaluate the \"greeting\" field against the \"expected\" field in the output dictionary. from langsmith import Clientfrom langsmith.evaluation import LangChainStringEvaluator, evaluatelabeled_criteria_evaluator = LangChainStringEvaluator(    \"labeled_criteria\",    config={        \"criteria\": {            \"helpfulness\": (                \"Is this submission helpful to the user,\"                \" taking into account the correct reference answer?\"            )        }    },    prepare_data=lambda run, example: {        \"prediction\": run.outputs[\"greeting\"],        \"reference\": example.outputs[\"expected\"],        \"input\": example.inputs[\"input\"],    })client = Client()evaluate(    lambda input: {\"greeting\": \"Hello \" + input[\"input\"], \"foo\": \"bar\"},    data=dataset_name,    evaluators=[        labeled_criteria_evaluator    ],)"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What topics are covered in the conceptual guide related to managing users, organizations, and workspaces within LangSmith?"}, {"role": "assistant", "content": "Admin This conceptual guide covers topics related to managing users, organizations, and workspaces within LangSmith."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key differences between a personal organization and a shared organization in LangSmith?"}, {"role": "assistant", "content": "Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.\nThere are a few important differences between your personal organization and shared organizations: FeaturePersonalSharedMaximum workspaces1Variable, depending on plan (see pricing page)CollaborationCannot invite usersCan invite usersBilling: paid plansDeveloper plan onlyAll other plans available"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the main functions and features associated with workspaces in an organization?"}, {"role": "assistant", "content": "Workspaces infoWorkspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. A workspace is a logical grouping of users and resources within an organization. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the setup guide. The following image shows a sample workspace settings page:\n The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: See the table below for details on which features are available in which scope (organization or workspace): Resource/SettingScopeTrace ProjectsWorkspaceAnnotation QueuesWorkspaceDeploymentsWorkspaceDatasets & TestingWorkspacePromptsWorkspaceAPI KeysWorkspaceSettings including Secrets, Feedback config, Models, Rules, and Shared URLsWorkspaceUser management: Invite User to WorkspaceWorkspaceRBAC: Assigning Workspace RolesWorkspaceData Retention, Usage LimitsWorkspace*Plans and Billing, Credits, InvoicesOrganizationUser management: Invite User to OrganizationOrganization**Adding WorkspacesOrganizationAssigning Organization RolesOrganizationRBAC: Creating/Editing/Deleting Custom RolesOrganization *Data retention settings and usage limits will be available soon for the organization level as well\n**Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag.\nSee the self-hosted user management docs for details."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a user in the context of LangSmith?"}, {"role": "assistant", "content": "Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What will happen to API keys prefixed with ls__ after August 15, 2024?"}, {"role": "assistant", "content": "API keys Dropping support August 15, 2024We will be dropping support for API keys on August 15, 2024 in favor of personal access tokens (PATs) and service keys. We recommend using PATs and service keys for all new integrations. API keys prefixed with ls__ will NO LONGER work after August 15, 2024. API keys are used to authenticate requests to the LangSmith API. They are created by users and scoped to a workspace. This means that all requests made with an API key will be associated with the workspace that the key was created in. The API key will have the ability to create, read, update, delete all resources within that workspace. API keys are prefixed with ls__. These keys will also show up in the UI under the service keys tab."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are Personal Access Tokens (PATs) used for in the LangSmith API?"}, {"role": "assistant", "content": "Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. PATs are prefixed with lsv2_pt_"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are service keys used for in relation to the LangSmith API?"}, {"role": "assistant", "content": "Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Service keys are prefixed with lsv2_sk_ noteTo see how to create a service key or Personal Access Token, see the setup guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the differences in permissions between an Organization Admin and an Organization User?"}, {"role": "assistant", "content": "Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here: Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. An Organization Admin has Admin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. infoThe Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins.\nCustom organization-scoped roles are not available yet. See the table below for all organization permissions: Organization UserOrganization AdminView organization configurationView organization rolesView organization membersView data retention settingsView usage limitsAdmin access to all workspacesManage billing settingsCreate workspacesCreate, edit, and delete organization rolesInvite new users to organizationDelete user invitesRemove users from an organizationUpdate data retention settings*Update usage limits*"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the built-in system roles available in the RBAC feature for workspaces?"}, {"role": "assistant", "content": "Workspace roles (RBAC) noteRBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, please contact our sales team at sales@langchain.dev\nOther plans default to using the Admin role for all users. Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited: Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Organization admins can also create/edit custom roles with specific permissions for different resources. Roles can be managed in organization settings under the Roles tab: For more details on assigning and creating roles, see the access control setup guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What function should be used to run pairwise evaluations of existing experiments in LangSmith?"}, {"role": "assistant", "content": "Run pairwise evaluations Recommended ReadingBefore diving into this content, it might be helpful to read the following:How-to guide on running regular evals LangSmith supports evaluating existing experiments in a comparative manner. This allows you to use automatic evaluators (especially, LLM-based evaluators) to score the outputs from multiple experiments against each other, rather than being confined to evaluating outputs one at a time. Think LMSYS Chatbot Arena - this is the same concept! To do this, use the evaluate_comparative / evaluateComparative function\nwith two existing experiments. If you haven't already created experiments to compare, check out our quick start or oue how-to guide to get started with evaluations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the required SDK versions for using the evaluate_comparative function and what arguments does it accept?"}, {"role": "assistant", "content": "Use theevaluate_comparativefunction notePairwise evaluations currently require langsmith SDK Python version >=0.1.55 or JS version >=0.1.24. At its simplest, evaluate_comparative / evaluateComparative function takes the following arguments: ArgumentDescriptionexperimentsA list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names.evaluatorsA list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. Along with these, you can also pass in the following optional args: ArgumentDescriptionrandomize_order / randomizeOrderAn optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False.experiment_prefix / experimentPrefixA prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.descriptionA description of the pairwise experiment. Defaults to None.max_concurrency / maxConcurrencyThe maximum number of concurrent evaluations to run. Defaults to 5.clientThe LangSmith client to use. Defaults to None.metadataMetadata to attach to your pairwise experiment. Defaults to None.load_nested / loadNestedWhether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the inputs and outputs required for configuring pairwise evaluators, and how should the output dictionary be structured?"}, {"role": "assistant", "content": "Configure inputs and outputs for pairwise evaluators Inputs: A list of Runs and a single Example. This is exactly the same as a normal evaluator, except with a list of Runs instead of a single Run. The list of runs will have a length of two. You can access the inputs and outputs with\nruns[0].inputs, runs[0].outputs, runs[1].inputs, runs[1].outputs, example.inputs, and example.outputs. Output: Your evaluator should return a dictionary with two keys: key, which represents the feedback key that will be loggedscores, which is a mapping from run ID to score for that run. We strongly encourage using 0 and 1 as the score values, where 1 is better. You may also set both to 0 to represent \"both equally bad\" or both to 1 for \"both equally good\". Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with pairwise_ or ranked_."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the process for using LLM-based pairwise evaluators to compare the quality of responses from two AI assistants?"}, {"role": "assistant", "content": "Compare two experiments with LLM-based pairwise evaluators The following example uses a prompt\nwhich asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2. Optional LangChain UsageIn the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain LLM wrapper.\nThe prompt asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example below uses the OpenAI API directly. PythonTypeScriptfrom langsmith.evaluation import evaluate_comparativefrom langchain import hubfrom langchain_openai import ChatOpenAIfrom langsmith.schemas import Run, Exampleprompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")def evaluate_pairwise(runs: list[Run], example: Example):    scores = {}        # Create the model to run your evaluator    model = ChatOpenAI(model_name=\"gpt-4\")        runnable = prompt | model    response = runnable.invoke({        \"question\": example.inputs[\"question\"],        \"answer_a\": runs[0].outputs[\"output\"] if runs[0].outputs is not None else \"N/A\",        \"answer_b\": runs[1].outputs[\"output\"] if runs[1].outputs is not None else \"N/A\",    })    score = response[\"Preference\"]    if score == 1:        scores[runs[0].id] = 1        scores[runs[1].id] = 0    elif score == 2:        scores[runs[0].id] = 0        scores[runs[1].id] = 1    else:        scores[runs[0].id] = 0        scores[runs[1].id] = 0    return {\"key\": \"ranked_preference\", \"scores\": scores}        evaluate_comparative(    # Replace the following array with the names or IDs of your experiments    [\"my-experiment-name-1\", \"my-experiment-name-2\"],    evaluators=[evaluate_pairwise],)Note: LangChain support inside evaluate / evaluateComparative is not supported yet. See this issue for more details.\nimport type { Run, Example } from \"langsmith\";import { evaluateComparative } from \"langsmith/evaluation\";import { wrapOpenAI } from \"langsmith/wrappers\";import OpenAI from \"openai\";const openai = wrapOpenAI(new OpenAI());import { z } from \"zod\";async function evaluatePairwise(runs: Run[], example: Example) {  const scores: Record<string, number> = {};  const [runA, runB] = runs;    if (!runA || !runB) throw new Error(\"Expected at least two runs\");    const payload = {    question: example.inputs?.question,    answer_a: runA?.outputs?.output ?? \"N/A\",    answer_b: runB?.outputs?.output ?? \"N/A\",  };    const output = await openai.chat.completions.create({    model: \"gpt-4-turbo\",    messages: [      {        role: \"system\",        content: [          \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.\",          \"You should choose the assistant that follows the user's instructions and answers the user's question better.\",          \"Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.\",          \"Begin your evaluation by comparing the two responses and provide a short explanation.\",          \"Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\",          \"Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible.\",        ].join(\" \"),      },      {        role: \"user\",        content: [          `[User Question] ${payload.question}`,          `[The Start of Assistant A's Answer] ${payload.answer_a} [The End of Assistant A's Answer]`,          `The Start of Assistant B's Answer] ${payload.answer_b} [The End of Assistant B's Answer]`,        ].join(\"\\n\\n\"),      },    ],    tool_choice: {      type: \"function\",      function: { name: \"Score\" },    },    tools: [      {        type: \"function\",        function: {          name: \"Score\",          description: [            `After providing your explanation, output your final verdict by strictly following this format:`,            `Output \"1\" if Assistant A answer is better based upon the factors above.`,            `Output \"2\" if Assistant B answer is better based upon the factors above.`,            `Output \"0\" if it is a tie.`,          ].join(\" \"),          parameters: {            type: \"object\",            properties: {              Preference: {                type: \"integer\",                description: \"Which assistant answer is preferred?\",              },            },          },        },      },    ],  });    const { Preference } = z    .object({ Preference: z.number() })    .parse(      JSON.parse(output.choices[0].message.tool_calls[0].function.arguments)    );      if (Preference === 1) {    scores[runA.id] = 1;    scores[runB.id] = 0;  } else if (Preference === 2) {    scores[runA.id] = 0;    scores[runB.id] = 1;  } else {    scores[runA.id] = 0;    scores[runB.id] = 0;  }    return { key: \"ranked_preference\", scores };}await evaluateComparative([\"earnest-name-40\", \"reflecting-pump-91\"], {  evaluators: [evaluatePairwise],});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the Comparison View of pairwise experiments?"}, {"role": "assistant", "content": "View pairwise experiments Navigate to the \"Pairwise Experiments\" tab from the dataset page: Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View: You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What resources are recommended for evaluating the performance of an LLM application?"}, {"role": "assistant", "content": "Evaluate an LLM Application Recommended ReadingBefore diving into this content, it might be helpful to read the following:Conceptual guide on evaluationHow-to guide on managing datasetsHow-to guide on managing datasets programmatically Evaluating the performance of your LLM application is a critical step in the development process. LangSmith makes it easy to run evaluations and track evaluation performance over time.\nThis section provides guidance on how to evaluate the performance of your LLM application."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key steps involved in the evaluation process for an LLM application?"}, {"role": "assistant", "content": "Run an evaluation At a high-level, the evaluation process involves the following steps: Define your LLM application or target task.Creating or selecting a dataset to evaluate your LLM application. Your evaluation criteria may or may not require expected outputs in the dataset.Configuring evaluators to score the outputs of your LLM application, sometimes against expected outputs.Running the evaluation and viewing the results. The following example involves evaluating a very simple LLM pipeline as classifier to label input data as \"Toxic\" or \"Not toxic\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the LLM pipeline described in the text?"}, {"role": "assistant", "content": "Step 1: Define your target task In this case, we are defining a simple evaluation target consisting of an LLM pipeline that classifies text as toxic or non-toxic.\nWe've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide. PythonTypeScriptfrom langsmith import traceable, wrappersfrom openai import Clientopenai = wrappers.wrap_openai(Client())@traceabledef label_text(text):    messages = [        {            \"role\": \"system\",            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        {\"role\": \"user\", \"content\": text},    ]    result = openai.chat.completions.create(        messages=messages, model=\"gpt-3.5-turbo\", temperature=0    )    return result.choices[0].message.contentimport { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";import { traceable } from \"langsmith/traceable\";const client = wrapOpenAI(new OpenAI());const labelText = traceable(  async (text: string) => {    const result = await client.chat.completions.create({      messages: [        {           role: \"system\",          content: \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",        },        { role: \"user\", content: text },      ],      model: \"gpt-3.5-turbo\",      temperature: 0,    });        return result.choices[0].message.content;  },  { name: \"labelText\" });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of text examples are included in the dataset created for evaluating the performance of the LLM application?"}, {"role": "assistant", "content": "Step 2: Create or select a dataset In this case, we are creating a dataset to evaluate the performance of our LLM application. The dataset contains examples of toxic and non-toxic text. Each Example in the dataset contains three dictionaries / objects: outputs: The reference labels or other context found in your datasetinputs: The inputs to your pipelinemetadata: Any other metadata you have stored in that example within the dataset These dictionaries / objects can have arbitrary keys and values, but the keys must be consistent across all examples in the dataset.\nThe values in the examples can also take any form, such as strings, numbers, lists, or dictionaries, but for this example, we are simply using strings. PythonTypeScriptfrom langsmith import Clientclient = Client()# Create a datasetexamples = [    (\"Shut up, idiot\", \"Toxic\"),    (\"You're a wonderful person\", \"Not toxic\"),    (\"This is the worst thing ever\", \"Toxic\"),    (\"I had a great day today\", \"Not toxic\"),    (\"Nobody likes you\", \"Toxic\"),    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),]dataset_name = \"Toxic Queries\"dataset = client.create_dataset(dataset_name=dataset_name)inputs, outputs = zip(    *[({\"text\": text}, {\"label\": label}) for text, label in examples])client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)import { Client } from \"langsmith\";const langsmith = new Client();// create a datasetconst toxicExamples = [  [\"Shut up, idiot\", \"Toxic\"],  [\"You're a wonderful person\", \"Not toxic\"],  [\"This is the worst thing ever\", \"Toxic\"],  [\"I had a great day today\", \"Not toxic\"],  [\"Nobody likes you\", \"Toxic\"],  [\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"],];const [inputs, outputs] = toxicExamples.reduce<  [Array<{ input: string }>, Array<{ outputs: string }>]>(  ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],  ],  [[], []]);const datasetName = \"Toxic Queries\";const toxicDataset = await langsmith.createDataset(datasetName);await langsmith.createExamples({ inputs, outputs, datasetId: toxicDataset.id });"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What type of evaluator is being used to score the outputs in the LLM pipeline?"}, {"role": "assistant", "content": "Step 3. Configure evaluators to score the outputs In this case, we are using a dead-simple evaluator that compares the output of our LLM pipeline to the expected output in the dataset.\nWriting evaluators is discussed in more detail in the following section. PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What arguments are required to use the evaluate method in Python and TypeScript for running an evaluation?"}, {"role": "assistant", "content": "Step 4. Run the evaluation and view the results You can use the evaluate method in Python and TypeScript to run an evaluation. At its simplest, the evaluate method takes the following arguments: a function that takes an input dictionary or object and returns an output dictionary or objectdata - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators - a list of evaluators to score the outputs of the functionexperiment_prefix - a string to prefix the experiment name with. A name will be generated if not provided. PythonTypeScriptfrom langsmith.evaluation import evaluatedataset_name = \"Toxic Queries\"results = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    description=\"Testing the baseline system.\",  # optional)import { evaluate } from \"langsmith/evaluation\";const datasetName = \"Toxic Queries\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); Each invocation of evaluate produces an experiment which is bound to the dataset, and can be viewed in the LangSmith UI.\nEvaluation scores are stored against each individual output produced by the target task as feedback, with the name and score configured in the evaluator. If you've annotated your code for tracing, you can open the trace of each row in a side panel view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the key components returned by custom evaluators in LangSmith?"}, {"role": "assistant", "content": "Use custom evaluators At a high-level, evaluators are functions that take in a Run and an Example and return a dictionary or object with a keys score (numeric) and key (string).\nThe key will be associated with the score in the LangSmith UI. advanced use-casesConfigure more feedback fields: you can configure other fields in the dictionary as well. Please see the feedback reference for more information.Evaluate on intermediate steps: to view a more advanced example that traverses the root_run / rootRun object, please refer to this guide on evaluating on intermediate steps.Return multiple scores: you can return multiple scores from a single evaluator. Please check out the example below for more information. To learn more about the Run format, you can read the following reference. However, many of the fields are not relevant nor required for writing evaluators.\nThe root_run / rootRun is always available and contains the inputs and outputs of the target task. If tracing is enabled, the root_run / rootRun will also contain child runs for each step in the pipeline. Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset: PythonTypeScriptfrom langsmith.schemas import Example, Rundef correct_label(root_run: Run, example: Example) -> dict:    score = root_run.outputs.get(\"output\") == example.outputs.get(\"label\")    return {\"score\": int(score), \"key\": \"correct_label\"}import type { EvaluationResult } from \"langsmith/evaluation\";import type { Run, Example } from \"langsmith/schemas\";// Row-level evaluatorfunction correctLabel(rootRun: Run, example: Example): EvaluationResult {  const score = rootRun.outputs?.outputs === example.outputs?.output;  return { key: \"correct_label\", score };} default feedback keyIf the \"key\" field is not provided, the default key name will be the name of the evaluator function."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a particular version of a dataset using examples from that version?"}, {"role": "assistant", "content": "Evaluate on a particular version of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on versioning datasets.\nAdditionally, it might be helpful to read the guide on fetching examples. You can take advantage of the fact that evaluate allows passing in an iterable of examples to evaluate on a particular version of a dataset.\nSimply use list_examples / listExamples to fetch examples from a particular version tag using as_of / asOf. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=toxic_dataset_name, as_of=\"latest\"),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    asOf: \"latest\",  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you fetch a subset of examples from a dataset to evaluate on?"}, {"role": "assistant", "content": "Evaluate on a subset of a dataset Recommended ReadingBefore diving into this content, it might be helpful to read the guide on fetching examples. You can use the list_examples / listExamples method to fetch a subset of examples from a dataset to evaluate on. You can refer to guide above to learn more about the different ways to fetch examples. One common workflow is to fetch examples that have a certain metadata key-value pair. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, metadata={\"desired_key\": \"desired_value\"}),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    metadata: {\"desired_key\": \"desired_value\"},  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate on specific splits of a dataset using the list_examples method?"}, {"role": "assistant", "content": "Evaluate on a dataset split Recommended ReadingBefore reading, it might be useful to check out the guide on creating/managing dataset splits. You can use the list_examples / listExamples method to evaluate on one or multiple splits of your dataset. The splits param takes a list of the splits you would like to evaluate. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=client.list_examples(dataset_name=dataset_name, splits=[\"test\", \"training\"]),    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: langsmith.listExamples({    datasetName: datasetName,    splits: [\"test\", \"training\"],  }),  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the num_repetitions parameter in the evaluate function?"}, {"role": "assistant", "content": "Evaluate on a dataset with repetitions The optional num_repetitions param to the evaluate function allows you to specify how many times\nto run/evaluate each example in your dataset. For instance, if you have 5 examples and set\nnum_repetitions=5, each example will be run 5 times, for a total of 25 runs. This can be useful for reducing\nnoise in systems prone to high variability, such as agents. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    lambda inputs: label_text(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",    num_repetitions=3,)import { evaluate } from \"langsmith/evaluation\";await evaluate((inputs) => labelText(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",  numReptitions=3,});"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of a summary evaluator in the context of evaluating an experiment?"}, {"role": "assistant", "content": "Use a summary evaluator Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment.\nFor example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset.\nThese are called summary_evaluators. Instead of taking in a single Run and Example, these evaluators take a list of each. Below, we'll implement a very simple summary evaluator that computes overall pass rate: PythonTypeScriptfrom langsmith.schemas import Example, Rundef summary_eval(runs: list[Run], examples: list[Example]) -> dict:    correct = 0    for i, run in enumerate(runs):        if run.outputs[\"output\"] == examples[i].outputs[\"label\"]:            correct += 1    if correct / len(runs) > 0.5:        return {\"key\": \"pass\", \"score\": True}    else:        return {\"key\": \"pass\", \"score\": False}import { Run, Example } from \"langsmith/schemas\";function summaryEval(runs: Run[], examples: Example[]) {  let correct = 0;    for (let i = 0; i < runs.length; i++) {    if (runs[i].outputs[\"output\"] === examples[i].outputs[\"label\"]) {      correct += 1;    }  }    return { key: \"pass\", score: correct / runs.length > 0.5 };} You can then pass this evaluator to the evaluate method as follows: PythonTypeScriptresults = evaluate(    lambda inputs: label_query(inputs[\"text\"]),    data=dataset_name,    evaluators=[correct_label],    summary_evaluators=[summary_eval],    experiment_prefix=\"Toxic Queries\",)await evaluate((inputs) => labelQuery(inputs[\"input\"]), {  data: datasetName,  evaluators: [correctLabel],  summaryEvaluators: [summaryEval],  experimentPrefix: \"Toxic Queries\",}); In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you evaluate a LangChain runnable in Python and TypeScript?"}, {"role": "assistant", "content": "Evaluate a LangChain runnable You can configure a LangChain runnable to be evaluated by passing runnable.invoke it to the evaluate method in Python, or just the runnable in TypeScript. First, define your LangChain runnable: PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([  (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),  (\"user\", \"{text}\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()chain = prompt | chat_model | output_parserimport { ChatOpenAI } from \"@langchain/openai\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { StringOutputParser } from \"@langchain/core/output_parsers\";const prompt = ChatPromptTemplate.fromMessages([  [\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"],  [\"user\", \"{text}\"]]);const chatModel = new ChatOpenAI();const outputParser = new StringOutputParser();const chain = prompt.pipe(chatModel).pipe(outputParser); Then, pass the runnable.invoke method to the evaluate method. Note that the input variables of the runnable must match the keys of the example inputs. PythonTypeScriptfrom langsmith.evaluation import evaluateresults = evaluate(    chain.invoke,    data=dataset_name,    evaluators=[correct_label],    experiment_prefix=\"Toxic Queries\",)import { evaluate } from \"langsmith/evaluation\";await evaluate(chain, {  data: datasetName,  evaluators: [correctLabel],  experimentPrefix: \"Toxic Queries\",}); The runnable is traced appropriately for each output."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you return multiple evaluation scores from a single evaluator in a structured format?"}, {"role": "assistant", "content": "Return multiple scores In most cases, each evaluator returns a single key or categorical value. Alternatively, you can return evaluation metrics from a single evaluator. This is useful if your metrics share intermediate values. For example, precision and recall but rely on the same true and false positives and negative values, or you may have an LLM generate multiple metrics in a single shot. To return multiple scores, simply return a dictionary/object of the following form: {    \"results\": [        {\"key\":string, \"score\": number},        {\"key\":string, \"score\": number},        # You may log as many as you wish    ]} Each of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information. Example: PythonTypeScriptfrom langsmith.schemas import Example, Rundef multiple_scores(root_run: Run, example: Example) -> dict:  # Your evaluation logic here  return {      \"results\": [          {\"key\": \"precision\", \"score\": 0.8},          {\"key\": \"recall\", \"score\": 0.9},          {\"key\": \"f1\", \"score\": 0.85},      ]  }    Support for multiple scores is available in langsmith@0.1.32 and higher\nimport type { Run, Example } from \"langsmith/schemas\";function multipleScores(rootRun: Run, example: Example) {  // Your evaluation logic here  return {      results: [          { key: \"precision\", score: 0.8 },          { key: \"recall\", score: 0.9 },          { key: \"f1\", score: 0.85 },      ],  };} Rows from the resulting experiment will display each of the scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of logging to LangSmith?"}, {"role": "assistant", "content": "Tracing This conceptual guide covers topics that are important to understand when logging traces to LangSmith. A Trace is essentially a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a Run. A Project is simply a collection of traces. The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer. Primitive datatypes in LangSmith"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Run in the context of an LLM application?"}, {"role": "assistant", "content": "Runs A Run is a span representing a single unit of work or operation within your LLM application. This could be anything from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span. To learn more about how runs are stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a Trace in the context of LangSmith, and how are runs related to it?"}, {"role": "assistant", "content": "Traces A Trace is a collection of runs that are related to a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a project in the context of traces?"}, {"role": "assistant", "content": "Projects A Project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the different ways to collect feedback on runs?"}, {"role": "assistant", "content": "Feedback Feedback allows you to score an individual run based on certain criteria.\nEach feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID.\nFeedback can currently be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. Collecting feedback on runs can be done in a number of ways: Sent up along with a trace from the LLM applicationGenerated by a user in the app inline or in an annotation queueGenerated by an automatic evaluator during offline evaluationGenerated by an online evaluator To learn more about how feedback is stored in the application, see this reference guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are tags used for in the LangSmith UI?"}, {"role": "assistant", "content": "Tags Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI. Tags can be used to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to attach tags to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of metadata in relation to runs?"}, {"role": "assistant", "content": "Metadata Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run.\nSimilar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis. Learn how to add metadata to your traces\n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How long does LangSmith retain trace data after it is inserted into the trace database?"}, {"role": "assistant", "content": "Data storage and retention For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata be retained for the purpose of showing accurate statistics such as historic usage and cost. noteIf you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you delete a trace from LangSmith before its expiration date?"}, {"role": "assistant", "content": "Deleting traces from LangSmith If you wish to remove a trace from LangSmith sooner than the expiration date, LangSmith supports deleting traces via deleting a project. This can be accomplished: in the LangSmith UI via the \"Delete\" option on the Project's overflow menuvia the Delete Tracer Sessions API endpointvia delete_project() (Python) or deleteProject() (JS/TS) in the LangSmith SDK LangSmith does not support self-service deletion of individual traces at this time. If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, please have your account owner reach out to LangSmith Support with your organization ID and trace IDs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What steps are required to integrate the Instructor library with the LangSmith SDK for tracing in Python?"}, {"role": "assistant", "content": "Trace withInstructor(Python only) We provide a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs. In order to use, you first need to set your LangSmith API key. export LANGCHAIN_API_KEY=<your-api-key> Next, you will need to install the LangSmith SDK: pip install -U langsmith Wrap your OpenAI client with langsmith.wrappers.wrap_openai from openai import OpenAIfrom langsmith import wrappersclient = wrappers.wrap_openai(OpenAI()) After this, you can patch the wrapped OpenAI client using instructor: import instructorclient = instructor.patch(client) Now, you can use instructor as you normally would, but now everything is logged to LangSmith! from pydantic import BaseModelclass UserDetail(BaseModel):    name: str    age: intuser = client.chat.completions.create(    model=\"gpt-3.5-turbo\",    response_model=UserDetail,    messages=[        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},    ]) Oftentimes, you use instructor inside of other functions.\nYou can get nested traces by using this wrapped client and decorating those functions with @traceable.\nPlease see this guide for more information on how to annotate your code for tracing with the @traceable decorator. # You can customize the run name with the `name` keyword argument@traceable(name=\"Extract User Details\")def my_function(text: str) -> UserDetail:    return client.chat.completions.create(        model=\"gpt-3.5-turbo\",        response_model=UserDetail,        messages=[            {\"role\": \"user\", \"content\": f\"Extract {text}\"},        ]    )my_function(\"Jason is 25 years old\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens when you add a webhook URL on an automation action?"}, {"role": "assistant", "content": "Set up webhook notifications for rules When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What information is included in the webhook payload sent to your endpoint?"}, {"role": "assistant", "content": "Webhook payload The payload we send to your webhook endpoint contains \"rule_id\" this is the ID of the automation that sent this payload\"start_time\" and \"end_time\" these are the time boundaries where we found matching runs\"runs\" this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.\"feedback_stats\" this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below. \"feedback_stats\": {    \"about_langchain\": {        \"n\": 1,        \"avg\": 0.0,        \"show_feedback_arrow\": true,        \"values\": {}    },    \"category\": {        \"n\": 0,        \"avg\": null,        \"show_feedback_arrow\": true,        \"values\": {            \"CONCEPTUAL\": 1        }    },    \"user_score\": {        \"n\": 2,        \"avg\": 0.0,        \"show_feedback_arrow\": false,        \"values\": {}    },    \"vagueness\": {        \"n\": 1,        \"avg\": 0.0,        \"show_feedback_arrow\": true,        \"values\": {}    }}, fetching from S3 URLsDepending on how recent your runs are, the inputs_s3_urls and outputs_s3_urls fields may contain S3 URLs to the actual data instead of the data itself.The inputs and outputs can be fetched by the ROOT.presigned_url provided in inputs_s3_urls and outputs_s3_urls respectively. This is an example of the entire payload we send to your webhook endpoint: {  \"rule_id\": \"d75d7417-0c57-4655-88fe-1db3cda3a47a\",  \"start_time\": \"2024-04-05T01:28:54.734491+00:00\",  \"end_time\": \"2024-04-05T01:28:56.492563+00:00\",  \"runs\": [    {      \"status\": \"success\",      \"is_root\": true,      \"trace_id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"dotted_order\": \"20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"run_type\": \"tool\",      \"modified_at\": \"2024-04-05T01:28:54.145062\",      \"tenant_id\": \"2ebda79f-2946-4491-a9ad-d642f49e0815\",      \"end_time\": \"2024-04-05T01:28:54.085649\",      \"name\": \"Search\",      \"start_time\": \"2024-04-05T01:28:54.085646\",      \"id\": \"6ab80f10-d79c-4fa2-b441-922ed6feb630\",      \"session_id\": \"6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5\",      \"parent_run_ids\": [],      \"child_run_ids\": null,      \"direct_child_run_ids\": null,      \"total_tokens\": 0,      \"completion_tokens\": 0,      \"prompt_tokens\": 0,      \"total_cost\": null,      \"completion_cost\": null,      \"prompt_cost\": null,      \"first_token_time\": null,      \"app_path\": \"/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809\",      \"in_dataset\": false,      \"last_queued_at\": null,      \"inputs\": null,      \"inputs_s3_urls\": null,      \"outputs\": null,      \"outputs_s3_urls\": null,      \"extra\": null,      \"events\": null,      \"feedback_stats\": null,      \"serialized\": null,      \"share_token\": null    }  ]}"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is a recommended method to enhance the security of a webhook URL?"}, {"role": "assistant", "content": "Webhook Security We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications. An example would be https://api.example.com/langsmith_webhook?secret=38ee77617c3a489ab6e871fbeb2ec87d"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you configure specific HTTP headers for a webhook?"}, {"role": "assistant", "content": "Webhook custom HTTP headers If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the Headers option next to the URL field and add your headers. noteHeaders are stored in encrypted format."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens if a webhook endpoint returns a 4xx status code?"}, {"role": "assistant", "content": "Webhook Delivery When delivering events to your webhook endpoint we follow these guidelines If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.Anything your endpoint returns in the body will be ignored"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the steps to set up a Modal account and install the Modal SDK?"}, {"role": "assistant", "content": "Setup For an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here. First, create a Modal account. Then, locally install the Modal SDK: pip install modal To finish setting up your account, run the command: modal setup and follow the instructions"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the name of the secret that needs to be created in Modal for LangSmith authentication?"}, {"role": "assistant", "content": "Secrets Next, you will need to set up some secrets in Modal. First, LangSmith will need to authenticate to Modal by passing in a secret.\nThe easiest way to do this is to pass in a secret in the query parameters.\nTo validate this secret, we will need to add a secret in Modal to validate it.\nWe will do that by creating a Modal secret.\nYou can see instructions for secrets here.\nFor this purpose, let's call our secret ls-webhook and have it set an environment variable with the name LS_WEBHOOK. We can also set up a LangSmith secret - luckily there is already an integration template for this!"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the Python file created in the service described?"}, {"role": "assistant", "content": "Service After that, you can create a Python file that will serve as your endpoint.\nAn example is below, with comments explaining what is going on: from fastapi import HTTPException, status, Request, Queryfrom modal import Secret, Stub, web_endpoint, Imagestub = Stub(\"auth-example\", image=Image.debian_slim().pip_install(\"langsmith\"))@stub.function(    secrets=[Secret.from_name(\"ls-webhook\"), Secret.from_name(\"my-langsmith-secret\")])# We want this to be a `POST` endpoint since we will post data here@web_endpoint(method=\"POST\")# We set up a `secret` query parameterdef f(data: dict, secret: str = Query(...)):    # You can import dependencies you don't have locally inside Modal funxtions    from langsmith import Client    # First, we validate the secret key we pass    import os    if secret != os.environ[\"LS_WEBHOOK\"]:        raise HTTPException(            status_code=status.HTTP_401_UNAUTHORIZED,            detail=\"Incorrect bearer token\",            headers={\"WWW-Authenticate\": \"Bearer\"},        )    # This is where we put the logic for what should happen inside this webhook    ls_client = Client()    runs = data[\"runs\"]    ids = [r[\"id\"] for r in runs]    feedback = list(ls_client.list_feedback(run_ids=ids))    for r, f in zip(runs, feedback):        try:            ls_client.create_example(                inputs=r[\"inputs\"],                outputs={\"output\": f.correction},                dataset_name=\"classifier-github-issues\",            )        except Exception:            raise ValueError(f\"{r} and {f}\")    # Function body    return \"success!\" We can now deploy this easily with modal deploy ... (see docs here). You should now get something like:  Created objects.  Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py  Created mount PythonPackage:langsmith  Created f => https://hwchase17--auth-example-f.modal.run App deployed! View Deployment: https://modal.com/apps/hwchase17/auth-example The important thing to remember is https://hwchase17--auth-example-f.modal.run - the function we created to run.\nNOTE: this is NOT the final deployment URL, make sure not to accidentally use that."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How should the secret key be included when adding a function URL as a webhook?"}, {"role": "assistant", "content": "Hooking it up We can now take the function URL we create above and add it as a webhook.\nWe have to remember to also pass in the secret key as a query parameter.\nPutting it all together, it should look something like: https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET} Replace {SECRET} with the secret key you created to access the Modal service."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the requirements for using custom TLS certificates in the LangSmith playground for self-hosted enterprise customers?"}, {"role": "assistant", "content": "Use custom TLS certificates Self-hosted enterprise only (version 0.6 and later)This feature is only available for self-hosted enterprise customers, starting from version 0.6.If you are interested in this plan, please contact sales@langchain.dev for more information. Azure OpenAI onlyThis feature is currently only available for the Azure OpenAI model provider. More model providers will be supported in the future.This will currently only affect model invocations through the LangSmith Playground, not Online Evaluation.\nThe TLS certificates will apply to all Azure Deployment configurations in the playground. You can use custom TLS certificates to connect to model providers in the LangSmith playground. This is useful if you are using a self-signed certificate, a certificate from a custom certificate authority or mutual TLS authentication. To use custom TLS certificates, you need to set the following environment variables. See the self hosted deployment section for more information on how to set up application configuration. LANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS: A comma-separated list of model providers that require custom TLS certificates. Note that azure_openai is currently the only supported model provider that supports custom TLS certificates, but more providers will be supported in the future.LANGSMITH_PLAYGROUND_TLS_CA: The custom certificate authority (CA) certificate in PEM format. This can be a file path (for a mounted volume) or a base64-encoded string.[Optional] LANGSMITH_PLAYGROUND_TLS_KEY: The private key in PEM format. This can be a file path (for a mounted volume) or a base64-encoded string. This is usually only necessary for mutual TLS authentication.[Optional] LANGSMITH_PLAYGROUND_TLS_CERT: The certificate in PEM format. This can be a file path (for a mounted volume) or a base64-encoded string. This is usually only necessary for mutual TLS authentication. Once you have set these environment variables, enter the playground and select the AzureChatOpenAI model provider. Enter the Deployment Name, Azure Endpoint, and API Version, as well as model invocation parameters. Then, the playground will be able to connect to the model provider using the custom TLS certificate."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the importance of accurately filtering runs in the application?"}, {"role": "assistant", "content": "Filter traces in the application Recommended readingBefore diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:Conceptual guide on tracing This page contains a series of guides for how to filter runs in the application. For a guide on how to accomplish something similar programmatically, please see this guide.\nBeing able to accurately filter runs is important for both manual inspection and setting up automations."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What are the two ways to create a filter?"}, {"role": "assistant", "content": "Create a filter There are two ways to create a filter.\nFirst, you can create a filter from the high level nav bar. By default, there is one filter applied: IsRoot is true. This restricts all runs to be top level traces. You can also define a filter from the Filter Shortcuts on the sidebar. This contains commonly used filters."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs (spans) in a pipeline?"}, {"role": "assistant", "content": "Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the default filter of IsRoot is true. After that, you can apply any filter you wish. A common way to do this is to filter by name for sub runs.\nThis relies on good naming, or tagging for all parts of your pipeline. To learn more, you can check out this guide"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for intermediate runs based on the attributes of their root run?"}, {"role": "assistant", "content": "Advanced: filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter for runs that have specific types of sub runs, such as those with the name Foo?"}, {"role": "assistant", "content": "Advanced: filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name Foo. This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters. This will make the rule you specific apply to all child runs of the individual runs you've already filtered for."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of filters can be used to search for specific content in the inputs and outputs of a run?"}, {"role": "assistant", "content": "Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use Full-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common json keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs based on specific key-value pairs in the inputs and outputs?"}, {"role": "assistant", "content": "Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data. To filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown. For example, to match the following input: {  \"input\": \"What is the capital of France?\"} Select Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output: {  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]} Select Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of saved filters in a trace project?"}, {"role": "assistant", "content": "Saved filters You can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do after constructing your filter to save it?"}, {"role": "assistant", "content": "Save a filter In the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What happens to saved filters when you have more than three?"}, {"role": "assistant", "content": "Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update a saved filter after making changes to its parameters?"}, {"role": "assistant", "content": "Update a saved filter With the filter selected, make any changes to filter parameters. Then click Save   Save to update the filter. In the same menu, you can also create a new saved filter by clicking Save   Save as."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How do you delete a saved filter?"}, {"role": "assistant", "content": "Delete a saved filter With the filter selected, click on the trash button to delete the saved filter."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you copy a filter that you have constructed in the UI?"}, {"role": "assistant", "content": "Copy the filter Sometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter runs in the trace view?"}, {"role": "assistant", "content": "Filtering runs within the trace view You can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\"."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you manually specify a raw query in LangSmith query language?"}, {"role": "assistant", "content": "Manually specify a raw query in LangSmith query language If you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI. In order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box. Note that this will add that query to the existing queries, not overwrite it."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What is the purpose of the AI Query functionality mentioned in the text?"}, {"role": "assistant", "content": "Use an AI Query to auto-generate a query Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added a AI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \"All runs longer than 10 seconds\" Experimental featureNote that this is an experimental feature and may not work for all queries."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What types of off-the-shelf evaluators does LangChain provide for evaluation scenarios?"}, {"role": "assistant", "content": "LangChain off-the-shelf evaluators LangChain's evaluation module provides evaluators you can use as-is for common evaluation scenarios.\nTo learn how to use these evaluators, please refer to the following guide. noteWe currently support off-the-shelf evaluators for Python only, but are adding support for TypeScript soon. noteMost of these evaluators are useful but imperfect! We recommend against blind trust of any single automated metric and to always incorporate them as a part of a holistic testing and evaluation strategy.\nMany of the LLM-based evaluators return a binary score for a given datapoint, so measuring differences in prompt or model performance are most reliable in aggregate over a larger dataset. The following table enumerates the off-the-shelf evaluators available in LangSmith, along with their output keys and a simple code sample. Evaluator nameOutput KeySimple Code ExampleQ&AcorrectnessLangChainStringEvaluator(\"qa\")Contextual Q&Acontextual accuracyLangChainStringEvaluator(\"context_qa\")Chain of Thought Q&Acot contextual accuracyLangChainStringEvaluator(\"cot_qa\")CriteriaDepends on criteria keyLangChainStringEvaluator(\"criteria\", config={ \"criteria\": <criterion> })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }Labeled CriteriaDepends on criteria keyLangChainStringEvaluator(\"labeled_criteria\", config={ \"criteria\": <criterion> })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }ScoreDepends on criteria keyLangChainStringEvaluator(\"score_string\", config={ \"criteria\": <criterion>, \"normalize_by\": 10 })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.Labeled ScoreDepends on criteria keyLangChainStringEvaluator(\"labeled_score_string\", config={ \"criteria\": <criterion>, \"normalize_by\": 10 })criterion may be one of the default implemented criteria: conciseness, relevance, correctness, coherence, harmfulness, maliciousness, helpfulness, controversiality, misogyny, and criminality.Or, you may define your own criteria in a custom dict as follows:{ \"criterion_key\": \"criterion description\" }. Scores are out of 10, so normalize_by will cast this to a score from 0 to 1.Embedding distanceembedding_cosine_distanceLangChainStringEvaluator(\"embedding_distance\")String Distancestring_distanceLangChainStringEvaluator(\"string_distance\", config={\"distance\": \"damerau_levenshtein\" }) distance defines the string difference metric to be applied, such as levenshtein or jaro_winkler.Exact Matchexact_matchLangChainStringEvaluator(\"exact_match\")Regex Matchregex_matchLangChainStringEvaluator(\"regex_match\")Json Validityjson_validityLangChainStringEvaluator(\"json_validity\")Json Equalityjson_equalityLangChainStringEvaluator(\"json_equality\")Json Edit Distancejson_edit_distanceLangChainStringEvaluator(\"json_edit_distance\")Json Schemajson_schemaLangChainStringEvaluator(\"json_schema\")"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made to evaluator scores in LangSmith?"}, {"role": "assistant", "content": "Audit evaluator scores LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you edit feedback details in the comparison view?"}, {"role": "assistant", "content": "In the comparison view In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the \"edit\" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under \"Make correction\".\nIf you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples\nin place of the few_shot_explanation prompt variable."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access the feedback details and corrections view in the runs table?"}, {"role": "assistant", "content": "In the runs table In the runs table, find the \"Feedback\" column and click on the feedback tag to bring up the feedback details. Again, click the \"edit\" icon on the right to bring up the corrections view."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can corrections be made using the SDK's update_feedback function?"}, {"role": "assistant", "content": "In the SDK Corrections can be made via the SDK's update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI. PythonTypeScriptimport langsmithclient = langsmith.Client()client.update_feedback(  my_feedback_id,  correction={      \"score\": 1,  },)import { Client } from 'langsmith';const client = new Client();await client.updateFeedback(  myFeedbackId,  {      correction: {          score: 1,      }  })"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How does LangSmith assist in comparing the results of different experiments in LLM applications?"}, {"role": "assistant", "content": "Compare experiment results Oftentimes, when you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments. LangSmith supports a powerful comparison view that lets you hone in on key differences, regressions, and improvements between different experiments."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the comparison view for experiments in a dataset?"}, {"role": "assistant", "content": "Open the comparison view To open the comparison view, select two or more experiments from the \"Experiments\" tab from a given dataset page. Then, click on the \"Compare\" button at the bottom of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How are runs that regressed and improved visually represented in the LangSmith comparison view?"}, {"role": "assistant", "content": "View regressions and improvements In the LangSmith comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved\nwill be highlighted in green. At the top of each column, you can see how many runs in that experiment did better and how many did worse than your baseline experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you filter the runs in an experiment to show only those that regressed or improved?"}, {"role": "assistant", "content": "Filter on regressions or improvements Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you change the baseline experiment for tracking regressions?"}, {"role": "assistant", "content": "Update baseline experiment In order to track regressions, you need a baseline experiment against which to compare. This will be automatically assigned as the first experiment in your comparison, but you can\nchange it from the dropdown at the top of the page."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What should you do to adjust the feedback key for evaluation metrics?"}, {"role": "assistant", "content": "Select feedback key You will also want to select the feedback key (evaluation metric) on which you would like focus on. This can be selected via another dropdown at the top. Again, one will be assigned by\ndefault, but you can adjust as needed."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you open the trace view for an evaluation run if tracing is enabled?"}, {"role": "assistant", "content": "Open a trace If tracing is enabled for the evaluation run, you can click on the trace icon in the hover state of any experiment cell to open the trace view for that run. This will open up a trace in the side panel."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you access a detailed view of experiment results for a specific example input?"}, {"role": "assistant", "content": "Expand detailed view From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you adjust the display settings for comparison view?"}, {"role": "assistant", "content": "Update display settings You can adjust the display settings for comparison view by clicking on \"Display\" in the top right corner. Here, you'll be able to toggle feedback, metrics, summary charts, and expand full text."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What benefits does LangSmith Self-Hosted offer regarding TTL and data retention of traces?"}, {"role": "assistant", "content": "TTL and Data Retention LangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if you're complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces.\nTraces will also have their data retention period automatically extended based on certain actions or run rule applications. For more details on Data Retention, take a look at the section on auto-upgrades in the data retention guide."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "What options can be configured for data retention in the system?"}, {"role": "assistant", "content": "Requirements You can configure retention through helm or environment variable settings. There are a few options that are\nconfigurable: Enabled: Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see data retention guide for details).Retention Periods: You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects. HelmDockerconfig:  ttl:    enabled: true    ttl_period_seconds:      # -- TTL seconds - 400 day longlived and 14 day shortlived      longlived: \"34560000\"      shortlived: \"1209600\"    # In your .env fileFF_TRACE_TIERS_ENABLED=trueTRACE_TIER_TTL_DURATION_SEC_MAP='{\"longlived\": 34560000, \"shortlived\": 1209600}'    "}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can I edit a prompt in the application?"}, {"role": "assistant", "content": "Update a prompt Navigate to the Prompts section in the left-hand sidebar or from the application homepage and click on the prompt you want to edit."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt metadata?"}, {"role": "assistant", "content": "Update metadata To update the prompt metadata (description, use cases, etc.) click the \"Edit\" pencil icon. Your prompt metadata will be updated upon save."}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you update the prompt content in the prompt playground?"}, {"role": "assistant", "content": "Update the prompt content To update the prompt content itself, you need to enter the prompt playground. Click \"Edit in playground\".\nNow you can make changes to the prompt and test it with different inputs. When you're happy with the prompt, click \"Commit\" to save it. \n"}]}
{"messages": [{"role": "system", "content": "You are an expert programmer with many years of experience"}, {"role": "user", "content": "How can you view all historical versions of a prompt after adding a commit?"}, {"role": "assistant", "content": "Version a prompt When you add a commit to a prompt, a new version of the prompt is created. You can view all historical versions by clicking the \"Commits\" tab in the prompt view."}]}
